[[["_relative_fp","oracle.html"],["title","oracle - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","mysql.html"],["title","mysql - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","sql通用优化.html"],["title","sql通用优化.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","数据库命名规范"],["heading","数据库命名规范"],["body","\n\n"],["body","所有数据库对象名称必须使用小写字母并用下划线分割"],["body","\n"],["body","所有数据库对象名称禁止使用mysql保留关键字"],["body","\n"],["body","数据库对象的命名要能做到见名识意，并且最后不要超过32个字符"],["body","\n"],["body","临时库表必须以tmp_为前缀并以日期为后缀，备份表必须以bak_为前缀并以日期(时间戳)为后缀"],["body","\n"],["body","所有存储相同数据的列名和列类型必须一致"],["body","\n\n"],["headingLink","数据库设计规范"],["heading","数据库设计规范"],["body","\n\n"],["body","\n"],["body","所有表必须使用Innodb存储引擎"],["body","\n"],["body","Innodb 支持事务，支持行级锁，更好的恢复性，高并发下性能更好"],["body","\n"],["body","\n"],["body","\n"],["body","数据库和表的字符集统一使用UTF8"],["body","\n"],["body","兼容性更好，统一字符集可以避免由于字符集转换产生的乱码，不同的字符集进行比较前需要进行转换会造成索引失效"],["body","\n"],["body","\n"],["body","\n"],["body","所有表和字段都需要添加注释"],["body","\n"],["body","\n"],["body","\n"],["body","尽量控制单表数据量的大小，建议控制在500万以内"],["body","\n\n"],["body","500万并不是MySQL数据库的限制，过大会造成修改表结构，备份，恢复都会有很大的问题"],["body","\n\n"],["body","\n"],["body","\n"],["body","可以用历史数据归档（应用于日志数据）,分库分表（应用于业务数据）等手段来控制数据量大小"],["body","\n"],["body","\n"],["body","\n"],["body","谨慎使用MySQL分区表"],["body","\n\n"],["body","分区表在物理上表现为多个文件，在逻辑上表现为一个表 谨慎选择分区键，跨分区查询效率可能更低 建议采用物理分表的方式管理大数据"],["body","\n\n"],["body","\n"],["body","\n"],["body","尽量做到冷热数据分离，减小表的宽度"],["body","\n"],["body","MySQL限制每个表最多存储4096列，并且每一行数据的大小不能超过65535字节 减少磁盘IO,保证热数据的内存缓存命中率（表越宽，把表装载进内存缓冲池时所占用的内存也就越大,也会消耗更多的IO） 更有效的利用缓存，避免读入无用的冷数据 经常一起使用的列放到一个表中（避免更多的关联操作）"],["body","\n"],["body","\n"],["body","\n"],["body","禁止在表中建立预留字段"],["body","\n"],["body","预留字段的命名很难做到见名识义 预留字段无法确认存储的数据类型，所以无法选择合适的类型 对预留字段类型的修改，会对表进行锁定"],["body","\n"],["body","\n"],["body","\n"],["body","禁止在数据库中存储图片，文件等大的二进制数据"],["body","\n"],["body","通常文件很大，会短时间内造成数据量快速增长，数据库进行数据库读取时，通常会进行大量的随机IO操作，文件很大时，IO操作很耗时 通常存储于文件服务器，数据库只存储文件地址信息"],["body","\n"],["body","\n"],["body","\n"],["body","禁止在线上做数据库压力测试"],["body","\n"],["body","\n"],["body","\n"],["body","禁止从开发环境，测试环境直接连接生成环境数据库"],["body","\n"],["body","\n\n"],["headingLink","数据库字段设计规范"],["heading","数据库字段设计规范"],["body","\n\n"],["body","\n"],["body","优先选择符合存储需要的最小的数据类型"],["body","\n\n"],["body","空间"],["body","\n"],["body","索引"],["body","\n\n"],["body","\n"],["body","\n"],["body","IO"],["body","\n"],["body","例如 IP地址转换成整型"],["body","\n"],["body","inet_aton 把ip转为无符号整型(4-8位)\ninet_ntoa 把整型的ip转为地址\n"],["body","\n"],["body","\n"],["body","\n"],["body","对于非负型的数据（如自增ID、整型IP）来说，要优先使用无符号整型来存储"],["body","\n"],["body","\n"],["body","\n"],["body","避免使用TEXT、BLOB数据类型，最常见的TEXT类型可以存储64k的数据"],["body","\n"],["body","建议把BLOB或是TEXT列分离到单独的扩展表中"],["body","\n"],["body","Mysql内存临时表不支持TEXT、BLOB这样的大数据类型，如果查询中包含这样的数据，在排序等操作时，就不能使用内存临时表，必须使用磁盘临时表进行。"],["body","\n"],["body","而且对于这种数据，Mysql还是要进行二次查询，会使sql性能变得很差，但是不是说一定不能使用这样的数据类型。"],["body","\n"],["body","如果一定要使用，建议把BLOB或是TEXT列分离到单独的扩展表中，查询时一定不要使用select * 而只需要取出必要的列，不需要TEXT列的数据时不要对该列进行查询。"],["body","\n"],["body","TEXT或BLOB类型只能使用前缀索引"],["body","\n"],["body","因为MySQL对索引字段长度是有限制的，所以TEXT类型只能使用前缀索引，并且TEXT列上是不能有默认值的。"],["body","\n"],["body","\n"],["body","\n"],["body","避免使用ENUM类型"],["body","\n"],["body","1、修改ENUM值需要使用ALTER语句"],["body","\n"],["body","2、ENUM类型的ORDER BY操作效率低，需要额外操作"],["body","\n"],["body","3、禁止使用数值作为ENUM的枚举值"],["body","\n"],["body","\n"],["body","\n"],["body","尽可能把所有列定义为NOT NULL"],["body","\n"],["body","1、索引NULL列需要额外的空间来保存，所以要占用更多的空间；"],["body","\n"],["body","2、进行比较和计算时要对NULL值做特别的处理"],["body","\n"],["body","\n"],["body","\n"],["body","使用TIMESTAMP（4个字节）或DATETIME类型（8个字节）存储时间"],["body","\n"],["body","经常会有人用字符串存储日期型的数据（不正确的做法）："],["body","\n"],["body","缺点1：无法用日期函数进行计算和比较"],["body","\n"],["body","缺点2：用字符串存储日期要占用更多的空间"],["body","\n"],["body","\n"],["body","\n"],["body","同财务相关的金额类数据必须使用decimal类型"],["body","\n"],["body","1、非精准浮点：float,double"],["body","\n"],["body","2、精准浮点：decimal"],["body","\n"],["body","\n"],["body","\n"],["body","尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型"],["body","\n"],["body","\n"],["body","\n"],["body","设计冗余列"],["body","\n"],["body","\n"],["body","\n"],["body","禁用外键"],["body","\n"],["body","\n"],["body","\n"],["body","分库分表"],["body","\n"],["body","\n"],["body","\n"],["body","外键"],["body","\n"],["body","\n\n"],["headingLink","索引设计规范"],["heading","索引设计规范"],["body","\n\n"],["body","限制每张表上的索引数量，建议单张表索引不超过5个\n\n"],["body","索引可以增加查询效率，但同样也会降低插入和更新的效率，甚至有些情况下会降低查询效率。"],["body","\n"],["body","因为mysql优化器在选择如何优化查询时，会根据统一信息，对每一个可以用到的索引来进行评估，以生成出一个最好的执行计划，如果同时有很多个索引都可以用于查询，就会增加mysql优化器生成执行计划的时间，同样会降低查询性能。"],["body","\n\n"],["body","\n"],["body","每个Innodb表必须有个主键\n\n"],["body","不要使用更新频繁的列作为主键"],["body","\n"],["body","不适用多列主键（相当于联合索引）"],["body","\n"],["body","不要使用UUID、MD5、HASH、字符串列作为主键（无法保证数据的顺序增长）。"],["body","\n"],["body","主键建议使用自增ID值。"],["body","\n\n"],["body","\n\n"],["body","如何选择索引列的顺序"],["body","\n\n"],["body","复合索引，哪个字段放在最前面，需要根据哪个字段经常出现在where条件中，哪个字段的选择性最好来判断的,区分度最高的放在联合索引的最左侧（区分度=列中不同值的数量/列的总行数）"],["body","\n"],["body","尽量把字段长度小的列放在联合索引的最左侧（因为字段长度越小，一页能存储的数据量越大，IO性能也就越好）；"],["body","\n"],["body","使用最频繁的列放到联合索引的左侧（这样可以比较少的建立一些索引）"],["body","\n\n"],["body","优先考虑覆盖索引"],["body","\n"],["body","**覆盖索引：**就是包含了所有查询字段(where,select,ordery by,group by包含的字段)的索引"],["body","\n"],["body","覆盖索引的好处："],["body","\n"],["body","避免Innodb表进行索引的二次查询"],["body","\n"],["body","Innodb是以聚集索引的顺序来存储的，对于Innodb来说，二级索引在叶子节点中所保存的是行的主键信息，"],["body","\n"],["body","如果是用二级索引查询数据的话，在查找到相应的键值后，还要通过主键进行二次查询才能获取我们真实所需要的数据。而在覆盖索引中，二级索引的键值中可以获取所有的数据，避免了对主键的二次查询 ，减少了IO操作，提升了查询效率。"],["body","\n"],["body","索引SET规范"],["body","\n\n"],["body","\n"],["body","尽量避免使用外键约束"],["body","\n"],["body","1、不建议使用外键约束（foreign key），但一定要在表与表之间的关联键上建立索引；"],["body","\n"],["body","2、外键可用于保证数据的参照完整性，但建议在业务端实现；"],["body","\n"],["body","3、外键会影响父表和子表的写操作从而降低性能。"],["body","\n"],["body","\n\n"],["headingLink","数据库sql开发规范"],["heading","数据库SQL开发规范"],["body","\n"],["headingLink","索引查询优化"],["heading","索引查询优化"],["body","\n\n"],["body","\n"],["body","避免数据类型的隐式转换导致的索引失效"],["body","\n"],["body","\n"],["body","\n"],["body","充分利用表上已经存在的索引"],["body","\n"],["body","\n"],["body","\n"],["body","WHERE从句中禁止对列进行函数转换和计算"],["body","\n"],["body","\n"],["body","\n"],["body","对应同一列进行or判断时，使用in代替or"],["body","\n"],["body","\n"],["body","\n"],["body","like语句 使用的是前缀索引"],["body","\n"],["body","\n"],["body","\n"],["body","尽量避免在索引列上使用 MySQL 的内置函数或者表达式操作"],["body","\n"],["body","-- 优化前\nselect userId,loginTime from loginuser where Date_ADD(loginTime,Interval 7 DAY) >=now();\n\n-- 优化后\nexplain  select userId,loginTime from loginuser where  loginTime >= Date_ADD(NOW(),INTERVAL - 7 DAY); \n"],["body","\n"],["body","\n"],["body","\n"],["body","避免使用 <> !=操作符"],["body","\n"],["body","反例："],["body","\n"],["body","select age,name from user where age <>18; \n"],["body","\n"],["body","正例："],["body","\n"],["body","//可以考虑分开两条sql写\nselect age,name  from user where age <18;\nselect age,name  from user where age >18;\n"],["body","\n"],["body","\n"],["body","\n"],["body","联合索引时,遵循最左匹配原则"],["body","\n"],["body","\n"],["body","\n"],["body","使用覆盖索引"],["body","\n"],["body","覆盖索引能够使得你的 SQL 语句不需要回表，仅仅访问索引就能够得到所有需要的数据，大大提高了查询效率。"],["body","\n"],["body","\n\n"],["headingLink","减少查询数据量"],["heading","减少查询数据量"],["body","\n\n"],["body","\n"],["body","查询 SQL 尽量不要使用 select *，而是 select 具体字段"],["body","\n\n"],["body","只取需要的字段，节省资源、减少网络开销。"],["body","\n"],["body","select * 进行查询时，很可能就不会使用到覆盖索引了，就会造成回表查询"],["body","\n\n"],["body","\n"],["body","\n"],["body","如果知道查询结果只有一条或者只要最大/最小一条记录，建议用 limit 1"],["body","\n"],["body","只要找到了对应的一条记录,就不会继续向下扫描了，效率将会大大提高。"],["body","\n"],["body","\n\n"],["headingLink","书写规范"],["heading","书写规范"],["body","\n\n"],["body","\n"],["body","建议使用预编译语句进行数据库操作"],["body","\n"],["body","\n"],["body","\n"],["body","禁止使用不含字段列表的INSERT语句"],["body","\n"],["body","\n"],["body","\n"],["body","在明显不会有重复值时使用UNION ALL而不是UNION"],["body","\n"],["body","\n"],["body","\n"],["body","程序连接不同的数据库使用不同的账号，禁止跨库查询"],["body","\n"],["body","\n"],["body","\n"],["body","避免使用子查询，可以把子查询优化为join操作"],["body","\n\n"],["body","子查询的结果集无法使用索引，通常子查询的结果集会被存储到临时表中，不论是内存临时表还是磁盘临时表都不会存在索引，所以查询性能会受到一定的影响；"],["body","\n"],["body","特别是对于返回结果集比较大的子查询，其对查询性能的影响也就越大；"],["body","\n"],["body","由于子查询会产生大量的临时表也没有索引，所以会消耗过多的CPU和IO资源，产生大量的慢查询。"],["body","\n\n"],["body","\n"],["body","\n"],["body","减少同数据库的交互次数"],["body","\n"],["body","\n"],["body","\n"],["body","拆分复杂的大SQL为多个小SQL"],["body","\n"],["body","\n"],["body","\n"],["body","查询优化时, 考虑对 where, order by 建立索引"],["body","\n"],["body","\n"],["body","\n"],["body","慎用 distinct 关键字"],["body","\n"],["body","\n"],["body","\n"],["body","如果数据量较大，优化你的修改/删除语句"],["body","\n"],["body","分批插入或者分批删除"],["body","\n"],["body","\n"],["body","\n"],["body","where 子句中考虑使用默认值代替 null"],["body","\n"],["body","\n"],["body","\n"],["body","不要有超过 5 个以上的表连接"],["body","\n"],["body","\n"],["body","\n"],["body","exist&in 的合理利用"],["body","\n"],["body","小表驱动大表"],["body","\n"],["body","\n"],["body","\n"],["body","尽量用 union all 替换 union"],["body","\n"],["body","\n\n"],["headingLink","数据库操作行为规范"],["heading","数据库操作行为规范"],["body","\n\n"],["body","\n"],["body","超100万行的批量写（UPDATE、DELETE、INSERT）操作，要分批多次进行操作。"],["body","\n"],["body","\n"],["body","\n"],["body","对于大表使用pt-online-schema-change修改表结构"],["body","\n"],["body","对大表数据结构的修改一定要谨慎，会造成严重的锁表操作，尤其是生产环境，是不能容忍的。\n\npt-online-schema-change它会首先建立一个与原表结构相同的新表，并且在新表上进行表结构的修改，然后再把原表中的数据复制到新表中，并在原表中增加一些触发器。\n\n把原表中新增的数据也复制到新表中，在行所有数据复制完成之后，把新表命名成原表，并把原来的表删除掉。\n\n把原来一个DDL操作，分解成多个小的批次进行。\n"],["body","\n"],["body","\n"],["body","\n"],["body","禁止为程序使用的账号赋予super权限"],["body","\n"],["body","当达到最大连接数限制时，还运行1个有super权限的用户连接super权限只能留给DBA处理问题的账号使用。\n"],["body","\n"],["body","\n"],["body","\n"],["body","对于程序连接数据库账号，遵循权限最小原则"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","MongoDB/README.html"],["title","MongoDB - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","nosql-简介"],["heading","NoSQL 简介"],["body","\n"],["body","NoSQL(NoSQL = Not Only SQL )，意即\"不仅仅是SQL\"。"],["body","\n"],["body","在现代的计算系统上每天网络上都会产生庞大的数据量。"],["body","\n"],["body","这些数据有很大一部分是由关系数据库管理系统（RDBMS）来处理。 1970年 E.F.Codd's提出的关系模型的论文 \"A relational model of data for large shared data banks\"，这使得数据建模和应用程序编程更加简单。"],["body","\n"],["body","通过应用实践证明，关系模型是非常适合于客户服务器编程，远远超出预期的利益，今天它是结构化数据存储在网络和商务应用的主导技术。"],["body","\n"],["headingLink","关系型数据库遵循acid规则"],["heading","关系型数据库遵循ACID规则"],["body","\n"],["body","事务在英文中是transaction，和现实世界中的交易很类似，它有如下四个特性："],["body","\n"],["body","1、A (Atomicity) 原子性"],["body","\n"],["body","2、C (Consistency) 一致性"],["body","\n"],["body","3、I (Isolation) 独立性"],["body","\n"],["body","4、D (Durability) 持久性"],["body","\n"],["headingLink","什么是nosql"],["heading","什么是NoSQL?"],["body","\n"],["body","NoSQL，指的是非关系型的数据库。NoSQL有时也称作Not Only SQL的缩写"],["body","\n"],["body","NoSQL用于超大规模数据的存储，这些类型的数据存储不需要固定的模式，无需多余操作就可以横向扩展。"],["body","\n"],["headingLink","nosql特性"],["heading","NoSQL特性"],["body","\n"],["body","- 代表着不仅仅是SQL\n- 没有声明性查询语言\n- 没有预定义的模式\n-键 - 值对存储，列存储，文档存储，图形数据库\n- 最终一致性，而非ACID属性\n- 非结构化和不可预知的数据\n- CAP定理\n- 高性能，高可用性和可伸缩性"],["body","\n"],["headingLink","cap定理cap-theorem"],["heading","CAP定理（CAP theorem）"],["body","\n"],["body","在计算机科学中, CAP定理（CAP theorem）, 又被称作 布鲁尔定理（Brewer's theorem）, 它指出对于一个分布式计算系统来说，不可能同时满足以下三点:"],["body","\n\n"],["body","一致性(Consistency) (所有节点在同一时间具有相同的数据) 一致"],["body","\n"],["body","可用性(Availability) (保证每个请求不管成功或者失败都有响应) 性能好"],["body","\n"],["body","分区容错(Partition tolerance) (系统中任意信息的丢失或失败不会影响系统的继续运作) 可扩展"],["body","\n\n"],["body","CAP理论的核心是：一个分布式系统不可能同时很好的满足一致性，可用性和分区容错性这三个需求，最多只能同时较好的满足两个。"],["body","\n"],["body","满足 CP 原则和满足 AP 原则三 大类："],["body","\n\n"],["body","CA - 单点集群，满足一致性，可用性的系统，通常在可扩展性上不太强大。"],["body","\n"],["body","CP - 满足一致性，分区容忍性的系统，通常性能不是特别高。"],["body","\n"],["body","AP - 满足可用性，分区容忍性的系统，通常可能对一致性要求低一些。"],["body","\n\n"],["body","\n"],["headingLink","nosql的优点缺点"],["heading","NoSQL的优点/缺点"],["body","\n"],["body","优点:"],["body","\n\n"],["body","- 高可扩展性"],["body","\n"],["body","- 分布式计算"],["body","\n"],["body","- 低成本"],["body","\n"],["body","- 架构的灵活性，半结构化数据"],["body","\n"],["body","- 没有复杂的关系"],["body","\n\n"],["body","缺点:"],["body","\n\n"],["body","- 没有标准化"],["body","\n"],["body","- 有限的查询功能（到目前为止）"],["body","\n"],["body","- 最终一致是不直观的程序"],["body","\n\n"],["headingLink","base"],["heading","BASE"],["body","\n"],["body","BASE：Basically Available, Soft-state, Eventually Consistent。 由 Eric Brewer 定义。"],["body","\n"],["body","CAP理论的核心是：一个分布式系统不可能同时很好的满足一致性，可用性和分区容错性这三个需求，最多只能同时较好的满足两个。"],["body","\n"],["body","BASE是NoSQL数据库通常对可用性及一致性的弱要求原则:"],["body","\n\n"],["body","Basically Available --基本可用"],["body","\n"],["body","Soft-state --软状态/柔性事务。 \"Soft state\" 可以理解为\"无连接\"的, 而 \"Hard state\" 是\"面向连接\"的"],["body","\n"],["body","Eventually Consistency -- 最终一致性， 也是 ACID 的最终目的。"],["body","\n\n"],["headingLink","acid-vs-base"],["heading","ACID vs BASE"],["body","\n"],["body","ACID"],["body","BASE"],["body","\n"],["body","原子性(Atomicity)"],["body","基本可用(Basically Available)"],["body","\n"],["body","一致性(Consistency)"],["body","软状态/柔性事务(Soft state)"],["body","\n"],["body","隔离性(Isolation)"],["body","最终一致性 (Eventual consistency)"],["body","\n"],["body","持久性 (Durable)"],["body","\n\n\n"],["headingLink","nosql-数据库分类"],["heading","NoSQL 数据库分类"],["body","\n"],["body","类型"],["body","部分代表"],["body","特点"],["body","\n"],["body","列存储"],["body","HbaseCassandraHypertable"],["body","顾名思义，是按列存储数据的。最大的特点是方便存储结构化和半结构化数据，方便做数据压缩，对针对某一列或者某几列的查询有非常大的IO优势。"],["body","\n"],["body","文档存储"],["body","MongoDBCouchDB"],["body","文档存储一般用类似json的格式存储，存储的内容是文档型的。这样也就有机会对某些字段建立索引，实现关系数据库的某些功能。"],["body","\n"],["body","key-value存储"],["body","Tokyo Cabinet / TyrantBerkeley DBMemcacheDBRedis"],["body","可以通过key快速查询到其value。一般来说，存储不管value的格式，照单全收。（Redis包含了其他功能）"],["body","\n"],["body","图存储"],["body","Neo4JFlockDB"],["body","图形关系的最佳存储。使用传统关系数据库来解决的话性能低下，而且设计使用不方便。"],["body","\n"],["body","对象存储"],["body","db4oVersant"],["body","通过类似面向对象语言的语法操作数据库，通过对象的方式存取数据。"],["body","\n"],["body","xml数据库"],["body","Berkeley DB XMLBaseX"],["body","高效的存储XML数据，并支持XML的内部查询语法，"],["body","\n\n\n"],["headingLink","mongodb简介"],["heading","Mongodb简介"],["body","\n\n"],["body","\n"],["body","MongoDB 是由C++语言编写的，是一个基于分布式文件存储的开源数据库系统。"],["body","\n"],["body","\n"],["body","\n"],["body","在高负载的情况下，添加更多的节点，可以保证服务器性能。"],["body","\n"],["body","\n"],["body","\n"],["body","MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。"],["body","\n"],["body","\n"],["body","\n"],["body","MongoDB 将数据存储为一个文档，数据结构由键值(key=>value)对组成。MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。"],["body","\n"],["body","\n\n"],["headingLink","主要特点"],["heading","主要特点"],["body","\n\n"],["body","MongoDB 是一个面向文档存储的数据库，操作起来比较简单和容易。"],["body","\n"],["body","你可以在MongoDB记录中设置任何属性的索引 (如：FirstName=\"Sameer\",Address=\"8 Gandhi Road\")来实现更快的排序。"],["body","\n"],["body","你可以通过本地或者网络创建数据镜像，这使得MongoDB有更强的扩展性。"],["body","\n"],["body","如果负载的增加（需要更多的存储空间和更强的处理能力） ，它可以分布在计算机网络中的其他节点上这就是所谓的分片。"],["body","\n"],["body","Mongo支持丰富的查询表达式。查询指令使用JSON形式的标记，可轻易查询文档中内嵌的对象及数组。"],["body","\n"],["body","MongoDb 使用update()命令可以实现替换完成的文档（数据）或者一些指定的数据字段 。"],["body","\n"],["body","Mongodb中的Map/reduce主要是用来对数据进行批量处理和聚合操作。"],["body","\n"],["body","Map和Reduce。Map函数调用emit(key,value)遍历集合中所有的记录，将key与value传给Reduce函数进行处理。"],["body","\n"],["body","Map函数和Reduce函数是使用Javascript编写的，并可以通过db.runCommand或mapreduce命令来执行MapReduce操作。"],["body","\n"],["body","GridFS是MongoDB中的一个内置功能，可以用于存放大量小文件。"],["body","\n"],["body","MongoDB允许在服务端执行脚本，可以用Javascript编写某个函数，直接在服务端执行，也可以把函数的定义存储在服务端，下次直接调用即可。"],["body","\n\n"],["headingLink","mongodb-下载"],["heading","MongoDB 下载"],["body","\n"],["body","你可以在mongodb官网下载该安装包，地址为：https://www.mongodb.com/download-center#community。MonggoDB支持以下平台:"],["body","\n\n"],["body","OS X 32-bit"],["body","\n"],["body","OS X 64-bit"],["body","\n"],["body","Linux 32-bit"],["body","\n"],["body","Linux 64-bit"],["body","\n"],["body","Windows 32-bit"],["body","\n"],["body","Windows 64-bit"],["body","\n"],["body","Solaris i86pc"],["body","\n"],["body","Solaris 64"],["body","\n\n"],["headingLink","mongodb-工具"],["heading","MongoDB 工具"],["body","\n"],["body","有几种可用于MongoDB的管理工具。"],["body","\n"],["headingLink","监控"],["heading","监控"],["body","\n"],["body","MongoDB提供了网络和系统监控工具Munin，它作为一个插件应用于MongoDB中。"],["body","\n"],["body","Gangila是MongoDB高性能的系统监视的工具，它作为一个插件应用于MongoDB中。"],["body","\n"],["body","基于图形界面的开源工具 Cacti, 用于查看CPU负载, 网络带宽利用率,它也提供了一个应用于监控 MongoDB 的插件。"],["body","\n"],["headingLink","gui"],["heading","GUI"],["body","\n\n"],["body","Fang of Mongo – 网页式,由Django和jQuery所构成。"],["body","\n"],["body","Futon4Mongo – 一个CouchDB Futon web的mongodb山寨版。"],["body","\n"],["body","Mongo3 – Ruby写成。"],["body","\n"],["body","MongoHub – 适用于OSX的应用程序。"],["body","\n"],["body","Opricot – 一个基于浏览器的MongoDB控制台, 由PHP撰写而成。"],["body","\n"],["body","Database Master — Windows的mongodb管理工具"],["body","\n"],["body","RockMongo — 最好的PHP语言的MongoDB管理工具，轻量级, 支持多国语言."],["body","\n\n"],["headingLink","mongodb-概念解析"],["heading","MongoDB 概念解析"],["body","\n"],["body","SQL术语/概念"],["body","MongoDB术语/概念"],["body","解释/说明"],["body","\n"],["body","database"],["body","database"],["body","数据库"],["body","\n"],["body","table"],["body","collection"],["body","数据库表/集合"],["body","\n"],["body","row"],["body","document"],["body","数据记录行/文档"],["body","\n"],["body","column"],["body","field"],["body","数据字段/域"],["body","\n"],["body","index"],["body","index"],["body","索引"],["body","\n"],["body","table joins"],["body","表连接,MongoDB不支持"],["body","\n"],["body","primary key"],["body","primary key"],["body","主键,MongoDB自动将_id字段设置为主键"],["body","\n\n\n"],["headingLink","数据库"],["heading","数据库"],["body","\n"],["body","一个mongodb中可以建立多个数据库。"],["body","\n"],["body","MongoDB的默认数据库为\"db\"，该数据库存储在data目录中。"],["body","\n"],["body","MongoDB的单个实例可以容纳多个独立的数据库，每一个都有自己的集合和权限，不同的数据库也放置在不同的文件中。"],["body","\n"],["body","命令"],["body","描述"],["body","\n"],["body","show dbs"],["body","显示所有数据的列表"],["body","\n"],["body","db"],["body","显示当前数据库对象或集合"],["body","\n"],["body","use db"],["body","连接到一个指定的数据库"],["body","\n\n\n"],["body","数据库也通过名字来标识。数据库名可以是满足以下条件的任意UTF-8字符串。"],["body","\n\n"],["body","不能是空字符串（\"\")。"],["body","\n"],["body","不得含有' '（空格)、.、$、/、\\和\\0 (空宇符)。"],["body","\n"],["body","应全部小写。"],["body","\n"],["body","最多64字节。"],["body","\n\n"],["body","有一些数据库名是保留的，可以直接访问这些有特殊作用的数据库。"],["body","\n"],["body","admin： 从权限的角度来看，这是\"root\"数据库。要是将一个用户添加到这个数据库，这个用户自动继承所有数据库的权限。一些特定的服务器端命令也只能从这个数据库运行，比如列出所有的数据库或者关闭服务器。"],["body","\n"],["body","**local:**这个数据永远不会被复制，可以用来存储限于本地单台服务器的任意集合"],["body","\n"],["body","config: 当Mongo用于分片设置时，config数据库在内部使用，用于保存分片的相关信息。"],["body","\n"],["headingLink","文档"],["heading","文档"],["body","\n"],["body","文档是一个键值(key-value)对(即BSON)。MongoDB 的文档不需要设置相同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别"],["body","\n"],["body","{\"site\":\"www.php.cn\", \"name\":\"php中文网\"}\n"],["body","\n"],["body","RDBMS"],["body","MongoDB"],["body","\n"],["body","数据库"],["body","数据库"],["body","\n"],["body","表格"],["body","集合"],["body","\n"],["body","行"],["body","文档"],["body","\n"],["body","列"],["body","字段"],["body","\n"],["body","表联合"],["body","嵌入文档"],["body","\n"],["body","主键"],["body","主键 (MongoDB 提供了 key  为 _id )"],["body","\n"],["body","数据库服务和客户端"],["body","\n"],["body","Mysqld/Oracle"],["body","mongod"],["body","\n"],["body","mysql/sqlplus"],["body","mongo"],["body","\n\n\n"],["headingLink","集合"],["heading","集合"],["body","\n"],["body","集合就是 MongoDB 文档组，类似于 RDBMS （关系数据库管理系统：Relational Database Management System)中的表格。"],["body","\n"],["body","集合存在于数据库中，集合没有固定的结构，这意味着你在对集合可以插入不同格式和类型的数据，但通常情况下我们插入集合的数据都会有一定的关联性。"],["body","\n"],["body","比如，我们可以将以下不同数据结构的文档插入到集合中："],["body","\n"],["body","{\"site\":\"www.baidu.com\"}\n{\"site\":\"www.google.com\",\"name\":\"Google\"}\n{\"site\":\"www.php.cn\",\"name\":\"php中文网\",\"num\":5}\n"],["body","\n"],["body","当第一个文档插入时，集合就会被创建。"],["body","\n"],["headingLink","合法的集合名"],["heading","合法的集合名"],["body","\n\n"],["body","集合名不能是空字符串\"\"。"],["body","\n"],["body","集合名不能含有\\0字符（空字符)，这个字符表示集合名的结尾。"],["body","\n"],["body","集合名不能以\"system.\"开头，这是为系统集合保留的前缀。"],["body","\n"],["body","用户创建的集合名字不能含有保留字符。有些驱动程序的确支持在集合名里面包含，这是因为某些系统生成的集合中包含该字符。除非你要访问这种系统创建的集合，否则千万不要在名字里出现$。　"],["body","\n\n"],["headingLink","capped-collections"],["heading","capped collections"],["body","\n"],["body","Capped collections 就是固定大小的collection。"],["body","\n"],["body","它有很高的性能以及队列过期的特性(过期按照插入的顺序). 有点和 \"RRD\" 概念类似。"],["body","\n"],["body","Capped collections是高性能自动的维护对象的插入顺序。它非常适合类似记录日志的功能 和标准的collection不同，你必须要显式的创建一个capped collection， 指定一个collection的大小，单位是字节。collection的数据存储空间值提前分配的。"],["body","\n"],["body","db.createCollection(\"mycoll\", {capped:true, size:100000})\n"],["body","\n\n"],["body","在capped collection中，你能添加新的对象。"],["body","\n"],["body","能进行更新，然而，对象不会增加存储空间。如果增加，更新就会失败 。"],["body","\n"],["body","数据库不允许进行删除。使用drop()方法删除collection所有的行。"],["body","\n"],["body","注意: 删除之后，你必须显式的重新创建这个collection。"],["body","\n"],["body","在32bit机器中，capped collection最大存储为1e9( 10^9)个字节。"],["body","\n\n"],["headingLink","元数据"],["heading","元数据"],["body","\n"],["body","数据库的信息是存储在集合中。它们使用了系统的命名空间："],["body","\n"],["body","dbname.system.*\n"],["body","\n"],["body","在MongoDB数据库中名字空间 .system.* 是包含多种系统信息的特殊集合(Collection)，如下:"],["body","\n"],["body","集合命名空间"],["body","描述"],["body","\n"],["body","dbname.system.namespaces"],["body","列出所有名字空间。"],["body","\n"],["body","dbname.system.indexes"],["body","列出所有索引。"],["body","\n"],["body","dbname.system.profile"],["body","包含数据库概要(profile)信息。"],["body","\n"],["body","dbname.system.users"],["body","列出所有可访问数据库的用户。"],["body","\n"],["body","dbname.local.sources"],["body","包含复制对端（slave）的服务器信息和状态。"],["body","\n\n\n"],["body","对于修改系统集合中的对象有如下限制。"],["body","\n\n"],["body","\n"],["body","在{{system.indexes}}插入数据，可以创建索引。但除此之外该表信息是不可变的(特殊的drop index命令将自动更新相关信息)。"],["body","\n"],["body","\n"],["body","\n"],["body","{{system.users}}是可修改的。 {{system.profile}}是可删除的。"],["body","\n"],["body","\n\n"],["headingLink","mongodb-数据类型"],["heading","MongoDB 数据类型"],["body","\n"],["body","下表为MongoDB中常用的几种数据类型。"],["body","\n"],["body","数据类型"],["body","描述"],["body","\n"],["body","String"],["body","字符串。存储数据常用的数据类型。在 MongoDB 中，UTF-8 编码的字符串才是合法的。"],["body","\n"],["body","Integer"],["body","整型数值。用于存储数值。根据你所采用的服务器，可分为 32 位或 64 位。"],["body","\n"],["body","Boolean"],["body","布尔值。用于存储布尔值（真/假）。"],["body","\n"],["body","Double"],["body","双精度浮点值。用于存储浮点值。"],["body","\n"],["body","Min/Max keys"],["body","将一个值与 BSON（二进制的 JSON）元素的最低值和最高值相对比。"],["body","\n"],["body","Arrays"],["body","用于将数组或列表或多个值存储为一个键。"],["body","\n"],["body","Timestamp"],["body","时间戳。记录文档修改或添加的具体时间。"],["body","\n"],["body","Object"],["body","用于内嵌文档。"],["body","\n"],["body","Null"],["body","用于创建空值。"],["body","\n"],["body","Symbol"],["body","符号。该数据类型基本上等同于字符串类型，但不同的是，它一般用于采用特殊符号类型的语言。"],["body","\n"],["body","Date"],["body","日期时间。用 UNIX 时间格式来存储当前日期或时间。你可以指定自己的日期时间：创建 Date 对象，传入年月日信息。"],["body","\n"],["body","Object ID"],["body","对象 ID。用于创建文档的 ID。"],["body","\n"],["body","Binary Data"],["body","二进制数据。用于存储二进制数据。"],["body","\n"],["body","Code"],["body","代码类型。用于在文档中存储 JavaScript 代码。"],["body","\n"],["body","Regular expression"],["body","正则表达式类型。用于存储正则表达式。"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","MongoDB/权限管理.html"],["title","权限管理.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","mongodb-security简介"],["heading","Mongodb Security简介"],["body","\n"],["body","MongoDB提供了各种功能，例如身份验证，访问控制，加密，以保护您的MongoDB部署安全。\n一些关键的安全功能包括："],["body","\n"],["body","Authentication"],["body","Authorization"],["body","TLS/SSL"],["body","\n"],["body","Authentication"],["body","Role-Based Access Control"],["body","TLS/SSL (Transport Encryption)"],["body","\n"],["body","x.509"],["body","Enable Access Control"],["body","Configure mongod and mongos for TLS/SSL"],["body","\n"],["body","SCRAM"],["body","Manage Users and Roles"],["body","TLS/SSL Configuration for Clients"],["body","\n\n\n"],["headingLink","security-checklist"],["heading","Security Checklist"],["body","\n"],["body","\n"],["body","安全自查手册"],["body","\n"],["body","\n"],["body","本文档提供了应实施的安全措施列表，以保护您的MongoDB安装。该列表并非详尽无遗。"],["body","\n"],["headingLink","部署前的注意事项"],["heading","部署前的注意事项"],["body","\n"],["headingLink","enable-access-control-and-enforce-authentication"],["heading","Enable Access Control and Enforce Authentication"],["body","\n"],["body","启用访问控制并指定身份验证机制。\n您可以使用MongoDB的SCRAM或x.509身份验证机制，也可以与现有的Kerberos / LDAP基础结构集成。\n身份验证要求所有客户端和服务器都必须提供有效的凭据，然后才能连接到系统。"],["body","\n\n"],["body","See also:\n\n"],["body","Authentication"],["body","\n"],["body","Enable Access Control"],["body","\n\n"],["body","\n\n"],["headingLink","configure-role-based-access-control"],["heading","Configure Role-Based Access Control"],["body","\n\n"],["body","首先创建一个用户管理员，然后创建其他用户。为访问系统的每个人/应用程序创建一个唯一的MongoDB用户。"],["body","\n"],["body","遵循最小特权原则。创建角色，以定义一组用户所需的确切访问权限。然后创建用户，并仅为其分配执行操作所需的角色。用户可以是个人或客户应用程序。"],["body","\n"],["body","用户可以在不同的数据库之间拥有特权。如果用户需要对多个数据库的特权，请创建一个具有授予适用数据库特权的角色的单个用户，而不要在不同的数据库中多次创建该用户。"],["body","\n\n\n"],["body","\n"],["body","See also:"],["body","\n\n"],["body","Role-Based Access Control"],["body","\n"],["body","Manage Users and Roles"],["body","\n\n"],["body","\n\n"],["headingLink","encrypt-communication-tlsssl"],["heading","Encrypt Communication (TLS/SSL)"],["body","\n"],["body","配置MongoDB以对所有传入和传出连接使用TLS / SSL。\n使用TLS / SSL加密MongoDB部署的mongod和mongos组件之间以及所有应用程序和MongoDB之间的通信。\n从版本4.0开始，MongoDB使用本机TLS / SSL OS库："],["body","\n"],["body","Platform"],["body","TLS/SSL Library"],["body","\n"],["body","Windows"],["body","Secure Channel (Schannel)"],["body","\n"],["body","Linux/BSD"],["body","OpenSSL"],["body","\n"],["body","macOS"],["body","Secure Transport"],["body","\n\n\n\n"],["body","See also: Configure mongod and mongos for TLS/SSL."],["body","\n\n"],["body","and so on..."],["body","\n"],["headingLink","认证"],["heading","认证"],["body","\n"],["headingLink","认证方法"],["heading","认证方法"],["body","\n"],["body","命令行认证"],["body","\n"],["body","mongo --username \"myTestDBUser\" --password --authenticationDatabase test --authenticationMechanism SCRAM-SHA-256\n或者 mongodb://userName:pwd@hostname\n"],["body","\n"],["body","登录后认证"],["body","\n"],["body","db.auth()\n//Use passwordPrompt() to prompt the user to enter a password:\ndb.auth( <username>, passwordPrompt() )\n\n//Specify a cleartext password:\ndb.auth( <username>, <password> )\n"],["body","\n"],["body","db.auth( {\n   user: <username>,\n   pwd: passwordPrompt(),   // Or \"<cleartext password>\"\n   mechanism: <authentication mechanism>,\n   digestPassword: <boolean>\n} )\n"],["body","\n"],["body","Parameter"],["body","Type"],["body","Description"],["body","\n"],["body","user"],["body","string"],["body","用户名"],["body","\n"],["body","pwd"],["body","string"],["body","值可以是：明文字符串中的用户密码 或者提示用户输入密码"],["body","\n"],["body","mechanism"],["body","string"],["body","可选，see authentication mechanisms.If unspecified, uses the isMaster to determine the SASL mechanism or mechanisms for the specified user. See saslSupportedMechs."],["body","\n"],["body","digestPassword"],["body","boolean"],["body","可选，是否对密码加密.For SCRAM-SHA-1, although you may specify true, setting this value to true does not improve security and may interfere with credentials using other mechanisms.For all other methods, this value must be set to false (default value). Any other value will result in authentication failure since those methods do not understand MongoDB pre-hashing.The default value is false."],["body","\n\n\n"],["headingLink","改密码"],["heading","改密码"],["body","\n"],["body","db.changeUserPassword(username, password)\n\n"],["body","\n"],["body","Parameter"],["body","Type"],["body","Description"],["body","\n"],["body","username"],["body","string"],["body","The name of the user whose password you wish to change."],["body","\n"],["body","password"],["body","string"],["body","The user's password. The value can be either:the user's password in cleartext string, orpasswordPrompt() to prompt for the user's password.TIPStarting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell."],["body","\n"],["body","writeConcern"],["body","document"],["body","Optional. The level of write concern for the creation operation. The writeConcern document takes the same fields as the getLastError command."],["body","\n\n\n"],["headingLink","创建用户"],["heading","创建用户"],["body","\n"],["body","db.createUser(user, writeConcern)\n"],["body","\n"],["body","The db.createUser() method has the following syntax:"],["body","\n"],["body","Field"],["body","Type"],["body","Description"],["body","\n"],["body","user"],["body","document"],["body","The document with authentication and access information about the user to create."],["body","\n"],["body","writeConcern"],["body","document"],["body","Optional. The level of write concern for the creation operation. The writeConcern document takes the same fields as the getLastError command."],["body","\n\n\n"],["body","{\n  user: \"<name>\",\n  pwd: passwordPrompt(),      // Or  \"<cleartext password>\"\n  customData: { <any information> },\n  roles: [\n    { role: \"<role>\", db: \"<database>\" } | \"<role>\",\n    ...\n  ],\n  authenticationRestrictions: [\n     {\n       clientSource: [\"<IP>\" | \"<CIDR range>\", ...],\n       serverAddress: [\"<IP>\" | \"<CIDR range>\", ...]\n     },\n     ...\n  ],\n  mechanisms: [ \"<SCRAM-SHA-1|SCRAM-SHA-256>\", ... ],\n  passwordDigestor: \"<server|client>\"\n}\n"],["body","\n"],["body","The user document has the following fields:"],["body","\n"],["body","Field"],["body","Type"],["body","Description"],["body","\n"],["body","user"],["body","string"],["body","The name of the new user."],["body","\n"],["body","pwd"],["body","string"],["body","The user's password. The pwd field is not required if you run db.createUser() on the $external database to create users who have credentials stored externally to MongoDB.The value can be either:the user's password in cleartext string, orpasswordPrompt() to prompt for the user's password.TIPStarting in version 4.2 of the mongo shell, you can use the passwordPrompt() method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the mongo shell."],["body","\n"],["body","customData"],["body","document"],["body","Optional. Any arbitrary information. This field can be used to store any data an admin wishes to associate with this particular user. For example, this could be the user's full name or employee id."],["body","\n"],["body","roles"],["body","array"],["body","The roles granted to the user. Can specify an empty array [] to create users without roles."],["body","\n"],["body","authenticationRestrictions"],["body","array"],["body","Optional. The authentication restrictions the server enforces on the created user. Specifies a list of IP addresses and CIDR ranges from which the user is allowed to connect to the server or from which the server can accept users.New in version 3.6."],["body","\n"],["body","mechanisms"],["body","array"],["body","Optional. Specify the specific SCRAM mechanism or mechanisms for creating SCRAM user credentials. If authenticationMechanisms is specified, you can only specify a subset of the authenticationMechanisms.Valid values are:\"SCRAM-SHA-1\"Uses the SHA-1 hashing function.\"SCRAM-SHA-256\"Uses the SHA-256 hashing function.Requires featureCompatibilityVersion set to 4.0.Requires passwordDigestor to be server.The default for featureCompatibilityVersion is 4.0 is both SCRAM-SHA-1 and SCRAM-SHA-256.The default for featureCompatibilityVersion is 3.6 is SCRAM-SHA-1.New in version 4.0."],["body","\n"],["body","passwordDigestor"],["body","string"],["body","Optional. Indicates whether the server or the client digests the password.Available values are:\"server\" (Default)The server receives undigested password from the client and digests the password.\"client\" (Not compatible with SCRAM-SHA-256)The client digests the password and passes the digested password to the server.Changed in version 4.0: The default value is \"server\". In earlier versions, the default value is \"client\"."],["body","\n\n\n"],["headingLink","roles"],["heading","Roles"],["body","\n"],["body","在角色字段中，您可以指定内置角色和用户定义的角色。"],["body","\n"],["body","要指定存在于其他数据库中的角色，请与文档一起指定该角色。"],["body","\n"],["body","\"readWrite\"\nor\n{ role: \"<role>\", db: \"<database>\" }\n"],["body","\n"],["headingLink","authentication-restrictions"],["heading","Authentication Restrictions"],["body","\n"],["body","Field Name"],["body","Value"],["body","Description"],["body","\n"],["body","clientSource"],["body","Array of IP addresses and/or CIDR ranges"],["body","如果存在，则在验证用户身份时，服务器将验证客户端的IP地址在给定列表中还是属于列表中的CIDR范围。"],["body","如果客户端的IP地址不存在，则服务器不会对用户进行身份验证。"],["body","\n"],["body","serverAddress"],["body","Array of IP addresses and/or CIDR ranges"],["body","客户端可以连接到的IP地址或CIDR范围的列表。"],["body","如果存在，服务器将通过给定列表中的IP地址验证是否接受了客户端的连接。"],["body","如果通过无法识别的IP地址接受了连接，则服务器不会对用户进行身份验证。"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","MongoDB/mongodb基本操作.html"],["title","mongodb基本操作.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","mongodb-连接"],["heading","MongoDB 连接"],["body","\n"],["body","格式"],["body","\n"],["body","//username:password@hostname/dbname\n./mongo mongodb://admin:123456@localhost/test\n"],["body","\n"],["headingLink","连接实例"],["heading","连接实例"],["body","\n"],["body","连接本地数据库服务器，端口是默认的。"],["body","\n"],["body","mongodb://localhost\n"],["body","\n"],["body","使用用户名fred，密码foobar登录localhost的admin数据库。"],["body","\n"],["body","mongodb://fred:foobar@localhost\n"],["body","\n"],["body","使用用户名fred，密码foobar登录localhost的baz数据库。"],["body","\n"],["body","mongodb://fred:foobar@localhost/baz\n"],["body","\n"],["body","连接 replica pair, 服务器1为example1.com服务器2为example2。"],["body","\n"],["body","mongodb://example1.com:27017,example2.com:27017\n"],["body","\n"],["body","连接 replica set 三台服务器 (端口 27017, 27018, 和27019):"],["body","\n"],["body","mongodb://localhost,localhost:27018,localhost:27019\n"],["body","\n"],["body","连接 replica set 三台服务器, 写入操作应用在主服务器 并且分布查询到从服务器。"],["body","\n"],["body","mongodb://host1,host2,host3/?slaveOk=true\n"],["body","\n"],["body","直接连接第一个服务器，无论是replica set一部分或者主服务器或者从服务器。"],["body","\n"],["body","mongodb://host1,host2,host3/?connect=direct;slaveOk=true\n"],["body","\n"],["body","当你的连接服务器有优先级，还需要列出所有服务器，你可以使用上述连接方式。"],["body","\n"],["body","安全模式连接到localhost:"],["body","\n"],["body","mongodb://localhost/?safe=true\n"],["body","\n"],["body","以安全模式连接到replica set，并且等待至少两个复制服务器成功写入，超时时间设置为2秒。"],["body","\n"],["body","mongodb://host1,host2,host3/?safe=true;w=2;wtimeoutMS=2000\n"],["body","\n"],["headingLink","标准格式"],["heading","标准格式"],["body","\n"],["body","mongodb://[username:password@]host1[:port1][,host2[:port2],...[,hostN[:portN]]][/[database][?options]]"],["body","\n"],["headingLink","选项"],["heading","选项"],["body","\n"],["body","选项"],["body","描述"],["body","\n"],["body","replicaSet=name"],["body","验证replica set的名称。 Impliesconnect=replicaSet."],["body","\n"],["body","slaveOk=true|false"],["body","true:在connect=direct模式下，驱动会连接第一台机器，即使这台服务器不是主。在connect=replicaSet模式下，驱动会发送所有的写请求到主并且把读取操作分布在其他从服务器。false: 在 connect=direct模式下，驱动会自动找寻主服务器. 在connect=replicaSet 模式下，驱动仅仅连接主服务器，并且所有的读写命令都连接到主服务器。"],["body","\n"],["body","safe=true|false"],["body","true: 在执行更新操作之后，驱动都会发送getLastError命令来确保更新成功。(还要参考 wtimeoutMS).false: 在每次更新之后，驱动不会发送getLastError来确保更新成功。"],["body","\n"],["body","w=n"],["body","驱动添加 { w : n } 到getLastError命令. 应用于safe=true。"],["body","\n"],["body","wtimeoutMS=ms"],["body","驱动添加 { wtimeout : ms } 到 getlasterror 命令. 应用于 safe=true."],["body","\n"],["body","fsync=true|false"],["body","true: 驱动添加 { fsync : true } 到 getlasterror 命令.应用于 safe=true.false: 驱动不会添加到getLastError命令中。"],["body","\n"],["body","journal=true|false"],["body","如果设置为 true, 同步到 journal (在提交到数据库前写入到实体中). 应用于 safe=true"],["body","\n"],["body","connectTimeoutMS=ms"],["body","可以打开连接的时间。"],["body","\n"],["body","socketTimeoutMS=ms"],["body","发送和接受sockets的时间。"],["body","\n\n\n"],["headingLink","mongodb数据库创建与删除"],["heading","MongoDB数据库创建与删除"],["body","\n"],["headingLink","创建数据库"],["heading","创建数据库"],["body","\n"],["body","MongoDB 创建数据库的语法格式如下："],["body","\n"],["body","use DATABASE_NAME\n"],["body","\n"],["body","如果数据库不存在，则创建数据库，否则切换到指定数据库。"],["body","\n"],["headingLink","实例"],["heading","实例"],["body","\n"],["body","以下实例我们创建了数据库 php:"],["body","\n"],["body","> use php\nswitched to db php\n> db\nphp\n>\n"],["body","\n"],["body","如果你想查看所有数据库，可以使用 show dbs 命令："],["body","\n"],["body","> show dbs\nlocal  0.078GB\ntest   0.078GB\n>\n"],["body","\n"],["body","可以看到，我们刚创建的数据库 php 并不在数据库的列表中， 要显示它，我们需要向 php 数据库插入一些数据。"],["body","\n"],["body","> db.php.insert({\"name\":\"php中文网\"})\nWriteResult({ \"nInserted\" : 1 })\n> show dbs\nlocal   0.078GB\nphp  0.078GB\ntest    0.078GB\n>\n"],["body","\n"],["headingLink","删除数据库"],["heading","删除数据库"],["body","\n"],["headingLink","语法"],["heading","语法"],["body","\n"],["body","MongoDB 删除数据库的语法格式如下："],["body","\n"],["body","db.dropDatabase()\n"],["body","\n"],["body","删除当前数据库，默认为 test，你可以使用 db 命令查看当前数据库名。"],["body","\n"],["headingLink","文档管理"],["heading","文档管理"],["body","\n"],["body","本章节中我们将向大家介绍如何将数据插入到MongoDB的集合中。"],["body","\n"],["body","文档的数据结构和JSON基本一样。"],["body","\n"],["body","所有存储在集合中的数据都是BSON格式。"],["body","\n"],["body","BSON是一种类json的一种二进制形式的存储格式,简称Binary JSON。"],["body","\n"],["headingLink","插入文档"],["heading","插入文档"],["body","\n"],["body","MongoDB 使用 insert() 或 save() 方法向集合中插入文档，语法如下："],["body","\n"],["body","db.COLLECTION_NAME.insert(document)\n"],["body","\n"],["headingLink","实例-1"],["heading","实例"],["body","\n"],["body","以下文档可以存储在 MongoDB 的 php 数据库 的 col集合中："],["body","\n"],["body",">db.col.insert({title: 'MongoDB 教程', \n    description: 'MongoDB 是一个 Nosql 数据库',\n    by: 'php中文网',\n    url: 'http://www.php.cn',\n    tags: ['mongodb', 'database', 'NoSQL'],\n    likes: 100\n})\n"],["body","\n"],["headingLink","更新文档"],["heading","更新文档"],["body","\n"],["body","MongoDB 使用 update() 和 save() 方法来更新集合中的文档。接下来让我们详细来看下两个函数的应用及其区别。"],["body","\n"],["headingLink","update-方法"],["heading","update() 方法"],["body","\n"],["body","update() 方法用于更新已存在的文档。语法格式如下："],["body","\n"],["body","db.collection.update(\n   <query>,\n   <update>,\n   {\n     upsert: <boolean>,\n     multi: <boolean>,\n     writeConcern: <document>\n   }\n)\n"],["body","\n"],["body","参数说明："],["body","\n\n"],["body","query : update的查询条件，类似sql update查询内where后面的。"],["body","\n"],["body","update : update的对象和一些更新的操作符（如$,$inc...）等，也可以理解为sql update查询内set后面的"],["body","\n"],["body","upsert : 可选，这个参数的意思是，如果不存在update的记录，是否插入objNew,true为插入，默认是false，不插入。"],["body","\n"],["body","multi : 可选，mongodb 默认是false,只更新找到的第一条记录，如果这个参数为true,就把按条件查出来多条记录全部更新。"],["body","\n"],["body","writeConcern :可选，抛出异常的级别。"],["body","\n\n"],["body","实例"],["body","\n"],["body","db.col.update(\n\t//query\n\t{'title':'MongoDB 教程'},\n\t//updates\n\t{$set:{'title':'MongoDB'}}\n\t//options\n\t{multi:true}\n)\n"],["body","\n"],["headingLink","save-方法"],["heading","save() 方法"],["body","\n"],["body","save() 方法通过传入的文档来替换已有文档。语法格式如下："],["body","\n"],["body","db.collection.save(\n   <document>,\n   {\n     writeConcern: <document>\n   }\n)\n"],["body","\n"],["body","参数说明："],["body","\n\n"],["body","document : 文档数据。"],["body","\n"],["body","writeConcern :可选，抛出异常的级别。"],["body","\n\n"],["body","以下实例中我们替换了 _id  为 56064f89ade2f21f36b03136 的文档数据："],["body","\n"],["body",">db.col.save({\n\t\"_id\" : ObjectId(\"56064f89ade2f21f36b03136\"),\n    \"title\" : \"MongoDB\",\n    \"description\" : \"MongoDB 是一个 Nosql 数据库\",\n    \"by\" : \"php\",\n    \"url\" : \"http://www.php.cn\",\n    \"tags\" : [\n            \"mongodb\",\n            \"NoSQL\"\n    ],\n    \"likes\" : 110\n})\n"],["body","\n"],["headingLink","更多实例"],["heading","更多实例"],["body","\n"],["body","只更新第一条记录：\ndb.col.update( { \"count\" : { $gt : 1 } } , { $set : { \"test2\" : \"OK\"} } );\n全部更新：\n\ndb.col.update( { \"count\" : { $gt : 3 } } , { $set : { \"test2\" : \"OK\"} },false,true );\n只添加第一条：\n\ndb.col.update( { \"count\" : { $gt : 4 } } , { $set : { \"test5\" : \"OK\"} },true,false );\n全部添加加进去:\n\ndb.col.update( { \"count\" : { $gt : 5 } } , { $set : { \"test5\" : \"OK\"} },true,true );\n全部更新：\n\ndb.col.update( { \"count\" : { $gt : 15 } } , { $inc : { \"count\" : 1} },false,true );\n只更新第一条记录：\n\ndb.col.update( { \"count\" : { $gt : 10 } } , { $inc : { \"count\" : 1} },false,false );\n"],["body","\n"],["headingLink","删除文档"],["heading","删除文档"],["body","\n"],["body","remove() 方法的基本语法格式如下所示："],["body","\n"],["body","db.collection.remove(\n   <query>,\n   <justOne>\n)\n"],["body","\n"],["body","如果你的 MongoDB 是 2.6 版本以后的，语法格式如下："],["body","\n"],["body","db.collection.remove(\n   <query>,\n   {\n     justOne: <boolean>,\n     writeConcern: <document>\n   }\n)\n"],["body","\n"],["body","参数说明："],["body","\n\n"],["body","query :（可选）删除的文档的条件。"],["body","\n"],["body","justOne : （可选）如果设为 true 或 1，则只删除一个文档。"],["body","\n"],["body","writeConcern :（可选）抛出异常的级别。"],["body","\n\n"],["headingLink","查询文档"],["heading","查询文档"],["body","\n"],["headingLink","mongodb-与-rdbms-where-语句比较"],["heading","MongoDB 与 RDBMS Where 语句比较"],["body","\n"],["body","如果你熟悉常规的 SQL 数据，通过下表可以更好的理解 MongoDB 的条件语句查询："],["body","\n"],["body","操作"],["body","格式"],["body","范例"],["body","RDBMS中的类似语句"],["body","\n"],["body","等于"],["body","{<key>:<value>}"],["body","db.col.find({\"by\":\"php中文网\"}).pretty()"],["body","where by = 'php中文网'"],["body","\n"],["body","小于"],["body","{<key>:{$lt:<value>}}"],["body","db.col.find({\"likes\":{$lt:50}}).pretty()"],["body","where likes < 50"],["body","\n"],["body","小于或等于"],["body","{<key>:{$lte:<value>}}"],["body","db.col.find({\"likes\":{$lte:50}}).pretty()"],["body","where likes <= 50"],["body","\n"],["body","大于"],["body","{<key>:{$gt:<value>}}"],["body","db.col.find({\"likes\":{$gt:50}}).pretty()"],["body","where likes > 50"],["body","\n"],["body","大于或等于"],["body","{<key>:{$gte:<value>}}"],["body","db.col.find({\"likes\":{$gte:50}}).pretty()"],["body","where likes >= 50"],["body","\n"],["body","不等于"],["body","{<key>:{$ne:<value>}}"],["body","db.col.find({\"likes\":{$ne:50}}).pretty()"],["body","where likes != 50"],["body","\n\n\n"],["headingLink","find"],["heading","find"],["body","\n"],["body","findOne用于查一个"],["body","\n"],["body","MongoDB 查询数据的语法格式如下："],["body","\n"],["body",">db.COLLECTION_NAME.find({query})\n"],["body","\n"],["body","find() 方法以非结构化的方式来显示所有文档。"],["body","\n"],["body","如果你需要以易读的方式来读取数据，可以使用 pretty() 方法，语法格式如下："],["body","\n"],["body",">db.col.find().pretty()\n"],["body","\n"],["headingLink","mongodb-and-条件"],["heading","MongoDB AND 条件"],["body","\n"],["body",">db.col.find({key1:value1, key2:value2}).pretty()\n"],["body","\n"],["headingLink","mongodb-or-条件"],["heading","MongoDB OR 条件"],["body","\n"],["body",">db.col.find(\n   {\n      $or: [\n\t     {key1: value1}, {key2:value2}\n      ]\n   }\n).pretty()\n\n"],["body","\n"],["headingLink","and-和-or-联合使用"],["heading","AND 和 OR 联合使用"],["body","\n"],["body","'where likes>50 AND (by = 'php中文网' OR title = 'MongoDB 教程')'"],["body","\n"],["body","db.col.find({\"likes\": {$gt:50}, $or: [{\"by\": \"php中文网\"},{\"title\": \"MongoDB 教程\"}]}).pretty()\n"],["body","\n"],["body","查看已插入文档："],["body","\n"],["body","> db.col.find()\n{ \"_id\" : ObjectId(\"56064886ade2f21f36b03134\"), \"title\" : \"MongoDB 教程\", \"description\" : \"MongoDB 是一个 Nosql 数据库\", \"by\" : \"php中文网\", \"url\" : \"http://www.php.cn\", \"tags\" : [ \"mongodb\", \"database\", \"NoSQL\" ], \"likes\" : 100 }\n>\n"],["body","\n"],["body","定义变量"],["body","\n"],["body","我们也可以将数据定义为一个变量，如下所示："],["body","\n"],["body","document=({title: 'MongoDB 教程', \n    description: 'MongoDB 是一个 Nosql 数据库',\n    by: 'php中文网',\n    url: 'http://www.php.cn',\n    tags: ['mongodb', 'database', 'NoSQL'],\n    likes: 100\n});\n\ndb.col.insert(document)\n"],["body","\n"],["body","插入文档你也可以使用 db.col.save(document) 命令。如果不指定 _id 字段 save() 方法类似于 insert() 方法。如果指定 _id 字段，则会更新该 _id 的数据。"],["body","\n"],["headingLink","其他关键字"],["heading","其他关键字"],["body","\n"],["headingLink","type-操作符"],["heading","$type 操作符"],["body","\n"],["body","MongoDB 中可以使用的类型如下表所示："],["body","\n"],["body","类型"],["body","数字"],["body","备注"],["body","\n"],["body","Double"],["body","1"],["body","\n"],["body","String"],["body","2"],["body","\n"],["body","Object"],["body","3"],["body","\n"],["body","Array"],["body","4"],["body","\n"],["body","Binary data"],["body","5"],["body","\n"],["body","Undefined"],["body","6"],["body","已废弃。"],["body","\n"],["body","Object id"],["body","7"],["body","\n"],["body","Boolean"],["body","8"],["body","\n"],["body","Date"],["body","9"],["body","\n"],["body","Null"],["body","10"],["body","\n"],["body","Regular Expression"],["body","11"],["body","\n"],["body","JavaScript"],["body","13"],["body","\n"],["body","Symbol"],["body","14"],["body","\n"],["body","JavaScript (with scope)"],["body","15"],["body","\n"],["body","32-bit integer"],["body","16"],["body","\n"],["body","Timestamp"],["body","17"],["body","\n"],["body","64-bit integer"],["body","18"],["body","\n"],["body","Min key"],["body","255"],["body","Query with -1."],["body","\n"],["body","Max key"],["body","127"],["body","\n\n\n"],["body","$type操作符是基于BSON类型来检索集合中匹配的数据类型，并返回结果。"],["body","\n"],["body","如果想获取 \"col\" 集合中 title 为 String 的数据，你可以使用以下命令："],["body","\n"],["body","db.col.find({\"title\" : {$type : 2}})\n"],["body","\n"],["headingLink","limit"],["heading","Limit()"],["body","\n"],["body","如果你需要在MongoDB中读取指定数量的数据记录，可以使用MongoDB的Limit方法，limit()方法接受一个数字参数，该参数指定从MongoDB中读取的记录条数。"],["body","\n"],["body",">db.COLLECTION_NAME.find().limit(NUMBER)\n"],["body","\n"],["headingLink","skip"],["heading","Skip()"],["body","\n"],["body","我们除了可以使用limit()方法来读取指定数量的数据外，还可以使用skip()方法来跳过指定数量的数据，skip方法同样接受一个数字参数作为跳过的记录条数。"],["body","\n"],["body",">db.COLLECTION_NAME.find().limit(NUMBER).skip(NUMBER)\n"],["body","\n"],["headingLink","sort"],["heading","sort()"],["body","\n"],["body","sort()方法可以通过参数指定排序的字段，并使用 1 和 -1 来指定排序的方式，其中 1 为升序排列，而-1是用于降序排列。"],["body","\n"],["body","db.COLLECTION_NAME.find().sort({KEY:1})\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","MongoDB/mongodb高级语法.html"],["title","mongodb高级语法.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","索引"],["heading","索引"],["body","\n"],["body","索引通常能够极大的提高查询的效率，如果没有索引，MongoDB在读取数据时必须扫描集合中的每个文件并选取那些符合查询条件的记录。"],["body","\n"],["body","索引存储在一个易于遍历读取的数据集合中，索引是对数据库表中一列或多列的值进行排序的一种结构"],["body","\n"],["headingLink","ensureindex-方法"],["heading","ensureIndex() 方法"],["body","\n"],["body","MongoDB使用 ensureIndex() 方法来创建索引。"],["body","\n"],["headingLink","语法"],["heading","语法"],["body","\n"],["body","ensureIndex()方法基本语法格式如下所示："],["body","\n"],["body",">db.COLLECTION_NAME.ensureIndex({KEY:1})\n"],["body","\n"],["body","ensureIndex() 方法中你也可以设置使用多个字段创建索引（关系型数据库中称作复合索引）。"],["body","\n"],["body",">db.col.ensureIndex({\"title\":1,\"description\":-1})\n>\n"],["body","\n"],["body","ensureIndex() 接收可选参数，可选参数列表如下："],["body","\n"],["body","Parameter"],["body","Type"],["body","Description"],["body","\n"],["body","background"],["body","Boolean"],["body","建索引过程会阻塞其它数据库操作，background可指定以后台方式创建索引，即增加 \"background\"  可选参数。  \"background\" 默认值为false。"],["body","\n"],["body","unique"],["body","Boolean"],["body","建立的索引是否唯一。指定为true创建唯一索引。默认值为false."],["body","\n"],["body","name"],["body","string"],["body","索引的名称。如果未指定，MongoDB的通过连接索引的字段名和排序顺序生成一个索引名称。"],["body","\n"],["body","dropDups"],["body","Boolean"],["body","在建立唯一索引时是否删除重复记录,指定 true 创建唯一索引。默认值为 false."],["body","\n"],["body","sparse"],["body","Boolean"],["body","对文档中不存在的字段数据不启用索引；这个参数需要特别注意，如果设置为true的话，在索引字段中不会查询出不包含对应字段的文档.。默认值为 false."],["body","\n"],["body","expireAfterSeconds"],["body","integer"],["body","指定一个以秒为单位的数值，完成 TTL设定，设定集合的生存时间。"],["body","\n"],["body","v"],["body","index version"],["body","索引的版本号。默认的索引版本取决于mongod创建索引时运行的版本。"],["body","\n"],["body","weights"],["body","document"],["body","索引权重值，数值在 1 到 99,999 之间，表示该索引相对于其他索引字段的得分权重。"],["body","\n"],["body","default_language"],["body","string"],["body","对于文本索引，该参数决定了停用词及词干和词器的规则的列表。 默认为英语"],["body","\n"],["body","language_override"],["body","string"],["body","对于文本索引，该参数指定了包含在文档中的字段名，语言覆盖默认的language，默认值为 language."],["body","\n\n\n"],["headingLink","实例"],["heading","实例"],["body","\n"],["body","在后台创建索引："],["body","\n"],["body","db.values.ensureIndex({open: 1, close: 1}, {background: true})\n"],["body","\n"],["headingLink","聚合"],["heading","聚合"],["body","\n"],["body","MongoDB中聚合(aggregate)主要用于处理数据(诸如统计平均值,求和等)，并返回计算后的数据结果。有点类似sql语句中的 count(*)。"],["body","\n"],["headingLink","aggregate-方法"],["heading","aggregate() 方法"],["body","\n"],["body","MongoDB中聚合的方法使用aggregate()。"],["body","\n"],["headingLink","语法-1"],["heading","语法"],["body","\n"],["body","aggregate() 方法的基本语法格式如下所示："],["body","\n"],["body",">db.COLLECTION_NAME.aggregate(AGGREGATE_OPERATION)\n"],["body","\n"],["body","现在我们通过以上集合计算每个作者所写的文章数，使用aggregate()计算结果如下："],["body","\n"],["body","以上实例类似sql语句： select by_user, count(*) from mycol group by by_user"],["body","\n"],["body","> db.mycol.aggregate([{$group : {_id : \"$by_user\", num_tutorial : {$sum : 1}}}])\n{\n   \"result\" : [\n      {\n         \"_id\" : \"w3cschool.cc\",\n         \"num_tutorial\" : 2\n      },\n      {\n         \"_id\" : \"Neo4j\",\n         \"num_tutorial\" : 1\n      }\n   ],\n   \"ok\" : 1\n}\n>\n"],["body","\n"],["body","下表展示了一些聚合的表达式:"],["body","\n"],["body","表达式"],["body","描述"],["body","实例"],["body","\n"],["body","$sum"],["body","计算总和。"],["body","db.mycol.aggregate([{$group : {_id : \"$by_user\", num_tutorial : {$sum : \"$likes\"}}}])"],["body","\n"],["body","$avg"],["body","计算平均值"],["body","db.mycol.aggregate([{$group : {_id : \"$by_user\", num_tutorial : {$avg : \"$likes\"}}}])"],["body","\n"],["body","$min"],["body","获取集合中所有文档对应值得最小值。"],["body","db.mycol.aggregate([{$group : {_id : \"$by_user\", num_tutorial : {$min : \"$likes\"}}}])"],["body","\n"],["body","$max"],["body","获取集合中所有文档对应值得最大值。"],["body","db.mycol.aggregate([{$group : {_id : \"$by_user\", num_tutorial : {$max : \"$likes\"}}}])"],["body","\n"],["body","$push"],["body","在结果文档中插入值到一个数组中。"],["body","db.mycol.aggregate([{$group : {_id : \"$by_user\", url : {$push: \"$url\"}}}])"],["body","\n"],["body","$addToSet"],["body","在结果文档中插入值到一个数组中，但不创建副本。"],["body","db.mycol.aggregate([{$group : {_id : \"$by_user\", url : {$addToSet : \"$url\"}}}])"],["body","\n"],["body","$first"],["body","根据资源文档的排序获取第一个文档数据。"],["body","db.mycol.aggregate([{$group : {_id : \"$by_user\", first_url : {$first : \"$url\"}}}])"],["body","\n"],["body","$last"],["body","根据资源文档的排序获取最后一个文档数据"],["body","db.mycol.aggregate([{$group : {_id : \"$by_user\", last_url : {$last : \"$url\"}}}])"],["body","\n\n\n"],["headingLink","管道的概念"],["heading","管道的概念"],["body","\n"],["body","管道在Unix和Linux中一般用于将当前命令的输出结果作为下一个命令的参数。"],["body","\n"],["body","MongoDB的聚合管道将MongoDB文档在一个管道处理完毕后将结果传递给下一个管道处理。管道操作是可以重复的。"],["body","\n"],["body","表达式：处理输入文档并输出。表达式是无状态的，只能用于计算当前聚合管道的文档，不能处理其它的文档。"],["body","\n"],["body","这里我们介绍一下聚合框架中常用的几个操作："],["body","\n\n"],["body","$project：修改输入文档的结构。可以用来重命名、增加或删除域，也可以用于创建计算结果以及嵌套文档。"],["body","\n"],["body","$match：用于过滤数据，只输出符合条件的文档。$match使用MongoDB的标准查询操作。"],["body","\n"],["body","$limit：用来限制MongoDB聚合管道返回的文档数。"],["body","\n"],["body","$skip：在聚合管道中跳过指定数量的文档，并返回余下的文档。"],["body","\n"],["body","$unwind：将文档中的某一个数组类型字段拆分成多条，每条包含数组中的一个值。"],["body","\n"],["body","$group：将集合中的文档分组，可用于统计结果。"],["body","\n"],["body","$sort：将输入文档排序后输出。"],["body","\n"],["body","$geoNear：输出接近某一地理位置的有序文档。"],["body","\n\n"],["body","db.article.aggregate(\n    { $project : {\n        title : 1 ,\n        author : 1 ,\n    }}\n );\n"],["body","\n"],["body","这样的话结果中就只还有_id,tilte和author三个字段了，默认情况下_id字段是被包含的，如果要想不包含_id话可以这样:"],["body","\n"],["body","db.article.aggregate(\n    { $project : {\n        _id : 0 ,\n        title : 1 ,\n        author : 1\n    }});\n"],["body","\n"],["body","$match实例"],["body","\n"],["body","db.articles.aggregate( [\n                        { $match : { score : { $gt : 70, $lte : 90 } } },\n                        { $group: { _id: null, count: { $sum: 1 } } }\n                       ] );\n"],["body","\n"],["body","$match用于获取分数大于70小于或等于90记录，然后将符合条件的记录送到下一阶段$group管道操作符进行处理。"],["body","\n"],["body","3.$skip实例"],["body","\n"],["body","db.article.aggregate(\n    { $skip : 5 });\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","index.html"],["title","MongoDB - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","nosql-简介"],["heading","NoSQL 简介"],["body","\n"],["body","NoSQL(NoSQL = Not Only SQL )，意即\"不仅仅是SQL\"。"],["body","\n"],["body","在现代的计算系统上每天网络上都会产生庞大的数据量。"],["body","\n"],["body","这些数据有很大一部分是由关系数据库管理系统（RDBMS）来处理。 1970年 E.F.Codd's提出的关系模型的论文 \"A relational model of data for large shared data banks\"，这使得数据建模和应用程序编程更加简单。"],["body","\n"],["body","通过应用实践证明，关系模型是非常适合于客户服务器编程，远远超出预期的利益，今天它是结构化数据存储在网络和商务应用的主导技术。"],["body","\n"],["headingLink","关系型数据库遵循acid规则"],["heading","关系型数据库遵循ACID规则"],["body","\n"],["body","事务在英文中是transaction，和现实世界中的交易很类似，它有如下四个特性："],["body","\n"],["body","1、A (Atomicity) 原子性"],["body","\n"],["body","2、C (Consistency) 一致性"],["body","\n"],["body","3、I (Isolation) 独立性"],["body","\n"],["body","4、D (Durability) 持久性"],["body","\n"],["headingLink","什么是nosql"],["heading","什么是NoSQL?"],["body","\n"],["body","NoSQL，指的是非关系型的数据库。NoSQL有时也称作Not Only SQL的缩写"],["body","\n"],["body","NoSQL用于超大规模数据的存储，这些类型的数据存储不需要固定的模式，无需多余操作就可以横向扩展。"],["body","\n"],["headingLink","nosql特性"],["heading","NoSQL特性"],["body","\n"],["body","- 代表着不仅仅是SQL\n- 没有声明性查询语言\n- 没有预定义的模式\n-键 - 值对存储，列存储，文档存储，图形数据库\n- 最终一致性，而非ACID属性\n- 非结构化和不可预知的数据\n- CAP定理\n- 高性能，高可用性和可伸缩性"],["body","\n"],["headingLink","cap定理cap-theorem"],["heading","CAP定理（CAP theorem）"],["body","\n"],["body","在计算机科学中, CAP定理（CAP theorem）, 又被称作 布鲁尔定理（Brewer's theorem）, 它指出对于一个分布式计算系统来说，不可能同时满足以下三点:"],["body","\n\n"],["body","一致性(Consistency) (所有节点在同一时间具有相同的数据) 一致"],["body","\n"],["body","可用性(Availability) (保证每个请求不管成功或者失败都有响应) 性能好"],["body","\n"],["body","分区容错(Partition tolerance) (系统中任意信息的丢失或失败不会影响系统的继续运作) 可扩展"],["body","\n\n"],["body","CAP理论的核心是：一个分布式系统不可能同时很好的满足一致性，可用性和分区容错性这三个需求，最多只能同时较好的满足两个。"],["body","\n"],["body","满足 CP 原则和满足 AP 原则三 大类："],["body","\n\n"],["body","CA - 单点集群，满足一致性，可用性的系统，通常在可扩展性上不太强大。"],["body","\n"],["body","CP - 满足一致性，分区容忍性的系统，通常性能不是特别高。"],["body","\n"],["body","AP - 满足可用性，分区容忍性的系统，通常可能对一致性要求低一些。"],["body","\n\n"],["body","\n"],["headingLink","nosql的优点缺点"],["heading","NoSQL的优点/缺点"],["body","\n"],["body","优点:"],["body","\n\n"],["body","- 高可扩展性"],["body","\n"],["body","- 分布式计算"],["body","\n"],["body","- 低成本"],["body","\n"],["body","- 架构的灵活性，半结构化数据"],["body","\n"],["body","- 没有复杂的关系"],["body","\n\n"],["body","缺点:"],["body","\n\n"],["body","- 没有标准化"],["body","\n"],["body","- 有限的查询功能（到目前为止）"],["body","\n"],["body","- 最终一致是不直观的程序"],["body","\n\n"],["headingLink","base"],["heading","BASE"],["body","\n"],["body","BASE：Basically Available, Soft-state, Eventually Consistent。 由 Eric Brewer 定义。"],["body","\n"],["body","CAP理论的核心是：一个分布式系统不可能同时很好的满足一致性，可用性和分区容错性这三个需求，最多只能同时较好的满足两个。"],["body","\n"],["body","BASE是NoSQL数据库通常对可用性及一致性的弱要求原则:"],["body","\n\n"],["body","Basically Available --基本可用"],["body","\n"],["body","Soft-state --软状态/柔性事务。 \"Soft state\" 可以理解为\"无连接\"的, 而 \"Hard state\" 是\"面向连接\"的"],["body","\n"],["body","Eventually Consistency -- 最终一致性， 也是 ACID 的最终目的。"],["body","\n\n"],["headingLink","acid-vs-base"],["heading","ACID vs BASE"],["body","\n"],["body","ACID"],["body","BASE"],["body","\n"],["body","原子性(Atomicity)"],["body","基本可用(Basically Available)"],["body","\n"],["body","一致性(Consistency)"],["body","软状态/柔性事务(Soft state)"],["body","\n"],["body","隔离性(Isolation)"],["body","最终一致性 (Eventual consistency)"],["body","\n"],["body","持久性 (Durable)"],["body","\n\n\n"],["headingLink","nosql-数据库分类"],["heading","NoSQL 数据库分类"],["body","\n"],["body","类型"],["body","部分代表"],["body","特点"],["body","\n"],["body","列存储"],["body","HbaseCassandraHypertable"],["body","顾名思义，是按列存储数据的。最大的特点是方便存储结构化和半结构化数据，方便做数据压缩，对针对某一列或者某几列的查询有非常大的IO优势。"],["body","\n"],["body","文档存储"],["body","MongoDBCouchDB"],["body","文档存储一般用类似json的格式存储，存储的内容是文档型的。这样也就有机会对某些字段建立索引，实现关系数据库的某些功能。"],["body","\n"],["body","key-value存储"],["body","Tokyo Cabinet / TyrantBerkeley DBMemcacheDBRedis"],["body","可以通过key快速查询到其value。一般来说，存储不管value的格式，照单全收。（Redis包含了其他功能）"],["body","\n"],["body","图存储"],["body","Neo4JFlockDB"],["body","图形关系的最佳存储。使用传统关系数据库来解决的话性能低下，而且设计使用不方便。"],["body","\n"],["body","对象存储"],["body","db4oVersant"],["body","通过类似面向对象语言的语法操作数据库，通过对象的方式存取数据。"],["body","\n"],["body","xml数据库"],["body","Berkeley DB XMLBaseX"],["body","高效的存储XML数据，并支持XML的内部查询语法，"],["body","\n\n\n"],["headingLink","mongodb简介"],["heading","Mongodb简介"],["body","\n\n"],["body","\n"],["body","MongoDB 是由C++语言编写的，是一个基于分布式文件存储的开源数据库系统。"],["body","\n"],["body","\n"],["body","\n"],["body","在高负载的情况下，添加更多的节点，可以保证服务器性能。"],["body","\n"],["body","\n"],["body","\n"],["body","MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。"],["body","\n"],["body","\n"],["body","\n"],["body","MongoDB 将数据存储为一个文档，数据结构由键值(key=>value)对组成。MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。"],["body","\n"],["body","\n\n"],["headingLink","主要特点"],["heading","主要特点"],["body","\n\n"],["body","MongoDB 是一个面向文档存储的数据库，操作起来比较简单和容易。"],["body","\n"],["body","你可以在MongoDB记录中设置任何属性的索引 (如：FirstName=\"Sameer\",Address=\"8 Gandhi Road\")来实现更快的排序。"],["body","\n"],["body","你可以通过本地或者网络创建数据镜像，这使得MongoDB有更强的扩展性。"],["body","\n"],["body","如果负载的增加（需要更多的存储空间和更强的处理能力） ，它可以分布在计算机网络中的其他节点上这就是所谓的分片。"],["body","\n"],["body","Mongo支持丰富的查询表达式。查询指令使用JSON形式的标记，可轻易查询文档中内嵌的对象及数组。"],["body","\n"],["body","MongoDb 使用update()命令可以实现替换完成的文档（数据）或者一些指定的数据字段 。"],["body","\n"],["body","Mongodb中的Map/reduce主要是用来对数据进行批量处理和聚合操作。"],["body","\n"],["body","Map和Reduce。Map函数调用emit(key,value)遍历集合中所有的记录，将key与value传给Reduce函数进行处理。"],["body","\n"],["body","Map函数和Reduce函数是使用Javascript编写的，并可以通过db.runCommand或mapreduce命令来执行MapReduce操作。"],["body","\n"],["body","GridFS是MongoDB中的一个内置功能，可以用于存放大量小文件。"],["body","\n"],["body","MongoDB允许在服务端执行脚本，可以用Javascript编写某个函数，直接在服务端执行，也可以把函数的定义存储在服务端，下次直接调用即可。"],["body","\n\n"],["headingLink","mongodb-下载"],["heading","MongoDB 下载"],["body","\n"],["body","你可以在mongodb官网下载该安装包，地址为：https://www.mongodb.com/download-center#community。MonggoDB支持以下平台:"],["body","\n\n"],["body","OS X 32-bit"],["body","\n"],["body","OS X 64-bit"],["body","\n"],["body","Linux 32-bit"],["body","\n"],["body","Linux 64-bit"],["body","\n"],["body","Windows 32-bit"],["body","\n"],["body","Windows 64-bit"],["body","\n"],["body","Solaris i86pc"],["body","\n"],["body","Solaris 64"],["body","\n\n"],["headingLink","mongodb-工具"],["heading","MongoDB 工具"],["body","\n"],["body","有几种可用于MongoDB的管理工具。"],["body","\n"],["headingLink","监控"],["heading","监控"],["body","\n"],["body","MongoDB提供了网络和系统监控工具Munin，它作为一个插件应用于MongoDB中。"],["body","\n"],["body","Gangila是MongoDB高性能的系统监视的工具，它作为一个插件应用于MongoDB中。"],["body","\n"],["body","基于图形界面的开源工具 Cacti, 用于查看CPU负载, 网络带宽利用率,它也提供了一个应用于监控 MongoDB 的插件。"],["body","\n"],["headingLink","gui"],["heading","GUI"],["body","\n\n"],["body","Fang of Mongo – 网页式,由Django和jQuery所构成。"],["body","\n"],["body","Futon4Mongo – 一个CouchDB Futon web的mongodb山寨版。"],["body","\n"],["body","Mongo3 – Ruby写成。"],["body","\n"],["body","MongoHub – 适用于OSX的应用程序。"],["body","\n"],["body","Opricot – 一个基于浏览器的MongoDB控制台, 由PHP撰写而成。"],["body","\n"],["body","Database Master — Windows的mongodb管理工具"],["body","\n"],["body","RockMongo — 最好的PHP语言的MongoDB管理工具，轻量级, 支持多国语言."],["body","\n\n"],["headingLink","mongodb-概念解析"],["heading","MongoDB 概念解析"],["body","\n"],["body","SQL术语/概念"],["body","MongoDB术语/概念"],["body","解释/说明"],["body","\n"],["body","database"],["body","database"],["body","数据库"],["body","\n"],["body","table"],["body","collection"],["body","数据库表/集合"],["body","\n"],["body","row"],["body","document"],["body","数据记录行/文档"],["body","\n"],["body","column"],["body","field"],["body","数据字段/域"],["body","\n"],["body","index"],["body","index"],["body","索引"],["body","\n"],["body","table joins"],["body","表连接,MongoDB不支持"],["body","\n"],["body","primary key"],["body","primary key"],["body","主键,MongoDB自动将_id字段设置为主键"],["body","\n\n\n"],["headingLink","数据库"],["heading","数据库"],["body","\n"],["body","一个mongodb中可以建立多个数据库。"],["body","\n"],["body","MongoDB的默认数据库为\"db\"，该数据库存储在data目录中。"],["body","\n"],["body","MongoDB的单个实例可以容纳多个独立的数据库，每一个都有自己的集合和权限，不同的数据库也放置在不同的文件中。"],["body","\n"],["body","命令"],["body","描述"],["body","\n"],["body","show dbs"],["body","显示所有数据的列表"],["body","\n"],["body","db"],["body","显示当前数据库对象或集合"],["body","\n"],["body","use db"],["body","连接到一个指定的数据库"],["body","\n\n\n"],["body","数据库也通过名字来标识。数据库名可以是满足以下条件的任意UTF-8字符串。"],["body","\n\n"],["body","不能是空字符串（\"\")。"],["body","\n"],["body","不得含有' '（空格)、.、$、/、\\和\\0 (空宇符)。"],["body","\n"],["body","应全部小写。"],["body","\n"],["body","最多64字节。"],["body","\n\n"],["body","有一些数据库名是保留的，可以直接访问这些有特殊作用的数据库。"],["body","\n"],["body","admin： 从权限的角度来看，这是\"root\"数据库。要是将一个用户添加到这个数据库，这个用户自动继承所有数据库的权限。一些特定的服务器端命令也只能从这个数据库运行，比如列出所有的数据库或者关闭服务器。"],["body","\n"],["body","**local:**这个数据永远不会被复制，可以用来存储限于本地单台服务器的任意集合"],["body","\n"],["body","config: 当Mongo用于分片设置时，config数据库在内部使用，用于保存分片的相关信息。"],["body","\n"],["headingLink","文档"],["heading","文档"],["body","\n"],["body","文档是一个键值(key-value)对(即BSON)。MongoDB 的文档不需要设置相同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别"],["body","\n"],["body","{\"site\":\"www.php.cn\", \"name\":\"php中文网\"}\n"],["body","\n"],["body","RDBMS"],["body","MongoDB"],["body","\n"],["body","数据库"],["body","数据库"],["body","\n"],["body","表格"],["body","集合"],["body","\n"],["body","行"],["body","文档"],["body","\n"],["body","列"],["body","字段"],["body","\n"],["body","表联合"],["body","嵌入文档"],["body","\n"],["body","主键"],["body","主键 (MongoDB 提供了 key  为 _id )"],["body","\n"],["body","数据库服务和客户端"],["body","\n"],["body","Mysqld/Oracle"],["body","mongod"],["body","\n"],["body","mysql/sqlplus"],["body","mongo"],["body","\n\n\n"],["headingLink","集合"],["heading","集合"],["body","\n"],["body","集合就是 MongoDB 文档组，类似于 RDBMS （关系数据库管理系统：Relational Database Management System)中的表格。"],["body","\n"],["body","集合存在于数据库中，集合没有固定的结构，这意味着你在对集合可以插入不同格式和类型的数据，但通常情况下我们插入集合的数据都会有一定的关联性。"],["body","\n"],["body","比如，我们可以将以下不同数据结构的文档插入到集合中："],["body","\n"],["body","{\"site\":\"www.baidu.com\"}\n{\"site\":\"www.google.com\",\"name\":\"Google\"}\n{\"site\":\"www.php.cn\",\"name\":\"php中文网\",\"num\":5}\n"],["body","\n"],["body","当第一个文档插入时，集合就会被创建。"],["body","\n"],["headingLink","合法的集合名"],["heading","合法的集合名"],["body","\n\n"],["body","集合名不能是空字符串\"\"。"],["body","\n"],["body","集合名不能含有\\0字符（空字符)，这个字符表示集合名的结尾。"],["body","\n"],["body","集合名不能以\"system.\"开头，这是为系统集合保留的前缀。"],["body","\n"],["body","用户创建的集合名字不能含有保留字符。有些驱动程序的确支持在集合名里面包含，这是因为某些系统生成的集合中包含该字符。除非你要访问这种系统创建的集合，否则千万不要在名字里出现$。　"],["body","\n\n"],["headingLink","capped-collections"],["heading","capped collections"],["body","\n"],["body","Capped collections 就是固定大小的collection。"],["body","\n"],["body","它有很高的性能以及队列过期的特性(过期按照插入的顺序). 有点和 \"RRD\" 概念类似。"],["body","\n"],["body","Capped collections是高性能自动的维护对象的插入顺序。它非常适合类似记录日志的功能 和标准的collection不同，你必须要显式的创建一个capped collection， 指定一个collection的大小，单位是字节。collection的数据存储空间值提前分配的。"],["body","\n"],["body","db.createCollection(\"mycoll\", {capped:true, size:100000})\n"],["body","\n\n"],["body","在capped collection中，你能添加新的对象。"],["body","\n"],["body","能进行更新，然而，对象不会增加存储空间。如果增加，更新就会失败 。"],["body","\n"],["body","数据库不允许进行删除。使用drop()方法删除collection所有的行。"],["body","\n"],["body","注意: 删除之后，你必须显式的重新创建这个collection。"],["body","\n"],["body","在32bit机器中，capped collection最大存储为1e9( 10^9)个字节。"],["body","\n\n"],["headingLink","元数据"],["heading","元数据"],["body","\n"],["body","数据库的信息是存储在集合中。它们使用了系统的命名空间："],["body","\n"],["body","dbname.system.*\n"],["body","\n"],["body","在MongoDB数据库中名字空间 .system.* 是包含多种系统信息的特殊集合(Collection)，如下:"],["body","\n"],["body","集合命名空间"],["body","描述"],["body","\n"],["body","dbname.system.namespaces"],["body","列出所有名字空间。"],["body","\n"],["body","dbname.system.indexes"],["body","列出所有索引。"],["body","\n"],["body","dbname.system.profile"],["body","包含数据库概要(profile)信息。"],["body","\n"],["body","dbname.system.users"],["body","列出所有可访问数据库的用户。"],["body","\n"],["body","dbname.local.sources"],["body","包含复制对端（slave）的服务器信息和状态。"],["body","\n\n\n"],["body","对于修改系统集合中的对象有如下限制。"],["body","\n\n"],["body","\n"],["body","在{{system.indexes}}插入数据，可以创建索引。但除此之外该表信息是不可变的(特殊的drop index命令将自动更新相关信息)。"],["body","\n"],["body","\n"],["body","\n"],["body","{{system.users}}是可修改的。 {{system.profile}}是可删除的。"],["body","\n"],["body","\n\n"],["headingLink","mongodb-数据类型"],["heading","MongoDB 数据类型"],["body","\n"],["body","下表为MongoDB中常用的几种数据类型。"],["body","\n"],["body","数据类型"],["body","描述"],["body","\n"],["body","String"],["body","字符串。存储数据常用的数据类型。在 MongoDB 中，UTF-8 编码的字符串才是合法的。"],["body","\n"],["body","Integer"],["body","整型数值。用于存储数值。根据你所采用的服务器，可分为 32 位或 64 位。"],["body","\n"],["body","Boolean"],["body","布尔值。用于存储布尔值（真/假）。"],["body","\n"],["body","Double"],["body","双精度浮点值。用于存储浮点值。"],["body","\n"],["body","Min/Max keys"],["body","将一个值与 BSON（二进制的 JSON）元素的最低值和最高值相对比。"],["body","\n"],["body","Arrays"],["body","用于将数组或列表或多个值存储为一个键。"],["body","\n"],["body","Timestamp"],["body","时间戳。记录文档修改或添加的具体时间。"],["body","\n"],["body","Object"],["body","用于内嵌文档。"],["body","\n"],["body","Null"],["body","用于创建空值。"],["body","\n"],["body","Symbol"],["body","符号。该数据类型基本上等同于字符串类型，但不同的是，它一般用于采用特殊符号类型的语言。"],["body","\n"],["body","Date"],["body","日期时间。用 UNIX 时间格式来存储当前日期或时间。你可以指定自己的日期时间：创建 Date 对象，传入年月日信息。"],["body","\n"],["body","Object ID"],["body","对象 ID。用于创建文档的 ID。"],["body","\n"],["body","Binary Data"],["body","二进制数据。用于存储二进制数据。"],["body","\n"],["body","Code"],["body","代码类型。用于在文档中存储 JavaScript 代码。"],["body","\n"],["body","Regular expression"],["body","正则表达式类型。用于存储正则表达式。"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/安装/2.InstallFromDeb.html"],["title","InstallFromDeb.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","import-the-elasticsearch-pgp-key"],["heading","Import the Elasticsearch PGP Key"],["body","\n"],["body","wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -\n\ncurl -L  https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -\n"],["body","\n"],["headingLink","installing-from-the-apt-repository"],["heading","Installing from the APT repository"],["body","\n"],["body","sudo apt-get install apt-transport-https\n"],["body","\n"],["body","echo \"deb https://artifacts.elastic.co/packages/7.x/apt stable main\" | sudo tee /etc/apt/sources.list.d/elastic-7.x.list\n"],["body","\n"],["body","These instructions do not use add-apt-repository for several reasons:"],["body","\n\n"],["body","\n"],["body","add-apt-repository adds entries to the system /etc/apt/sources.list file rather than a clean per-repository file in /etc/apt/sources.list.d"],["body","\n"],["body","\n"],["body","\n"],["body","add-apt-repository is not part of the default install on many distributions and requires a number of non-default dependencies."],["body","\n"],["body","\n"],["body","\n"],["body","Older versions of add-apt-repository always add a deb-src entry which will cause errors because we do not provide a source package. If you have added the deb-src entry, you will see an error like the following until you delete the deb-src line:"],["body","\n"],["body","Unable to find expected entry 'main/source/Sources' in Release file\n(Wrong sources.list entry or malformed file)\n"],["body","\n"],["body","\n\n"],["body","sudo apt-get update && sudo apt-get install elasticsearch\n"],["body","\n"],["body","If two entries exist for the same Elasticsearch repository, you will see an error like this during apt-get update:"],["body","\n"],["body","Duplicate sources.list entry https://artifacts.elastic.co/packages/7.x/apt/ ...`\n"],["body","\n"],["body","Examine /etc/apt/sources.list.d/elasticsearch-7.x.list for the duplicate entry or locate the duplicate entry amongst the files in /etc/apt/sources.list.d/ and the /etc/apt/sources.list file."],["body","\n"],["headingLink","手动下载"],["heading","手动下载"],["body","\n"],["body","wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.13.4-amd64.deb\nwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.13.4-amd64.deb.sha512\nshasum -a 512 -c elasticsearch-7.13.4-amd64.deb.sha512 \nsudo dpkg -i elasticsearch-7.13.4-amd64.deb\n"],["body","\n"],["headingLink","running-elasticsearch-with-sysv-init"],["heading","Running Elasticsearch with SysV init"],["body","\n"],["body","自启"],["body","\n"],["body","sudo update-rc.d elasticsearch defaults 95 10\nsudo -i service elasticsearch start\nsudo -i service elasticsearch stop\n"],["body","\n"],["headingLink","running-elasticsearch-with-systemd"],["heading","Running Elasticsearch with systemd"],["body","\n"],["body","sudo /bin/systemctl daemon-reload\nsudo /bin/systemctl enable elasticsearch.service\nsudo systemctl start elasticsearch.service\nsudo systemctl stop elasticsearch.service\n"],["body","\n"],["headingLink","密码文件"],["heading","密码文件"],["body","\n"],["body","echo \"keystore_password\" > /path/to/my_pwd_file.tmp\nchmod 600 /path/to/my_pwd_file.tmp\nsudo systemctl set-environment ES_KEYSTORE_PASSPHRASE_FILE=/path/to/my_pwd_file.tmp\nsudo systemctl start elasticsearch.service\n"],["body","\n"],["headingLink","journactl集成"],["heading","journactl集成"],["body","\n"],["body","By default the Elasticsearch service doesn’t log information in the systemd journal. To enable journalctl logging, the --quiet option must be removed from the ExecStart command line in the elasticsearch.service file."],["body","\n"],["body","sudo journalctl -f\nsudo journalctl --unit elasticsearch\nsudo journalctl --unit elasticsearch --since  \"2016-10-30 18:17:16\"\n\n"],["body","\n"],["headingLink","directory-layout-of-debian-package"],["heading","Directory layout of Debian package"],["body","\n"],["body","The Debian package places config files, logs, and the data directory in the appropriate locations for a Debian-based system:"],["body","\n"],["body","Type"],["body","Description"],["body","Default Location"],["body","Setting"],["body","\n"],["body","home"],["body","Elasticsearch home directory or $ES_HOME"],["body","/usr/share/elasticsearch"],["body","\n"],["body","bin"],["body","Binary scripts including elasticsearch to start a node and elasticsearch-plugin to install plugins"],["body","/usr/share/elasticsearch/bin"],["body","\n"],["body","conf"],["body","Configuration files including elasticsearch.yml"],["body","/etc/elasticsearch"],["body","ES_PATH_CONF"],["body","\n"],["body","conf"],["body","Environment variables including heap size, file descriptors."],["body","/etc/default/elasticsearch"],["body","\n"],["body","data"],["body","The location of the data files of each index / shard allocated on the node."],["body","/var/lib/elasticsearch"],["body","path.data"],["body","\n"],["body","jdk"],["body","The bundled Java Development Kit used to run Elasticsearch. Can be overridden by setting the ES_JAVA_HOME environment variable in /etc/default/elasticsearch."],["body","/usr/share/elasticsearch/jdk"],["body","\n"],["body","logs"],["body","Log files location."],["body","/var/log/elasticsearch"],["body","path.logs"],["body","\n"],["body","plugins"],["body","Plugin files location. Each plugin will be contained in a subdirectory."],["body","/usr/share/elasticsearch/plugins"],["body","\n"],["body","repo"],["body","Shared file system repository locations. Can hold multiple locations. A file system repository can be placed in to any subdirectory of any directory specified here."],["body","Not configured"],["body","path.repoe"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/安装/1.InstallingFromRpm.html"],["title","InstallingFromRpm.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","手动安装"],["heading","手动安装"],["body","\n"],["body","平台"],["body","资源包"],["body","\n"],["body","Linux and MacOS tar.gz archives"],["body","The tar.gz archives are available for installation on any Linux distribution and MacOS.Install Elasticsearch from archive on Linux or MacOS"],["body","\n"],["body","Windows .zip archive"],["body","The zip archive is suitable for installation on Windows.Install Elasticsearch with .zip on Windows"],["body","\n"],["body","deb"],["body","The deb package is suitable for Debian, Ubuntu, and other Debian-based systems. Debian packages may be downloaded from the Elasticsearch website or from our Debian repository.Install Elasticsearch with Debian Package"],["body","\n"],["body","rpm"],["body","The rpm package is suitable for installation on Red Hat, Centos, SLES, OpenSuSE and other RPM-based systems. RPMs may be downloaded from the Elasticsearch website or from our RPM repository.Install Elasticsearch with RPM"],["body","\n"],["body","msi"],["body","[beta] This functionality is in beta and is subject to change. The design and code is less mature than official GA features and is being provided as-is with no warranties. Beta features are not subject to the support SLA of official GA features.The msi package is suitable for installation on Windows 64-bit systems with at least .NET 4.5 framework installed, and is the easiest choice for getting started with Elasticsearch on Windows. MSIs may be downloaded from the Elasticsearch website.Install Elasticsearch with Windows MSI Installer"],["body","\n"],["body","docker"],["body","Images are available for running Elasticsearch as Docker containers. They may be downloaded from the Elastic Docker Registry.Install Elasticsearch with Docker"],["body","\n"],["body","brew"],["body","Formulae are available from the Elastic Homebrew tap for installing Elasticsearch on macOS with the Homebrew package manager.Install Elasticsearch on macOS with Homebrew"],["body","\n\n\n"],["headingLink","使用rpm-安装"],["heading","使用RPM 安装"],["body","\n"],["body","The RPM for Elasticsearch can be downloaded from our website or from our RPM repository. It can be used to install Elasticsearch on any RPM-based system such as OpenSuSE, SLES, Centos, Red Hat, and Oracle Enterprise."],["body","\n"],["body","RPM install is not supported on distributions with old versions of RPM, such as SLES 11 and CentOS 5. Please see Install Elasticsearch from archive on Linux or MacOS instead."],["body","\n\n"],["body","可以通过上述链接下载 RPM包。支持任务 基于RPM的包管理系统"],["body","\n"],["body","不支持旧版本的 RPM"],["body","\n\n"],["headingLink","import-the-elasticsearch-gpg-key"],["heading","Import the Elasticsearch GPG Key"],["body","\n"],["body","We sign all of our packages with the Elasticsearch Signing Key (PGP key D88E42B4, available from https://pgp.mit.edu) with fingerprint:"],["body","\n"],["body","4609 5ACC 8548 582C 1A26 99A9 D27D 666C D88E 42B4\n"],["body","\n"],["body","rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch\n"],["body","\n"],["headingLink","installing-from-the-rpm-repository"],["heading","Installing from the RPM repository"],["body","\n"],["body","Create a file called elasticsearch.repo in the /etc/yum.repos.d/ directory for RedHat based distributions, or in the /etc/zypp/repos.d/ directory for OpenSuSE based distributions, containing:"],["body","\n"],["body","[elasticsearch]\nname=Elasticsearch repository for 7.x packages\nbaseurl=https://artifacts.elastic.co/packages/7.x/yum\ngpgcheck=1\ngpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch\nenabled=0\nautorefresh=1\ntype=rpm-md\n"],["body","\n"],["body","// centos7\nsudo yum install --enablerepo=elasticsearch elasticsearch \n//centos8\nsudo dnf install --enablerepo=elasticsearch elasticsearch \n//opensuse\nsudo zypper modifyrepo --enable elasticsearch && \\\n  sudo zypper install elasticsearch; \\\n  sudo zypper modifyrepo --disable elasticsearch \n"],["body","\n"],["body","默认情况下，已配置的存储库处于禁用状态。"],["body","\n"],["body","这消除了升级系统其余部分时意外升级elasticsearch的可能性。"],["body","\n"],["body","每个安装或升级命令都必须明确启用存储库，如上面的示例命令所示。"],["body","\n"],["headingLink","download-and-install-the-rpm-manually"],["heading","Download and install the RPM manually"],["body","\n"],["body","wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.13.4-x86_64.rpm\nwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.13.4-x86_64.rpm.sha512\nshasum -a 512 -c elasticsearch-7.13.4-x86_64.rpm.sha512 \nsudo rpm --install elasticsearch-7.13.4-x86_64.rpm\n"],["body","\n"],["body","比较下载的RPM和发布的校验和的SHA，应该输出elasticsearch-{version}-x86_64.rpm: OK。"],["body","\n"],["body","On systemd-based distributions, the installation scripts will attempt to set kernel parameters (e.g., vm.max_map_count); you can skip this by masking the systemd-sysctl.service unit."],["body","\n"],["headingLink","enable-automatic-creation-of-system-indices"],["heading","Enable automatic creation of system indices"],["body","\n"],["body","一些商业功能会自动在Elasticsearch内创建索引。"],["body","\n"],["body","默认情况下，Elasticsearch配置为允许自动创建索引，并且不需要其他步骤。"],["body","\n"],["body","但是，如果禁用了Elasticsearch中的自动索引创建，则必须在elasticsearch.yml中配置action.auto_create_index，以允许商业功能创建以下索引:"],["body","\n"],["body","action.auto_create_index: .monitoring*,.watches,.triggered_watches,.watcher-history*,.ml*\n"],["body","\n\n"],["body","\n"],["body","如果您使用Logstash或Beats，则很可能在action.auto_create_index设置中需要其他索引名称，确切的值将取决于您的本地配置。"],["body","\n"],["body","\n"],["body","\n"],["body","如果不确定环境的正确值，则可以考虑将该值设置为 *，这将允许自动创建所有索引。"],["body","\n"],["body","\n\n"],["headingLink","sysv-init-vs-systemd"],["heading","SysV init vs systemd"],["body","\n"],["body","安装后，Elasticsearch不会自动启动。如何启动和停止Elasticsearch取决于您的系统是使用SysV init还是systemd (由较新的发行版使用)。您可以通过运行此命令来判断正在使用哪个命令:"],["body","\n"],["body","ps -p 1\n"],["body","\n"],["headingLink","running-elasticsearch-with-sysv-init"],["heading","Running Elasticsearch with SysV init"],["body","\n"],["body","sudo chkconfig --add elasticsearch\nsudo -i service elasticsearch start\nsudo -i service elasticsearch stop\n//日志文件\n/var/log/elasticsearch/\n"],["body","\n"],["headingLink","running-elasticsearch-with-systemd"],["heading","Running Elasticsearch with systemd"],["body","\n"],["body","sudo /bin/systemctl daemon-reload\nsudo /bin/systemctl enable elasticsearch.service\nsudo systemctl start elasticsearch.service\nsudo systemctl stop elasticsearch.service\n//日志文件\n/var/log/elasticsearch/\n\n\n"],["body","\n"],["headingLink","密码文件"],["heading","密码文件"],["body","\n"],["body","如果您的Elasticsearch密钥库受到密码保护，则需要使用本地文件和systemd环境变量为systemd提供密钥库密码。"],["body","\n"],["body","此本地文件应在存在时受到保护，并且一旦Elasticsearch启动并运行，就可以安全地删除该本地文件。"],["body","\n"],["body","echo \"keystore_password\" > /path/to/my_pwd_file.tmp\nchmod 600 /path/to/my_pwd_file.tmp\nsudo systemctl set-environment ES_KEYSTORE_PASSPHRASE_FILE=/path/to/my_pwd_file.tmp\nsudo systemctl start elasticsearch.service\n"],["body","\n"],["headingLink","启用journalctl日志"],["heading","启用journalctl日志"],["body","\n"],["body","默认情况下，Elasticsearch服务不会将信息记录在systemd日志中。要启用journalctl日志记录，必须从elasticsearch.service文件中的ExecStart命令行中删除 -- quiet选项。"],["body","\n"],["body","sudo journalctl -f\nsudo journalctl --unit elasticsearch\nsudo journalctl --unit elasticsearch --since  \"2016-10-30 18:17:16\"\n"],["body","\n"],["body","Check man journalctl or https://www.freedesktop.org/software/systemd/man/journalctl.html for more command line options.."],["body","\n"],["headingLink","checking-that-elasticsearch-is-running"],["heading","Checking that Elasticsearch is running"],["body","\n"],["body","GET /\n"],["body","\n"],["body","{\n  \"name\" : \"Cp8oag6\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"cluster_uuid\" : \"AT69_T_DTp-1qgIJlatQqA\",\n  \"version\" : {\n    \"number\" : \"7.13.4\",\n    \"build_flavor\" : \"default\",\n    \"build_type\" : \"tar\",\n    \"build_hash\" : \"f27399d\",\n    \"build_date\" : \"2016-03-30T09:51:41.449Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"8.8.2\",\n    \"minimum_wire_compatibility_version\" : \"1.2.3\",\n    \"minimum_index_compatibility_version\" : \"1.2.3\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n"],["body","\n"],["headingLink","configuring-elasticsearch"],["heading","Configuring Elasticsearch"],["body","\n\n"],["body","\n"],["body","/etc/elasticsearch目录包含Elasticsearch的默认运行时配置。此目录和所有包含文件的所有权设置为root:elasticsearch。"],["body","\n"],["body","\n"],["body","\n"],["body","setgid标志在/etc/elasticsearch目录上应用组权限，以确保Elasticsearch可以读取任何包含的文件和子目录。所有文件和子目录都继承 root:elasticsearch 所有权。"],["body","\n"],["body","\n"],["body","\n"],["body","Running commands from this directory or any subdirectories, such as the elasticsearch-keystore tool, requires root:elasticsearch permissions."],["body","\n"],["body","\n\n"],["body","默认情况下，Elasticsearch从/etc/elasticsearch.yml文件加载其配置。"],["body","\n"],["body","此配置文件的格式在 Configuring Elasticsearch.中进行了说明。"],["body","\n"],["headingLink","系统配置"],["heading","系统配置"],["body","\n"],["body","RPM还有一个系统配置文件 (/etc/sysconfig/elasticsearch)，它允许您设置以下参数:"],["body","\n"],["body","配置项"],["body","描述"],["body","\n"],["body","ES_JAVA_HOME"],["body","Set a custom Java path to be used."],["body","\n"],["body","MAX_OPEN_FILES"],["body","Maximum number of open files, defaults to 65535."],["body","\n"],["body","MAX_LOCKED_MEMORY"],["body","Maximum locked memory size. Set to unlimited if you use the bootstrap.memory_lock option in elasticsearch.yml."],["body","\n"],["body","MAX_MAP_COUNT"],["body","Maximum number of memory map areas a process may have. If you use mmapfs as index store type, make sure this is set to a high value. For more information, check the linux kernel documentation about max_map_count. This is set via sysctl before starting Elasticsearch. Defaults to 262144."],["body","\n"],["body","ES_PATH_CONF"],["body","Configuration file directory (which needs to include elasticsearch.yml, jvm.options, and log4j2.properties files); defaults to /etc/elasticsearch."],["body","\n"],["body","ES_JAVA_OPTS"],["body","Any additional JVM system properties you may want to apply."],["body","\n"],["body","RESTART_ON_UPGRADE"],["body","Configure restart on package upgrade, defaults to false. This means you will have to restart your Elasticsearch instance after installing a package manually. The reason for this is to ensure, that upgrades in a cluster do not result in a continuous shard reallocation resulting in high network traffic and reducing the response times of your cluster."],["body","\n\n\n"],["body","使用systemd 配置时 ，可以通过 Systemd configuration 修改 配置 而不是 /etc/sysconfig/elasticsearch"],["body","\n"],["headingLink","directory-layout-of-rpm"],["heading","Directory layout of RPM"],["body","\n"],["body","Type"],["body","Description"],["body","Default Location"],["body","Setting"],["body","\n"],["body","home"],["body","Elasticsearch home directory or $ES_HOME"],["body","/usr/share/elasticsearch"],["body","\n"],["body","bin"],["body","Binary scripts including elasticsearch to start a node and elasticsearch-plugin to install plugins"],["body","/usr/share/elasticsearch/bin"],["body","\n"],["body","conf"],["body","Configuration files including elasticsearch.yml"],["body","/etc/elasticsearch"],["body","ES_PATH_CONF"],["body","\n"],["body","conf"],["body","Environment variables including heap size, file descriptors."],["body","/etc/sysconfig/elasticsearch"],["body","\n"],["body","data"],["body","The location of the data files of each index / shard allocated on the node."],["body","/var/lib/elasticsearch"],["body","path.data"],["body","\n"],["body","jdk"],["body","The bundled Java Development Kit used to run Elasticsearch. Can be overridden by setting the ES_JAVA_HOME environment variable in /etc/sysconfig/elasticsearch."],["body","/usr/share/elasticsearch/jdk"],["body","\n"],["body","logs"],["body","Log files location."],["body","/var/log/elasticsearch"],["body","path.logs"],["body","\n"],["body","plugins"],["body","Plugin files location. Each plugin will be contained in a subdirectory."],["body","/usr/share/elasticsearch/plugins"],["body","\n"],["body","repo"],["body","Shared file system repository locations. Can hold multiple locations. A file system repository can be placed in to any subdirectory of any directory specified here."],["body","Not configured"],["body","path.repo"],["body","\n\n\n"],["headingLink","管理工具"],["heading","管理工具"],["body","\n"],["body","工具，名"],["body","链接"],["body","\n"],["body","Puppet"],["body","puppet-elasticsearch"],["body","\n"],["body","Chef"],["body","cookbook-elasticsearch"],["body","\n"],["body","Ansible"],["body","ansible-elasticsearch"],["body","\n\n\n"],["body","参考链接"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/安装/README.html"],["title","安装 - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","安装-elasticsearch"],["heading","安装 Elasticsearch"],["body","\n"],["body","本节包括有关如何设置Elasticsearch并使其运行的信息，包括:"],["body","\n\n"],["body","Downloading"],["body","\n"],["body","Installing"],["body","\n"],["body","Starting"],["body","\n"],["body","Configuring"],["body","\n\n"],["headingLink","supported-platforms"],["heading","Supported platforms"],["body","\n"],["body","所有版本支持的： Support Matrix"],["body","\n"],["headingLink","java-jvm-version"],["heading","Java (JVM) Version"],["body","\n"],["body","Elasticsearch is built using Java, and includes a bundled version of OpenJDK from the JDK maintainers (GPLv2+CE) within each distribution. The bundled JVM is the recommended JVM and is located within the jdk directory of the Elasticsearch home directory."],["body","\n"],["body","To use your own version of Java, set the ES_JAVA_HOME environment variable. If you must use a version of Java that is different from the bundled JVM, we recommend using a supported LTS version of Java. Elasticsearch will refuse to start if a known-bad version of Java is used. The bundled JVM directory may be removed when using your own JVM."],["body","\n\n"],["body","内置JDK"],["body","\n"],["body","可以  使用 ES_JAVA_HOME  自定义JDK"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/深入搜索/全文搜索.html"],["title","全文搜索.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","全文搜索"],["heading","全文搜索"],["body","\n"],["body","全文搜索两个最重要的方面是："],["body","\n\n"],["body","相关性（Relevance）"],["body","\n"],["body","**分析（Analysis）**它是将文本块转换为有区别的、规范化的 token 的一个过程，目的是为了创建倒排索引"],["body","\n\n"],["headingLink","基于词项与基于全文"],["heading","基于词项与基于全文"],["body","\n"],["body","所有查询会或多或少的执行相关度计算，但不是所有查询都有分析阶段，和一些特殊的完全不会对文本进行操作的查询（如 bool 或 function_score ）不同，文本查询可以划分成两大家族："],["body","\n"],["body","基于词项的查询"],["body","\n"],["body","如 term 或 fuzzy 这样的底层查询不需要分析阶段，它们对单个词项进行操作"],["body","\n\n"],["body","用 term 查询词项 Foo 只要在倒排索引中查找 准确词项 ，"],["body","\n"],["body","并且用 TF/IDF 算法为每个包含该词项的文档计算相关度评分 _score 。"],["body","\n\n"],["body","term 查询只对倒排索引的词项精确匹配，这点很重要，它不会对词的多样性进行处理（如， foo 或 FOO ）"],["body","\n"],["body","基于全文的查询"],["body","\n"],["body","像 match 或 query_string 这样的查询是高层查询，它们了解字段映射的信息："],["body","\n\n"],["body","如果查询 日期（date） 或 整数（integer） 字段，它们会将查询字符串分别作为日期或整数对待。"],["body","\n"],["body","如果查询一个（ not_analyzed ）未分析的精确值字符串字段，它们会将整个查询字符串作为单个词项对待。"],["body","\n"],["body","但如果要查询一个（ analyzed ）已分析的全文字段，它们会先将查询字符串传递到一个合适的分析器，然后生成一个供查询的词项列表。"],["body","\n\n"],["body","一旦组成了词项列表，这个查询会对每个词项逐一执行底层的查询，再将结果合并，然后为每个文档生成一个最终的相关度评分。"],["body","\n"],["body","我们很少直接使用基于词项的搜索，通常情况下都是对全文进行查询，这只需要简单的执行一个高层全文查询（进而在高层查询内部会以基于词项的底层查询完成搜索）。"],["body","\n"],["headingLink","匹配查询"],["heading","匹配查询"],["body","\n"],["body","匹配查询 match 是个 核心 查询。无论需要查询什么字段， match 查询都应该会是首选的查询方式。它是一个高级 全文查询 ，这表示它既能处理全文字段，又能处理精确字段。"],["body","\n"],["body","这就是说， match 查询主要的应用场景就是进行全文搜索，我们以下面一个简单例子来说明全文搜索是如何工作的："],["body","\n"],["body","GET /my_index/my_type/_search\n{\n    \"query\": {\n        \"match\": {\n            \"title\": \"QUICK!\"\n        }\n    }\n}\n"],["body","\n"],["body","Elasticsearch 执行上面这个 match 查询的步骤是："],["body","\n\n"],["body","\n"],["body","检查字段类型 。"],["body","\n"],["body","标题 title 字段是一个 string 类型（ analyzed ）已分析的全文字段，这意味着查询字符串本身也应该被分析。"],["body","\n"],["body","\n"],["body","\n"],["body","分析查询字符串"],["body","\n\n"],["body","将查询的字符串 QUICK! 传入标准分析器中，输出的结果是单个项 quick 。因为只有一个单词项，所以 match 查询执行的是单个底层 term 查询。"],["body","\n\n"],["body","\n"],["body","\n"],["body","查找匹配文档 。"],["body","\n\n"],["body","用 term 查询在倒排索引中查找 quick 然后获取一组包含该项的文档，本例的结果是文档：1、2 和 3 。"],["body","\n\n"],["body","\n"],["body","\n"],["body","为每个文档评分 。"],["body","\n\n"],["body","用 term 查询计算每个文档相关度评分 _score ，这是种将词频（term frequency，即词 quick 在相关文档的 title 字段中出现的频率）和反向文档频率（inverse document frequency，即词 quick 在所有文档的 title 字段中出现的频率），以及字段的长度（即字段越短相关度越高）相结合的计算方式"],["body","\n\n"],["body","\n\n"],["headingLink","多词查询"],["heading","多词查询"],["body","\n"],["body","幸运的是 match 查询让多词查询变得简单："],["body","\n"],["body","GET /my_index/my_type/_search\n{\n    \"query\": {\n        \"match\": {\n            \"title\": \"BROWN DOG!\"\n        }\n    }\n}\n"],["body","\n"],["body","因为 match 查询必须查找两个词（ [\"brown\",\"dog\"] ），它在内部实际上先执行两次 term 查询，然后将两次查询的结果合并作为最终结果输出。为了做到这点，它将两个 term 查询包入一个 bool 查询中，"],["body","\n"],["body","上面这个查询返回所有四个文档："],["body","\n"],["body","{\n  \"hits\": [\n     {\n        \"_id\":      \"4\",\n        \"_score\":   0.73185337, \n        \"_source\": {\n           \"title\": \"Brown fox brown dog\"\n        }\n     },\n     {\n        \"_id\":      \"2\",\n        \"_score\":   0.47486103, \n        \"_source\": {\n           \"title\": \"The quick brown fox jumps over the lazy dog\"\n        }\n     },\n     {\n        \"_id\":      \"3\",\n        \"_score\":   0.47486103, \n        \"_source\": {\n           \"title\": \"The quick brown fox jumps over the quick dog\"\n        }\n     },\n     {\n        \"_id\":      \"1\",\n        \"_score\":   0.11914785, \n        \"_source\": {\n           \"title\": \"The quick brown fox\"\n        }\n     }\n  ]\n}\n"],["body","\n\n"],["body","文档 4 最相关，因为它包含词 \"brown\" 两次以及 \"dog\" 一次。"],["body","\n"],["body","文档 2、3 同时包含 brown 和 dog 各一次，而且它们 title 字段的长度相同，所以具有相同的评分。"],["body","\n"],["body","文档 1 也能匹配，尽管它只有 brown 没有 dog 。"],["body","\n\n"],["body","因为 match 查询必须查找两个词（ [\"brown\",\"dog\"] ），它在内部实际上先执行两次 term 查询，然后将两次查询的结果合并作为最终结果输出。为了做到这点，它将两个 term 查询包入一个 bool 查询中"],["body","\n"],["headingLink","提高精度"],["heading","提高精度"],["body","\n"],["body","用 任意 查询词项匹配文档可能会导致结果中出现不相关的长尾。这是种散弹式搜索。可能我们只想搜索包含 所有 词项的文档，也就是说，不去匹配 brown OR dog ，而通过匹配 brown AND dog 找到所有文档。"],["body","\n"],["body","match 查询还可以接受 operator 操作符作为输入参数，默认情况下该操作符是 or 。我们可以将它修改成 and 让所有指定词项都必须匹配："],["body","\n"],["body","GET /my_index/my_type/_search\n{\n    \"query\": {\n        \"match\": {\n            \"title\": {      \n                \"query\":    \"BROWN DOG!\",\n                \"operator\": \"and\"\n            }\n        }\n    }\n}\n"],["body","\n"],["headingLink","控制精度"],["heading","控制精度"],["body","\n"],["body","在 所有 与 任意 间二选一有点过于非黑即白。"],["body","\n"],["body","如果用户给定 5 个查询词项，想查找只包含其中 4 个的文档，该如何处理？"],["body","\n"],["body","match 查询支持 minimum_should_match 最小匹配参数，这让我们可以指定必须匹配的词项数用来表示一个文档是否相关。我们可以将其设置为某个具体数字，更常用的做法是将其设置为一个百分数，因为我们无法控制用户搜索时输入的单词数量："],["body","\n"],["body","GET /my_index/my_type/_search\n{\n  \"query\": {\n    \"match\": {\n      \"title\": {\n        \"query\":                \"quick brown dog\",\n        \"minimum_should_match\": \"75%\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","当给定百分比的时候， minimum_should_match 会做合适的事情：在之前三词项的示例中， 75% 会自动被截断成 66.6% ，即三个里面两个词。无论这个值设置成什么，至少包含一个词项的文档才会被认为是匹配的。"],["body","\n"],["body","参数 minimum_should_match 的设置非常灵活，可以根据用户输入词项的数目应用不同的规则。完整的信息参考文档 https://www.elastic.co/guide/en/elasticsearch/reference/5.6/query-dsl-minimum-should-match.html#query-dsl-minimum-should-match"],["body","\n"],["headingLink","组合查询"],["heading","组合查询"],["body","\n"],["body","与bool过滤器一样， bool 查询也可以接受 must 、 must_not 和 should 参数下的多个查询语句。比如："],["body","\n"],["body","GET /my_index/my_type/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\":     { \"match\": { \"title\": \"quick\" }},\n      \"must_not\": { \"match\": { \"title\": \"lazy\"  }},\n      \"should\": [\n                  { \"match\": { \"title\": \"brown\" }},\n                  { \"match\": { \"title\": \"dog\"   }}\n      ]\n    }\n  }\n}\n"],["body","\n\n"],["body","以上的查询结果返回 title 字段包含词项 quick 但不包含 lazy 的任意文档。目前为止，这与 bool 过滤器的工作方式非常相似。"],["body","\n"],["body","区别就在于两个 should 语句，也就是说：一个文档不必包含 brown 或 dog 这两个词项，但如果一旦包含，我们就认为它们 更相关 ："],["body","\n\n"],["body","{\n  \"hits\": [\n     {\n        \"_id\":      \"3\",\n        \"_score\":   0.70134366, \n        \"_source\": {\n           \"title\": \"The quick brown fox jumps over the quick dog\"\n        }\n     },\n     {\n        \"_id\":      \"1\",\n        \"_score\":   0.3312608,\n        \"_source\": {\n           \"title\": \"The quick brown fox\"\n        }\n     }\n  ]\n}\n"],["body","\n"],["body","文档 3 会比文档 1 有更高评分是因为它同时包含 brown 和 dog 。"],["body","\n"],["headingLink","评分计算"],["heading","评分计算"],["body","\n"],["body","bool 查询会为每个文档计算相关度评分 _score ，再将所有匹配的 must 和 should 语句的分数 _score 求和，最后除以 must 和 should 语句的总数。"],["body","\n"],["body","must_not 语句不会影响评分；它的作用只是将不相关的文档排除。"],["body","\n"],["headingLink","控制精度-1"],["heading","控制精度"],["body","\n\n"],["body","\n"],["body","所有 must 语句必须匹配，所有 must_not 语句都必须不匹配"],["body","\n"],["body","\n"],["body","\n"],["body","但有多少 should 语句应该匹配呢？默认情况下，没有 should 语句是必须匹配的"],["body","\n"],["body","\n"],["body","\n"],["body","那就是当没有 must 语句的时候，至少有一个 should 语句必须匹配。"],["body","\n"],["body","\n\n"],["body","就像我们能控制 match 查询的精度 一样，我们可以通过 minimum_should_match 参数控制需要匹配的 should 语句的数量，它既可以是一个绝对的数字，又可以是个百分比："],["body","\n"],["headingLink","布尔匹配与match"],["heading","布尔匹配与Match"],["body","\n"],["body","目前为止，可能已经意识到多词 match 查询只是简单地将生成的 term 查询包裹在一个 bool 查询中。如果使用默认的 or 操作符，每个 term 查询都被当作 should 语句，这样就要求必须至少匹配一条语句。以下两个查询是等价的："],["body","\n"],["body","{\n    \"match\": { \"title\": \"brown fox\"}\n}\n"],["body","\n"],["body","{\n  \"bool\": {\n    \"should\": [\n      { \"term\": { \"title\": \"brown\" }},\n      { \"term\": { \"title\": \"fox\"   }}\n    ]\n  }\n}\n"],["body","\n"],["body","如果使用 and 操作符，所有的 term 查询都被当作 must 语句，所以 所有（all） 语句都必须匹配。以下两个查询是等价的："],["body","\n"],["body","{\n    \"match\": {\n        \"title\": {\n            \"query\":    \"brown fox\",\n            \"operator\": \"and\"\n        }\n    }\n}\n"],["body","\n"],["body","{\n  \"bool\": {\n    \"must\": [\n      { \"term\": { \"title\": \"brown\" }},\n      { \"term\": { \"title\": \"fox\"   }}\n    ]\n  }\n"],["body","\n"],["body","如果指定参数 minimum_should_match ，它可以通过 bool 查询直接传递，使以下两个查询等价："],["body","\n"],["body","{\n    \"match\": {\n        \"title\": {\n            \"query\":                \"quick brown fox\",\n            \"minimum_should_match\": \"75%\"\n        }\n    }\n}\n"],["body","\n"],["body","{\n  \"bool\": {\n    \"should\": [\n      { \"term\": { \"title\": \"brown\" }},\n      { \"term\": { \"title\": \"fox\"   }},\n      { \"term\": { \"title\": \"quick\" }}\n    ],\n    \"minimum_should_match\": 2 \n  }\n}\n"],["body","\n"],["body","因为只有三条语句，match 查询的参数 minimum_should_match 值 75% 会被截断成 2 。即三条 should 语句中至少有两条必须匹配。"],["body","\n"],["headingLink","查询语句提升权重"],["heading","查询语句提升权重"],["body","\n"],["body","当然 bool 查询不仅限于组合简单的单个词 match 查询，它可以组合任意其他的查询，以及其他 bool 查询。普遍的用法是通过汇总多个独立查询的分数，从而达到为每个文档微调其相关度评分 _score 的目的。"],["body","\n"],["body","假设想要查询关于 “full-text search（全文搜索）” 的文档，但我们希望为提及 “Elasticsearch” 或 “Lucene” 的文档给予更高的 权重 ，这里 更高权重 是指如果文档中出现 “Elasticsearch” 或 “Lucene” ，它们会比没有的出现这些词的文档获得更高的相关度评分 _score ，也就是说，它们会出现在结果集的更上面。"],["body","\n"],["body","GET /_search\n{\n    \"query\": {\n        \"bool\": {\n            \"must\": {\n                \"match\": {\n                    \"content\": { \n                        \"query\":    \"full text search\",\n                        \"operator\": \"and\"\n                    }\n                }\n            },\n            \"should\": [ \n                { \"match\": { \"content\": \"Elasticsearch\" }},\n                { \"match\": { \"content\": \"Lucene\"        }}\n            ]\n        }\n    }\n}\n"],["body","\n\n"],["body","content 字段必须包含 full 、 text 和 search 所有三个词。"],["body","\n"],["body","如果 content 字段也包含 Elasticsearch 或 Lucene ，文档会获得更高的评分 _score 。"],["body","\n\n"],["body","但是如果我们想让包含 Lucene 的有更高的权重，并且包含 Elasticsearch 的语句比 Lucene 的权重更高，该如何处理?"],["body","\n"],["body","GET /_search\n{\n    \"query\": {\n        \"bool\": {\n            \"must\": {\n                \"match\": {  //这些语句使用默认的 boost 值 1 。\n                    \"content\": {\n                        \"query\":    \"full text search\",\n                        \"operator\": \"and\"\n                    }\n                }\n            },\n            \"should\": [\n                { \"match\": {\n                    \"content\": {\n                        \"query\": \"Elasticsearch\",\n                        \"boost\": 3  //这条语句更为重要，因为它有最高的 boost 值。\n                    }\n                }},\n                { \"match\": {\n                    \"content\": {\n                        \"query\": \"Lucene\",\n                        \"boost\": 2  //这条语句比使用默认值的更重要，但它的重要性不及 Elasticsearch 语句。\n                    }\n                }}\n            ]\n        }\n    }\n}\n"],["body","\n"],["body","boost 参数被用来提升一个语句的相对权重（ boost 值大于 1 ）或降低相对权重（ boost 值处于 0 到 1 之间），但是这种提升或降低并不是线性的，换句话说，如果一个 boost 值为 2 ，并不能获得两倍的评分 _score 。"],["body","\n"],["body","相反，新的评分 _score 会在应用权重提升之后被 归一化 ，每种类型的查询都有自己的归一算法，细节超出了本书的范围，所以不作介绍。简单的说，更高的 boost 值为我们带来更高的评分 _score 。"],["body","\n"],["body","如果不基于 TF/IDF 要实现自己的评分模型，我们就需要对权重提升的过程能有更多控制，可以使用 function_score 查询操纵一个文档的权重提升方式而跳过归一化这一步骤。"],["body","\n"],["headingLink","控制分析"],["heading","控制分析"],["body","\n"],["body","查询只能查找倒排索引表中真实存在的项，所以保证文档在索引时与查询字符串在搜索时应用相同的分析过程非常重要，这样查询的项才能够匹配倒排索引中的项。"],["body","\n"],["body","不过分析器可以由每个字段决定。每个字段都可以有不同的分析器，既可以通过配置为字段指定分析器，也可以使用更高层的类型（type）、索引（index）或节点（node）的默认配置。在索引时，一个字段值是根据配置或默认分析器分析的。"],["body","\n"],["body","例如为 my_index 新增一个字段："],["body","\n"],["body","PUT /my_index/_mapping/my_type\n{\n    \"my_type\": {\n        \"properties\": {\n            \"english_title\": {\n                \"type\":     \"string\",\n                \"analyzer\": \"english\"\n            }\n        }\n    }\n}\n"],["body","\n"],["body","现在我们就可以通过使用 analyze API 来分析单词 Foxes ，进而比较 english_title 字段和 title 字段在索引时的分析结果："],["body","\n"],["body","//字段 title ，使用默认的 standard 标准分析器，返回词项 foxes 。\nGET /my_index/_analyze\n{\n  \"field\": \"my_type.title\",   \n  \"text\": \"Foxes\"\n}\n//字段 english_title ，使用 english 英语分析器，返回词项 fox 。\nGET /my_index/_analyze\n{\n  \"field\": \"my_type.english_title\",   \n  \"text\": \"Foxes\"\n}\n"],["body","\n"],["body","如果使用底层 term 查询精确项 fox 时， english_title 字段会匹配但 title 字段不会。"],["body","\n"],["headingLink","默认分析器"],["heading","默认分析器"],["body","\n"],["body","分析器可以从三个层面进行定义：按字段（per-field）、按索引（per-index）或全局缺省（global default）。Elasticsearch 会按照以下顺序依次处理，直到它找到能够使用的分析器。索引时的顺序如下："],["body","\n\n"],["body","字段映射里定义的 analyzer ，否则"],["body","\n"],["body","索引设置中名为 default 的分析器，默认为"],["body","\n"],["body","standard 标准分析器"],["body","\n\n"],["body","在搜索时，顺序有些许不同："],["body","\n\n"],["body","查询自己定义的 analyzer ，否则"],["body","\n"],["body","字段映射里定义的 analyzer ，否则"],["body","\n"],["body","索引设置中名为 default 的分析器，默认为"],["body","\n"],["body","standard 标准分析器"],["body","\n\n"],["body","有时，在索引时和搜索时使用不同的分析器是合理的。"],["body","\n"],["body","我们可能要想为同义词建索引（例如，所有 quick 出现的地方，同时也为 fast 、 rapid 和 speedy 创建索引）。"],["body","\n"],["body","但在搜索时，我们不需要搜索所有的同义词，取而代之的是寻找用户输入的单词是否是 quick 、 fast 、 rapid 或 speedy 。"],["body","\n"],["body","为了区分，Elasticsearch 也支持一个可选的 search_analyzer 映射，它仅会应用于搜索时（ analyzer 还用于索引时）。还有一个等价的 default_search 映射，用以指定索引层的默认配置。"],["body","\n"],["body","如果考虑到这些额外参数，一个搜索时的 完整 顺序会是下面这样："],["body","\n\n"],["body","查询自己定义的 analyzer ，否则"],["body","\n"],["body","字段映射里定义的 search_analyzer ，否则"],["body","\n"],["body","字段映射里定义的 analyzer ，否则"],["body","\n"],["body","索引设置中名为 default_search 的分析器，默认为"],["body","\n"],["body","索引设置中名为 default 的分析器，默认为"],["body","\n"],["body","standard 标准分析器"],["body","\n\n"],["headingLink","被破坏的相关度"],["heading","被破坏的相关度！"],["body","\n"],["body","在讨论更复杂的 多字段搜索 之前，让我们先快速解释一下为什么只在主分片上 创建测试索引 。"],["body","\n"],["body","用户会时不时的抱怨无法按相关度排序并提供简短的重现步骤"],["body","\n"],["body","用户索引了一些文档，运行一个简单的查询，然后发现明显低相关度的结果出现在高相关度结果之上。"],["body","\n"],["body","我们在两个主分片上创建了索引和总共 10 个文档，其中 6 个文档有单词 foo 。可能是分片 1 有其中 3 个 foo 文档，而分片 2 有其中另外 3 个文档，换句话说，所有文档是均匀分布存储的。"],["body","\n"],["body","相关度"],["body","\n"],["body","在 什么是相关度？中，我们描述了 Elasticsearch 默认使用的相似度算法，这个算法叫做 词频/逆向文档频率 或 TF/IDF 。词频是计算某个词在当前被查询文档里某个字段中出现的频率，出现的频率越高，文档越相关。 逆向文档频率 将 某个词在索引内所有文档出现的百分数 考虑在内，出现的频率越高，它的权重就越低。"],["body","\n"],["body","但是由于性能原因， Elasticsearch 不会计算索引内所有文档的 IDF 。相反，每个分片会根据 该分片 内的所有文档计算一个本地 IDF 。"],["body","\n"],["body","因为文档是均匀分布存储的，两个分片的 IDF 是相同的。相反，设想如果有 5 个 foo 文档存于分片 1 ，而第 6 个文档存于分片 2 ，在这种场景下， foo 在一个分片里非常普通（所以不那么重要），但是在另一个分片里非常出现很少（所以会显得更重要）。这些 IDF 之间的差异会导致不正确的结果。"],["body","\n"],["body","在实际应用中，这并不是一个问题，本地和全局的 IDF 的差异会随着索引里文档数的增多渐渐消失，在真实世界的数据量下，局部的 IDF 会被迅速均化，所以上述问题并不是相关度被破坏所导致的，而是由于数据太少。"],["body","\n"],["body","为了测试，我们可以通过两种方式解决这个问题。第一种是只在主分片上创建索引，正如 match 查询 里介绍的那样，如果只有一个分片，那么本地的 IDF 就是 全局的 IDF。"],["body","\n"],["body","第二个方式就是在搜索请求后添加 ?search_type=dfs_query_then_fetch ， dfs 是指 分布式频率搜索（Distributed Frequency Search） ， 它告诉 Elasticsearch ，先分别获得每个分片本地的 IDF ，然后根据结果再计算整个索引的全局 IDF 。"],["body","\n"],["body","不要在生产环境上使用 dfs_query_then_fetch 。完全没有必要。只要有足够的数据就能保证词频是均匀分布的。没有理由给每个查询额外加上 DFS 这步。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/深入搜索/多字段搜索.html"],["title","多字段搜索.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","多字段搜索"],["heading","多字段搜索"],["body","\n"],["body","查询很少是简单一句话的 match 匹配查询"],["body","\n"],["body","通常我们需要用相同或不同的字符串查询一个或多个字段，也就是说，需要对多个查询语句以及它们相关度评分进行合理的合并。"],["body","\n"],["headingLink","多字符串查询"],["heading","多字符串查询"],["body","\n"],["body","最简单的多字段查询可以将搜索项映射到具体的字段"],["body","\n"],["body","如果我们知道 War and Peace 是标题，Leo Tolstoy 是作者，很容易就能把两个条件用 match 语句表示，并将它们用 bool 查询 组合起来："],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        { \"match\": { \"title\":  \"War and Peace\" }},\n        { \"match\": { \"author\": \"Leo Tolstoy\"   }}\n      ]\n    }\n  }\n}\n"],["body","\n"],["body","bool 查询采取 more-matches-is-better 匹配越多越好的方式，"],["body","\n"],["body","所以每条 match 语句的评分结果会被加在一起，从而为每个文档提供最终的分数 _score 。能与两条语句同时匹配的文档比只与一条语句匹配的文档得分要高。"],["body","\n"],["body","当然，并不是只能使用 match 语句：可以用 bool 查询来包裹组合任意其他类型的查询，甚至包括其他的 bool 查询。我们可以在上面的示例中添加一条语句来指定译者版本的偏好：."],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        { \"match\": { \"title\":  \"War and Peace\" }},\n        { \"match\": { \"author\": \"Leo Tolstoy\"   }},\n        { \"bool\":  {\n          \"should\": [\n            { \"match\": { \"translator\": \"Constance Garnett\" }},\n            { \"match\": { \"translator\": \"Louise Maude\"      }}\n          ]\n        }}\n      ]\n    }\n  }\n}\n"],["body","\n"],["body","为什么将译者条件语句放入另一个独立的 bool 查询中呢？所有的四个 match 查询都是 should 语句，所以为什么不将 translator 语句与其他如 title 、 author 这样的语句放在同一层呢？"],["body","\n"],["body","答案在于评分的计算方式。 bool 查询运行每个 match 查询，再把评分加在一起，然后将结果与所有匹配的语句数量相乘，最后除以所有的语句数量。处于同一层的每条语句具有相同的权重。在前面这个例子中，包含 translator 语句的 bool 查询，只占总评分的三分之一。如果将 translator 语句与 title 和 author 两条语句放入同一层，那么 title 和 author 语句只贡献四分之一评分。"],["body","\n"],["headingLink","语句的优先级"],["heading","语句的优先级"],["body","\n"],["body","前例中每条语句贡献三分之一评分的这种方式可能并不是我们想要的，我们可能对 title 和 author 两条语句更感兴趣，这样就需要调整查询，使 title 和 author 语句相对来说更重要。"],["body","\n"],["body","在武器库中，最容易使用的就是 boost 参数。为了提升 title 和 author 字段的权重，为它们分配的 boost 值大于 1 ："],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        { \"match\": { \n            \"title\":  {\n              \"query\": \"War and Peace\",\n              \"boost\": 2\n        }}},\n        { \"match\": { \n            \"author\":  {\n              \"query\": \"Leo Tolstoy\",\n              \"boost\": 2\n        }}},\n        { \"bool\":  { \n            \"should\": [\n              { \"match\": { \"translator\": \"Constance Garnett\" }},\n              { \"match\": { \"translator\": \"Louise Maude\"      }}\n            ]\n        }}\n      ]\n    }\n  }\n}\n"],["body","\n"],["body","要获取 boost 参数 “最佳” 值，较为简单的方式就是不断试错：设定 boost 值，运行测试查询，如此反复。 boost 值比较合理的区间处于 1 到 10 之间，当然也有可能是 15 。如果为 boost 指定比这更高的值，将不会对最终的评分结果产生更大影响，因为评分是被 归一化的（normalized） 。"],["body","\n"],["headingLink","单字符串查询"],["heading","单字符串查询"],["body","\n"],["body","bool 查询是多语句查询的主干。它的适用场景很多，特别是当需要将不同查询字符串映射到不同字段的时候。"],["body","\n"],["body","问题在于，目前有些用户期望将所有的搜索项堆积到单个字段中，并期望应用程序能为他们提供正确的结果"],["body","\n"],["body","有意思的是多字段搜索的表单通常被称为 高级查询 （Advanced Search） —— 只是因为它对用户而言是高级的，而多字段搜索的实现却非常简单。"],["body","\n"],["body","对于多词（multiword）、多字段（multifield）查询来说，不存在简单的 万能 方案。为了获得最好结果，需要 了解我们的数据 ，并了解如何使用合适的工具。"],["body","\n"],["headingLink","了解我们的数据"],["heading","了解我们的数据"],["body","\n"],["body","当用户输入了单个字符串查询的时候，通常会遇到以下三种情形："],["body","\n"],["body","最佳字段"],["body","\n"],["body","当搜索词语具体概念的时候，比如 “brown fox” ，词组比各自独立的单词更有意义。像 title 和 body 这样的字段，尽管它们之间是相关的，但同时又彼此相互竞争。文档在 相同字段 中包含的词越多越好，评分也来自于 最匹配字段 。"],["body","\n"],["body","多数字段"],["body","\n"],["body","为了对相关度进行微调，常用的一个技术就是将相同的数据索引到不同的字段，它们各自具有独立的分析链。"],["body","\n"],["body","主字段可能包括它们的词源、同义词以及 变音词 或口音词，被用来匹配尽可能多的文档。"],["body","\n"],["body","相同的文本被索引到其他字段，以提供更精确的匹配。一个字段可以包括未经词干提取过的原词，另一个字段包括其他词源、口音，还有一个字段可以提供 词语相似性 信息的瓦片词（shingles）。"],["body","\n"],["body","其他字段是作为匹配每个文档时提高相关度评分的 信号 ， 匹配字段越多 则越好。"],["body","\n"],["body","混合字段"],["body","\n"],["body","对于某些实体，我们需要在多个字段中确定其信息，单个字段都只能作为整体的一部分："],["body","\n\n"],["body","Person： first_name 和 last_name （人：名和姓）"],["body","\n"],["body","Book： title 、 author 和 description （书：标题、作者、描述）"],["body","\n"],["body","Address： street 、 city 、 country 和 postcode （地址：街道、市、国家和邮政编码）"],["body","\n\n"],["body","在这种情况下，我们希望在 任何 这些列出的字段中找到尽可能多的词，这有如在一个大字段中进行搜索，这个大字段包括了所有列出的字段。"],["body","\n"],["headingLink","最佳字段"],["heading","最佳字段"],["body","\n"],["body","假设有个网站允许用户搜索博客的内容，以下面两篇博客内容文档为例："],["body","\n"],["body","PUT /my_index/my_type/1\n{\n    \"title\": \"Quick brown rabbits\",\n    \"body\":  \"Brown rabbits are commonly seen.\"\n}\n\nPUT /my_index/my_type/2\n{\n    \"title\": \"Keeping pets healthy\",\n    \"body\":  \"My quick brown fox eats rabbits on a regular basis.\"\n}\n"],["body","\n"],["body","用户输入词组 “Brown fox” 然后点击搜索按钮。事先，我们并不知道用户的搜索项是会在 title 还是在 body 字段中被找到，但是，用户很有可能是想搜索相关的词组。用肉眼判断，文档 2 的匹配度更高，因为它同时包括要查找的两个词："],["body","\n"],["body","{\n    \"query\": {\n        \"bool\": {\n            \"should\": [\n                { \"match\": { \"title\": \"Brown fox\" }},\n                { \"match\": { \"body\":  \"Brown fox\" }}\n            ]\n        }\n    }\n}\n"],["body","\n"],["body","但是我们发现查询的结果是文档 1 的评分更高："],["body","\n\n"],["body","它会执行 should 语句中的两个查询。"],["body","\n"],["body","加和两个查询的评分。"],["body","\n"],["body","乘以匹配语句的总数。"],["body","\n"],["body","除以所有语句总数（这里为：2）。"],["body","\n\n"],["body","文档 1 的两个字段都包含 brown 这个词，所以两个 match 语句都能成功匹配并且有一个评分。文档 2 的 body 字段同时包含 brown 和 fox 这两个词，但 title 字段没有包含任何词。这样， body 查询结果中的高分，加上 title 查询中的 0 分，然后乘以二分之一，就得到比文档 1 更低的整体评分。"],["body","\n"],["body","在本例中， title 和 body 字段是相互竞争的关系，所以就需要找到单个 最佳匹配 的字段。"],["body","\n"],["body","如果不是简单将每个字段的评分结果加在一起，而是将 最佳匹配 字段的评分作为查询的整体评分，"],["body","\n"],["body","结果会怎样？这样返回的结果可能是： 同时 包含 brown 和 fox 的单个字段比反复出现相同词语的多个不同字段有更高的相关度。"],["body","\n"],["headingLink","dis_max-查询"],["heading","dis_max 查询"],["body","\n"],["body","不使用 bool 查询，可以使用 dis_max 即分离 最大化查询（Disjunction Max Query） 。"],["body","\n"],["body","分离（Disjunction）的意思是 或（or） ，这与可以把结合（conjunction）理解成 与（and） 相对应"],["body","\n"],["body","分离最大化查询（Disjunction Max Query）指的是： 将任何与任一查询匹配的文档作为结果返回，但只将最佳匹配的评分作为查询的评分结果返回 ："],["body","\n"],["body","{\n    \"query\": {\n        \"dis_max\": {\n            \"queries\": [\n                { \"match\": { \"title\": \"Brown fox\" }},\n                { \"match\": { \"body\":  \"Brown fox\" }}\n            ]\n        }\n    }\n}\n"],["body","\n"],["body","{\n  \"hits\": [\n     {\n        \"_id\":      \"2\",\n        \"_score\":   0.21509302,\n        \"_source\": {\n           \"title\": \"Keeping pets healthy\",\n           \"body\":  \"My quick brown fox eats rabbits on a regular basis.\"\n        }\n     },\n     {\n        \"_id\":      \"1\",\n        \"_score\":   0.12713557,\n        \"_source\": {\n           \"title\": \"Quick brown rabbits\",\n           \"body\":  \"Brown rabbits are commonly seen.\"\n        }\n     }\n  ]\n}\n"],["body","\n"],["headingLink","最佳字段查询调优"],["heading","最佳字段查询调优"],["body","\n"],["body","当用户搜索 “quick pets” 时会发生什么呢？在前面的例子中，两个文档都包含词 quick ，但是只有文档 2 包含词 pets ，两个文档中都不具有同时包含 两个词 的 相同字段 。"],["body","\n"],["body","{\n    \"query\": {\n        \"dis_max\": {\n            \"queries\": [\n                { \"match\": { \"title\": \"Quick pets\" }},\n                { \"match\": { \"body\":  \"Quick pets\" }}\n            ]\n        }\n    }\n}\n"],["body","\n"],["body","{\n  \"hits\": [\n     {\n        \"_id\": \"1\",\n        \"_score\": 0.12713557, \n        \"_source\": {\n           \"title\": \"Quick brown rabbits\",\n           \"body\": \"Brown rabbits are commonly seen.\"\n        }\n     },\n     {\n        \"_id\": \"2\",\n        \"_score\": 0.12713557, \n        \"_source\": {\n           \"title\": \"Keeping pets healthy\",\n           \"body\": \"My quick brown fox eats rabbits on a regular basis.\"\n        }\n     }\n   ]\n}\n"],["body","\n"],["headingLink","tie_breaker-参数"],["heading","tie_breaker 参数"],["body","\n"],["body","可以通过指定 tie_breaker 这个参数将其他匹配语句的评分也考虑其中："],["body","\n"],["body","{\n    \"query\": {\n        \"dis_max\": {\n            \"queries\": [\n                { \"match\": { \"title\": \"Quick pets\" }},\n                { \"match\": { \"body\":  \"Quick pets\" }}\n            ],\n            \"tie_breaker\": 0.3\n        }\n    }\n}\n"],["body","\n"],["body","{\n  \"hits\": [\n     {\n        \"_id\": \"2\",\n        \"_score\": 0.14757764, \n        \"_source\": {\n           \"title\": \"Keeping pets healthy\",\n           \"body\": \"My quick brown fox eats rabbits on a regular basis.\"\n        }\n     },\n     {\n        \"_id\": \"1\",\n        \"_score\": 0.124275915, \n        \"_source\": {\n           \"title\": \"Quick brown rabbits\",\n           \"body\": \"Brown rabbits are commonly seen.\"\n        }\n     }\n   ]\n}\n"],["body","\n"],["body","tie_breaker 参数提供了一种 dis_max 和 bool 之间的折中选择，它的评分方式如下："],["body","\n\n"],["body","获得最佳匹配语句的评分 _score 。"],["body","\n"],["body","将其他匹配语句的评分结果与 tie_breaker 相乘。"],["body","\n"],["body","对以上评分求和并规范化。"],["body","\n\n"],["body","有了 tie_breaker ，会考虑所有匹配语句，但最佳匹配语句依然占最终结果里的很大一部分。"],["body","\n"],["body","tie_breaker 可以是 0 到 1 之间的浮点数，其中 0 代表使用 dis_max 最佳匹配语句的普通逻辑， 1 表示所有匹配语句同等重要。最佳的精确值需要根据数据与查询调试得出，但是合理值应该与零接近（处于 0.1 - 0.4 之间），这样就不会颠覆 dis_max 最佳匹配性质的根本。"],["body","\n"],["headingLink","multi_match-查询"],["heading","multi_match 查询"],["body","\n"],["body","multi_match 查询为能在多个字段上反复执行相同查询提供了一种便捷方式。"],["body","\n"],["body","multi_match 多匹配查询的类型有多种，其中的三种恰巧与 了解我们的数据 中介绍的三个场景对应，即： best_fields 、 most_fields 和 cross_fields （最佳字段、多数字段、跨字段）。"],["body","\n"],["body","默认情况下，查询的类型是 best_fields ，这表示它会为每个字段生成一个 match 查询，然后将它们组合到 dis_max 查询的内部，如下："],["body","\n"],["body","{\n  \"dis_max\": {\n    \"queries\":  [\n      {\n        \"match\": {\n          \"title\": {\n            \"query\": \"Quick brown fox\",\n            \"minimum_should_match\": \"30%\"\n          }\n        }\n      },\n      {\n        \"match\": {\n          \"body\": {\n            \"query\": \"Quick brown fox\",\n            \"minimum_should_match\": \"30%\"\n          }\n        }\n      },\n    ],\n    \"tie_breaker\": 0.3\n  }\n}\n"],["body","\n"],["body","上面这个查询用 multi_match 重写成更简洁的形式："],["body","\n"],["body","{\n    \"multi_match\": {\n        \"query\":                \"Quick brown fox\",\n        \"type\":                 \"best_fields\",  //best_fields 类型是默认值，可以不指定。\n        \"fields\":               [ \"title\", \"body\" ],\n        \"tie_breaker\":          0.3,\n        \"minimum_should_match\": \"30%\" //如 minimum_should_match 或 operator 这样的参数会被传递到生成的 match 查询中。\n    }\n}\n"],["body","\n"],["headingLink","查询字段名称的模糊匹配"],["heading","查询字段名称的模糊匹配"],["body","\n"],["body","字段名称可以用模糊匹配的方式给出：任何与模糊模式正则匹配的字段都会被包括在搜索条件中，例如可以使用以下方式同时匹配 book_title 、 chapter_title 和 section_title （书名、章名、节名）这三个字段："],["body","\n"],["body","{\n    \"multi_match\": {\n        \"query\":  \"Quick brown fox\",\n        \"fields\": \"*_title\"\n    }\n}\n"],["body","\n"],["headingLink","提升单个字段的权重"],["heading","提升单个字段的权重"],["body","\n"],["body","可以使用 ^ 字符语法为单个字段提升权重，在字段名称的末尾添加 ^boost ，其中 boost 是一个浮点数："],["body","\n"],["body","{\n    \"multi_match\": {\n        \"query\":  \"Quick brown fox\",\n        \"fields\": [ \"*_title\", \"chapter_title^2\" ] \n    }\n}\n"],["body","\n"],["body","chapter_title 这个字段的 boost 值为 2 ，而其他两个字段 book_title 和 section_title 字段的默认 boost 值为 1 。"],["body","\n"],["headingLink","多数字段"],["heading","多数字段"],["body","\n"],["body","全文搜索被称作是 召回率（Recall） 与 精确率（Precision） 的战场"],["body","\n"],["body","召回率 ——返回所有的相关文档； 精确率 ——不返回无关文档"],["body","\n"],["body","目的是在结果的第一页中为用户呈现最为相关的文档。"],["body","\n"],["body","提高全文相关性精度的常用方式是为同一文本建立多种方式的索引，每种方式都提供了一个不同的相关度信号 signal 。主字段会以尽可能多的形式的去匹配尽可能多的文档"],["body","\n\n"],["body","使用词干提取来索引 jumps 、 jumping 和 jumped 样的词，将 jump 作为它们的词根形式。这样即使用户搜索 jumped ，也还是能找到包含 jumping 的匹配的文档。"],["body","\n"],["body","将同义词包括其中，如 jump 、 leap 和 hop 。"],["body","\n"],["body","移除变音或口音词：如 ésta 、 está 和 esta 都会以无变音形式 esta 来索引。"],["body","\n\n"],["body","如果我们有两个文档，其中一个包含词 jumped ，另一个包含词 jumping ，用户很可能期望前者能排的更高，因为它正好与输入的搜索条件一致。"],["body","\n"],["body","为了达到目的，我们可以将相同的文本索引到其他字段从而提供更为精确的匹配。"],["body","\n"],["body","一个字段可能是为词干未提取过的版本，"],["body","\n"],["body","另一个字段可能是变音过的原始词，"],["body","\n"],["body","第三个可能使用 shingles 提供 词语相似性 信息。"],["body","\n"],["body","这些附加的字段可以看成提高每个文档的相关度评分的信号 signals ，能匹配字段的越多越好。"],["body","\n"],["body","一个文档如果与广度匹配的主字段相匹配，那么它会出现在结果列表中。如果文档同时又与 signal 信号字段匹配，那么它会获得额外加分，系统会提升它在结果列表中的位置。"],["body","\n"],["body","我们会在本书稍后对同义词、词相似性、部分匹配以及其他潜在的信号进行讨论，但这里只使用词干已提取（stemmed）和未提取（unstemmed）的字段作为简单例子来说明这种技术"],["body","\n"],["headingLink","多字段映射"],["heading","多字段映射"],["body","\n"],["body","首先要做的事情就是对我们的字段索引两次：一次使用词干模式以及一次非词干模式。为了做到这点，采用 multifields 来实现，已经在 multifields 有所介绍："],["body","\n"],["body","DELETE /my_index\n\nPUT /my_index\n{\n    \"settings\": { \"number_of_shards\": 1 }, \n    \"mappings\": {\n        \"my_type\": {\n            \"properties\": {\n                \"title\": { \n                    \"type\":     \"string\",\n                    \"analyzer\": \"english\", //title 字段使用 english 英语分析器来提取词干。\n                    \"fields\": {\n                        \"std\":   { \n                            \"type\":     \"string\",\n                            \"analyzer\": \"standard\" //title.std 字段使用 standard 标准分析器，所以没有词干提取。\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n"],["body","\n"],["body","PUT /my_index/my_type/1\n{ \"title\": \"My rabbit jumps\" }\n\nPUT /my_index/my_type/2\n{ \"title\": \"Jumping jack rabbits\" }\n"],["body","\n"],["body","GET /my_index/_search\n{\n   \"query\": {\n        \"match\": {\n            \"title\": \"jumping rabbits\"\n        }\n    }\n}\n"],["body","\n"],["body","因为有了 english 分析器，这个查询是在查找以 jump 和 rabbit 这两个被提取词的文档。两个文档的 title 字段都同时包括这两个词，所以两个文档得到的评分也相同："],["body","\n"],["body","{\n  \"hits\": [\n     {\n        \"_id\": \"1\",\n        \"_score\": 0.42039964,\n        \"_source\": {\n           \"title\": \"My rabbit jumps\"\n        }\n     },\n     {\n        \"_id\": \"2\",\n        \"_score\": 0.42039964,\n        \"_source\": {\n           \"title\": \"Jumping jack rabbits\"\n        }\n     }\n  ]\n}\n"],["body","\n"],["body","如果同时查询两个字段，然后使用 bool 查询将评分结果 合并 ，那么两个文档都是匹配的（ title 字段的作用），而且文档 2 的相关度评分更高（ title.std 字段的作用）："],["body","\n"],["body","GET /my_index/_search\n{\n   \"query\": {\n        \"multi_match\": {\n            \"query\":  \"jumping rabbits\",\n            \"type\":   \"most_fields\", \n            \"fields\": [ \"title\", \"title.std\" ]\n        }\n    }\n}\n"],["body","\n"],["body","{\n  \"hits\": [\n     {\n        \"_id\": \"2\",\n        \"_score\": 0.8226396, \n        \"_source\": {\n           \"title\": \"Jumping jack rabbits\"\n        }\n     },\n     {\n        \"_id\": \"1\",\n        \"_score\": 0.10741998, \n        \"_source\": {\n           \"title\": \"My rabbit jumps\"\n        }\n     }\n  ]\n}\n"],["body","\n"],["body","用广度匹配字段 title 包括尽可能多的文档——以提升召回率——"],["body","\n"],["body","同时又使用字段 title.std 作为 信号 将相关度更高的文档置于结果顶部。"],["body","\n"],["body","每个字段对于最终评分的贡献可以通过自定义值 boost 来控制。比如，使 title 字段更为重要，这样同时也降低了其他信号字段的作用："],["body","\n"],["body","GET /my_index/_search\n{\n   \"query\": {\n        \"multi_match\": {\n            \"query\":       \"jumping rabbits\",\n            \"type\":        \"most_fields\",\n            \"fields\":      [ \"title^10\", \"title.std\" ] \n        }\n    }\n}\n"],["body","\n"],["headingLink","跨字段实体搜索"],["heading","跨字段实体搜索"],["body","\n"],["body","现在讨论一种普遍的搜索模式：跨字段实体搜索（cross-fields entity search）"],["body","\n"],["body","如 person 、 product 或 address （人、产品或地址）这样的实体中，需要使用多个字段来唯一标识它的信息。 person 实体可能是这样索引的："],["body","\n"],["body","{\n    \"firstname\":  \"Peter\",\n    \"lastname\":   \"Smith\"\n}\n"],["body","\n"],["body","或地址："],["body","\n"],["body","{\n    \"street\":   \"5 Poland Street\",\n    \"city\":     \"London\",\n    \"country\":  \"United Kingdom\",\n    \"postcode\": \"W1V 3DG\"\n}\n"],["body","\n"],["body","在本例中，我们想使用 单个 字符串在多个字段中进行搜索。"],["body","\n"],["headingLink","简单的方式"],["heading","简单的方式"],["body","\n"],["body","依次查询每个字段并将每个字段的匹配评分结果相加，听起来真像是 bool 查询："],["body","\n"],["body","{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        { \"match\": { \"street\":    \"Poland Street W1V\" }},\n        { \"match\": { \"city\":      \"Poland Street W1V\" }},\n        { \"match\": { \"country\":   \"Poland Street W1V\" }},\n        { \"match\": { \"postcode\":  \"Poland Street W1V\" }}\n      ]\n    }\n  }\n}\n"],["body","\n"],["body","为每个字段重复查询字符串会使查询瞬间变得冗长，可以采用 multi_match 查询，将 type 设置成 most_fields 然后告诉 Elasticsearch 合并所有匹配字段的评分："],["body","\n"],["body","{\n  \"query\": {\n    \"multi_match\": {\n      \"query\":       \"Poland Street W1V\",\n      \"type\":        \"most_fields\",\n      \"fields\":      [ \"street\", \"city\", \"country\", \"postcode\" ]\n    }\n  }\n}\n"],["body","\n"],["headingLink","most_fields-方式的问题"],["heading","most_fields 方式的问题"],["body","\n"],["body","用 most_fields 这种方式搜索也存在某些问题，这些问题并不会马上显现："],["body","\n\n"],["body","它是为多数字段匹配 任意 词设计的，而不是在 所有字段 中找到最匹配的。"],["body","\n"],["body","它不能使用 operator 或 minimum_should_match 参数来降低次相关结果造成的长尾效应。"],["body","\n"],["body","词频对于每个字段是不一样的，而且它们之间的相互影响会导致不好的排序结果。"],["body","\n\n"],["headingLink","字段中心式查询"],["heading","字段中心式查询"],["body","\n"],["body","以上三个源于 most_fields 的问题都因为它是 字段中心式（field-centric） 而不是 词中心式（term-centric） 的：当真正感兴趣的是匹配词的时候，它为我们查找的是最匹配的 字段 。"],["body","\n"],["body","best_fields 类型也是字段中心式的，它也存在类似的问题。"],["body","\n"],["body","首先查看这些问题存在的原因，再想如何解决它们。"],["body","\n"],["headingLink","问题-1-在多个字段中匹配相同的词"],["heading","问题 1 ：在多个字段中匹配相同的词"],["body","\n"],["body","回想一下 most_fields 查询是如何执行的：Elasticsearch 为每个字段生成独立的 match 查询，再用 bool 查询将他们包起来。"],["body","\n"],["body","可以通过 validate-query API 查看："],["body","\n"],["body","GET /_validate/query?explain\n{\n  \"query\": {\n    \"multi_match\": {\n      \"query\":   \"Poland Street W1V\",\n      \"type\":    \"most_fields\",\n      \"fields\":  [ \"street\", \"city\", \"country\", \"postcode\" ]\n    }\n  }\n}\n"],["body","\n"],["body","生成 explanation 解释："],["body","\n"],["body","(street:poland   street:street   street:w1v)\n(city:poland     city:street     city:w1v)\n(country:poland  country:street  country:w1v)\n(postcode:poland postcode:street postcode:w1v)\n"],["body","\n"],["headingLink","问题-2-剪掉长尾"],["heading","问题 2 ：剪掉长尾"],["body","\n"],["body","在 匹配精度 中，我们讨论过使用 and 操作符或设置 minimum_should_match 参数来消除结果中几乎不相关的长尾，或许可以尝试以下方式："],["body","\n"],["body","{\n    \"query\": {\n        \"multi_match\": {\n            \"query\":       \"Poland Street W1V\",\n            \"type\":        \"most_fields\",\n            \"operator\":    \"and\", \n            \"fields\":      [ \"street\", \"city\", \"country\", \"postcode\" ]\n        }\n    }\n}\n"],["body","\n"],["body","但是对于 best_fields 或 most_fields 这些参数会在 match 查询生成时被传入，这个查询的 explanation 解释如下："],["body","\n"],["body","(+street:poland   +street:street   +street:w1v)\n(+city:poland     +city:street     +city:w1v)\n(+country:poland  +country:street  +country:w1v)\n(+postcode:poland +postcode:street +postcode:w1v)\n"],["body","\n"],["headingLink","问题-3-词频"],["heading","问题 3 ：词频"],["body","\n\n"],["body","\n"],["body","词频"],["body","\n"],["body","一个词在单个文档的某个字段中出现的频率越高，这个文档的相关度就越高。"],["body","\n"],["body","\n"],["body","\n"],["body","逆向文档频率"],["body","\n"],["body","一个词在所有文档某个字段索引中出现的频率越高，这个词的相关度就越低。"],["body","\n"],["body","\n\n"],["body","当搜索多个字段时，TF/IDF 会带来某些令人意外的结果。"],["body","\n"],["body","想想用字段 first_name 和 last_name 查询 “Peter Smith” 的例子， Peter 是个平常的名 Smith 也是平常的姓，这两者都具有较低的 IDF 值。但当索引中有另外一个人的名字是 “Smith Williams” 时， Smith 作为名来说很不平常，以致它有一个较高的 IDF 值！"],["body","\n"],["body","下面这个简单的查询可能会在结果中将 “Smith Williams” 置于 “Peter Smith” 之上，尽管事实上是第二个人比第一个人更为匹配。"],["body","\n"],["body","{\n    \"query\": {\n        \"multi_match\": {\n            \"query\":       \"Peter Smith\",\n            \"type\":        \"most_fields\",\n            \"fields\":      [ \"*_name\" ]\n        }\n    }\n}\n"],["body","\n"],["body","这里的问题是 smith 在名字段中具有高 IDF ，它会削弱 “Peter” 作为名和 “Smith” 作为姓时低 IDF 的所起作用。"],["body","\n"],["headingLink","解决方案"],["heading","解决方案"],["body","\n"],["body","存在这些问题仅仅是因为我们在处理着多个字段，如果将所有这些字段组合成单个字段，问题就会消失。可以为 person 文档添加 full_name 字段来解决这个问题："],["body","\n"],["body","{\n    \"first_name\":  \"Peter\",\n    \"last_name\":   \"Smith\",\n    \"full_name\":   \"Peter Smith\"\n}\n"],["body","\n"],["body","当查询 full_name 字段时："],["body","\n\n"],["body","具有更多匹配词的文档会比只有一个重复匹配词的文档更重要。"],["body","\n"],["body","minimum_should_match 和 operator 参数会像期望那样工作。"],["body","\n"],["body","姓和名的逆向文档频率被合并，所以 Smith 到底是作为姓还是作为名出现，都会变得无关紧要。"],["body","\n\n"],["headingLink","自定义-_all-字段"],["heading","自定义 _all 字段"],["body","\n"],["body","在 all-field 字段中，我们解释过 _all 字段的索引方式是将所有其他字段的值作为一个大字符串索引的。然而这么做并不十分灵活，为了灵活我们可以给人名添加一个自定义 _all 字段，再为地址添加另一个 _all 字段。"],["body","\n"],["body","Elasticsearch 在字段映射中为我们提供 copy_to 参数来实现这个功能："],["body","\n"],["body","PUT /my_index\n{\n    \"mappings\": {\n        \"person\": {\n            \"properties\": {\n                \"first_name\": {\n                    \"type\":     \"string\",\n                    \"copy_to\":  \"full_name\" \n                },\n                \"last_name\": {\n                    \"type\":     \"string\",\n                    \"copy_to\":  \"full_name\" \n                },\n                \"full_name\": {\n                    \"type\":     \"string\"\n                }\n            }\n        }\n    }\n}\n"],["body","\n"],["body","first_name 和 last_name 字段中的值会被复制到 full_name 字段。"],["body","\n"],["body","有了这个映射，我们可以用 first_name 来查询名，用 last_name 来查询姓，或者直接使用 full_name 查询整个姓名。"],["body","\n"],["body","first_name 和 last_name 的映射并不影响 full_name 如何被索引， full_name 将两个字段的内容复制到本地，然后根据 full_name 的映射自行索引。"],["body","\n"],["body","copy_to 设置对multi-field无效。如果尝试这样配置映射，Elasticsearch 会抛异常。"],["body","\n"],["headingLink","跨字段实体搜索-1"],["heading","跨字段实体搜索"],["body","\n\n"],["body","搜索  username 为xxx，password 为 xxx"],["body","\n\n"],["headingLink","字段中心式与词中心式"],["heading","字段中心式与词中心式"],["body","\n"],["body","字段中心式：为每个字段 生成一个 match查询"],["body","\n"],["body","GET /_validate/query?explain\n{\n  \"query\": {\n    \"multi_match\": {\n      \"query\":   \"Poland Street W1V\",\n      \"type\":    \"most_fields\",\n      \"fields\":  [ \"street\", \"city\", \"country\", \"postcode\" ]\n    }\n  }\n}\n\n(street:poland   street:street   street:w1v)\n(city:poland     city:street     city:w1v)\n(country:poland  country:street  country:w1v)\n(postcode:poland postcode:street postcode:w1v)\n"],["body","\n"],["body","词中心式：为每个词在所有 的字段中 查找匹配的文档，每个文档都要包含该词"],["body","\n"],["body","出现的问题"],["body","\n\n"],["body","多词匹配：多个字段匹配多个词项导致的相关度计算有误，场景：两个字段 同时匹配 poland 比 一个 字段匹配 poland street 的相关度要高"],["body","\n"],["body","长尾：多个字段匹配多个词项导致许多细小的匹配"],["body","\n"],["body","反向文档词频：搜索 Peter Smith  可能会在结果中将 “Smith Williams” 置于 “Peter Smith” 之上，因为 Smith作为lastName 的IDF过高会拉低分数\n\n"],["body","解决方案：first_name与 last_name合并为 full_name这样IDF就会被 合并"],["body","\n\n"],["body","\n\n"],["headingLink","cross-fields-跨字段查询"],["heading","cross-fields 跨字段查询"],["body","\n"],["body","自定义 _all 的方式是一个好的解决方案，只需在索引文档前为其设置好映射"],["body","\n"],["body","不过， Elasticsearch 还在搜索时提供了相应的解决方案：使用 cross_fields 类型进行 multi_match 查询。"],["body","\n"],["body","cross_fields 使用词中心式（term-centric）的查询方式,这与 best_fields 和 most_fields 使用字段中心式（field-centric）的查询方式非常不同"],["body","\n"],["body","它将所有字段当成一个大字段，并在 每个字段 中查找 每个词 。"],["body","\n"],["body","为了说明字段中心式（field-centric）与词中心式（term-centric）这两种查询方式的不同，先看看以下字段中心式的 most_fields 查询的 explanation 解释："],["body","\n"],["body","GET /_validate/query?explain\n{\n    \"query\": {\n        \"multi_match\": {\n            \"query\":       \"peter smith\",\n            \"type\":        \"most_fields\",\n            \"operator\":    \"and\", \n            \"fields\":      [ \"first_name\", \"last_name\" ]\n        }\n    }\n}\n"],["body","\n"],["body","对于匹配的文档， peter 和 smith 都必须同时出现在相同字段中，要么是 first_name 字段，要么 last_name 字段："],["body","\n"],["body","(+first_name:peter +first_name:smith)\n(+last_name:peter  +last_name:smith)\n"],["body","\n"],["body","词中心式 会使用以下逻辑："],["body","\n"],["body","+(first_name:peter last_name:peter)\n+(first_name:smith last_name:smith)\n"],["body","\n"],["body","换句话说，词 peter 和 smith 都必须出现，但是可以出现在任意字段中。"],["body","\n"],["body","cross_fields 类型首先分析查询字符串并生成一个词列表，然后它从所有字段中依次搜索每个词"],["body","\n"],["body","GET /_validate/query?explain\n{\n    \"query\": {\n        \"multi_match\": {\n            \"query\":       \"peter smith\",\n            \"type\":        \"cross_fields\", \n            \"operator\":    \"and\",\n            \"fields\":      [ \"first_name\", \"last_name\" ]\n        }\n    }\n}\n"],["body","\n"],["body","用 cross_fields 词中心式匹配。"],["body","\n"],["body","它通过 混合 不同字段逆向索引文档频率的方式解决了词频的问题："],["body","\n"],["body","+blended(\"peter\", fields: [first_name, last_name])\n+blended(\"smith\", fields: [first_name, last_name])\n"],["body","\n"],["body","换句话说，它会同时在 first_name 和 last_name 两个字段中查找 smith 的 IDF ，然后用两者的最小值作为两个字段的 IDF 。结果实际上就是 smith 会被认为既是个平常的姓，也是平常的名。"],["body","\n"],["body","为了让 cross_fields 查询以最优方式工作，所有的字段都须使用相同的分析器，具有相同分析器的字段会被分组在一起作为混合字段使用。"],["body","\n"],["body","如果包括了不同分析链的字段，它们会以 best_fields 的相同方式被加入到查询结果中。例如：我们将 title 字段加到之前的查询中（假设它们使用的是不同的分析器）， explanation 的解释结果如下："],["body","\n"],["body","(+title:peter +title:smith)\n(\n  +blended(\"peter\", fields: [first_name, last_name])\n  +blended(\"smith\", fields: [first_name, last_name])\n)\n"],["body","\n"],["body","当在使用 minimum_should_match 和 operator 参数时，这点尤为重要。"],["body","\n"],["headingLink","按字段提高权重"],["heading","按字段提高权重"],["body","\n"],["body","采用 cross_fields 查询与 自定义 _all 字段 相比，其中一个优势就是它可以在搜索时为单个字段提升权重。"],["body","\n"],["body","这对像 first_name 和 last_name 具有相同值的字段并不是必须的，但如果要用 title 和 description 字段搜索图书，可能希望为 title 分配更多的权重，这同样可以使用前面介绍过的 ^ 符号语法来实现："],["body","\n"],["body","GET /books/_search\n{\n    \"query\": {\n        \"multi_match\": {\n            \"query\":       \"peter smith\",\n            \"type\":        \"cross_fields\",\n            \"fields\":      [ \"title^2\", \"description\" ] \n        }\n    }\n}\n"],["body","\n"],["body","自定义单字段查询是否能够优于多字段查询，取决于在多字段查询与单字段自定义 _all 之间代价的权衡，即哪种解决方案会带来更大的性能优化就选择哪一种。"],["body","\n"],["headingLink","exact-value-精确值字段"],["heading","Exact-Value 精确值字段"],["body","\n"],["body","在结束多字段查询这个话题之前，我们最后要讨论的是精确值 not_analyzed 未分析字段。将 not_analyzed 字段与 multi_match 中 analyzed 字段混在一起没有多大用处。"],["body","\n"],["body","原因可以通过查看查询的 explanation 解释得到，设想将 title 字段设置成 not_analyzed ："],["body","\n"],["body","GET /_validate/query?explain\n{\n    \"query\": {\n        \"multi_match\": {\n            \"query\":       \"peter smith\",\n            \"type\":        \"cross_fields\",\n            \"fields\":      [ \"title\", \"first_name\", \"last_name\" ]\n        }\n    }\n}\n"],["body","\n"],["body","因为 title 字段是未分析过的，Elasticsearch 会将 “peter smith” 这个完整的字符串作为查询条件来搜索！"],["body","\n"],["body","title:peter smith\n(\n    blended(\"peter\", fields: [first_name, last_name])\n    blended(\"smith\", fields: [first_name, last_name])\n)\n"],["body","\n"],["body","显然这个项不在 title 的倒排索引中，所以需要在 multi_match 查询中避免使用 not_analyzed 字段。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/深入搜索/结构化搜索.html"],["title","结构化搜索.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","精确值查找"],["heading","精确值查找"],["body","\n"],["body","当进行精确值查找时， 我们会使用过滤器（filters）。过滤器很重要，因为它们执行速度非常快，不会计算相关度（直接跳过了整个评分阶段）而且很容易被缓存。"],["body","\n"],["headingLink","term-查询数字"],["heading","term 查询数字"],["body","\n"],["body","可以用它处理数字（numbers）、布尔值（Booleans）、日期（dates）以及文本（text）。"],["body","\n"],["body","term 查询会查找我们指定的精确值。"],["body","\n"],["body","非评分"],["body","\n"],["body","通常当查找一个精确值的时候，我们不希望对查询进行评分计算。只希望对文档进行包括或排除的计算，所以我们会使用 constant_score 查询以非评分模式来执行 term 查询并以一作为统一评分。"],["body","\n"],["body","GET /my_store/products/_search\n{\n    \"query\" : {\n        \"constant_score\" : { \n            \"filter\" : {\n                \"term\" : { \n                    \"price\" : 20\n                }\n            }\n        }\n    }\n}\n"],["body","\n"],["headingLink","term-查询文本"],["heading","term 查询文本"],["body","\n"],["body","GET /my_store/products/_search\n{\n    \"query\" : {\n        \"constant_score\" : {\n            \"filter\" : {\n                \"term\" : {\n                    \"productID\" : \"XHDK-A-1293-#fJ3\"\n                }\n            }\n        }\n    }\n}\n"],["body","\n"],["body","这里有几点需要注意："],["body","\n\n"],["body","\n"],["body","Elasticsearch 用 4 个不同的 token 而不是单个 token 来表示这个 UPC 。"],["body","\n"],["body","\n"],["body","\n"],["body","所有字母都是小写的。"],["body","\n"],["body","\n"],["body","\n"],["body","丢失了连字符和哈希符（ # ）。"],["body","\n"],["body","\n\n"],["body","所以当我们用 term 查询查找精确值 XHDK-A-1293-#fJ3 的时候，找不到任何文档，因为它并不在我们的倒排索引中，正如前面呈现出的分析结果，索引里有四个 token 。"],["body","\n"],["headingLink","内部过滤器的操作"],["heading","内部过滤器的操作"],["body","\n\n"],["body","查找匹配文档."],["body","\n\n"],["body","​\tterm 查询在倒排索引中查找 XHDK-A-1293-#fJ3 然后获取包含该 term 的所有文档。本例中，只有文档 1 满足我们要求。"],["body","\n"],["body","2. *创建 bitset*.\n\n过滤器会创建一个 *bitset* （一个包含 0 和 1 的数组），它描述了哪个文档会包含该 term 。匹配文档的标志位是 1 。本例中，bitset 的值为 `[1,0,0,0]` 。在内部，它表示成一个 [\"roaring bitmap\"](https://www.elastic.co/blog/frame-of-reference-and-roaring-bitmaps)，可以同时对稀疏或密集的集合进行高效编码。\n\n3. *迭代 bitset(s)*\n\n一旦为每个查询生成了 bitsets ，Elasticsearch 就会循环迭代 bitsets 从而找到满足所有过滤条件的匹配文档的集合。执行顺序是启发式的，但一般来说先迭代稀疏的 bitset （因为它可以排除掉大量的文档）。\n\n4. *增量使用计数*\n"],["body","\n"],["body","​\tElasticsearch 能够缓存非评分查询从而获取更快的访问，但是它也会不太聪明地缓存一些使用极少的东西。非评分计算因为倒排索引已经足够快了，所以我们只想缓存那些我们 知道 在将来会被再次使用的查询，以避免资源的浪费。"],["body","\n"],["body","为了实现以上设想，Elasticsearch 会为每个索引跟踪保留查询使用的历史状态。如果查询在最近的 256 次查询中会被用到，那么它就会被缓存到内存中。当 bitset 被缓存后，缓存会在那些低于 10,000 个文档（或少于 3% 的总索引数）的段（segment）中被忽略。这些小的段即将会消失，所以为它们分配缓存是一种浪费。"],["body","\n"],["headingLink","组合过滤器"],["heading","组合过滤器"],["body","\n"],["headingLink","布尔过滤器"],["heading","布尔过滤器"],["body","\n"],["body","一个 bool 过滤器由三部分组成："],["body","\n"],["body","{\n   \"bool\" : {\n      \"must\" :     [],\n      \"should\" :   [],\n      \"must_not\" : [],\n   }\n}\n"],["body","\n"],["body","must"],["body","\n"],["body","所有的语句都 必须（must） 匹配，与 AND 等价。"],["body","\n"],["body","must_not"],["body","\n"],["body","所有的语句都 不能（must not） 匹配，与 NOT 等价。"],["body","\n"],["body","should"],["body","\n"],["body","至少有一个语句要匹配，与 OR 等价。"],["body","\n"],["body","SELECT product\nFROM   products\nWHERE  (price = 20 OR productID = \"XHDK-A-1293-#fJ3\")\n  AND  (price != 30)\n"],["body","\n"],["body","等价于"],["body","\n"],["body","GET /my_store/products/_search\n{\n   \"query\" : {\n      \"filtered\" : { \n         \"filter\" : {\n            \"bool\" : {\n              \"should\" : [\n                 { \"term\" : {\"price\" : 20}}, \n                 { \"term\" : {\"productID\" : \"XHDK-A-1293-#fJ3\"}} \n              ],\n              \"must_not\" : {\n                 \"term\" : {\"price\" : 30} \n              }\n           }\n         }\n      }\n   }\n}\n"],["body","\n"],["headingLink","嵌套布尔过滤器"],["heading","嵌套布尔过滤器"],["body","\n"],["body","SELECT document\nFROM   products\nWHERE  productID      = \"KDKE-B-9947-#kL5\"\n  OR (     productID = \"JODL-X-1937-#pV7\"\n       AND price     = 30 )\n"],["body","\n"],["body","GET /my_store/products/_search\n{\n   \"query\" : {\n      \"filtered\" : {\n         \"filter\" : {\n            \"bool\" : {\n              \"should\" : [\n                { \"term\" : {\"productID\" : \"KDKE-B-9947-#kL5\"}}, \n                { \"bool\" : { \n                  \"must\" : [\n                    { \"term\" : {\"productID\" : \"JODL-X-1937-#pV7\"}}, \n                    { \"term\" : {\"price\" : 30}} \n                  ]\n                }}\n              ]\n           }\n         }\n      }\n   }\n}\n"],["body","\n"],["headingLink","查找多个精确值"],["heading","查找多个精确值"],["body","\n"],["body","term 查询对于查找单个值非常有用，但通常我们可能想搜索多个值。 如果我们想要查找价格字段值为 $20 或 $30 的文档该如何处理呢？"],["body","\n"],["body","{\n    \"terms\" : {\n        \"price\" : [20, 30]\n    }\n}\n"],["body","\n"],["body","GET /my_store/products/_search\n{\n    \"query\" : {\n        \"constant_score\" : {\n            \"filter\" : {\n                \"terms\" : { \n                    \"price\" : [20, 30]\n                }\n            }\n        }\n    }\n}\n"],["body","\n"],["headingLink","包含而不是相等"],["heading","包含，而不是相等"],["body","\n"],["body","一定要了解 term 和 terms 是 包含（contains） 操作，而非 等值（equals） （判断）。"],["body","\n"],["headingLink","精确相等"],["heading","精确相等"],["body","\n"],["body","如果一定期望得到我们前面说的那种行为（即整个字段完全相等），最好的方式是增加并索引另一个字段， 这个字段用以存储该字段包含词项的数量，同样以上面提到的两个文档为例，现在我们包括了一个维护标签数的新字段："],["body","\n"],["body","{ \"tags\" : [\"search\"], \"tag_count\" : 1 }\n{ \"tags\" : [\"search\", \"open_source\"], \"tag_count\" : 2 }\n"],["body","\n"],["body","一旦增加这个用来索引项 term 数目信息的字段，我们就可以构造一个 constant_score 查询，来确保结果中的文档所包含的词项数量与要求是一致的："],["body","\n"],["headingLink","范围"],["heading","范围"],["body","\n"],["body","\"range\" : {\n    \"price\" : {\n        \"gte\" : 20,\n        \"lte\" : 40\n    }\n}\n"],["body","\n"],["body","range 查询可同时提供包含（inclusive）和不包含（exclusive）这两种范围表达式，可供组合的选项如下："],["body","\n\n"],["body","gt: > 大于（greater than）"],["body","\n"],["body","lt: < 小于（less than）"],["body","\n"],["body","gte: >= 大于或等于（greater than or equal to）"],["body","\n"],["body","lte: <= 小于或等于（less than or equal to）"],["body","\n\n"],["headingLink","日期范围"],["heading","日期范围"],["body","\n"],["body","\"range\" : {\n    \"timestamp\" : {\n        \"gt\" : \"2014-01-01 00:00:00\",\n        \"lt\" : \"2014-01-07 00:00:00\"\n    }\n}\n"],["body","\n"],["headingLink","字符串范围"],["heading","字符串范围"],["body","\n"],["body","字符串范围可采用 字典顺序（lexicographically） 或字母顺序（alphabetically）。例如，下面这些字符串是采用字典序（lexicographically）排序的："],["body","\n"],["body","在倒排索引中的词项就是采取字典顺序（lexicographically）排列的，这也是字符串范围可以使用这个顺序来确定的原因。"],["body","\n"],["body","\"range\" : {\n    \"title\" : {\n        \"gte\" : \"a\",\n        \"lt\" :  \"b\"\n    }\n}\n"],["body","\n"],["headingLink","处理-null-值"],["heading","处理 Null 值"],["body","\n"],["body","如何将某个不存在的字段存储在这个数据结构中呢？无法做到！简单的说，一个倒排索引只是一个 token 列表和与之相关的文档信息，如果字段不存在，那么它也不会持有任何 token，也就无法在倒排索引结构中表现。"],["body","\n"],["body","最终，这也就意味着，null, [] （空数组）和 [null] 所有这些都是等价的，它们无法存于倒排索引中。"],["body","\n"],["headingLink","存在查询"],["heading","存在查询"],["body","\n"],["body","这个查询会返回那些在指定字段有任何值的文档"],["body","\n"],["body","GET /my_index/posts/_search\n{\n    \"query\" : {\n        \"constant_score\" : {\n            \"filter\" : {\n                \"exists\" : { \"field\" : \"tags\" }\n            }\n        }\n    }\n}\n"],["body","\n"],["headingLink","缺失查询"],["heading","缺失查询"],["body","\n"],["body","这个 missing 查询本质上与 exists 恰好相反：它返回某个特定 无 值字段的文档，与以下 SQL 表达的意思类似："],["body","\n"],["body","SELECT tags\nFROM   posts\nWHERE  tags IS NULL\n"],["body","\n"],["body","GET /my_index/posts/_search\n{\n    \"query\" : {\n        \"constant_score\" : {\n            \"filter\": {\n                \"missing\" : { \"field\" : \"tags\" }\n            }\n        }\n    }\n}\n"],["body","\n"],["headingLink","对象上的存在与缺失"],["heading","对象上的存在与缺失"],["body","\n"],["body","{\n   \"name\" : {\n      \"first\" : \"John\",\n      \"last\" :  \"Smith\"\n   }\n}\n"],["body","\n"],["body","我们不仅可以检查 name.first 和 name.last 的存在性，也可以检查 name ，不过在 映射 中，如上对象的内部是个扁平的字段与值（field-value）的简单键值结构，类似下面这样："],["body","\n"],["body","{\n   \"name.first\" : \"John\",\n   \"name.last\"  : \"Smith\"\n}\n"],["body","\n"],["body","那么我们如何用 exists 或 missing 查询 name 字段呢？ name 字段并不真实存在于倒排索引中。"],["body","\n"],["body","原因是当我们执行下面这个过滤的时候："],["body","\n"],["body","{\n    \"exists\" : { \"field\" : \"name\" }\n}\n"],["body","\n"],["body","实际执行的是："],["body","\n"],["body","{\n    \"bool\": {\n        \"should\": [\n            { \"exists\": { \"field\": \"name.first\" }},\n            { \"exists\": { \"field\": \"name.last\" }}\n        ]\n    }\n}\n"],["body","\n"],["body","这也就意味着，如果 first 和 last 都是空，那么 name 这个命名空间才会被认为不存在。"],["body","\n"],["headingLink","关于缓存"],["heading","关于缓存"],["body","\n"],["body","我们已经简单介绍了过滤器是如何计算的。其核心实际是采用一个 bitset 记录与过滤器匹配的文档。Elasticsearch 积极地把这些 bitset 缓存起来以备随后使用。一旦缓存成功，bitset 可以复用 任何 已使用过的相同过滤器，而无需再次计算整个过滤器。"],["body","\n"],["body","这些 bitsets 缓存是“智能”的：它们以增量方式更新。当我们索引新文档时，只需将那些新文档加入已有 bitset，而不是对整个缓存一遍又一遍的重复计算。和系统其他部分一样，过滤器是实时的，我们无需担心缓存过期问题。"],["body","\n"],["headingLink","独立的过滤器缓存"],["heading","独立的过滤器缓存"],["body","\n"],["body","属于一个查询组件的 bitsets 是独立于它所属搜索请求其他部分的"],["body","\n"],["body","这就意味着，一旦被缓存，一个查询可以被用作多个搜索请求"],["body","\n"],["body","bitsets 并不依赖于它所存在的查询上下文"],["body","\n"],["body","这样使得缓存可以加速查询中经常使用的部分，从而降低较少、易变的部分所带来的消耗。"],["body","\n"],["body","它查找满足以下任意一个条件的电子邮件："],["body","\n\n"],["body","在收件箱中，且没有被读过的"],["body","\n"],["body","不在 收件箱中，但被标注重要的"],["body","\n\n"],["body","GET /inbox/emails/_search\n{\n  \"query\": {\n      \"constant_score\": {\n          \"filter\": {\n              \"bool\": {\n                 \"should\": [\n                    { \"bool\": {\n                          \"must\": [\n                             { \"term\": { \"folder\": \"inbox\" }}, \n                             { \"term\": { \"read\": false }}\n                          ]\n                    }},\n                    { \"bool\": {\n                          \"must_not\": {\n                             \"term\": { \"folder\": \"inbox\" } \n                          },\n                          \"must\": {\n                             \"term\": { \"important\": true }\n                          }\n                    }}\n                 ]\n              }\n            }\n        }\n    }\n}\n"],["body","\n"],["body","两个过滤器是相同的，所以会使用同一 bitset 。"],["body","\n"],["body","尽管其中一个收件箱的条件是 must 语句，另一个是 must_not 语句，但他们两者是完全相同的。这意味着在第一个语句执行后， bitset 就会被计算然后缓存起来供另一个使用。当再次执行这个查询时，收件箱的这个过滤器已经被缓存了，所以两个语句都会使用已缓存的 bitset 。"],["body","\n"],["headingLink","自动缓存行为"],["heading","自动缓存行为"],["body","\n"],["body","早起版本的缓存"],["body","\n"],["body","在 Elasticsearch 的较早版本中，默认的行为是缓存一切可以缓存的对象。这也通常意味着系统缓存 bitsets 太富侵略性，从而因为清理缓存带来性能压力"],["body","\n"],["body","小查询易过滤"],["body","\n"],["body","不仅如此，很多过滤器都很容易被评价，。缓存这些过滤器的意义不大，因为可以简单地再次执行过滤器。"],["body","\n"],["body","用户ID类数据不重复"],["body","\n"],["body","例如 term 过滤字段 \"user_id\" ：如果有上百万的用户，每个具体的用户 ID 出现的概率都很小，那么为这个过滤器缓存 bitsets 就不是很合算，因为缓存的结果很可能在重用之前就被剔除了。"],["body","\n"],["body","这种缓存的扰动对性能有着严重的影响。更严重的是，它让开发者难以区分有良好表现的缓存以及无用缓存。"],["body","\n"],["body","Elasticsearch 会基于使用频次自动缓存查询"],["body","\n"],["body","查询频次"],["body","\n\n"],["body","如果一个非评分查询在最近的 256 次查询中被使用过（次数取决于查询类型），那么这个查询就会作为缓存的候选"],["body","\n\n"],["body","大段"],["body","\n\n"],["body","文档数量超过 10,000 （或超过总文档数量的 3% )的段才会缓存 bitset，因为小的片段可以很快的进行搜索和合并，这里缓存的意义不大。"],["body","\n\n"],["body","LRU驱逐"],["body","\n\n"],["body","least recently used 最近最少使用"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/映射.html"],["title","映射.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","映射"],["heading","映射"],["body","\n"],["body","为了能够将时间域视为时间，数字域视为数字，字符串域视为全文或精确值字符串， Elasticsearch 需要知道每个域中数据的类型。这个信息包含在映射中。"],["body","\n"],["headingLink","核心简单域类型"],["heading","核心简单域类型"],["body","\n"],["body","Elasticsearch 支持如下简单域类型："],["body","\n\n"],["body","字符串: string"],["body","\n"],["body","整数 : byte, short, integer, long"],["body","\n"],["body","浮点数: float, double"],["body","\n"],["body","布尔型: boolean"],["body","\n"],["body","日期: date"],["body","\n\n"],["body","如果索引一个新域时，通过JSON中基本数据类型，尝试猜测域类型，使用如下规则："],["body","\n"],["body","JSON type"],["body","域 type"],["body","\n"],["body","布尔型: true 或者 false"],["body","boolean"],["body","\n"],["body","整数: 123"],["body","long"],["body","\n"],["body","浮点数: 123.45"],["body","double"],["body","\n"],["body","字符串，有效日期: 2014-09-15"],["body","date"],["body","\n"],["body","字符串: foo bar"],["body","string"],["body","\n\n\n"],["body","\n"],["body","这意味着如果你通过引号( \"123\" )索引一个数字，它会被映射为 string 类型，而不是 long 。但是，如果这个域已经映射为 long ，那么 Elasticsearch 会尝试将这个字符串转化为 long ，如果无法转化，则抛出一个异常。"],["body","\n"],["body","\n"],["headingLink","复杂核心域类型"],["heading","复杂核心域类型"],["body","\n"],["headingLink","多值域"],["heading","多值域"],["body","\n"],["body","很有可能，我们希望 tag 域包含多个标签。我们可以以数组的形式索引标签："],["body","\n"],["body","对于数组，没有特殊的映射需求。任何域都可以包含0、1或者多个值，就像全文域分析得到多个词条。"],["body","\n"],["body","数组同类型"],["body","\n"],["body","这暗示 数组中所有的值必须是相同数据类型的 。你不能将日期和字符串混在一起。如果你通过索引数组来创建新的域，Elasticsearch 会用数组中第一个值的数据类型作为这个域的 类型 。"],["body","\n"],["body","_source不变形\n当你从 Elasticsearch 得到一个文档，每个数组的顺序和你当初索引文档时一样。你得到的 _source 域，包含与你索引的一模一样的 JSON 文档。"],["body","\n"],["body","数组的无序搜索"],["body","\n"],["body","但是，数组是以多值域 索引的—可以搜索，但是无序的。 在搜索的时候，你不能指定 “第一个” 或者 “最后一个”。 更确切的说，把数组想象成 装在袋子里的值 。"],["body","\n"],["headingLink","空域"],["heading","空域"],["body","\n"],["body","当然，数组可以为空。这相当于存在零值。 事实上，在 Lucene 中是不能存储 null 值的，所以我们认为存在 null 值的域为空域。"],["body","\n"],["body","下面三种域被认为是空的，它们将不会被索引："],["body","\n"],["body","\"null_value\":               null,\n\"empty_array\":              [],\n\"array_with_null_value\":    [ null ]\n"],["body","\n"],["headingLink","多层级对象"],["heading","多层级对象"],["body","\n"],["body","我们讨论的最后一个 JSON 原生数据类是 对象 -- 在其他语言中称为哈希，哈希 map，字典或者关联数组。"],["body","\n"],["body","内部对象 经常用于嵌入一个实体或对象到其它对象中。例如，与其在 tweet 文档中包含 user_name 和 user_id 域，我们也可以这样写："],["body","\n"],["body","{\n    \"tweet\":            \"Elasticsearch is very flexible\",\n    \"user\": {\n        \"id\":           \"@johnsmith\",\n        \"gender\":       \"male\",\n        \"age\":          26,\n        \"name\": {\n            \"full\":     \"John Smith\",\n            \"first\":    \"John\",\n            \"last\":     \"Smith\"\n        }\n    }\n}\n"],["body","\n"],["headingLink","内部对象的映射"],["heading","内部对象的映射"],["body","\n"],["body","Elasticsearch 会动态监测新的对象域并映射它们为 对象 ，在 properties 属性下列出内部域："],["body","\n"],["body","{\n  \"gb\": {\n    \"tweet\": { \n      \"properties\": {\n        \"tweet\":            { \"type\": \"string\" },\n        \"user\": { \n          \"type\":             \"object\",\n          \"properties\": {\n            \"id\":           { \"type\": \"string\" },\n            \"gender\":       { \"type\": \"string\" },\n            \"age\":          { \"type\": \"long\"   },\n            \"name\":   { \n              \"type\":         \"object\",\n              \"properties\": {\n                \"full\":     { \"type\": \"string\" },\n                \"first\":    { \"type\": \"string\" },\n                \"last\":     { \"type\": \"string\" }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","内部对象是如何索引的"],["heading","内部对象是如何索引的"],["body","\n"],["body","Lucene 不理解内部对象。 Lucene 文档是由一组键值对列表组成的。为了能让 Elasticsearch 有效地索引内部类，它把我们的文档转化成这样："],["body","\n"],["body","{\n    \"tweet\":            [elasticsearch, flexible, very],\n    \"user.id\":          [@johnsmith],\n    \"user.gender\":      [male],\n    \"user.age\":         [26],\n    \"user.name.full\":   [john, smith],\n    \"user.name.first\":  [john],\n    \"user.name.last\":   [smith]\n}\n"],["body","\n"],["body","内部域 可以通过名称引用（例如， first ）。为了区分同名的两个域，我们可以使用全 路径 （例如， user.name.first ） 或 type 名加路径（ tweet.user.name.first ）。"],["body","\n"],["body","在前面简单扁平的文档中，没有 user 和 user.name 域。Lucene 索引只有标量和简单值，没有复杂数据结构。"],["body","\n"],["headingLink","内部对象数组"],["heading","内部对象数组"],["body","\n"],["body","最后，考虑包含内部对象的数组是如何被索引的。 假设我们有个 followers 数组："],["body","\n"],["body","{\n    \"followers\": [\n        { \"age\": 35, \"name\": \"Mary White\"},\n        { \"age\": 26, \"name\": \"Alex Jones\"},\n        { \"age\": 19, \"name\": \"Lisa Smith\"}\n    ]\n}\n"],["body","\n"],["body","这个文档会像我们之前描述的那样被扁平化处理，结果如下所示："],["body","\n"],["body","{\n    \"followers.age\":    [19, 26, 35],\n    \"followers.name\":   [alex, jones, lisa, smith, mary, white]\n}\n"],["body","\n"],["body","{age: 35} 和 {name: Mary White} 之间的相关性已经丢失了"],["body","\n"],["body","相关内部对象被称为 nested 对象，可以回答上面的查询"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/6.DataStreams/README.html"],["title","DataStreams - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","data-streams"],["heading","Data streams"],["body","\n\n"],["body","dataStreams 使您可以跨多个索引存储仅追加的时间序列数据，同时为您提供用于请求的单个命名资源"],["body","\n"],["body","dataStreams 非常适合日志、事件、度量和其他连续生成的数据"],["body","\n"],["body","您可以直接向 dataStreams 提交索引和搜索请求。流自动将请求路由到存储流数据的后备索引"],["body","\n"],["body","可以使用 index lifecycle management (ILM) 自动化这些后备索引的管理.\n\n"],["body","例如，您可以使用ILM自动将较旧的备份索引移动到较便宜的硬件并删除不需要的索引。随着数据的增长，ILM可以帮助您降低成本和开销。"],["body","\n\n"],["body","\n\n"],["headingLink","backing-indices"],["heading","Backing Indices"],["body","\n\n"],["body","数据流由一个或多个 隐藏，自动生成的后备索引组成。"],["body","\n"],["body","数据流需要一个匹配的   template. 模板包含用于配置流的后备索引的映射和设置。"],["body","\n"],["body","索引到数据流的每个文档都必须包含 “@timestamp” 字段，映射为 date or date_nanos  字段类型。"],["body","\n"],["body","如果索引模板没有 为 @timestamp  字段指定映射，则Elasticsearch将 @timestamp  映射为具有默认选项的 “日期” 字段。"],["body","\n"],["body","相同的索引模板可以用于多个数据流。不能删除数据流使用的索引模板。"],["body","\n\n"],["headingLink","read-requests"],["heading","Read requests"],["body","\n"],["body","当您向数据流提交读取请求时，该流将请求路由到其所有支持索引。"],["body","\n"],["headingLink","write-index"],["heading","Write index"],["body","\n\n"],["body","最近创建的后备索引是数据流的写入索引。流仅将新文档添加到此索引。"],["body","\n"],["body","即使直接向索引发送请求，也无法将新文档添加到其他支持索引。"],["body","\n\n"],["body","您也不能对可能Block索引的写的索引执行操作，例如:"],["body","\n\n"],["body","Clone"],["body","\n"],["body","Delete"],["body","\n"],["body","Freeze"],["body","\n"],["body","Shrink"],["body","\n"],["body","Split"],["body","\n\n"],["headingLink","rollover"],["heading","Rollover"],["body","\n\n"],["body","RollOver翻转 创建一个新的后备索引，该索引成为流的新写入索引。"],["body","\n"],["body","我们建议使用 ILM 在写入索引达到指定的年龄或大小时自动滚动数据流"],["body","\n"],["body","如果需要，您还可以手动翻滚数据流。"],["body","\n\n"],["headingLink","generation"],["heading","Generation"],["body","\n"],["body","每个数据流跟踪其代数（generation）"],["body","\n\n"],["body","\n"],["body","一个六位数的零填充整数，作为流的翻转的累积计数：开始于 000001."],["body","\n"],["body","\n"],["body","\n"],["body","创建后备索引时，索引将使用以下约定命名:"],["body","\n"],["body","\n\n"],["body",".ds-<data-stream>-<yyyy.MM.dd>-<generation>\n"],["body","\n\n"],["body","<yyyy.MM.dd> is the backing index’s creation date."],["body","\n"],["body","Backing indices with a higher generation contain more recent data."],["body","\n"],["body","For example, the web-server-logs data stream has a generation of 34."],["body","\n"],["body","The stream’s most recent backing index, created on 7 March 2099, is named .ds-web-server-logs-2099.03.07-000034."],["body","\n"],["body","Some operations, such as a shrink or restore, can change a backing index’s name. These name changes do not remove a backing index from its data stream."],["body","\n\n"],["headingLink","append-only"],["heading","Append-only"],["body","\n\n"],["body","数据流是为现有数据很少 (如果有的话) 更新的用例而设计的"],["body","\n"],["body","您不能将现有文档的更新或删除请求直接发送到数据流"],["body","\n"],["body","Instead, use the update by query and delete by query APIs."],["body","\n"],["body","如果需要，您可以通过直接向文档的后备索引提交请求来 更新或删除文档。"],["body","\n"],["body","如果您经常更新或删除现有的时间序列数据，请使用带有写索引的索引别名，而不是数据流"],["body","\n"],["body","See Manage time series data without data streams."],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/6.DataStreams/2.UseADataStream.html"],["title","UseADataStream.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","use-a-data-stream"],["heading","Use a data stream"],["body","\n"],["body","After you set up a data stream, you can do the following:"],["body","\n\n"],["body","Add documents to a data stream"],["body","\n"],["body","Search a data stream"],["body","\n"],["body","Get statistics for a data stream"],["body","\n"],["body","Manually roll over a data stream"],["body","\n"],["body","Open closed backing indices"],["body","\n"],["body","Reindex with a data stream"],["body","\n"],["body","Update documents in a data stream by query"],["body","\n"],["body","Delete documents in a data stream by query"],["body","\n"],["body","Update or delete documents in a backing index"],["body","\n\n"],["headingLink","add-documents-to-a-data-stream"],["heading","Add documents to a data stream"],["body","\n"],["body","To add an individual document, use the index API. Ingest pipelines are supported."],["body","\n"],["body","POST /my-data-stream/_doc/\n{\n  \"@timestamp\": \"2099-03-08T11:06:07.000Z\",\n  \"user\": {\n    \"id\": \"8a4f500d\"\n  },\n  \"message\": \"Login successful\"\n}\n"],["body","\n"],["body","You cannot add new documents to a data stream using the index API’s PUT /<target>/_doc/<_id> request format. To specify a document ID, use the PUT /<target>/_create/<_id> format instead. Only an op_type of create is supported."],["body","\n"],["body","To add multiple documents with a single request, use the bulk API. Only create actions are supported."],["body","\n"],["body","PUT /my-data-stream/_bulk?refresh\n{\"create\":{ }}\n{ \"@timestamp\": \"2099-03-08T11:04:05.000Z\", \"user\": { \"id\": \"vlb44hny\" }, \"message\": \"Login attempt failed\" }\n{\"create\":{ }}\n{ \"@timestamp\": \"2099-03-08T11:06:07.000Z\", \"user\": { \"id\": \"8a4f500d\" }, \"message\": \"Login successful\" }\n{\"create\":{ }}\n{ \"@timestamp\": \"2099-03-09T11:07:08.000Z\", \"user\": { \"id\": \"l7gk7f82\" }, \"message\": \"Logout successful\" }\n"],["body","\n"],["headingLink","search-a-data-stream"],["heading","Search a data stream"],["body","\n"],["body","以下搜索api支持数据流:"],["body","\n\n"],["body","Search"],["body","\n"],["body","Async search"],["body","\n"],["body","Multi search"],["body","\n"],["body","Field capabilities"],["body","\n"],["body","EQL search"],["body","\n\n"],["headingLink","get-statistics-for-a-data-stream"],["heading","Get statistics for a data stream"],["body","\n"],["body","Use the data stream stats API to get statistics for one or more data streams:"],["body","\n"],["body","GET /_data_stream/my-data-stream/_stats?human=true\n"],["body","\n"],["headingLink","manually-roll-over-a-data-stream"],["heading","Manually roll over a data stream"],["body","\n"],["body","Use the rollover API to manually roll over a data stream:"],["body","\n"],["body","POST /my-data-stream/_rollover/\n"],["body","\n"],["headingLink","open-closed-backing-indices"],["heading","Open closed backing indices"],["body","\n"],["body","You cannot search a closed backing index, even by searching its data stream. You also cannot update or delete documents in a closed index."],["body","\n"],["body","To re-open a closed backing index, submit an open index API request directly to the index:"],["body","\n"],["body","POST /.ds-my-data-stream-2099.03.07-000001/_open/\n"],["body","\n"],["body","若要重新打开数据流的所有封闭备份索引，请向流提交一个open index API请求:"],["body","\n"],["body","POST /my-data-stream/_open/\n"],["body","\n"],["headingLink","reindex-with-a-data-stream"],["heading","Reindex with a data stream"],["body","\n\n"],["body","\n"],["body","使用 reindex API 将文档从现有索引、别名或数据流复制到数据流。"],["body","\n"],["body","\n"],["body","\n"],["body","Because data streams are append-only, a reindex into a data stream must use an op_type of create. A reindex cannot update existing documents in a data stream."],["body","\n"],["body","\n\n"],["body","POST /_reindex\n{\n  \"source\": {\n    \"index\": \"archive\"\n  },\n  \"dest\": {\n    \"index\": \"my-data-stream\",\n    \"op_type\": \"create\"\n  }\n}\n"],["body","\n"],["headingLink","update-documents-in-a-data-stream-by-query"],["heading","Update documents in a data stream by query"],["body","\n"],["body","使用 update by query API 更新数据流中与提供的查询匹配的文档:"],["body","\n"],["body","POST /my-data-stream/_update_by_query\n{\n  \"query\": {\n    \"match\": {\n      \"user.id\": \"l7gk7f82\"\n    }\n  },\n  \"script\": {\n    \"source\": \"ctx._source.user.id = params.new_id\",\n    \"params\": {\n      \"new_id\": \"XgdX0NoX\"\n    }\n  }\n}\n"],["body","\n"],["headingLink","delete-documents-in-a-data-stream-by-query"],["heading","Delete documents in a data stream by query"],["body","\n"],["body","Use the delete by query API to delete documents in a data stream that match a provided query:"],["body","\n"],["body","POST /my-data-stream/_delete_by_query\n{\n  \"query\": {\n    \"match\": {\n      \"user.id\": \"vlb44hny\"\n    }\n  }\n}\n"],["body","\n"],["headingLink","update-or-delete-documents-in-a-backing-index"],["heading","Update or delete documents in a backing index"],["body","\n"],["body","如果需要，您可以通过向包含该文档的后备索引发送请求来更新或删除数据流中的文档。你需要:"],["body","\n\n"],["body","The document ID"],["body","\n"],["body","包含文档的后备索引的名称"],["body","\n"],["body","If updating the document, its sequence number and primary term"],["body","\n\n"],["body","To get this information, use a search request:"],["body","\n"],["body","GET /my-data-stream/_search\n{\n  \"seq_no_primary_term\": true,\n  \"query\": {\n    \"match\": {\n      \"user.id\": \"yWIumJd7\"\n    }\n  }\n}\n"],["body","\n"],["body","Response:"],["body","\n"],["body","{\n  \"took\": 20,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 3,\n    \"successful\": 3,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": {\n      \"value\": 1,\n      \"relation\": \"eq\"\n    },\n    \"max_score\": 0.2876821,\n    \"hits\": [\n      {\n        \"_index\": \".ds-my-data-stream-2099.03.08-000003\",      \n        \"_type\": \"_doc\",\n        \"_id\": \"bfspvnIBr7VVZlfp2lqX\",              \n        \"_seq_no\": 0,                               \n        \"_primary_term\": 1,                         \n        \"_score\": 0.2876821,\n        \"_source\": {\n          \"@timestamp\": \"2099-03-08T11:06:07.000Z\",\n          \"user\": {\n            \"id\": \"yWIumJd7\"\n          },\n          \"message\": \"Login successful\"\n        }\n      }\n    ]\n  }\n}\n"],["body","\n"],["body","Backing index containing the matching document"],["body","\n\n"],["body","Document ID for the document"],["body","\n"],["body","Current sequence number for the document"],["body","\n"],["body","Primary term for the document"],["body","\n\n"],["body","To update the document, use an index API request with valid if_seq_no and if_primary_term arguments:"],["body","\n"],["body","PUT /.ds-my-data-stream-2099-03-08-000003/_doc/bfspvnIBr7VVZlfp2lqX?if_seq_no=0&if_primary_term=1\n{\n  \"@timestamp\": \"2099-03-08T11:06:07.000Z\",\n  \"user\": {\n    \"id\": \"8a4f500d\"\n  },\n  \"message\": \"Login successful\"\n}\n"],["body","\n"],["body","To delete the document, use the delete API:"],["body","\n"],["body","DELETE /.ds-my-data-stream-2099.03.08-000003/_doc/bfspvnIBr7VVZlfp2lqX\n"],["body","\n"],["body","To delete or update multiple documents with a single request, use the bulk API's delete, index, and update actions. For index actions, include valid if_seq_no and if_primary_term arguments."],["body","\n"],["body","PUT /_bulk?refresh\n{ \"index\": { \"_index\": \".ds-my-data-stream-2099.03.08-000003\", \"_id\": \"bfspvnIBr7VVZlfp2lqX\", \"if_seq_no\": 0, \"if_primary_term\": 1 } }\n{ \"@timestamp\": \"2099-03-08T11:06:07.000Z\", \"user\": { \"id\": \"8a4f500d\" }, \"message\": \"Login successful\" }\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/6.DataStreams/DataStreams.mm.html"],["title","DataStreams.mm.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","数据流"],["heading","数据流"],["body","\n"],["headingLink","backing-indices"],["heading","Backing Indices"],["body","\n\n"],["body","数据流由一个或多个隐藏的后备索引构成"],["body","\n"],["body","数据流需要一个匹配的索引模板用于配置后备索引的映射和设置。"],["body","\n"],["body","索引中的映射需要 @timestamp字段，类型为date、date_nanos"],["body","\n\n"],["headingLink","read-requests"],["heading","Read requests"],["body","\n\n"],["body","当您向数据流提交读取请求时，该流将请求路由到其所有支持索引。"],["body","\n\n"],["headingLink","write-index"],["heading","Write index"],["body","\n\n"],["body","最近创建的后备索引是数据流的写入索引。流仅将新文档添加到此索引。"],["body","\n"],["body","即使直接向索引发送请求，也无法将新文档添加到其他后备索引"],["body","\n"],["body","也不能对后备索引执行以下操作：Clone、Delete、Freeze、Shrink、Split"],["body","\n\n"],["headingLink","rollover"],["heading","Rollover"],["body","\n\n"],["body","创建一个新的后备索引，该索引成为流的新写入索引。"],["body","\n"],["body","建议使用ILM在写入索引达到指定的年龄或大小时自动滚动数据流"],["body","\n"],["body","您还可以手动翻滚数据流。"],["body","\n\n"],["headingLink","generation"],["heading","Generation"],["body","\n\n"],["body","每个数据流跟踪其代数"],["body","\n"],["body","一个六位数的零填充整数，作为流的翻转的累积计数：开始于 000001."],["body","\n"],["body","后备索引约定命名"],["body","\n"],["body","\n"],["body",".ds-<data-stream>-<yyyy.MM.dd>-<generation>\n"],["body","\n"],["body","\n\n"],["headingLink","构建datastream步骤"],["heading","构建DataStream步骤"],["body","\n"],["headingLink","注意测试环境要调高ilm的轮询间隔"],["heading","注意测试环境要调高ILM的轮询间隔"],["body","\n"],["body","PUT _cluster/settings\n{\n  \"persistent\": {\n    \"indices.lifecycle.poll_interval\": \"1s\"\n  }\n}\n"],["body","\n"],["headingLink","新建ilm规则定义索引翻转逻辑"],["heading","新建ILM规则、定义索引翻转逻辑"],["body","\n"],["body","PUT _ilm/policy/my-lifecycle-policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"actions\": {\n          \"rollover\": {\n            \"max_docs\": 5\n          }\n        }\n      },\n      \"delete\": {\n        \"actions\": {\n          \"delete\": {\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","创建索引组件模板"],["heading","创建索引、组件模板"],["body","\n"],["body","PUT _component_template/my-mappings\n{\n  \"template\": {\n    \"mappings\": {\n      \"properties\": {\n        \"@timestamp\": {\n          \"type\": \"date\",\n          \"format\": \"date_optional_time||epoch_millis\"\n        },\n        \"message\": {\n          \"type\": \"wildcard\"\n        }\n      }\n    }\n  },\n  \"_meta\": {\n    \"description\": \"Mappings for @timestamp and message fields\",\n    \"my-custom-meta-field\": \"More arbitrary metadata\"\n  }\n}\n"],["body","\n"],["body","PUT _component_template/my-settings\n{\n  \"template\": {\n    \"settings\": {\n      \"index.lifecycle.name\": \"my-lifecycle-policy\",\n       //禁用软删除\n      \"index.soft_deletes.enabled\":false\n    }\n  },\n  \"_meta\": {\n    \"description\": \"Settings for ILM\",\n    \"my-custom-meta-field\": \"More arbitrary metadata\"\n  }\n}\n"],["body","\n"],["body","PUT _index_template/my-index-template\n{\n  \"index_patterns\": [\"my-data-stream*\"],\n  \"data_stream\": { },\n  \"composed_of\": [ \"my-mappings\", \"my-settings\" ],\n  \"priority\": 500,\n  \"_meta\": {\n    \"description\": \"Template for my time series data\",\n    \"my-custom-meta-field\": \"More arbitrary metadata\"\n  }\n}\n"],["body","\n"],["headingLink","创建数据流"],["heading","创建数据流"],["body","\n\n"],["body","PUT _data_stream/my-data-stream"],["body","\n"],["body","\n"],["body","    POST my-data-stream/_doc\n    {\n    \"@timestamp\": \"2099-05-06T16:21:15.000Z\",\n    \"message\": \"192.0.2.42 - - [06/May/2099:16:21:15 +0000] \\\"GET /images/bg.jpg HTTP/1.0\\\" 200 24736\"\n    }\n"],["body","\n"],["body","\n"],["body","\n"],["body","PUT my-data-stream/_bulk\n{ \"create\":{ } }\n{ \"@timestamp\": \"2099-05-06T16:21:15.000Z\", \"message\": \"192.0.2.42 - - [06/May/2099:16:21:15 +0000] \\\"GET /images/bg.jpg HTTP/1.0\\\" 200 24736\" }\n"],["body","\n"],["body","\n\n"],["headingLink","对数据流进行安全配置"],["heading","对数据流进行安全配置"],["body","\n\n"],["body","详见 Data stream privileges"],["body","\n\n"],["headingLink","将索引别名迁移到数据流"],["heading","将索引别名迁移到数据流"],["body","\n\n"],["body","使用迁移到数据流API"],["body","\n"],["body","POST _data_stream/_migrate/my-time-series-data"],["body","\n\n"],["headingLink","获取数据流信息"],["heading","获取数据流信息"],["body","\n\n"],["body","get data stream API"],["body","\n"],["body","GET _data_stream/my-data-stream"],["body","\n\n"],["headingLink","删除数据流"],["heading","删除数据流"],["body","\n\n"],["body","delete data stream API"],["body","\n"],["body","DELETE _data_stream/my-data-stream"],["body","\n\n"],["headingLink","使用数据流进行各种操作"],["heading","使用数据流进行各种操作"],["body","\n"],["headingLink","添加数据"],["heading","添加数据"],["body","\n"],["body","POST my-data-stream/_doc\n{\n\"@timestamp\": \"2099-05-06T16:21:15.000Z\",\n\"message\": \"192.0.2.42 - - [06/May/2099:16:21:15 +0000] \\\"GET /images/bg.jpg HTTP/1.0\\\" 200 24736\"\n}\n"],["body","\n"],["headingLink","检索数据"],["heading","检索数据"],["body","\n\n"],["body","Search"],["body","\n"],["body","Async search"],["body","\n"],["body","Multi search"],["body","\n"],["body","Field capabilities"],["body","\n"],["body","EQL search"],["body","\n\n"],["headingLink","获取数据流统计信息"],["heading","获取数据流统计信息"],["body","\n\n"],["body","data stream stats API"],["body","\n"],["body","GET /_data_stream/my-data-stream/_stats?human=true"],["body","\n\n"],["headingLink","手动翻滚数据流"],["heading","手动翻滚数据流"],["body","\n\n"],["body","POST /my-data-stream/_rollover/"],["body","\n\n"],["headingLink","打开关闭的索引"],["heading","打开关闭的索引"],["body","\n\n"],["body","手动打开特定索引\n\n"],["body","open index API request"],["body","\n"],["body","POST /.ds-my-data-stream-2099.03.07-000001/_open/"],["body","\n\n"],["body","\n"],["body","打开数据流的所有封闭索引\n\n"],["body","POST /my-data-stream/_open/"],["body","\n\n"],["body","\n\n"],["headingLink","重新索引数据流"],["heading","重新索引数据流"],["body","\n\n"],["body","reindex API"],["body","\n\n"],["body","POST /_reindex\n{\n  \"source\": {\n    \"index\": \"archive\"\n  },\n  \"dest\": {\n    \"index\": \"my-data-stream\",\n    \"op_type\": \"create\"\n  }\n}\n"],["body","\n"],["headingLink","更新数据流中的文档--by-query"],["heading","更新数据流中的文档  by query"],["body","\n"],["body","POST /my-data-stream/_update_by_query\n{\n  \"query\": {\n    \"match\": {\n      \"user.id\": \"l7gk7f82\"\n    }\n  },\n  \"script\": {\n    \"source\": \"ctx._source.user.id = params.new_id\",\n    \"params\": {\n      \"new_id\": \"XgdX0NoX\"\n    }\n  }\n}\n"],["body","\n"],["headingLink","删除数据流中的文档--by-query"],["heading","删除数据流中的文档  by query"],["body","\n"],["body","POST /my-data-stream/_delete_by_query\n{\n  \"query\": {\n    \"match\": {\n      \"user.id\": \"vlb44hny\"\n    }\n  }\n"],["body","\n"],["headingLink","使用后备索引对文档新增删除"],["heading","使用后备索引对文档新增、删除"],["body","\n"],["headingLink","使用后备索引对文档进行删除需要三个字段"],["heading","使用后备索引对文档进行删除需要三个字段"],["body","\n\n"],["body","DocumentId"],["body","\n"],["body","if_seq_no"],["body","\n"],["body","if_primary_term"],["body","\n\n"],["headingLink","search请求返回-if_seq_noif_primary_term"],["heading","Search请求返回 if_seq_no、if_primary_term"],["body","\n"],["body","GET /my-data-stream/_search\n{\n\"seq_no_primary_term\": true,\n\"query\": {\n    \"match\": {\n    \"user.id\": \"yWIumJd7\"\n    }\n}\n}\n"],["body","\n"],["headingLink","更新数据"],["heading","更新数据"],["body","\n"],["body","PUT /.ds-my-data-stream-2099-03-08-000003/_doc/bfspvnIBr7VVZlfp2lqX?if_seq_no=0&if_primary_term=1\n{\n\"@timestamp\": \"2099-03-08T11:06:07.000Z\",\n\"user\": {\n    \"id\": \"8a4f500d\"\n},\n\"message\": \"Login successful\"\n}\n"],["body","\n"],["headingLink","删除数据"],["heading","删除数据"],["body","\n\n"],["body","DELETE /.ds-my-data-stream-2099.03.08-000003/_doc/bfspvnIBr7VVZlfp2lqX"],["body","\n\n"],["headingLink","批量操作"],["heading","批量操作"],["body","\n"],["body","PUT /_bulk?refresh\n{ \"index\": { \"_index\": \".ds-my-data-stream-2099.03.08-000003\", \"_id\": \"bfspvnIBr7VVZlfp2lqX\", \"if_seq_no\": 0, \"if_primary_term\": 1 } }\n{ \"@timestamp\": \"2099-03-08T11:06:07.000Z\", \"user\": { \"id\": \"8a4f500d\" }, \"message\": \"Login successful\" }\n"],["body","\n"],["headingLink","改变数据流的映射和设置"],["heading","改变数据流的映射和设置"],["body","\n"],["headingLink","新增字段映射"],["heading","新增字段映射"],["body","\n"],["headingLink","修改字段映射"],["heading","修改字段映射"],["body","\n"],["headingLink","修改索引运行时设置"],["heading","修改索引运行时设置"],["body","\n"],["headingLink","修改索引静态不可变设置"],["heading","修改索引静态不可变设置"],["body","\n"],["headingLink","通过reindex修改索引运行时或不可变设置"],["heading","通过ReIndex修改索引、运行时、或不可变设置"],["body","\n"],["headingLink","混合新旧数据"],["heading","混合新旧数据"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/6.DataStreams/3.ChangeMappingsAndSettings.html"],["title","ChangeMappingsAndSettings.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","change-mappings-and-settings-for-a-data-stream"],["heading","Change mappings and settings for a data stream"],["body","\n"],["body","\n"],["body","修改数据流的 mappings and settings"],["body","\n"],["body","\n\n"],["body","\n"],["body","每个数据流都有一个 匹配索引模板。来自此模板的映射和索引设置应用于为流创建的新备份索引"],["body","\n"],["body","\n"],["body","\n"],["body","这包括流的第一个后备索引，该索引在创建流时自动生成。"],["body","\n"],["body","\n"],["body","\n"],["body","在创建数据流之前，我们建议您仔细考虑要在此模板中包含哪些映射和设置"],["body","\n"],["body","\n"],["body","\n"],["body","如果以后需要更改数据流的映射或设置，则有以下几种选择:"],["body","\n\n"],["body","\n"],["body","Add a new field mapping to a data stream"],["body","\n"],["body","\n"],["body","\n"],["body","Change an existing field mapping in a data stream"],["body","\n"],["body","\n"],["body","\n"],["body","Change a dynamic index setting for a data stream"],["body","\n"],["body","\n"],["body","\n"],["body","Change a static index setting for a data stream"],["body","\n"],["body","\n\n"],["body","\n\n"],["body","如果您的更改包括对现有字段映射或 静态索引设置的修改通常需要重新索引才能将更改应用于数据流的支持索引。"],["body","\n"],["body","如果您已经在执行重新索引，则可以使用相同的过程来添加新的字段映射并更改 动态索引设置。请参阅 使用重新索引更改映射或设置。"],["body","\n"],["headingLink","add-a-new-field-mapping-to-a-data-stream"],["heading","Add a new field mapping to a data stream"],["body","\n"],["body","\n"],["body","在数据流上添加新字段"],["body","\n"],["body","\n"],["body","要将新字段的映射添加到数据流，请执行以下步骤:"],["body","\n\n"],["body","\n"],["body","更新数据流使用的索引模板. 这样可以确保将新的字段映射添加到为流创建的将来的备份索引中。"],["body","\n"],["body","For example, my-data-stream-template is an existing index template used by my-data-stream."],["body","\n"],["body","The following create or update index template request adds a mapping for a new field, message, to the template."],["body","\n"],["body","PUT /_index_template/my-data-stream-template\n{\n  \"index_patterns\": [ \"my-data-stream*\" ],\n  \"data_stream\": { },\n  \"priority\": 500,\n  \"template\": {\n    \"mappings\": {\n      \"properties\": {\n        \"message\": {                              \n          \"type\": \"text\"\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","\n"],["body","\n"],["body","使用 更新映射API 将新的字段映射添加到数据流。默认情况下，这会将映射添加到流的现有备份索引 (包括写入索引)。"],["body","\n"],["body","以下更新映射API请求将新的 'message' 字段映射添加到 'my-data-stream'。"],["body","\n"],["body","PUT /my-data-stream/_mapping\n{\n  \"properties\": {\n    \"message\": {\n      \"type\": \"text\"\n    }\n  }\n}\n"],["body","\n"],["body","要将映射仅添加到流的写索引中，请将更新映射API的 “write_index_only” 查询参数设置为 “true”。"],["body","\n"],["body","以下更新映射请求仅将新的 'message' 字段映射添加到 'my-data-stream' 的写入索引。新的字段映射不会添加到流的其他支持索引中。"],["body","\n"],["body","PUT /my-data-stream/_mapping?write_index_only=true\n{\n  \"properties\": {\n    \"message\": {\n      \"type\": \"text\"\n    }\n  }\n}\n"],["body","\n"],["body","\n\n"],["headingLink","change-an-existing-field-mapping-in-a-data-stream"],["heading","Change an existing field mapping in a data stream"],["body","\n"],["body","\n"],["body","改变现有字段Mapping"],["body","\n"],["body","\n"],["body","每个 映射参数 的文档指示是否可以使用 [更新映射API](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.17/indices-put-mapping.html)。要更新现有字段的这些参数，请执行以下步骤:"],["body","\n\n"],["body","\n"],["body","更新数据流使用的索引模板。"],["body","\n"],["body","\n"],["body","\n"],["body","这样可以确保将更新的字段映射添加到为流创建的将来的后备索引中。"],["body","\n"],["body","例如，'my-data-stream-templat' 是 'my-data-stream' 使用的现有索引模板。"],["body","\n"],["body","The following create or update index template request changes the argument for the host.ip field’s ignore_malformed mapping parameter to true."],["body","\n"],["body","PUT /_index_template/my-data-stream-template\n{\n  \"index_patterns\": [ \"my-data-stream*\" ],\n  \"data_stream\": { },\n  \"priority\": 500,\n  \"template\": {\n    \"mappings\": {\n      \"properties\": {\n        \"host\": {\n          \"properties\": {\n            \"ip\": {\n              \"type\": \"ip\",\n              \"ignore_malformed\": true            \n            }\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","\n"],["body","\n"],["body","Use the update mapping API to apply the mapping changes to the data stream. By default, this applies the changes to the stream’s existing backing indices, including the write index."],["body","\n"],["body","The following update mapping API request targets my-data-stream. The request changes the argument for the host.ip field’s ignore_malformed mapping parameter to true."],["body","\n"],["body","PUT /my-data-stream/_mapping\n{\n  \"properties\": {\n    \"host\": {\n      \"properties\": {\n        \"ip\": {\n          \"type\": \"ip\",\n          \"ignore_malformed\": true\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","要将映射更改仅应用于流的写索引，请将put映射API的 write_index_only 查询参数设置为 “true”。"],["body","\n"],["body","PUT /my-data-stream/_mapping?write_index_only=true\n{\n  \"properties\": {\n    \"host\": {\n      \"properties\": {\n        \"ip\": {\n          \"type\": \"ip\",\n          \"ignore_malformed\": true\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","\n\n"],["body","除了支持的映射参数外，我们不建议您更改现有字段的映射或字段数据类型，即使在数据流的匹配索引模板或其支持索引中也是如此。更改现有字段的映射可能会使已索引的任何数据无效。"],["body","\n"],["body","如果您需要更改现有字段的映射，请创建一个新的数据流并将您的数据重新索引到其中. See Use reindex to change mappings or settings."],["body","\n"],["headingLink","change-a-dynamic-index-setting-for-a-data-stream"],["heading","Change a dynamic index setting for a data stream"],["body","\n"],["body","\n"],["body","改变数据流动态的设置"],["body","\n"],["body","\n"],["body","To change a dynamic index setting for a data stream, follow these steps:"],["body","\n\n"],["body","\n"],["body","更新数据流使用的索引模板. 这确保该设置应用于后续的后备索引\nThe following create or update index template request changes the template’s index.refresh_interval index setting to 30s (30 seconds)."],["body","\n"],["body","PUT /_index_template/my-data-stream-template\n{\n  \"index_patterns\": [ \"my-data-stream*\" ],\n  \"data_stream\": { },\n  \"priority\": 500,\n  \"template\": {\n    \"settings\": {\n      \"index.refresh_interval\": \"30s\"             \n    }\n  }\n}\n"],["body","\n"],["body","\n"],["body","\n"],["body","Use the update index settings API to update the index setting for the data stream. By default, this applies the setting to the stream’s existing backing indices, including the write index."],["body","\n"],["body","The following update index settings API request updates the index.refresh_interval setting for my-data-stream."],["body","\n"],["body","PUT /my-data-stream/_settings\n{\n  \"index\": {\n    \"refresh_interval\": \"30s\"\n  }\n}\n"],["body","\n"],["body","\n\n"],["body","To change the index.lifecycle.name setting, first use the remove policy API to remove the existing ILM policy. See Switch lifecycle policies."],["body","\n"],["headingLink","change-a-static-index-setting-for-a-data-stream"],["heading","Change a static index setting for a data stream"],["body","\n"],["body","\n"],["body","改变数据流的静态设置"],["body","\n"],["body","\n\n"],["body","[静态索引设置](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/index-modules.html # index-modules-settings) 只能在创建后备索引时设置"],["body","\n"],["body","You cannot update static index settings using the update index settings API."],["body","\n"],["body","要将新的静态设置应用于未来的支持索引，请更新数据流使用的索引模板，该设置将自动应用于更新后创建的任何后备索引。"],["body","\n"],["body","例如，'my-data-stream-templat' 是 'my-data-stream' 使用的现有索引模板。"],["body","\n\n"],["body","The following create or update index template API requests adds new sort.field and sort.order index settings to the template."],["body","\n"],["body","PUT /_index_template/my-data-stream-template\n{\n  \"index_patterns\": [ \"my-data-stream*\" ],\n  \"data_stream\": { },\n  \"priority\": 500,\n  \"template\": {\n    \"settings\": {\n      \"sort.field\": [ \"@timestamp\"],             \n      \"sort.order\": [ \"desc\"]                    \n    }\n  }\n}\n"],["body","\n\n"],["body","如果需要，您可以[滚动数据流](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/use-a-data-stream.html # 手动-roll-over-a-data-stream) 立即将设置应用于数据流的写入索引。"],["body","\n"],["body","这会影响在翻转后添加到流中的新数据，但是不会影响数据流的现有后备索引的现有数据。"],["body","\n"],["body","要将静态设置更改应用于现有的备份索引，您必须创建一个新的数据流并将您的数据重新索引到其中。See Use reindex to change mappings or settings."],["body","\n\n"],["headingLink","use-reindex-to-change-mappings-or-settings"],["heading","Use reindex to change mappings or settings"],["body","\n"],["body","\n"],["body","使用Reindex重新索引来变更 mappings or settings"],["body","\n"],["body","\n\n"],["body","\n"],["body","您可以使用重新索引来更改数据流的映射或设置"],["body","\n"],["body","\n"],["body","\n"],["body","更改现有字段的数据类型或更新备份索引的静态索引设置通常需要这样做。"],["body","\n"],["body","\n"],["body","\n"],["body","要重新索引数据流，请首先创建或更新索引模板，使其包含所需的映射或设置更改。然后，您可以将现有数据流重新索引为与模板匹配的新数据流"],["body","\n"],["body","\n"],["body","\n"],["body","这将模板中的映射和设置更改应用于添加到新数据流中的每个文档和后备索引，这些更改还会影响新流创建的任何未来支持索引。"],["body","\n"],["body","\n\n"],["body","这些更改还会影响新流创建的任何之后的后备索引。"],["body","\n"],["body","按照下列步骤操作:"],["body","\n\n"],["body","您可以使用resolve index API来检查名称或模式是否与任何现有索引、别名或数据流匹配。如果是这样，则应考虑使用其他名称或模式。"],["body","\n"],["body","以下resolve index API请求会检查以 “new-data-stream” 开头的任何现有索引、别名或数据流。如果不是，则可以使用 “new-data-stream” 索引模式来创建新数据流。"],["body","\n\n"],["body","GET /_resolve/index/new-data-stream*\n"],["body","\n"],["body","API返回以下响应，指示没有现有目标与此模式匹配。"],["body","\n"],["body","{\n  \"indices\": [ ],\n  \"aliases\": [ ],\n  \"data_streams\": [ ]\n}\n"],["body","\n\n"],["body","\n"],["body","创建或更新索引模板。此模板应包含要应用于新数据流的后备索引的映射和设置。"],["body","\n"],["body","\n"],["body","\n"],["body","此索引模板必须满足 [数据流模板的要求](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/set-up-a-data-stream.html # create-index-template)。它还应该在 “index_patterns” 属性中包含您以前选择的名称或索引模式。"],["body","\n"],["body","\n"],["body","\n"],["body","如果您仅添加或更改了一些内容，我们建议您通过复制现有模板并根据需要对其进行修改来创建新模板。"],["body","\n\n"],["body","For example, my-data-stream-template is an existing index template used by my-data-stream."],["body","\n"],["body","The following create or update index template API request creates a new index template, new-data-stream-template. new-data-stream-template uses my-data-stream-template as its basis, 以下变更:\n\n"],["body","The index pattern in index_patterns matches any index or data stream starting with new-data-stream."],["body","\n"],["body","index_patterns 中的索引模式与以 new-data-stream 开头的任何索引或数据流匹配。"],["body","\n"],["body","The @timestamp field mapping uses the date_nanos field data type rather than the date data type."],["body","\n"],["body","The template includes sort.field and sort.order index settings, which were not in the original my-data-stream-template template."],["body","\n\n"],["body","\n\n"],["body","PUT /_index_template/new-data-stream-template\n{\n  \"index_patterns\": [ \"new-data-stream*\" ],\n  \"data_stream\": { },\n  \"priority\": 500,\n  \"template\": {\n    \"mappings\": {\n      \"properties\": {\n        \"@timestamp\": {//Changes the `@timestamp` field mapping to the `date_nanos` field data type.\n          \"type\": \"date_nanos\"                 \n        }\n      }\n    },\n    \"settings\": {\n    //Adds the `sort.field` index setting.\n      \"sort.field\": [ \"@timestamp\"],          \n    //Adds the `sort.order` index setting.\n      \"sort.order\": [ \"desc\"]                 \n    }\n  }\n}\n"],["body","\n"],["body","\n"],["body","\n"],["body","Use the create data stream API to manually create the new data stream. 数据流的名称必须与新模板的 “index_patterns” 属性中定义的索引模式匹配。\n我们不建议 索引新数据以创建此数据流。稍后，您将把现有数据流中的旧数据重新索引到这个新流中。这可能导致包含新旧数据混合的一个或多个支持索引。"],["body","\n"],["body","\n\n"],["headingLink","mixing-new-and-old-data-in-a-data-stream"],["heading","Mixing new and old data in a data stream"],["body","\n\n"],["body","虽然混合新旧数据是安全的，但它可能会干扰数据保留。"],["body","\n"],["body","如果删除较旧的索引，则可能会意外删除包含新数据和旧数据的后备索引。为了防止过早的数据丢失，您需要保留这样的后备索引，直到准备删除其最新数据为止。"],["body","\n"],["body","下面的 create data stream API 请求以 new-data-stream 为目标，它与 “new-data-stream-templat'” 的索引模式相匹配。由于没有现有的索引或数据流使用此名称，因此 此请求将创建 new-data-stream 数据流。"],["body","\n\n"],["body","PUT /_data_stream/new-data-stream\n"],["body","\n\n"],["body","\n"],["body","如果您不想在新数据流中混合新数据和旧数据，请暂停新文档的索引.虽然混合新旧数据是安全的，但它可能会干扰数据保留. See Mixing new and old data in a data stream."],["body","\n"],["body","\n"],["body","\n"],["body","如果使用ILM 自动翻转，请减少ILM轮询间隔。这样可以确保当前的写入索引在等待滚动检查时不会增长太大。默认情况下，ILM每10分钟检查一次翻转条件。\n以下 群集更新设置API 请求将 “indices.lifecycle.poll_interval” 设置降低到 “1m” (一分钟)。"],["body","\n"],["body","PUT /_cluster/settings\n{\n  \"persistent\": {\n    \"indices.lifecycle.poll_interval\": \"1m\"\n  }\n}\n"],["body","\n"],["body","\n"],["body","\n"],["body","Reindex your data to the new data stream using an op_type of create."],["body","\n\n"],["body","如果要按照最初索引的顺序对数据进行分区，则可以运行单独的重新索引请求"],["body","\n"],["body","这些重新索引请求可以使用单个后备索引作为源。您可以使用获取数据流API检索支持索引列表。"],["body","\n"],["body","例如，您计划将数据从 “my-data-stream” 重新索引为 “new-data-stream”。但是，您希望为 “my-data-stream” 中的每个后备索引提交单独的重新索引请求，从最旧的备份索引开始。这保留了数据最初索引的顺序。"],["body","\n"],["body","下面的get数据流API请求检索关于 'my-data-stream' 的信息，包括其后备的索引的列表。"],["body","\n\n"],["body","GET /_data_stream/my-data-stream\n"],["body","\n"],["body","响应的 indices 属性包含当前后备索引的数组。数组中的第一项包含有关流最旧的后备索引的信息。"],["body","\n"],["body","{\n  \"data_streams\": [\n    {\n      \"name\": \"my-data-stream\",\n      \"timestamp_field\": {\n        \"name\": \"@timestamp\"\n      },\n      \"indices\": [\n        {\n          \"index_name\": \".ds-my-data-stream-2099.03.07-000001\", \n          \"index_uuid\": \"Gpdiyq8sRuK9WuthvAdFbw\"\n        },\n        {\n          \"index_name\": \".ds-my-data-stream-2099.03.08-000002\",\n          \"index_uuid\": \"_eEfRrFHS9OyhqWntkgHAQ\"\n        }\n      ],\n      \"generation\": 2,\n      \"status\": \"GREEN\",\n      \"template\": \"my-data-stream-template\",\n      \"hidden\": false,\n      \"system\": false,\n      \"replicated\": false\n    }\n  ]\n}\n"],["body","\n"],["body","以下 reindex API 请求将文档从 “.ds-my-data-stream-2099.03.07-000001” 复制到 “新数据流”。请求的 op_type is  create。"],["body","\n"],["body","POST /_reindex\n{\n  \"source\": {\n    \"index\": \".ds-my-data-stream-2099.03.07-000001\"\n  },\n  \"dest\": {\n    \"index\": \"new-data-stream\",\n    \"op_type\": \"create\"\n  }\n}\n"],["body","\n"],["body","您还可以使用查询来仅对每个请求的文档子集进行重新索引。\nThe following reindex API request copies documents from my-data-stream to new-data-stream. The request uses a range query to only reindex documents with a timestamp within the last week. Note the request’s op_type is create."],["body","\n"],["body","以下 reindex API 请求将文档从 my-data-stream 复制到 new-data-stream。该请求使用 RangeQuery 仅重新索引上周的文档。请注意，请求的 “opType” 是 create。"],["body","\n"],["body","POST /_reindex\n{\n  \"source\": {\n    \"index\": \"my-data-stream\",\n    \"query\": {\n      \"range\": {\n        \"@timestamp\": {\n          \"gte\": \"now-7d/d\",\n          \"lte\": \"now/d\"\n        }\n      }\n    }\n  },\n  \"dest\": {\n    \"index\": \"new-data-stream\",\n    \"op_type\": \"create\"\n  }\n}\n"],["body","\n"],["body","\n"],["body","\n"],["body","如果您以前更改了ILM轮询间隔，请在重新索引完成后将其更改回其原始值。这样可以防止主节点上不必要的负载。\n以下群集更新设置API请求将 “indices.lifecycle.poll_interval” 设置重置为其默认值。"],["body","\n"],["body","PUT /_cluster/settings\n{\n  \"persistent\": {\n    \"indices.lifecycle.poll_interval\": null\n  }\n}\n"],["body","\n"],["body","\n"],["body","\n"],["body","使用新数据流恢复索引。现在，在此流上的搜索将查询您的新数据和重新索引的旧数据。"],["body","\n"],["body","\n"],["body","\n"],["body","一旦您验证了新数据流中所有重新索引的数据都可用，就可以安全地删除旧流。\n以下 删除数据流API 请求删除 “my-data-stream'”。此请求还删除流的备份索引及其包含的任何数据。"],["body","\n"],["body","DELETE /_data_stream/my-data-stream\n"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/6.DataStreams/1.setUpADataStream.html"],["title","setUpADataStream.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","set-up-a-data-stream"],["heading","Set up a data stream"],["body","\n"],["body","要配置数据流，请执行以下步骤:"],["body","\n\n"],["body","Create an index lifecycle policy"],["body","\n"],["body","Create component templates"],["body","\n"],["body","Create an index template"],["body","\n"],["body","Create the data stream"],["body","\n"],["body","Secure the data stream"],["body","\n\n"],["body","You can also convert an index alias to a data stream."],["body","\n"],["body","If you use Fleet or Elastic Agent, skip this tutorial. Fleet and Elastic Agent set up data streams for you. See Fleet’s data streams documentation."],["body","\n"],["headingLink","create-an-index-lifecycle-policy"],["heading","Create an index lifecycle policy"],["body","\n\n"],["body","\n"],["body","虽然可选，但我们建议使用ILM来自动管理数据流的支持索引"],["body","\n"],["body","\n"],["body","\n"],["body","ILM requires an index lifecycle policy."],["body","\n"],["body","\n"],["body","\n"],["body","To create an index lifecycle policy in Kibana, open the main menu and go to Stack Management > Index Lifecycle Policies. Click Create policy."],["body","\n"],["body","\n\n"],["body","您也可以使用 create lifecycle policy API."],["body","\n"],["body","PUT _ilm/policy/my-lifecycle-policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"actions\": {\n          \"rollover\": {\n            \"max_primary_shard_size\": \"50gb\"\n          }\n        }\n      },\n      \"warm\": {\n        \"min_age\": \"30d\",\n        \"actions\": {\n          \"shrink\": {\n            \"number_of_shards\": 1\n          },\n          \"forcemerge\": {\n            \"max_num_segments\": 1\n          }\n        }\n      },\n      \"cold\": {\n        \"min_age\": \"60d\",\n        \"actions\": {\n          \"searchable_snapshot\": {\n            \"snapshot_repository\": \"found-snapshots\"\n          }\n        }\n      },\n      \"frozen\": {\n        \"min_age\": \"90d\",\n        \"actions\": {\n          \"searchable_snapshot\": {\n            \"snapshot_repository\": \"found-snapshots\"\n          }\n        }\n      },\n      \"delete\": {\n        \"min_age\": \"735d\",\n        \"actions\": {\n          \"delete\": {}\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","create-component-templates"],["heading","Create component templates"],["body","\n\n"],["body","数据流需要匹配的索引模板。"],["body","\n"],["body","在大多数情况下，您使用一个或多个组件模板组成此索引模板"],["body","\n"],["body","您通常使用单独的组件模板进行映射和索引设置，这允许您在多个索引模板中重用组件模板"],["body","\n\n"],["body","创建组件模板时，包括:"],["body","\n\n"],["body","A date or date_nanos mapping for the @timestamp field. If you don’t specify a mapping, Elasticsearch maps @timestamp as a date field with default options."],["body","\n"],["body","Your lifecycle policy in the index.lifecycle.name index setting."],["body","\n"],["body","Use the Elastic Common Schema (ECS) when mapping your fields. ECS fields integrate with several Elastic Stack features by default."],["body","\n"],["body","如果你不确定如何提取你的字段， use runtime fields to extract fields from unstructured content at search time."],["body","\n"],["body","例如，您可以将日志消息索引到 wildcard 字段，然后在搜索过程中从该字段中提取ip地址和其他数据。"],["body","\n"],["body","To create a component template in Kibana, open the main menu and go to Stack Management > Index Management. In the Index Templates view, click Create component template."],["body","\n\n"],["body","You can also use the create component template API."],["body","\n"],["body","# Creates a component template for mappings\nPUT _component_template/my-mappings\n{\n  \"template\": {\n    \"mappings\": {\n      \"properties\": {\n        \"@timestamp\": {\n          \"type\": \"date\",\n          \"format\": \"date_optional_time||epoch_millis\"\n        },\n        \"message\": {\n          \"type\": \"wildcard\"\n        }\n      }\n    }\n  },\n  \"_meta\": {\n    \"description\": \"Mappings for @timestamp and message fields\",\n    \"my-custom-meta-field\": \"More arbitrary metadata\"\n  }\n}\n\n# Creates a component template for index settings\nPUT _component_template/my-settings\n{\n  \"template\": {\n    \"settings\": {\n      \"index.lifecycle.name\": \"my-lifecycle-policy\"\n    }\n  },\n  \"_meta\": {\n    \"description\": \"Settings for ILM\",\n    \"my-custom-meta-field\": \"More arbitrary metadata\"\n  }\n}\n"],["body","\n"],["headingLink","create-an-index-template"],["heading","Create an index template"],["body","\n"],["body","使用组件模板创建索引模板。指定:"],["body","\n\n"],["body","与数据流的名称匹配的一个或多个索引模式 我们建议使用我们的 data stream naming scheme."],["body","\n"],["body","模板已启用数据流。"],["body","\n"],["body","包含您的映射和索引设置的任何组件模板."],["body","\n"],["body","高于 “200” 的优先级，以避免与内置模板发生冲突。 See Avoid index pattern collisions."],["body","\n"],["body","To create an index template in Kibana, open the main menu and go to Stack Management > Index Management. In the Index Templates view, click Create template."],["body","\n"],["body","You can also use the create index template API. Include the data_stream object to enable data streams."],["body","\n\n"],["body","PUT _index_template/my-index-template\n{\n  \"index_patterns\": [\"my-data-stream*\"],\n  \"data_stream\": { },\n  \"composed_of\": [ \"my-mappings\", \"my-settings\" ],\n  \"priority\": 500,\n  \"_meta\": {\n    \"description\": \"Template for my time series data\",\n    \"my-custom-meta-field\": \"More arbitrary metadata\"\n  }\n}\n"],["body","\n"],["headingLink","create-the-data-stream"],["heading","Create the data stream"],["body","\n\n"],["body","Indexing requests add documents to a data stream. These requests must use an op_type of create. Documents must include a @timestamp field."],["body","\n"],["body","要自动创建数据流，请提交针对流名称的索引请求。此名称必须与索引模板的索引模式之一匹配。"],["body","\n\n"],["body","PUT my-data-stream/_bulk\n{ \"create\":{ } }\n{ \"@timestamp\": \"2099-05-06T16:21:15.000Z\", \"message\": \"192.0.2.42 - - [06/May/2099:16:21:15 +0000] \\\"GET /images/bg.jpg HTTP/1.0\\\" 200 24736\" }\n{ \"create\":{ } }\n{ \"@timestamp\": \"2099-05-06T16:25:42.000Z\", \"message\": \"192.0.2.255 - - [06/May/2099:16:25:42 +0000] \\\"GET /favicon.ico HTTP/1.0\\\" 200 3638\" }\n\nPOST my-data-stream/_doc\n{\n  \"@timestamp\": \"2099-05-06T16:21:15.000Z\",\n  \"message\": \"192.0.2.42 - - [06/May/2099:16:21:15 +0000] \\\"GET /images/bg.jpg HTTP/1.0\\\" 200 24736\"\n}\n"],["body","\n\n"],["body","You can also manually create the stream using the create data stream API. The stream’s name must still match one of your template’s index patterns."],["body","\n\n"],["body","PUT _data_stream/my-data-stream\n"],["body","\n"],["headingLink","secure-the-data-stream"],["heading","Secure the data stream"],["body","\n\n"],["body","\n"],["body","Use index privileges to control access to a data stream."],["body","\n"],["body","\n"],["body","\n"],["body","对数据流授予特权对其支持索引授予相同的特权。"],["body","\n"],["body","\n"],["body","\n"],["body","For an example, see Data stream privileges."],["body","\n"],["body","\n\n"],["headingLink","convert-an-index-alias-to-a-data-stream"],["heading","Convert an index alias to a data stream"],["body","\n\n"],["body","Prior to Elasticsearch 7.9, you’d typically use an index alias with a write index to manage time series data."],["body","\n"],["body","Data streams replace this functionality, require less maintenance, and automatically integrate with data tiers."],["body","\n"],["body","要将具有写索引的索引别名转换为具有相同名称的数据流，请使用 迁移到数据流API。在转换期间，别名的索引将成为流的隐藏备份索引。别名的写索引变成流的写索引。流仍然需要启用数据流的匹配索引模板。"],["body","\n\n"],["body","POST _data_stream/_migrate/my-time-series-data\n"],["body","\n"],["headingLink","get-information-about-a-data-stream"],["heading","Get information about a data stream"],["body","\n\n"],["body","\n"],["body","To get information about a data stream in Kibana, open the main menu and go to Stack Management > Index Management. In the Data Streams view, click the data stream’s name."],["body","\n"],["body","\n"],["body","\n"],["body","You can also use the get data stream API."],["body","\n"],["body","\n\n"],["body","GET _data_stream/my-data-stream\n"],["body","\n"],["headingLink","delete-a-data-stream"],["heading","Delete a data stream"],["body","\n\n"],["body","\n"],["body","To delete a data stream and its backing indices in Kibana, open the main menu and go to Stack Management > Index Management. In the Data Streams view, click the trash icon. The icon only displays if you have the delete_index security privilege for the data stream."],["body","\n"],["body","\n"],["body","\n"],["body","You can also use the delete data stream API."],["body","\n"],["body","\n\n"],["body","DELETE _data_stream/my-data-stream\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/聚合/RareTermsAggregation.html"],["title","RareTermsAggregation.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","rare-terms-aggregation"],["heading","Rare terms aggregation"],["body","\n"],["body","基于多桶值源的聚合，可找到 “稀有” 术语-分布的长尾且不常见的术语。从概念上讲，这就像一个按 _count升序排序的术语聚合。如术语聚合文档中所述，实际上按计数升序排序术语agg具有无界错误。相反，您应该使用rare_terms聚合"],["body","\n"],["body","A multi-bucket value source based aggregation which finds \"rare\" terms —词项处于分布的长尾且不频繁出现的"],["body","\n"],["body","从概念上来讲，这就是一个 terms 聚合 ，通过 _count  升序排序"],["body","\n"],["body","实际上，通过 count 升序对术语agg进行排序具有无界错误(unbounded error)。相反，您应该使用rare_terms聚合"],["body","\n"],["headingLink","syntax"],["heading","Syntax"],["body","\n"],["body","{\n  \"rare_terms\": {\n    \"field\": \"the_field\",\n    \"max_doc_count\": 1\n  }\n}\n"],["body","\n"],["headingLink","parameters"],["heading","Parameters"],["body","\n"],["body","Parameter Name"],["body","Description"],["body","Required"],["body","Default Value"],["body","\n"],["body","field"],["body","检索的字段"],["body","Required"],["body","\n"],["body","max_doc_count"],["body","term 出现在的  最大文档个数"],["body","Optional"],["body","1"],["body","\n"],["body","precision"],["body","The precision of the internal CuckooFilters. Smaller precision leads to better approximation, but higher memory usage. Cannot be smaller than 0.00001"],["body","Optional"],["body","0.01"],["body","\n"],["body","include"],["body","Terms that should be included in the aggregation"],["body","Optional"],["body","\n"],["body","exclude"],["body","Terms that should be excluded from the aggregation"],["body","Optional"],["body","\n"],["body","missing"],["body","The value that should be used if a document does not have the field being aggregated"],["body","Optional"],["body","\n\n\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","GET /_search\n{\n  \"aggs\": {\n    \"genres\": {\n      \"rare_terms\": {\n        \"field\": \"genre\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","{\n  ...\n  \"aggregations\": {\n    \"genres\": {\n      \"buckets\": [\n        {\n          \"key\": \"swing\",\n          \"doc_count\": 1\n        }\n      ]\n    }\n  }\n}\n"],["body","\n"],["body","在此示例中，我们看到的唯一存储桶是 swing 存储桶，因为它是一个文档中出现的唯一术语。如果我们将max_doc_count增加到2，我们将看到更多的存储桶:"],["body","\n"],["body","{\n  ...\n  \"aggregations\": {\n    \"genres\": {\n      \"buckets\": [\n        {\n          \"key\": \"swing\",\n          \"doc_count\": 1\n        },\n        {\n          \"key\": \"jazz\",\n          \"doc_count\": 2\n        }\n      ]\n    }\n  }\n}\n"],["body","\n"],["headingLink","maximum-document-count"],["heading","Maximum document count"],["body","\n"],["body","max_doc_count参数用于控制 term 可以具有的文档计数的上限"],["body","\n"],["body","rare_terms agg没有像terms agg那样的大小限制。"],["body","\n"],["body","这意味着将返回与max_doc_count标准匹配的术语"],["body","\n"],["body","但是，这确实意味着如果选择不正确，可以返回大量结果。为了限制此设置的危险，maximum max_doc_count 最大为100"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/聚合/Term.html"],["title","Term.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","简介"],["heading","简介"],["body","\n"],["body","从例子中可以了解到，Terms Aggregation 以指定域的唯一值（term）构建出多个桶，每个桶包含了 key 为域的值（term），doc_count 为含有这个值（term）的文档数。"],["body","\n"],["body","GET /_search\n{\n    \"aggs\" : {\n        \"genres\" : {\n            \"terms\" : { \"field\" : \"genre\" }\n        }\n    }\n}\n"],["body","\n"],["body","{\n    ...\n    \"aggregations\" : {\n        \"genres\" : {\n            \"doc_count_error_upper_bound\": 0, \n            \"sum_other_doc_count\": 0, \n            \"buckets\" : [ \n                {\n                    \"key\" : \"electronic\",\n                    \"doc_count\" : 6\n                },\n                {\n                    \"key\" : \"rock\",\n                    \"doc_count\" : 3\n                },\n                {\n                    \"key\" : \"jazz\",\n                    \"doc_count\" : 2\n                }\n            ]\n        }\n    }\n}\n"],["body","\n"],["body","默认情况下，响应将返回 10 个桶。当然也可以指定 size 参数改变返回的桶的数量。"],["body","\n"],["headingLink","size-与精度"],["heading","Size 与精度"],["body","\n"],["body","Terms Aggregation 返回结果中的 doc_count- 是近似的，并不是一个准确数。"],["body","\n"],["body","根据 Elasticsearch 的机制，每个分片都会根据各自拥有的数据进行计算并且进行排序，最后协调节点对各个分片的计算结果进行整理并返回给客户端。"],["body","\n"],["body","而就是因为这样，产生出精度的问题，也就是计算的结果有误差。"],["body","\n"],["body","下面为官方给出的例子：有一组数据，数据中包含一个 product 字段，记录了产品的数量。请求获取产品数量 TOP 5 的数据。而数据存在有 3 个分片的索引中。"],["body","\n"],["body","GET /_search\n{\n    \"aggs\" : {\n        \"products\" : {\n            \"terms\" : {\n                \"field\" : \"product\",\n                \"size\" : 5\n            }\n        }\n    }\n}\n"],["body","\n"],["body","会有如下动作发生："],["body","\n"],["body","各个分片计算得出的结果。"],["body","\n"],["body","ShardA"],["body","ShardB"],["body","ShardC"],["body","\n"],["body","1"],["body","productA 25"],["body","productA 30"],["body","productA 45"],["body","\n"],["body","2"],["body","productB 18"],["body","productB 25"],["body","productC 44"],["body","\n"],["body","3"],["body","productC 6"],["body","productF 17"],["body","productZ 36"],["body","\n"],["body","4"],["body","productD 3"],["body","productZ 16"],["body","productG 30"],["body","\n"],["body","5"],["body","productE 2"],["body","productG 15"],["body","productE 29"],["body","\n"],["body","6"],["body","productF 2"],["body","productH 14"],["body","productH 28"],["body","\n"],["body","7"],["body","productG 2"],["body","productI 10"],["body","productQ 2"],["body","\n"],["body","8"],["body","productH 2"],["body","productQ 8"],["body","productD 1"],["body","\n"],["body","9"],["body","productI 1"],["body","productJ 6"],["body","\n"],["body","10"],["body","productJ 1"],["body","productC 4"],["body","\n\n\n"],["body","然后分片将会把 TOP 5 的数据返回给协调节点。"],["body","\n"],["body","ShardA"],["body","ShardB"],["body","ShardC"],["body","\n"],["body","1"],["body","productA 25"],["body","productA 30"],["body","productA 45"],["body","\n"],["body","2"],["body","productB 18"],["body","productB 25"],["body","productC 44"],["body","\n"],["body","3"],["body","productC 6"],["body","productF 17"],["body","productZ 36"],["body","\n"],["body","4"],["body","productD 3"],["body","productZ 16"],["body","productG 30"],["body","\n"],["body","5"],["body","productE 2"],["body","productG 15"],["body","productE 29"],["body","\n\n\n"],["body","最后，协调节点将会根据各个节点给出的数据进行整理得出最后 TOP 5 的数据并返回给客户端。"],["body","\n"],["body","最终数据"],["body","\n"],["body","1"],["body","productA(100)"],["body","\n"],["body","2"],["body","productZ(52)"],["body","\n"],["body","3"],["body","productC(50)"],["body","\n"],["body","4"],["body","productG(45)"],["body","\n"],["body","5"],["body","productB(43)"],["body","\n\n\n"],["body","可以看出，在第二步中，由于各个分片的数据有所不同，数据ProductC在分片 A 能排得上 TOP 5 的 term 在分片 B 却排不上，所以统计的 count个数可能不准确"],["body","\n"],["body","但是，只要各个分片都返回足够多的数据给协调节点，客户端得到的结果将是精准的。"],["body","\n"],["body","而开始提到的参数 size 就会控制分片返回给节点的数据量以及返回给客户端的数据量。可见，参数 size 越大，获取的结果的精度越高。"],["body","\n"],["headingLink","shard-size"],["heading","Shard Size"],["body","\n"],["body","上面提到，size 的大小会影响到聚合结果的精准度，size 值越大，精度越高。为了更高得精度，请求的时候将 size 值设置得偏大，这时会有一个问题，就是客户端将会得到大量的响应数据，而且这些响应数据对于客户端来说大部分都是没用的，而大量的响应数据还会耗费网络资源。"],["body","\n"],["body","这时，就要使用到另一个参数 shard_size 。shard_size 只会控制分片返回给协调节点的数据量，而最后协调节点整理并返回的数据量由 size 控制，这样既能提升精度，也避免了上述由于要提升精度而导致协调节点返回大量响应数据给客户端的问题。"],["body","\n"],["body","上面的内容由提到，size 会控制分片返回给协调节点的数据量，这段描述即正确也不正确。默认情况下，shard_size 的大小为 (size * 1.5 + 10) ，确实由 size 值控制，但是如果在请求时显式提供 shard_size 参数，自然 size 与分片返回给节点的数据量无关。"],["body","\n"],["headingLink","没显示的文档数"],["heading","没显示的文档数"],["body","\n"],["body","在响应结果中，有一个 sum_other_doc_count 值。假如 size 设定为 5，那么响应中只有 doc_count 前 5 的桶的数据，而 sum_other_doc_count 表示的就是没有返回的其他桶的文档数的总和。"],["body","\n"],["headingLink","文档数计算错误上限"],["heading","文档数计算错误上限"],["body","\n"],["body","Terms Aggreagation 的响应结果中，有一个 doc_count_error_upper_bound 值。这个值表示的是在聚合中，没有在最终结果（响应给客户端的结果）中的 term 最大可能有 doc_count_error_upper_bound 个文档含有。这是 ES 预估可能出现的最坏的结果。"],["body","\n"],["body","doc_count_error_upper_bound 是这样计算出来的：假如请求像上面 Product 的例子一样，"],["body","\n"],["body","size = 5，协调节点会将各个分片的排第五的 term 的文档数相加起来（根据上面的例子就是 2+15+29），得出的结果便是 doc_count_error_upper_bound 。"],["body","\n"],["body","根据 doc_count_error_upper_bound 的计算是基于这样的猜想（继续以上面的 Product 为例子），"],["body","\n"],["body","可能存在 Product Z 1，它在分片 A 中，包含它的文档数是 2，在分片 B 中它的文档数是 15，在分片 C 中它的文档数是 29，然后在各个分片的排名均是第六位，这样在协调节点将获取不到有关 Product Z1 的数据，便会将这个 Product Z1 排除在外，然而实际上这个 Product Z1 是足以排进前 5 的。"],["body","\n"],["body","当然上述提到的情况并不没有这么容易发生，但是 doc_count_error_upper_bound 越大，错误发生的可能性也越大（这个大是指与响应的结果作比较）。这时候可以适当增大 size 的值，让更多的数据参与到协调节点的整理过程中。"],["body","\n"],["headingLink","每个桶的错误上限"],["heading","每个桶的错误上限"],["body","\n"],["body","如上面提到的文档数计算错误上限类似，不过这个是精确到每个桶的。\n这个默认是关闭的，要开启就需要传递 show_term_doc_count_error 参数。"],["body","\n"],["body","GET /_search\n{\n    \"aggs\" : {\n        \"products\" : {\n            \"terms\" : {\n                \"field\" : \"product\",\n                \"size\" : 5,\n                \"show_term_doc_count_error\": true\n            }\n        }\n    }\n}\n"],["body","\n"],["body","\n\n{\n    ...\n    \"aggregations\" : {\n        \"products\" : {\n            \"doc_count_error_upper_bound\" : 46,\n            \"sum_other_doc_count\" : 79,\n            \"buckets\" : [\n                {\n                    \"key\" : \"Product A\",\n                    \"doc_count\" : 100,\n                    \"doc_count_error_upper_bound\" : 0\n                },\n                {\n                    \"key\" : \"Product Z\",\n                    \"doc_count\" : 52,\n                    \"doc_count_error_upper_bound\" : 2\n                }\n                ...\n            ]\n        }\n    }\n}\n\n\n"],["body","\n"],["body","每个桶的错误上限是这样计算的：响应结果中的 term 在没有返回相关数据的分片的最后一名的文档数之和。以上述例子中的 Product Z 为例，分片 B，分片 C 响应给协调节点的数据均包含了 Product Z 相关的数据，但是分片 A 却没有，那么就有可能是 Product Z 在分片 A 中排不到前 5，那么 Product Z 在分片 A 中最大的可能值就是与 Product E 一样，也就是 2。"],["body","\n"],["body","当然像 Product A 一样各个分片都有返回相关的数据的话，这个错误上限就是 0。"],["body","\n"],["body","1 ...... 10000\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/6.Date.html"],["title","Date.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","date-field-type"],["heading","Date field type"],["body","\n"],["body","JSON没有日期数据类型，所以Elasticsearch中的日期可以是:"],["body","\n\n"],["body","strings containing formatted dates, e.g. \"2015-01-01\" or \"2015/01/01 12:10:30\"."],["body","\n"],["body","a number representing milliseconds-since-the-epoch."],["body","\n"],["body","a number representing seconds-since-the-epoch (configuration)."],["body","\n\n\n"],["body","milliseconds-since-the-epoch.  的值必须是非负的。"],["body","\n"],["body","在内部，日期将转换为UTC (如果指定了时区)，并将其存储为代表时间间隔的长数。"],["body","\n"],["body","关于日期的查询在内部转换为  long类型的范围查询，聚合和存储字段的结果将转换回字符串，具体取决于与该字段关联的日期格式。"],["body","\n"],["body","日期将始终呈现为字符串，即使它们最初在JSON文档中作为long提供。"],["body","\n"],["body","可以自定义日期格式，但是如果未指定 “格式”，则它将使用默认值:"],["body","\n\n"],["body","    \"strict_date_optional_time||epoch_millis\"\n"],["body","\n"],["body","For instance:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"date\": {//The `date` field uses the default `format`.\n        \"type\": \"date\" \n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/1\n{ \"date\": \"2015-01-01\" } //This document uses a plain date.\n\nPUT my-index-000001/_doc/2\n{ \"date\": \"2015-01-01T12:10:30Z\" } //This document includes a time.\n\nPUT my-index-000001/_doc/3\n{ \"date\": 1420070400001 } //This document uses milliseconds-since-the-epoch.\n\nGET my-index-000001/_search //Note that the `sort` values that are returned are all in milliseconds-since-the-epoch.\n{\n  \"sort\": { \"date\": \"asc\"} \n}\n"],["body","\n"],["body","日期将接受小数点的数字，如 {\"date\": 1618249875.123456}。但是在某些情况下 (#70085)，我们会在这些日期上失去精度，因此应避免使用它们。"],["body","\n"],["headingLink","multiple-date-formats"],["heading","Multiple date formats"],["body","\n\n"],["body","可以通过以 '| |' 作为分隔符来分隔它们来指定多种格式"],["body","\n"],["body","将依次尝试每种格式，直到找到匹配的格式"],["body","\n"],["body","将会使用 第一种格式 将  milliseconds-since-the-epoch  转换回string"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"date\": {\n        \"type\":   \"date\",\n        \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\"\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","parameters-for-date-fields"],["heading","Parameters for date fields"],["body","\n"],["body","The following parameters are accepted by date fields:"],["body","\n"],["body","\n"],["body","boost"],["body","Mapping field-level query time boosting. Accepts a floating point number, defaults to 1.0."],["body","\n"],["body","doc_values"],["body","Should the field be stored on disk in a column-stride fashion, so that it can later be used for sorting, aggregations, or scripting? Accepts true (default) or false."],["body","\n"],["body","format"],["body","The date format(s) that can be parsed. Defaults to `strict_date_optional_time"],["body","\n"],["body","locale"],["body","The locale to use when parsing dates since months do not have the same names and/or abbreviations in all languages. The default is the ROOT locale,"],["body","\n"],["body","ignore_malformed"],["body","If true, malformed numbers are ignored. If false (default), malformed numbers throw an exception and reject the whole document. Note that this cannot be set if the script parameter is used."],["body","\n"],["body","index"],["body","Should the field be searchable? Accepts true (default) and false."],["body","\n"],["body","null_value"],["body","Accepts a date value in one of the configured format's as the field which is substituted for any explicit null values. Defaults to null, which means the field is treated as missing. Note that this cannot be set of the script parameter is used."],["body","\n"],["body","on_script_error"],["body","Defines what to do if the script defined by the script parameter throws an error at indexing time. Accepts fail (default), which will cause the entire document to be rejected, and continue, which will register the field in the document’s _ignored metadata field and continue indexing. This parameter can only be set if the script field is also set."],["body","\n"],["body","script"],["body","If this parameter is set, then the field will index values generated by this script, rather than reading the values directly from the source. If a value is set for this field on the input document, then the document will be rejected with an error. Scripts are in the same format as their runtime equivalent, and should emit long-valued timestamps."],["body","\n"],["body","store"],["body","Whether the field value should be stored and retrievable separately from the _source field. Accepts true or false (default)."],["body","\n"],["body","meta"],["body","Metadata about the field."],["body","\n\n\n"],["headingLink","epoch-seconds"],["heading","Epoch seconds"],["body","\n"],["body","If you need to send dates as seconds-since-the-epoch then make sure the format lists epoch_second:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"date\": {\n        \"type\":   \"date\",\n        \"format\": \"strict_date_optional_time||epoch_second\"\n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/example?refresh\n{ \"date\": 1618321898 }\n\nPOST my-index-000001/_search\n{\n  \"fields\": [ {\"field\": \"date\"}],\n  \"_source\": false\n}\n"],["body","\n"],["body","Which will reply with a date like:"],["body","\n"],["body","{\n  \"hits\": {\n    \"hits\": [\n      {\n        \"_id\": \"example\",\n        \"_index\": \"my-index-000001\",\n        \"_type\": \"_doc\",\n        \"_score\": 1.0,\n        \"fields\": {\n          \"date\": [\"2021-04-13T13:51:38.000Z\"]\n        }\n      }\n    ]\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/14.Join.html"],["title","Join.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","join-field-type"],["heading","Join field type"],["body","\n\n"],["body","'join' 数据类型是一个特殊字段，用于在相同索引的文档中创建父/子关系"],["body","\n"],["body","“relations” 部分定义了文档中的一组可能的关系，每个关系都是父名称和子名称。父/子关系可以定义如下:"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_id\": {\n        \"type\": \"keyword\"\n      },\n      \"my_join_field\": { \n        \"type\": \"join\",\n        \"relations\": {\n          \"question\": \"answer\"  //Defines a single relation where `question` is parent of `answer`.\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n"],["body","\n"],["body","要使用联接对文档进行索引，必须在 “_source” 中提供关系的名称和文档的可选父级。"],["body","\n"],["body","\n"],["body","\n"],["body","例如，以下示例在 “问题” 上下文中创建了两个 “parent” 文档:"],["body","\n"],["body","\n\n"],["body","PUT my-index-000001/_doc/1?refresh\n{\n  \"my_id\": \"1\",\n  \"text\": \"This is a question\",\n  \"my_join_field\": {\n    \"name\": \"question\" \n  }\n}\n\nPUT my-index-000001/_doc/2?refresh //This document is a `question` document.\n{\n  \"my_id\": \"2\",\n  \"text\": \"This is another question\",\n  \"my_join_field\": {\n    \"name\": \"question\"\n  }\n}\n"],["body","\n"],["body","索引父文档时，您可以选择仅指定关系的名称作为快捷方式，而不是将其封装在普通对象表示法中:"],["body","\n"],["body","PUT my-index-000001/_doc/1?refresh\n{\n  \"my_id\": \"1\",\n  \"text\": \"This is a question\",\n  \"my_join_field\": \"question\" \n}\n\nPUT my-index-000001/_doc/2?refresh\n{\n  \"my_id\": \"2\",\n  \"text\": \"This is another question\",\n  \"my_join_field\": \"question\" //Simpler notation for a parent document just uses the relation name.\n}\n"],["body","\n\n"],["body","索引子文档时，必须在 “_ source” 中添加关系的名称以及文档的父id。"],["body","\n"],["body","必须使用父文档的ID来路由。"],["body","\n\n"],["body","例如，下面的示例显示了如何索引两个 “子” 文档:"],["body","\n"],["body","PUT my-index-000001/_doc/3?routing=1&refresh //The routing value is mandatory because parent and child documents must be indexed on the same shard\n{\n  \"my_id\": \"3\",\n  \"text\": \"This is an answer\",\n  \"my_join_field\": {\n    \"name\": \"answer\", //`answer` is the name of the join for this document\n    \"parent\": \"1\" The parent id of this child document\n  }\n}\n\nPUT my-index-000001/_doc/4?routing=1&refresh //\n{\n  \"my_id\": \"4\",\n  \"text\": \"This is another answer\",\n  \"my_join_field\": {\n    \"name\": \"answer\",\n    \"parent\": \"1\"\n  }\n}\n"],["body","\n"],["headingLink","parent-join-and-performance"],["heading","Parent-join and performance"],["body","\n\n"],["body","Join field 不应像关系数据库中的join那样使用。"],["body","\n"],["body","在Elasticsearch中，良好性能的关键是将数据 de-normalize 为文档。"],["body","\n"],["body","每个联接字段 “has_child” 或 “has_parent” 查询都会给您的查询性能带来很大的负担。它还可以触发 全局序数。"],["body","\n"],["body","它还可以触发 全局序数。"],["body","\n\n"],["body","连接字段有意义的唯一情况是，如果您的数据包含一对多关系，其中一个实体明显超过另一个实体"],["body","\n"],["body","这种情况的一个例子是带有产品和这些产品的报价的用例。在产品报价数量明显超过产品数量的情况下，将产品建模为父文档，将产品报价建模为子文档是有意义的。"],["body","\n"],["headingLink","parent-join-restrictions"],["heading","Parent-join restrictions"],["body","\n\n"],["body","每个索引只允许一个 “join” 字段映射。"],["body","\n"],["body","父文档和子文档必须在同一分片上建立索引"],["body","\n"],["body","这意味着在 获取，[删除](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.13/docs-delete.html)，或 updating 子文档。要提供同一个routeing值"],["body","\n"],["body","一个元素可以有多个子代，但只有一个父代。"],["body","\n"],["body","可以向现有的 “join” 字段添加新的关系。也可以将子元素添加到现有元素，但前提是该元素已经是父元素。"],["body","\n\n"],["headingLink","searching-with-parent-join"],["heading","Searching with parent-join"],["body","\n\n"],["body","\n"],["body","父连接创建一个字段来索引文档中关系的名称"],["body","\n"],["body","\n"],["body","\n"],["body","它还为每个父/子关系创建一个字段。"],["body","\n"],["body","\n"],["body","\n"],["body","字段名是： joib_field_name#parent_field_name"],["body","\n"],["body","\n"],["body","\n"],["body","如是子文档 这个字段包含 parent的 _id。如果文档是父项 则包含文档的ID"],["body","\n"],["body","\n\n"],["body","GET my-index-000001/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": [\"my_id\"]\n}\n"],["body","\n"],["body","Will return:"],["body","\n"],["body","{\n  ...,\n  \"hits\": {\n    \"total\": {\n      \"value\": 4,\n      \"relation\": \"eq\"\n    },\n    \"max_score\": null,\n    \"hits\": [\n      {\n        \"_index\": \"my-index-000001\",\n        \"_type\": \"_doc\",\n        \"_id\": \"1\",\n        \"_score\": null,\n        \"_source\": {\n          \"my_id\": \"1\",\n          \"text\": \"This is a question\",\n          \"my_join_field\": \"question\" //This document belongs to the `question` join \n        },\n        \"sort\": [\n          \"1\"\n        ]\n      },\n      {\n        \"_index\": \"my-index-000001\",\n        \"_type\": \"_doc\",\n        \"_id\": \"2\",\n        \"_score\": null,\n        \"_source\": {\n          \"my_id\": \"2\",\n          \"text\": \"This is another question\",\n          \"my_join_field\": \"question\"       This document belongs to the `question` join   \n        },\n        \"sort\": [\n          \"2\"\n        ]\n      },\n      {\n        \"_index\": \"my-index-000001\",\n        \"_type\": \"_doc\",\n        \"_id\": \"3\",\n        \"_score\": null,\n        \"_routing\": \"1\",\n        \"_source\": {\n          \"my_id\": \"3\",\n          \"text\": \"This is an answer\",\n          \"my_join_field\": {\n            \"name\": \"answer\",                 \n            \"parent\": \"1\"                     \n          }\n        },\n        \"sort\": [\n          \"3\"\n        ]\n      },\n      {\n        \"_index\": \"my-index-000001\",\n        \"_type\": \"_doc\",\n        \"_id\": \"4\",\n        \"_score\": null,\n        \"_routing\": \"1\",\n        \"_source\": {\n          \"my_id\": \"4\",\n          \"text\": \"This is another answer\",\n          \"my_join_field\": {\n            \"name\": \"answer\",\n            \"parent\": \"1\"\n          }\n        },\n        \"sort\": [\n          \"4\"\n        ]\n      }\n    ]\n  }\n}\n"],["body","\n"],["headingLink","parent-join-queries-and-aggregations"],["heading","Parent-join queries and aggregations"],["body","\n\n"],["body","\n"],["body","See the has_child and has_parent queries, the children aggregation, and inner hits for more information."],["body","\n"],["body","\n"],["body","\n"],["body","The value of the join field is accessible in aggregations and scripts, and may be queried with the parent_id query:"],["body","\n"],["body","\n"],["body","\n"],["body","查找某个parent的所有子文档"],["body","\n"],["body","\n\n"],["body","GET my-index-000001/_search\n{\n  \"query\": {\n    \"parent_id\": { \n      \"type\": \"answer\",\n      \"id\": \"1\"\n    }\n  },\n  \"aggs\": {\n    \"parents\": {\n      \"terms\": {\n        \"field\": \"my_join_field#question\", \n        \"size\": 10\n      }\n    }\n  },\n  \"runtime_mappings\": {\n    \"parent\": {\n      \"type\": \"long\",\n      \"script\": \"\"\"\n        emit(Integer.parseInt(doc['my_join_field#question'].value)) \n      \"\"\"\n    }\n  },\n  \"fields\": [\n    { \"field\": \"parent\" }\n  ]\n}\n"],["body","\n\n"],["body","Querying the parent id field (also see the has_parent query and the has_child query)"],["body","\n"],["body","Aggregating on the parent id field (also see the children aggregation)"],["body","\n"],["body","Accessing the parent id field in scripts."],["body","\n\n"],["headingLink","global-ordinals"],["heading","Global ordinals"],["body","\n\n"],["body","\n"],["body","The join field uses global ordinals to speed up joins."],["body","\n"],["body","\n"],["body","\n"],["body","在对碎片进行任何更改后，都需要重建全局序号."],["body","\n"],["body","\n"],["body","\n"],["body","在分片中存储的父id值越多，为 “join” 字段重建全局序号所需的时间就越长。"],["body","\n"],["body","\n"],["body","\n"],["body","默认情况下，全局序号是急切地构建的"],["body","\n"],["body","\n"],["body","\n"],["body","如果索引已更改，则 “join' 字段的全局序号将作为刷新的一部分重建，这可能会为刷新增加大量时间。"],["body","\n"],["body","\n"],["body","\n"],["body","但是，在大多数情况下，这是正确的权衡，否则在使用第一个父联接查询或聚合时会重建全局序号"],["body","\n"],["body","\n"],["body","\n"],["body","这可能会为您的用户带来显著的延迟峰值，通常情况更糟，因为当发生多次写入时，可能会在单个刷新间隔内尝试重建 “join' 字段的多个全局序数。"],["body","\n"],["body","\n\n"],["body","当  join 字段不经常使用并且写入频繁发生时，禁用急切加载可能是有意义的:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_join_field\": {\n        \"type\": \"join\",\n        \"relations\": {\n           \"question\": \"answer\"\n        },\n        \"eager_global_ordinals\": false\n      }\n    }\n  }\n}\n"],["body","\n"],["body","可以按父关系检查全局序数使用的堆数量，如下所示:"],["body","\n"],["body","# Per-index\nGET _stats/fielddata?human&fields=my_join_field#question\n\n# Per-node per-index\nGET _nodes/stats/indices/fielddata?human&fields=my_join_field#question\n"],["body","\n"],["headingLink","multiple-children-per-parent"],["heading","Multiple children per parent"],["body","\n"],["body","也可以为单亲定义多个子:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_join_field\": {\n        \"type\": \"join\",\n        \"relations\": {\n          \"question\": [\"answer\", \"comment\"]  \n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","multiple-levels-of-parent-join"],["heading","Multiple levels of parent join"],["body","\n\n"],["body","不建议使用多个级别的关系来复制关系模型"],["body","\n"],["body","每个级别的关系都会在查询时增加内存和计算方面的开销。"],["body","\n"],["body","如果你关心性能，你应该去规范化你的数据。"],["body","\n\n"],["body","Multiple levels of parent/child:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_join_field\": {\n        \"type\": \"join\",\n        \"relations\": {\n          \"question\": [\"answer\", \"comment\"],  // `question` is parent of `answer` and `comment`\n          \"answer\": \"vote\"  //`answer` is parent of `vote`\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","The mapping above represents the following tree:"],["body","\n"],["body","   question\n    /    \\\n   /      \\\ncomment  answer\n           |\n           |\n          vote\n"],["body","\n"],["body","索引孙子文档需要一个 “路由” 值等于祖父母 (血统的较大父母):"],["body","\n"],["body","PUT my-index-000001/_doc/3?routing=1&refresh //This child document must be on the same shard than its grand-parent and parent\n{\n  \"text\": \"This is a vote\",\n  \"my_join_field\": {\n    \"name\": \"vote\",\n    \"parent\": \"2\"  //The parent id of this document (must points to an `answer` document)\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/15.KeywordType.html"],["title","KeywordType.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","keyword-type-family"],["heading","Keyword type family"],["body","\n"],["body","关键字族包括以下字段类型:"],["body","\n\n"],["body","keyword, 用于结构化内容，例如id，电子邮件地址，主机名，状态代码，邮政编码或标签。"],["body","\n"],["body","constant_keyword 对于始终包含相同值的关键字字段。"],["body","\n"],["body","wildcard 对于非结构化机器生成的内容。“通配符” 类型针对具有大值或高基数的字段进行了优化。"],["body","\n\n"],["body","Keyword fields are often used in sorting, aggregations, and term-level queries, such as term."],["body","\n"],["body","Avoid using keyword fields for full-text search. Use the text field type instead."],["body","\n"],["headingLink","keyword-field-type"],["heading","Keyword field type"],["body","\n"],["body","Below is an example of a mapping for a basic keyword field:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"tags\": {\n        \"type\":  \"keyword\"\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","mapping-numeric-identifiers"],["heading","Mapping numeric identifiers"],["body","\n"],["body","Not all numeric data should be mapped as a numeric field data type."],["body","\n\n"],["body","并非所有数字数据都应映射为 数字 字段数据类型"],["body","\n"],["body","Elasticsearch optimizes numeric fields, such as integer or long, for range queries."],["body","\n"],["body","但是，'keyword' 字段更适合 ['term'](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.13/query-dsl-term-query.html) 和其他 术语级别 查询。"],["body","\n\n"],["body","标识符 (例如ISBN或产品ID) 很少在 “范围” 查询中使用。但是，通常使用术语级查询来检索它们。"],["body","\n"],["body","考虑将数字标识符映射为 “关键词”，如果:"],["body","\n\n"],["body","You don’t plan to search for the identifier data using range queries."],["body","\n"],["body","快速检索很重要. term query searches on keyword fields are often faster than term searches on numeric fields."],["body","\n"],["body","If you’re unsure which to use, you can use a multi-field to map the data as both a keyword and a numeric data type."],["body","\n\n"],["headingLink","parameters-for-basic-keyword-fields"],["heading","Parameters for basic keyword fields"],["body","\n"],["body","\n"],["body","boost"],["body","Mapping field-level query time boosting. Accepts a floating point number, defaults to 1.0."],["body","\n"],["body","doc_values"],["body","Should the field be stored on disk in a column-stride fashion, so that it can later be used for sorting, aggregations, or scripting? Accepts true (default) or false."],["body","\n"],["body","eager_global_ordinals"],["body","Should global ordinals be loaded eagerly on refresh? Accepts true or false (default). Enabling this is a good idea on fields that are frequently used for terms aggregations."],["body","\n"],["body","fields"],["body","Multi-fields allow the same string value to be indexed in multiple ways for different purposes, such as one field for search and a multi-field for sorting and aggregations."],["body","\n"],["body","ignore_above"],["body","Do not index any string longer than this value. Defaults to 2147483647 so that all values would be accepted. Please however note that default dynamic mapping rules create a sub keyword field that overrides this default by setting ignore_above: 256."],["body","\n"],["body","index"],["body","Should the field be searchable? Accepts true (default) or false."],["body","\n"],["body","index_options"],["body","What information should be stored in the index, for scoring purposes. Defaults to docs but can also be set to freqs to take term frequency into account when computing scores."],["body","\n"],["body","norms"],["body","Whether field-length should be taken into account when scoring queries. Accepts true or false (default)."],["body","\n"],["body","null_value"],["body","Accepts a string value which is substituted for any explicit null values. Defaults to null, which means the field is treated as missing. Note that this cannot be set if the script value is used."],["body","\n"],["body","on_script_error"],["body","Defines what to do if the script defined by the script parameter throws an error at indexing time. Accepts fail (default), which will cause the entire document to be rejected, and continue, which will register the field in the document’s _ignored metadata field and continue indexing. This parameter can only be set if the script field is also set."],["body","\n"],["body","script"],["body","If this parameter is set, then the field will index values generated by this script, rather than reading the values directly from the source. If a value is set for this field on the input document, then the document will be rejected with an error. Scripts are in the same format as their runtime equivalent. Values emitted by the script are normalized as usual, and will be ignored if they are longer that the value set on ignore_above."],["body","\n"],["body","store"],["body","Whether the field value should be stored and retrievable separately from the _source field. Accepts true or false (default)."],["body","\n"],["body","similarity"],["body","Which scoring algorithm or similarity should be used. Defaults to BM25."],["body","\n"],["body","normalizer"],["body","How to pre-process the keyword prior to indexing. Defaults to null, meaning the keyword is kept as-is."],["body","\n"],["body","split_queries_on_whitespace"],["body","Whether full text queries should split the input on whitespace when building a query for this field. Accepts true or false (default)."],["body","\n"],["body","meta"],["body","Metadata about the field."],["body","\n\n\n"],["headingLink","constant-keyword-field-type"],["heading","Constant keyword field type"],["body","\n"],["body","Constant keyword is a specialization of the keyword field for the case that all documents in the index have the same value."],["body","\n"],["body","PUT logs-debug\n{\n  \"mappings\": {\n    \"properties\": {\n      \"@timestamp\": {\n        \"type\": \"date\"\n      },\n      \"message\": {\n        \"type\": \"text\"\n      },\n      \"level\": {\n        \"type\": \"constant_keyword\",\n        \"value\": \"debug\"\n      }\n    }\n  }\n}\n"],["body","\n\n"],["body","“constant_keyword” 支持与 “keyword” 字段相同的查询和聚合，但是利用了所有文档每个索引具有相同值的事实，可以更有效地执行查询。"],["body","\n"],["body","既允许提交没有字段值的文档，也允许提交与映射中配置的值相等的文档。以下两个索引请求是等效的:"],["body","\n\n"],["body","POST logs-debug/_doc\n{\n  \"date\": \"2019-12-12\",\n  \"message\": \"Starting up Elasticsearch\",\n  \"level\": \"debug\"\n}\n\nPOST logs-debug/_doc\n{\n  \"date\": \"2019-12-12\",\n  \"message\": \"Starting up Elasticsearch\"\n}\n"],["body","\n\n"],["body","但是，不允许提供与映射中配置的值不同的值。"],["body","\n"],["body","如果映射中没有提供 “值”，则该字段将根据第一个索引文档中包含的值自动配置自身"],["body","\n"],["body","虽然这种行为可能很方便，但请注意，这意味着单个有毒文档如果具有错误的值，可能会导致所有其他文档被拒绝。"],["body","\n"],["body","在提供值之前 (通过映射或来自文档)，该字段上的查询将不匹配任何文档.This includes exists queries."],["body","\n\n"],["body","字段的 “值” 在设置后不能更改。"],["body","\n"],["headingLink","parameters-for-constant-keyword-fields"],["heading","Parameters for constant keyword fields"],["body","\n"],["body","\n"],["body","meta"],["body","Metadata about the field."],["body","\n"],["body","value"],["body","The value to associate with all documents in the index. If this parameter is not provided, it is set based on the first document that gets indexed."],["body","\n\n\n"],["headingLink","wildcard-field-type"],["heading","Wildcard field type"],["body","\n\n"],["body","通配符 字段类型 是keyword 的特化。对于非结构化的 机器生成的 内容 ，使用 grep-like  wildcard and regexp queries 有特别优化"],["body","\n"],["body","“通配符” 类型针对具有大值或高基数的字段进行了优化。"],["body","\n\n"],["body","Mapping unstructured content"],["body","\n\n"],["body","\n"],["body","您可以将包含非结构化内容的字段映射到 “文本” 或关键字族字段。"],["body","\n"],["body","\n"],["body","\n"],["body","最佳字段类型取决于内容的性质以及您计划如何搜索该字段。"],["body","\n"],["body","\n\n"],["body","Use the text field type if:"],["body","\n\n"],["body","内容是人类可读的，例如电子邮件正文或产品描述。"],["body","\n"],["body","您计划在字段中搜索单个单词或短语, such as the brown fox jumped, using full text queries. Elasticsearch analyzes text fields to return the most relevant results for these queries."],["body","\n\n"],["body","Use a keyword family field type if:"],["body","\n\n"],["body","内容是机器生成的，例如日志消息或HTTP请求信息。"],["body","\n"],["body","You plan to search the field for exact full values, such as org.foo.bar, or partial character sequences, such as org.foo.*, using term-level queries."],["body","\n\n"],["body","Choosing a keyword family field type"],["body","\n\n"],["body","\n"],["body","如果选择关键字族字段类型，则可以根据字段值的基数和大小将字段映射为 “关键字” 或 “通配符” 字段"],["body","\n"],["body","\n"],["body","\n"],["body","Use the wildcard type if you plan to regularly search the field using a wildcard or regexp query and meet one of the following criteria:"],["body","\n\n"],["body","\n"],["body","该字段包含超过一百万个唯一值，您计划使用带有前导通配符的模式定期搜索字段，例如 “* foo” 或 “* baz'”。"],["body","\n"],["body","\n"],["body","\n"],["body","该字段包含大于32KB的值，您计划使用任何通配符模式定期搜索字段。"],["body","\n"],["body","\n\n"],["body","\n"],["body","\n"],["body","否则，请使用 “关键字” 字段类型来实现更快的搜索、更快的索引和更低的存储成本。有关深入的比较和决策流程图，请参阅我们的 相关博客文章。"],["body","\n"],["body","\n\n"],["body","Switching from a text field to a keyword field"],["body","\n\n"],["body","\n"],["body","如果您以前使用 “文本” 字段来索引非结构化机器生成的内容，"],["body","\n"],["body","\n"],["body","\n"],["body","you can reindex to update the mapping to a keyword or wildcard field."],["body","\n"],["body","\n"],["body","\n"],["body","我们还建议您更新您的应用程序或工作流程，以替换字段上的任何基于单词的 全文查询 为等效的 [术语级查询](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.13/term-level-queries.html)。"],["body","\n"],["body","\n"],["body","\n"],["body","在内部，“通配符” 字段使用ngram索引整个字段值，并存储完整字符串"],["body","\n"],["body","\n"],["body","\n"],["body","索引用作粗略过滤器，以减少值的数量，然后通过检索和检查完整值来检查。"],["body","\n"],["body","\n"],["body","\n"],["body","此字段特别适合在日志行上运行类似grep的查询"],["body","\n"],["body","\n"],["body","\n"],["body","存储成本通常低于 “关键字” 字段的存储成本，但是完整terms 的精确匹配的搜索速度较慢。"],["body","\n"],["body","\n"],["body","\n"],["body","如果字段值共享许多前缀，例如同一网站的url，则 “通配符” 字段的存储成本可能高于等效的 “关键字” 字段。"],["body","\n"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_wildcard\": {\n        \"type\": \"wildcard\"\n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/1\n{\n  \"my_wildcard\" : \"This string can be quite lengthy\"\n}\n\nGET my-index-000001/_search\n{\n  \"query\": {\n    \"wildcard\": {\n      \"my_wildcard\": {\n        \"value\": \"*quite*lengthy\"\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","parameters-for-wildcard-fields"],["heading","Parameters for wildcard fields"],["body","\n"],["body","\n"],["body","null_value"],["body","Accepts a string value which is substituted for any explicit null values. Defaults to null, which means the field is treated as missing."],["body","\n"],["body","ignore_above"],["body","Do not index any string longer than this value. Defaults to 2147483647 so that all values would be accepted."],["body","\n\n\n"],["headingLink","limitations"],["heading","Limitations"],["body","\n\n"],["body","“通配符” 字段像关键字字段一样是未分词的，因此不支持依赖单词位置的查询，例如短语查询。"],["body","\n"],["body","当运行 “通配符” 查询时，任何 “重写” 参数都将被忽略。得分总是恒定的得分。"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/7.DateNanoseconds.html"],["title","DateNanoseconds.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","date-nanoseconds-field-type"],["heading","Date nanoseconds field type"],["body","\n\n"],["body","此数据是对日期数据类型的添加，但是两者有很大的区别"],["body","\n"],["body","date数据类型 最多支持 毫秒精度"],["body","\n"],["body","date_nanos 支持 纳秒精度，这将其日期范围从大约1970限制到2262"],["body","\n"],["body","纳秒的查询会在内部转换为 long类型的 范围查询，并且聚合和存储字段的结果会根据与该字段关联的日期格式转换回字符串。"],["body","\n"],["body","可以自定义日期格式，但是如果未指定 “格式”，则它将使用默认值:"],["body","\n\n"],["body","    \"strict_date_optional_time_nanos||epoch_millis\"\n"],["body","\n"],["body","For instance:"],["body","\n"],["body","PUT my-index-000001?include_type_name=true\n{\n  \"mappings\": {\n    \"_doc\": {\n      \"properties\": {\n        \"date\": {//The `date` field uses the default `format`.\n          \"type\": \"date_nanos\" \n        }\n      }\n    }\n  }\n}\n\nPUT my-index-000001/_bulk?refresh\n{ \"index\" : { \"_id\" : \"1\" } }\n{ \"date\": \"2015-01-01\" }  //This document uses a plain date.\n{ \"index\" : { \"_id\" : \"2\" } }\n{ \"date\": \"2015-01-01T12:10:30.123456789Z\" } //This document includes a time.\n{ \"index\" : { \"_id\" : \"3\" } }\n{ \"date\": 1420070400000 } //This document uses milliseconds-since-the-epoch.\n\nGET my-index-000001/_search\n{\n  \"sort\": { \"date\": \"asc\"},   Note that the `sort` values that are returned are all in nanoseconds-since-the-epoch.\n  \"runtime_mappings\": {\n    \"date_has_nanos\": { \n      \"type\": \"boolean\",\n      \"script\": \"emit(doc['date'].value.nano != 0)\"  Use `.nano` in scripts to return the nanosecond component of the date.\n    }\n  },\n  \"fields\": [\n    {\n      \"field\": \"date\", \n      \"format\": \"strict_date_optional_time_nanos\" \n    },\n    {\n      \"field\": \"date_has_nanos\" \n    }\n  ]\n}\n"],["body","\n\n"],["body","\n"],["body","You can specify the format when fetching data using the fields parameter. Use strict_date_optional_time_nanos or you’ll get a rounded result."],["body","\n"],["body","\n"],["body","\n"],["body","You can also specify multiple date formats separated by ||. The same mapping parameters than with the date field can be used."],["body","\n"],["body","\n"],["body","\n"],["body","Date nanoseconds will accept numbers with a decimal point like {\"date\": 1618249875.123456} but there are some cases (#70085) where we’ll lose precision on those dates so should avoid them."],["body","\n"],["body","\n\n"],["headingLink","limitations"],["heading","Limitations"],["body","\n"],["body","Aggregations are still on millisecond resolution, even when using a date_nanos field. This limitation also affects transforms."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/23.RankFeatures.html"],["title","RankFeatures.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","rank-features-field-type"],["heading","Rank features field type"],["body","\n\n"],["body","\n"],["body","“rank_features” 字段可以对数字特征向量进行索引，以便它们以后可以用于使用 “rank_features” 查询来 boost documents"],["body","\n"],["body","\n"],["body","\n"],["body","它类似于 [“rank_feature”](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.13/rank-feature.html) 数据类型，但当 features 列表稀疏时更适合，因此将一个字段添加到每个映射中并不合理。"],["body","\n"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"topics\": {\n        \"type\": \"rank_features\" \n      },\n      \"negative_reviews\" : {\n        \"type\": \"rank_features\", \n        \"positive_score_impact\": false  //Rank features that correlate negatively with the score need to declare it\n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/1\n{\n  \"topics\": { \n    \"politics\": 20, //Rank features fields must be a hash with string keys and strictly positive numeric values\n    \"economics\": 50.8\n  },\n  \"negative_reviews\": {\n    \"1star\": 10,\n    \"2star\": 100\n  }\n}\n\nPUT my-index-000001/_doc/2\n{\n  \"topics\": {\n    \"politics\": 5.2,\n    \"sports\": 80.1\n  },\n  \"negative_reviews\": {\n    \"1star\": 1,\n    \"2star\": 10\n  }\n}\n\nGET my-index-000001/_search\n{\n  \"query\": { \n    \"rank_feature\": {Rank features fields must use the `rank_features` field type\n      \"field\": \"topics.politics\" //This query ranks documents by how much they are about the \"politics\" topic.\n    }\n  }\n}\n\nGET my-index-000001/_search\n{\n  \"query\": { \n    \"rank_feature\": {\n      \"field\": \"negative_reviews.1star\"//This query ranks documents inversely to the number of \"1star\" reviews they received.\n    }\n  }\n}\n"],["body","\n\n"],["body","\n"],["body","rank_features fields only support single-valued features and strictly positive values. Multi-valued fields and zero or negative values will be rejected."],["body","\n"],["body","\n"],["body","\n"],["body","rank_features fields do not support sorting or aggregating and may only be queried using rank_feature queries."],["body","\n"],["body","\n"],["body","\n"],["body","rank_features fields only preserve 9 significant bits for the precision, which translates to a relative error of about 0.4%."],["body","\n"],["body","\n"],["body","\n"],["body","Rank features that correlate negatively with the score should set positive_score_impact to false (defaults to true). This will be used by the rank_feature query to modify the scoring formula in such a way that the score decreases with the value of the feature instead of increasing."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/20.Point.html"],["title","Point.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","point-field-type"],["heading","Point field type"],["body","\n\n"],["body","\n"],["body","“Point” 数据类型有助于索引和搜索属于二维平面坐标系的任意 “x，y” pairs"],["body","\n"],["body","\n"],["body","\n"],["body","You can query documents using this type using shape Query."],["body","\n"],["body","\n\n"],["body","可以通过四种方式指定点，如下所示:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"location\": {\n        \"type\": \"point\"\n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/1\n{\n  \"text\": \"Point as an object\",\n  \"location\": { \n    \"x\": 41.12,\n    \"y\": -71.34\n  }\n}\n\nPUT my-index-000001/_doc/2\n{\n  \"text\": \"Point as a string\",\n  \"location\": \"41.12,-71.34\" \n}\n\n\nPUT my-index-000001/_doc/4\n{\n  \"text\": \"Point as an array\",\n  \"location\": [41.12, -71.34] \n}\n\nPUT my-index-000001/_doc/5\n{\n  \"text\": \"Point as a WKT POINT primitive\",\n  \"location\" : \"POINT (41.12 -71.34)\" \n}\n"],["body","\n"],["body","提供给索引器的坐标是单精度浮点值，因此该字段可确保java虚拟机提供的相同精度 (通常为 “1E-38”)。"],["body","\n"],["headingLink","parameters-for-point-fields"],["heading","Parameters for point fields"],["body","\n"],["body","\n"],["body","ignore_malformed"],["body","If true, malformed points are ignored. If false (default), malformed points throw an exception and reject the whole document."],["body","\n"],["body","ignore_z_value"],["body","If true (default) three dimension points will be accepted (stored in source) but only x and y values will be indexed; the third dimension is ignored. If false, points containing any more than x and y (two dimensions) values throw an exception and reject the whole document."],["body","\n"],["body","null_value"],["body","Accepts an point value which is substituted for any explicit null values. Defaults to null, which means the field is treated as missing."],["body","\n\n\n"],["headingLink","sorting-and-retrieving-points"],["heading","Sorting and retrieving points"],["body","\n"],["body","目前无法直接对点进行排序或检索其字段。“point” 值只能通过 “_ source” 字段检索。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/3.Arrays.html"],["title","Arrays.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","arrays"],["heading","Arrays"],["body","\n"],["body","在Elasticsearch中，没有专用的 数组数据类型。默认情况下，任何字段都可以包含零个或多个值，但是，数组中的所有值必须具有相同的数据类型。例如:"],["body","\n\n"],["body","an array of strings: [ \"one\", \"two\" ]"],["body","\n"],["body","an array of integers: [ 1, 2 ]"],["body","\n"],["body","an array of arrays: [ 1, [ 2, 3 ]] which is the equivalent of [ 1, 2, 3 ]"],["body","\n"],["body","an array of objects: [ { \"name\": \"Mary\", \"age\": 12 }, { \"name\": \"John\", \"age\": 10 }]"],["body","\n\n"],["headingLink","arrays-of-objects"],["heading","Arrays of objects"],["body","\n\n"],["body","\n"],["body","对象数组不能像你期望的那样工作:"],["body","\n"],["body","\n"],["body","\n"],["body","不能查询数组中的每个独立对象，如果要这样查询，详见 nested data type"],["body","\n"],["body","\n"],["body","\n"],["body","动态添加字段时，数组中的第一个值确定字段 “类型”。所有后续值必须具有相同的数据类型，或者至少可以将 强制 后续值转换为相同的数据类型。"],["body","\n"],["body","\n\n"],["body","PUT my-index-000001/_doc/1\n{\n  \"message\": \"some arrays in this document...\",\n  \"tags\":  [ \"elasticsearch\", \"wow\" ],  //The `tags` field is dynamically added as a `string` field.\n  \"lists\": [  //The `lists` field is dynamically added as an `object` field.\n    {\n      \"name\": \"prog_list\",\n      \"description\": \"programming list\"\n    },\n    {\n      \"name\": \"cool_list\",\n      \"description\": \"cool stuff list\"\n    }\n  ]\n}\n\nPUT my-index-000001/_doc/2 \n{\n  \"message\": \"no arrays in this document...\",\n  \"tags\":  \"elasticsearch\", //The second document contains no arrays, but can be indexed into the same fields.\n  \"lists\": {\n    \"name\": \"prog_list\",\n    \"description\": \"programming list\"\n  }\n}\n\n//The query looks for `elasticsearch` in the `tags` field, and matches both documents.\nGET my-index-000001/_search\n{\n  \"query\": {\n    \"match\": {\n      \"tags\": \"elasticsearch\" \n    }\n  }\n}\n"],["body","\n"],["body","Multi-value fields and the inverted index"],["body","\n\n"],["body","所有字段类型都支持开箱即用的多值字段的事实是Lucene起源的结果"],["body","\n"],["body","Lucene被设计为全文搜索引擎。为了能够在一大块文本中搜索单个单词，Lucene将文本分词为单个术语，并将每个术语分别添加到倒排索引中。"],["body","\n"],["body","这意味着即使是简单的文本字段也必须能够默认支持多个值。当添加其他数据类型 (如数字和日期) 时，它们使用与字符串相同的数据结构，因此开箱即用"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/8.DenseVector.html"],["title","DenseVector.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","dense-vector-field-type"],["heading","Dense vector field type"],["body","\n\n"],["body","“dense_vector” 字段存储浮点值的密集向量。向量中的最大维数不应超过2048。"],["body","\n"],["body","“dense_vector” 字段是单值字段。"],["body","\n"],["body","dense_vector` fields do not support querying, sorting or aggregating."],["body","\n"],["body","They can only be accessed in scripts through the dedicated vector functions."],["body","\n\n"],["body","您将密集向量索引为浮点数数组。"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 3  \n      },\n      \"my_text\" : {\n        \"type\" : \"keyword\"\n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/1\n{\n  \"my_text\" : \"text1\",\n  \"my_vector\" : [0.5, 10, 6]\n}\n\nPUT my-index-000001/_doc/2\n{\n  \"my_text\" : \"text2\",\n  \"my_vector\" : [-0.5, 10, 10]\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/9.Flattened.html"],["title","Flattened.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","flattened-field-type"],["heading","Flattened field type"],["body","\n\n"],["body","默认情况下，对象中的每个子字段都分别映射和索引"],["body","\n"],["body","如果预先不知道子字段的名称或类型，则它们将被 动态映射。"],["body","\n"],["body","“flattened” 类型提供了一种替代方法，整个对象被映射为 单字段"],["body","\n"],["body","给定一个对象，“扁平化” 映射将解析出其叶值并将其作为关键字索引到一个字段中，然后可以通过简单的查询和聚合搜索对象的内容。"],["body","\n"],["body","这种数据类型对于索引具有大量或未知数量的唯一键的对象非常有用，仅为整个JSON对象创建了一个字段映射，这可以帮助防止 [mappings爆炸](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/mapping.html # mapping-limit-settings) 有太多不同的字段映射。"],["body","\n"],["body","另一方面，扁平化的对象字段在搜索功能方面存在权衡，只允许基本查询，不支持数字范围查询或突出显示，详见 Supported operations section."],["body","\n"],["body","“扁平化” 映射类型不应 用于索引所有文档内容，因为它将所有值视为关键字，并且不提供完整的搜索功能。默认策略（每个子字段在映射中都有自己的条目）在大多数情况下都工作得很好，"],["body","\n\n"],["body","An flattened object field can be created as follows:"],["body","\n"],["body","PUT bug_reports\n{\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\"\n      },\n      \"labels\": {\n        \"type\": \"flattened\"\n      }\n    }\n  }\n}\n\nPOST bug_reports/_doc/1\n{\n  \"title\": \"Results are not sorted correctly.\",\n  \"labels\": {\n    \"priority\": \"urgent\",\n    \"release\": [\"v1.2.5\", \"v1.3.0\"],\n    \"timestamp\": {\n      \"created\": 1541458026,\n      \"closed\": 1541457010\n    }\n  }\n}\n"],["body","\n\n"],["body","在索引过程中，为JSON对象中的每个叶值创建词项"],["body","\n"],["body","这些值被索引为字符串关键字"],["body","\n"],["body","查询顶级 'flattened' 字段会搜索对象中的所有叶值:"],["body","\n\n"],["body","POST bug_reports/_search\n{\n  \"query\": {\n    \"term\": {\"labels\": \"urgent\"}\n  }\n}\n"],["body","\n"],["body","要查询展平对象中的特定键，请使用对象点表示法:"],["body","\n"],["body","POST bug_reports/_search\n{\n  \"query\": {\n    \"term\": {\"labels.release\": \"v1.3.0\"}\n  }\n}\n"],["body","\n"],["headingLink","supported-operations"],["heading","Supported operations"],["body","\n\n"],["body","\n"],["body","由于索引的方式相似，“flattened” 字段与 “关键字” 字段共享许多相同的映射和搜索功能。"],["body","\n"],["body","\n"],["body","\n"],["body","Currently, flattened object fields can be used with the following query types:"],["body","\n\n"],["body","\n"],["body","term, terms, and terms_set"],["body","\n"],["body","\n"],["body","\n"],["body","prefix"],["body","\n"],["body","\n"],["body","\n"],["body","range"],["body","\n"],["body","\n"],["body","\n"],["body","match and multi_match"],["body","\n"],["body","\n"],["body","\n"],["body","query_string and simple_query_string"],["body","\n"],["body","\n"],["body","\n"],["body","exists"],["body","\n"],["body","\n\n"],["body","\n"],["body","\n"],["body","查询时，无法使用通配符引用字段键，如  { \"term\": {\"labels.time*\": 1541457010}}."],["body","\n"],["body","\n"],["body","\n"],["body","请注意，所有查询 (包括 'range') 都将值视为字符串关键字。“扁平化” 字段不支持Highlighting"],["body","\n"],["body","\n"],["body","\n"],["body","可以在扁平的对象字段上进行排序，并执行简单的关键字样式聚合，例如 “terms”"],["body","\n"],["body","\n"],["body","\n"],["body","与查询一样，没有对数字的特殊支持，JSON对象中的所有值都被视为关键字，。排序时，这意味着值是按字典比较的。"],["body","\n"],["body","\n"],["body","\n"],["body","当前无法存储展平的对象字段。无法在映射中指定 'store' 参数"],["body","\n"],["body","\n\n"],["headingLink","retrieving-flattened-fields"],["heading","Retrieving flattened fields"],["body","\n\n"],["body","\n"],["body","可以使用 [fields参数](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/search-fields.html # search-fields-param) 检索字段值和具体子字段。"],["body","\n"],["body","\n"],["body","\n"],["body","由于 “扁平化” 字段将具有潜在许多子字段的整个对象映射为一个字段，因此响应包含来自 “_ source” 的未更改结构。"],["body","\n"],["body","\n"],["body","\n"],["body","但是，可以通过在请求中显式指定单个子字段来获取它们。这仅适用于具体路径，但不使用通配符:"],["body","\n"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"flattened_field\": {\n        \"type\": \"flattened\"\n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/1?refresh=true\n{\n  \"flattened_field\" : {\n    \"subfield\" : \"value\"\n  }\n}\n\nPOST my-index-000001/_search\n{\n  \"fields\": [\"flattened_field.subfield\"],\n  \"_source\": false\n}\n"],["body","\n"],["body","{\n  \"took\": 2,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": {\n      \"value\": 1,\n      \"relation\": \"eq\"\n    },\n    \"max_score\": 1.0,\n    \"hits\": [{\n      \"_index\": \"my-index-000001\",\n      \"_type\" : \"_doc\",\n      \"_id\": \"1\",\n      \"_score\": 1.0,\n      \"fields\": {\n        \"flattened_field.subfield\" : [ \"value\" ]\n      }\n    }]\n  }\n}\n"],["body","\n\n"],["body","可以使用  Painless script 获取子字段的值。与其使用  doc['<field_name>'].value ，不如使用 doc['<field_name>.<sub-field_name>'].value."],["body","\n"],["body","例如：doc['labels.release'].value."],["body","\n"],["body","例如，假设您的映射包含两个字段，其中一个是 'flattened' 类型:"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\"\n      },\n      \"labels\": {\n        \"type\": \"flattened\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","Index a few documents containing your mapped fields. The labels field has three sub-fields:"],["body","\n"],["body","POST /my-index-000001/_bulk?refresh\n{\"index\":{}}\n{\"title\":\"Something really urgent\",\"labels\":{\"priority\":\"urgent\",\"release\":[\"v1.2.5\",\"v1.3.0\"],\"timestamp\":{\"created\":1541458026,\"closed\":1541457010}}}\n{\"index\":{}}\n{\"title\":\"Somewhat less urgent\",\"labels\":{\"priority\":\"high\",\"release\":[\"v1.3.0\"],\"timestamp\":{\"created\":1541458026,\"closed\":1541457010}}}\n{\"index\":{}}\n{\"title\":\"Not urgent\",\"labels\":{\"priority\":\"low\",\"release\":[\"v1.2.0\"],\"timestamp\":{\"created\":1541458026,\"closed\":1541457010}}}\n"],["body","\n"],["body","Because labels is a flattened field type, the entire object is mapped as a single field. To retrieve values from this sub-field in a Painless script, use the doc['<field_name>.<sub-field_name>'].value format."],["body","\n"],["body","\"script\": {\n  \"source\": \"\"\"\n    if (doc['labels.release'].value.equals('v1.3.0'))\n    {emit(doc['labels.release'].value)}\n    else{emit('Version mismatch')}\n  \"\"\"\n"],["body","\n"],["headingLink","parameters-for-flattened-object-fields"],["heading","Parameters for flattened object fields"],["body","\n"],["body","The following mapping parameters are accepted:"],["body","\n"],["body","\n"],["body","boost"],["body","Mapping field-level query time boosting. Accepts a floating point number, defaults to 1.0."],["body","\n"],["body","depth_limit"],["body","The maximum allowed depth of the flattened object field, in terms of nested inner objects. If a flattened object field exceeds this limit, then an error will be thrown. Defaults to 20. Note that depth_limit can be updated dynamically through the update mapping API."],["body","\n"],["body","doc_values"],["body","Should the field be stored on disk in a column-stride fashion, so that it can later be used for sorting, aggregations, or scripting? Accepts true (default) or false."],["body","\n"],["body","eager_global_ordinals"],["body","Should global ordinals be loaded eagerly on refresh? Accepts true or false (default). Enabling this is a good idea on fields that are frequently used for terms aggregations."],["body","\n"],["body","ignore_above"],["body","Leaf values longer than this limit will not be indexed. By default, there is no limit and all values will be indexed. Note that this limit applies to the leaf values within the flattened object field, and not the length of the entire field."],["body","\n"],["body","index"],["body","Determines if the field should be searchable. Accepts true (default) or false."],["body","\n"],["body","index_options"],["body","What information should be stored in the index for scoring purposes. Defaults to docs but can also be set to freqs to take term frequency into account when computing scores."],["body","\n"],["body","null_value"],["body","A string value which is substituted for any explicit null values within the flattened object field. Defaults to null, which means null sields are treated as if it were missing."],["body","\n"],["body","similarity"],["body","Which scoring algorithm or similarity should be used. Defaults to BM25."],["body","\n"],["body","split_queries_on_whitespace"],["body","Whether full text queries should split the input on whitespace when building a query for this field. Accepts true or false (default)."],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/README.html"],["title","FieldDataTypes - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","field-data-types"],["heading","Field data types"],["body","\n\n"],["body","每个字段都有字段数据类型。"],["body","\n"],["body","Filed types 是按 family 分类的。在同一个 family 的数据类型 提供同样的查询功能。但是不同的空间使用率、性能特性等"],["body","\n"],["body","目前唯一的  family 是 keyword 包含  keyword、 constant_keyword、wildcard 其他的family  只有一个数据类型"],["body","\n\n"],["headingLink","common-types"],["heading","Common types"],["body","\n\n"],["body","\n"],["body","binary"],["body","\n"],["body","Binary value encoded as a Base64 string."],["body","\n"],["body","\n"],["body","\n"],["body","boolean"],["body","\n"],["body","true and false values."],["body","\n"],["body","\n"],["body","\n"],["body","Keywords"],["body","\n"],["body","The keyword family, including keyword, constant_keyword, and wildcard."],["body","\n"],["body","\n"],["body","\n"],["body","Numbers"],["body","\n"],["body","Numeric types, such as long and double, used to express amounts."],["body","\n"],["body","\n"],["body","\n"],["body","Dates"],["body","\n"],["body","Date types, including date and date_nanos."],["body","\n"],["body","\n"],["body","\n"],["body","alias"],["body","\n"],["body","Defines an alias for an existing field."],["body","\n"],["body","\n\n"],["headingLink","objects-and-relational-types"],["heading","Objects and relational types"],["body","\n\n"],["body","\n"],["body","object"],["body","\n"],["body","A JSON object."],["body","\n"],["body","\n"],["body","\n"],["body","flattened"],["body","\n"],["body","作为单个字段值的整个JSON对象。"],["body","\n"],["body","\n"],["body","\n"],["body","nested"],["body","\n"],["body","一个JSON对象，保留其子字段之间的关系。"],["body","\n"],["body","\n"],["body","\n"],["body","join"],["body","\n"],["body","为同一索引中的文档定义父/子关系。"],["body","\n"],["body","\n\n"],["headingLink","structured-data-types"],["heading","Structured data types"],["body","\n\n"],["body","\n"],["body","Range"],["body","\n"],["body","Range types, such as long_range, double_range, date_range, and ip_range."],["body","\n"],["body","\n"],["body","\n"],["body","ip"],["body","\n"],["body","IPv4 and IPv6 addresses."],["body","\n"],["body","\n"],["body","\n"],["body","version"],["body","\n"],["body","Software versions. Supports Semantic Versioning precedence rules."],["body","\n"],["body","\n"],["body","\n"],["body","murmur3"],["body","\n"],["body","Compute and stores hashes of values."],["body","\n"],["body","\n\n"],["headingLink","aggregate-data-types"],["heading","Aggregate data types"],["body","\n\n"],["body","\n"],["body","aggregate_metric_double"],["body","\n"],["body","Pre-aggregated metric values."],["body","\n"],["body","\n"],["body","\n"],["body","histogram"],["body","\n"],["body","Pre-aggregated numerical values in the form of a histogram."],["body","\n"],["body","\n\n"],["headingLink","text-search-types"],["heading","Text search types"],["body","\n\n"],["body","\n"],["body","text"],["body","\n"],["body","Analyzed, unstructured text."],["body","\n"],["body","\n"],["body","\n"],["body","annotated-text"],["body","\n"],["body","Text containing special markup. Used for identifying named entities."],["body","\n"],["body","\n"],["body","\n"],["body","completion"],["body","\n"],["body","Used for auto-complete suggestions."],["body","\n"],["body","\n"],["body","\n"],["body","search_as_you_type"],["body","\n"],["body","text-like type for as-you-type completion."],["body","\n"],["body","\n"],["body","\n"],["body","token_count"],["body","\n"],["body","A count of tokens in a text."],["body","\n"],["body","\n\n"],["headingLink","document-ranking-types"],["heading","Document ranking types"],["body","\n\n"],["body","\n"],["body","dense_vector"],["body","\n"],["body","Records dense vectors of float values."],["body","\n"],["body","\n"],["body","\n"],["body","sparse_vector\nsparse_vector是一种稀疏向量，也就是说，它的元素大部分都是零，比如[1, 0, 0, 0, 0, 0, 0, 2, 0]\n1   存储所有的零会浪费内存，所以通常用字典来记录非零元素的位置和值 1 稀疏向量可以用于表示高维度但数据稀疏的情况，比如文本或图像  2"],["body","\n"],["body","\n"],["body","\n"],["body","rank_feature"],["body","\n"],["body","\n\n"],["body","rank_feature是一种查询类型，用于根据rank_feature或rank_features字段的数值来提升文档的相关性得分12  rank_feature查询通常用在bool查询的should子句中，这样它的相关性得分就可以和bool查询的其他得分相加12rank_feature或rank_features字段可以存储数值型的特征向量，用于表示文档的某些特征，比如点击率、点赞数等34rank_feature字段只支持单值和正值，而rank_features字段支持多值和稀疏向量35"],["body","\n"],["body","需要搭配 rank_feature query 语句使用，该语句可以根据特征数值提高文档的相关性分数。"],["body","\n\n"],["body","rank_features\n相比较 rank_feature 可以保存多值"],["body","\n\n"],["headingLink","spatial-data-types"],["heading","Spatial data types"],["body","\n\n"],["body","\n"],["body","geo_point"],["body","\n"],["body","Latitude and longitude points."],["body","\n"],["body","\n"],["body","\n"],["body","geo_shape"],["body","\n"],["body","Complex shapes, such as polygons."],["body","\n"],["body","\n"],["body","\n"],["body","point"],["body","\n"],["body","Arbitrary cartesian points.\npoint 和shape 都是用来存储和查询二维平面坐标系的字段，但有以下区别："],["body","\n"],["body","\n"],["body","\n"],["body","point 只能存储一个坐标点，shape 可以存储多种形状，如点集、线、多边形等。"],["body","\n"],["body","\n"],["body","\n"],["body","point 和 shape 的用法类似于 geo_point 和 geo_shape ，但不依赖于地理位置或投影。"],["body","\n"],["body","\n"],["body","\n"],["body","point 可以用于计算距离、排序、打分和聚合，shape 不能。"],["body","\n"],["body","\n"],["body","\n"],["body","point 和 shape 都需要搭配 shape query 语句使用，该语句可以根据坐标或形状匹配文档。"],["body","\n"],["body","\n"],["body","\n"],["body","shape"],["body","\n"],["body","Arbitrary cartesian geometries."],["body","\n"],["body","\n\n"],["headingLink","other-types"],["heading","Other types"],["body","\n\n"],["body","\n"],["body","percolator"],["body","\n"],["body","Indexes queries written in Query DSL."],["body","\n"],["body","\n\n"],["headingLink","arrays"],["heading","Arrays"],["body","\n\n"],["body","在Elasticsearch中，数组不需要专用的字段数据类型"],["body","\n"],["body","默认情况下，任何字段都可以包含零个或多个值"],["body","\n"],["body","however, all values in the array must be of the same field type. See Arrays."],["body","\n\n"],["headingLink","multi-fields"],["heading","Multi-fields"],["body","\n\n"],["body","为了不同的目的，以不同的方式对同一个字段进行索引通常是有用的"],["body","\n"],["body","For instance, a string field could be mapped as a text field for full-text search, and as a keyword field for sorting or aggregations."],["body","\n"],["body","Alternatively, you could index a text field with the standard analyzer, the english analyzer, and the french analyzer."],["body","\n"],["body","This is the purpose of multi-fields. Most field types support multi-fields via the fields parameter."],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/25.Shape.html"],["title","Shape.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","shape-field-type"],["heading","Shape field type"],["body","\n\n"],["body","\n"],["body","“Shape” 数据类型有助于对任意 “x，y” 笛卡尔形状 (例如矩形和多边形) 进行索引和搜索。它可用于索引和查询坐标落在二维平面坐标系中的几何形状。"],["body","\n"],["body","\n"],["body","\n"],["body","You can query documents using this type using shape Query."],["body","\n"],["body","\n\n"],["headingLink","mapping-options"],["heading","Mapping Options"],["body","\n"],["body","Like the geo_shape field type, the shape field mapping maps GeoJSON or Well-Known Text (WKT) geometry objects to the shape type."],["body","\n"],["body","要启用它，用户必须将字段显式映射到形状类型。"],["body","\n"],["body","Option"],["body","Description"],["body","Default"],["body","\n"],["body","orientation"],["body","Optionally define how to interpret vertex order for polygons / multipolygons. This parameter defines one of two coordinate system rules (Right-hand or Left-hand) each of which can be specified in three different ways. 1. Right-hand rule: right, ccw, counterclockwise, 2. Left-hand rule: left, cw, clockwise. The default orientation (counterclockwise) complies with the OGC standard which defines outer ring vertices in counterclockwise order with inner ring(s) vertices (holes) in clockwise order. Setting this parameter in the geo_shape mapping explicitly sets vertex order for the coordinate list of a geo_shape field but can be overridden in each individual GeoJSON or WKT document."],["body","ccw"],["body","\n"],["body","ignore_malformed"],["body","If true, malformed GeoJSON or WKT shapes are ignored. If false (default), malformed GeoJSON and WKT shapes throw an exception and reject the entire document."],["body","false"],["body","\n"],["body","ignore_z_value"],["body","If true (default) three dimension points will be accepted (stored in source) but only latitude and longitude values will be indexed; the third dimension is ignored. If false, geo-points containing any more than latitude and longitude (two dimensions) values throw an exception and reject the whole document."],["body","true"],["body","\n"],["body","coerce"],["body","If true unclosed linear rings in polygons will be automatically closed."],["body","false"],["body","\n\n\n"],["headingLink","indexing-approach"],["heading","Indexing approach"],["body","\n"],["body","Like geo_shape, the shape field type is indexed by decomposing geometries into a triangular mesh and indexing each triangle as a 7 dimension point in a BKD tree."],["body","\n\n"],["body","像 “geo_shape” 一样，“shape” 字段类型是通过将几何分解为三角形网格并将每个三角形索引为BKD树中的7维点来索引的"],["body","\n"],["body","提供给索引器的坐标是单精度浮点值，因此该字段可确保java虚拟机提供的相同精度 (通常为 “1E-38”)。"],["body","\n"],["body","对于多边形/多多边形，tessellator 的性能主要取决于定义几何形状的顶点数量。"],["body","\n\n"],["body","IMPORTANT NOTES"],["body","\n"],["body","CONTAINS relation query - shape queries with relation defined as contains are supported for indices created with ElasticSearch 7.5.0 or higher."],["body","\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","PUT /example\n{\n  \"mappings\": {\n    \"properties\": {\n      \"geometry\": {\n        \"type\": \"shape\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","此映射定义将几何字段映射到形状类型"],["body","\n"],["body","索引器对顶点值使用单精度浮点，因此可以确保精度与java虚拟机提供的 “浮点” 值相同 (通常为1E-38)。"],["body","\n"],["headingLink","input-structure"],["heading","Input Structure"],["body","\n"],["body","Shapes can be represented using either the GeoJSON or Well-Known Text (WKT) format. The following table provides a mapping of GeoJSON and WKT to Elasticsearch types:"],["body","\n"],["body","GeoJSON Type"],["body","WKT Type"],["body","Elasticsearch Type"],["body","Description"],["body","\n"],["body","Point"],["body","POINT"],["body","point"],["body","A single x, y coordinate."],["body","\n"],["body","LineString"],["body","LINESTRING"],["body","linestring"],["body","An arbitrary line given two or more points."],["body","\n"],["body","Polygon"],["body","POLYGON"],["body","polygon"],["body","A closed polygon whose first and last point must match, thus requiring n + 1 vertices to create an n-sided polygon and a minimum of 4 vertices."],["body","\n"],["body","MultiPoint"],["body","MULTIPOINT"],["body","multipoint"],["body","An array of unconnected, but likely related points."],["body","\n"],["body","MultiLineString"],["body","MULTILINESTRING"],["body","multilinestring"],["body","An array of separate linestrings."],["body","\n"],["body","MultiPolygon"],["body","MULTIPOLYGON"],["body","multipolygon"],["body","An array of separate polygons."],["body","\n"],["body","GeometryCollection"],["body","GEOMETRYCOLLECTION"],["body","geometrycollection"],["body","A shape collection similar to the multi* shapes except that multiple types can coexist (e.g., a Point and a LineString)."],["body","\n"],["body","N/A"],["body","BBOX"],["body","envelope"],["body","A bounding rectangle, or envelope, specified by specifying only the top left and bottom right points."],["body","\n\n\n"],["body","For all types, both the inner type and coordinates fields are required."],["body","\n"],["body","In GeoJSON and WKT, and therefore Elasticsearch, the correct coordinate order is (X, Y) within coordinate arrays. This differs from many Geospatial APIs (e.g., geo_shape) that typically use the colloquial latitude, longitude (Y, X) ordering."],["body","\n"],["headingLink","point"],["heading","Point"],["body","\n"],["body","A point is a single coordinate in cartesian x, y space. It may represent the location of an item of interest in a virtual world or projected space. The following is an example of a point in GeoJSON."],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : {\n    \"type\" : \"point\",\n    \"coordinates\" : [-377.03653, 389.897676]\n  }\n}\n"],["body","\n"],["body","The following is an example of a point in WKT:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : \"POINT (-377.03653 389.897676)\"\n}\n"],["body","\n"],["headingLink","linestring"],["heading","LineString"],["body","\n"],["body","A linestring defined by an array of two or more positions. By specifying only two points, the linestring will represent a straight line. Specifying more than two points creates an arbitrary path. The following is an example of a LineString in GeoJSON."],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : {\n    \"type\" : \"linestring\",\n    \"coordinates\" : [[-377.03653, 389.897676], [-377.009051, 389.889939]]\n  }\n}\n"],["body","\n"],["body","The following is an example of a LineString in WKT:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : \"LINESTRING (-377.03653 389.897676, -377.009051 389.889939)\"\n}\n"],["body","\n"],["headingLink","polygon"],["heading","Polygon"],["body","\n"],["body","A polygon is defined by a list of a list of points. The first and last points in each (outer) list must be the same (the polygon must be closed). The following is an example of a Polygon in GeoJSON."],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : {\n    \"type\" : \"polygon\",\n    \"coordinates\" : [\n      [ [1000.0, -1001.0], [1001.0, -1001.0], [1001.0, -1000.0], [1000.0, -1000.0], [1000.0, -1001.0] ]\n    ]\n  }\n}\n"],["body","\n"],["body","The following is an example of a Polygon in WKT:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : \"POLYGON ((1000.0 -1001.0, 1001.0 -1001.0, 1001.0 -1000.0, 1000.0 -1000.0, 1000.0 -1001.0))\"\n}\n"],["body","\n"],["body","The first array represents the outer boundary of the polygon, the other arrays represent the interior shapes (\"holes\"). The following is a GeoJSON example of a polygon with a hole:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : {\n    \"type\" : \"polygon\",\n    \"coordinates\" : [\n      [ [1000.0, -1001.0], [1001.0, -1001.0], [1001.0, -1000.0], [1000.0, -1000.0], [1000.0, -1001.0] ],\n      [ [1000.2, -1001.2], [1000.8, -1001.2], [1000.8, -1001.8], [1000.2, -1001.8], [1000.2, -1001.2] ]\n    ]\n  }\n}\n"],["body","\n"],["body","The following is an example of a Polygon with a hole in WKT:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : \"POLYGON ((1000.0 1000.0, 1001.0 1000.0, 1001.0 1001.0, 1000.0 1001.0, 1000.0 1000.0), (1000.2 1000.2, 1000.8 1000.2, 1000.8 1000.8, 1000.2 1000.8, 1000.2 1000.2))\"\n}\n"],["body","\n"],["body","IMPORTANT NOTE: WKT does not enforce a specific order for vertices. GeoJSON mandates that the outer polygon must be counterclockwise and interior shapes must be clockwise, which agrees with the Open Geospatial Consortium (OGC) Simple Feature Access specification for vertex ordering."],["body","\n"],["body","By default Elasticsearch expects vertices in counterclockwise (right hand rule) order. If data is provided in clockwise order (left hand rule) the user can change the orientation parameter either in the field mapping, or as a parameter provided with the document."],["body","\n"],["body","The following is an example of overriding the orientation parameters on a document:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : {\n    \"type\" : \"polygon\",\n    \"orientation\" : \"clockwise\",\n    \"coordinates\" : [\n      [ [1000.0, 1000.0], [1000.0, 1001.0], [1001.0, 1001.0], [1001.0, 1000.0], [1000.0, 1000.0] ]\n    ]\n  }\n}\n"],["body","\n"],["headingLink","multipoint"],["heading","MultiPoint"],["body","\n"],["body","The following is an example of a list of geojson points:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : {\n    \"type\" : \"multipoint\",\n    \"coordinates\" : [\n      [1002.0, 1002.0], [1003.0, 2000.0]\n    ]\n  }\n}\n"],["body","\n"],["body","The following is an example of a list of WKT points:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : \"MULTIPOINT (1002.0 2000.0, 1003.0 2000.0)\"\n}\n"],["body","\n"],["headingLink","multilinestring"],["heading","MultiLineString"],["body","\n"],["body","The following is an example of a list of geojson linestrings:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : {\n    \"type\" : \"multilinestring\",\n    \"coordinates\" : [\n      [ [1002.0, 200.0], [1003.0, 200.0], [1003.0, 300.0], [1002.0, 300.0] ],\n      [ [1000.0, 100.0], [1001.0, 100.0], [1001.0, 100.0], [1000.0, 100.0] ],\n      [ [1000.2, 100.2], [1000.8, 100.2], [1000.8, 100.8], [1000.2, 100.8] ]\n    ]\n  }\n}\n"],["body","\n"],["body","The following is an example of a list of WKT linestrings:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : \"MULTILINESTRING ((1002.0 200.0, 1003.0 200.0, 1003.0 300.0, 1002.0 300.0), (1000.0 100.0, 1001.0 100.0, 1001.0 100.0, 1000.0 100.0), (1000.2 0.2, 1000.8 100.2, 1000.8 100.8, 1000.2 100.8))\"\n}\n"],["body","\n"],["headingLink","multipolygon"],["heading","MultiPolygon"],["body","\n"],["body","The following is an example of a list of geojson polygons (second polygon contains a hole):"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : {\n    \"type\" : \"multipolygon\",\n    \"coordinates\" : [\n      [ [[1002.0, 200.0], [1003.0, 200.0], [1003.0, 300.0], [1002.0, 300.0], [1002.0, 200.0]] ],\n      [ [[1000.0, 200.0], [1001.0, 100.0], [1001.0, 100.0], [1000.0, 100.0], [1000.0, 100.0]],\n        [[1000.2, 200.2], [1000.8, 100.2], [1000.8, 100.8], [1000.2, 100.8], [1000.2, 100.2]] ]\n    ]\n  }\n}\n"],["body","\n"],["body","The following is an example of a list of WKT polygons (second polygon contains a hole):"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : \"MULTIPOLYGON (((1002.0 200.0, 1003.0 200.0, 1003.0 300.0, 1002.0 300.0, 102.0 200.0)), ((1000.0 100.0, 1001.0 100.0, 1001.0 100.0, 1000.0 100.0, 1000.0 100.0), (1000.2 100.2, 1000.8 100.2, 1000.8 100.8, 1000.2 100.8, 1000.2 100.2)))\"\n}\n"],["body","\n"],["headingLink","geometry-collection"],["heading","Geometry Collection"],["body","\n"],["body","The following is an example of a collection of geojson geometry objects:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : {\n    \"type\": \"geometrycollection\",\n    \"geometries\": [\n      {\n        \"type\": \"point\",\n        \"coordinates\": [1000.0, 100.0]\n      },\n      {\n        \"type\": \"linestring\",\n        \"coordinates\": [ [1001.0, 100.0], [1002.0, 100.0] ]\n      }\n    ]\n  }\n}\n"],["body","\n"],["body","The following is an example of a collection of WKT geometry objects:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : \"GEOMETRYCOLLECTION (POINT (1000.0 100.0), LINESTRING (1001.0 100.0, 1002.0 100.0))\"\n}\n"],["body","\n"],["headingLink","envelope"],["heading","Envelope"],["body","\n"],["body","Elasticsearch supports an envelope type, which consists of coordinates for upper left and lower right points of the shape to represent a bounding rectangle in the format [[minX, maxY], [maxX, minY]]:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : {\n    \"type\" : \"envelope\",\n    \"coordinates\" : [ [1000.0, 100.0], [1001.0, 100.0] ]\n  }\n}\n"],["body","\n"],["body","The following is an example of an envelope using the WKT BBOX format:"],["body","\n"],["body","NOTE: WKT specification expects the following order: minLon, maxLon, maxLat, minLat."],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : \"BBOX (1000.0, 1002.0, 2000.0, 1000.0)\"\n}\n"],["body","\n"],["headingLink","sorting-and-retrieving-index-shapes"],["heading","Sorting and Retrieving index Shapes"],["body","\n"],["body","Due to the complex input structure and index representation of shapes, it is not currently possible to sort shapes or retrieve their fields directly. The shape value is only retrievable through the _source field."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/28.UnsignedLong.html"],["title","UnsignedLong.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","unsigned-long-field-type"],["heading","Unsigned long field type"],["body","\n"],["body","Unsigned long是一种数字字段类型，表示最小值为0，最大值为 “264-1” (包括0至18446744073709551615) 的无符号64位整数。"],["body","\n"],["body","PUT my_index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_counter\": {\n        \"type\": \"unsigned_long\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","Unsigned long可以以数字或字符串形式索引，表示范围 [0，18446744073709551615] 中的整数值。他们不能有小数部分。"],["body","\n"],["body","POST /my_index/_bulk?refresh\n{\"index\":{\"_id\":1}}\n{\"my_counter\": 0}\n{\"index\":{\"_id\":2}}\n{\"my_counter\": 9223372036854775808}\n{\"index\":{\"_id\":3}}\n{\"my_counter\": 18446744073709551614}\n{\"index\":{\"_id\":4}}\n{\"my_counter\": 18446744073709551615}\n"],["body","\n"],["body","Term queries accept any numbers in a numeric or string form."],["body","\n"],["body","GET /my_index/_search\n{\n    \"query\": {\n        \"term\" : {\n            \"my_counter\" : 18446744073709551615\n        }\n    }\n}\n"],["body","\n"],["body","范围查询术语可以包含带有小数部分的值。在这种情况下，Elasticsearch将它们转换为整数值: 'gte' 和 'gt' 项被转换为最接近的包含整数，并且 'lt' 和 'lte' 范围被转换为最接近的包含整数。"],["body","\n"],["body","建议将范围作为字符串传递，以确保解析它们而不会损失精度。"],["body","\n"],["body","GET /my_index/_search\n{\n    \"query\": {\n        \"range\" : {\n            \"my_counter\" : {\n                \"gte\" : \"9223372036854775808.5\",\n                \"lte\" : \"18446744073709551615\"\n            }\n        }\n    }\n}\n"],["body","\n"],["body","对于在 “unsigned_long” 字段上进行排序的查询，对于特定文档，如果此文档的值在long范围内，则Elasticsearch返回类型为 “long” 的排序值，如果该值超出此范围，则返回类型为 “biginteger” 的排序值。"],["body","\n"],["body","REST客户端需要能够处理JSON中的大整数值，以正确支持此字段类型。"],["body","\n"],["body","GET /my_index/_search\n{\n    \"query\": {\n        \"match_all\" : {}\n    },\n    \"sort\" : {\"my_counter\" : \"desc\"}\n}\n"],["body","\n"],["headingLink","unsigned-long-in-scripts"],["heading","Unsigned long in scripts"],["body","\n"],["body","Currently unsigned_long is not supported in scripts."],["body","\n"],["headingLink","stored-fields"],["heading","Stored fields"],["body","\n"],["body","A stored field of unsigned_long is stored and returned as String."],["body","\n"],["headingLink","aggregations"],["heading","Aggregations"],["body","\n"],["body","For terms aggregations, similarly to sort values, Long or BigInteger values are used. For other aggregations, values are converted to the double type."],["body","\n"],["headingLink","queries-with-mixed-numeric-types"],["heading","Queries with mixed numeric types"],["body","\n"],["body","Searches with mixed numeric types one of which is unsigned_long are supported, except queries with sort. Thus, a sort query across two indexes where the same field name has an unsigned_long type in one index, and long type in another, doesn’t produce correct results and must be avoided. If there is a need for such kind of sorting, script based sorting can be used instead."],["body","\n"],["body","Aggregations across several numeric types one of which is unsigned_long are supported. In this case, values are converted to the double type."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/2.AliasFieldType.html"],["title","AliasFieldType.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","alias-field-type"],["heading","Alias field type"],["body","\n"],["body","字段别名"],["body","\n"],["body","PUT trips\n{\n  \"mappings\": {\n    \"properties\": {\n      \"distance\": {\n        \"type\": \"long\"\n      },\n      \"route_length_miles\": {\n        \"type\": \"alias\",\n        \"path\": \"distance\" \n      },\n      \"transit_mode\": {\n        \"type\": \"keyword\"\n      }\n    }\n  }\n}\n\nGET _search\n{\n  \"query\": {\n    \"range\" : {\n      \"route_length_miles\" : {\n        \"gte\" : 39\n      }\n    }\n  }\n}\n"],["body","\n\n"],["body","\n"],["body","搜索请求的几乎所有组件都接受字段别名。特别是，别名可以在查询，聚合和排序字段中以及在请求 'docvalue_field'，'stored_field'，建议和突出显示时使用。脚本在访问字段值时也支持别名"],["body","\n"],["body","\n"],["body","\n"],["body","Please see the section on unsupported APIs for exceptions."],["body","\n"],["body","\n"],["body","\n"],["body","在搜索请求的某些部分以及请求字段功能时，可以提供字段通配符模式。在这些情况下，除了具体字段之外，通配符模式将匹配字段别名"],["body","\n"],["body","\n\n"],["body","GET trips/_field_caps?fields=route_*,transit_mode\n"],["body","\n"],["headingLink","alias-targets"],["heading","Alias targets"],["body","\n"],["body","别名的目标有一些限制:"],["body","\n\n"],["body","目标必须是具体字段，而不是对象或其他字段别名。"],["body","\n"],["body","目标字段必须在创建别名时存在。"],["body","\n"],["body","如果定义了嵌套对象，则字段别名必须具有与其目标相同的嵌套作用域。"],["body","\n"],["body","此外，字段别名只能有一个目标。这意味着不可能使用字段别名来查询单个子句中的多个目标字段。"],["body","\n"],["body","一个已知的限制是，别名更新时，如果任何存储的percolator查询包含字段别名，它们仍将引用其原始目标。详见：percolator documentation."],["body","\n\n"],["headingLink","unsupported-apis"],["heading","Unsupported APIs"],["body","\n"],["body","在写入的时候不支持字段别名:"],["body","\n"],["body","不支持 copy_to 或者 multi-fields"],["body","\n"],["body","别名不存在于文档源中"],["body","\n"],["body","GET /_search\n{\n  \"query\" : {\n    \"match_all\": {}\n  },\n  \"_source\": \"route_length_miles\"\n}\n"],["body","\n\n"],["body","\n"],["body","Currently only the search and field capabilities APIs will accept and resolve field aliases."],["body","\n"],["body","\n"],["body","\n"],["body","Finally, some queries, such as terms, geo_shape, and more_like_this, allow for fetching query information from an indexed document. Because field aliases aren’t supported when fetching documents, the part of the query that specifies the lookup path cannot refer to a field by its alias."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/18.Numberic.html"],["title","Numberic.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","numeric-field-types"],["heading","Numeric field types"],["body","\n"],["body","The following numeric types are supported:"],["body","\n"],["body","\n"],["body","long"],["body","A signed 64-bit integer with a minimum value of -263 and a maximum value of 263-1."],["body","\n"],["body","integer"],["body","A signed 32-bit integer with a minimum value of -231 and a maximum value of 231-1."],["body","\n"],["body","short"],["body","A signed 16-bit integer with a minimum value of -32,768 and a maximum value of 32,767."],["body","\n"],["body","byte"],["body","A signed 8-bit integer with a minimum value of -128 and a maximum value of 127."],["body","\n"],["body","double"],["body","A double-precision 64-bit IEEE 754 floating point number, restricted to finite values."],["body","\n"],["body","float"],["body","A single-precision 32-bit IEEE 754 floating point number, restricted to finite values."],["body","\n"],["body","half_float"],["body","A half-precision 16-bit IEEE 754 floating point number, restricted to finite values."],["body","\n"],["body","scaled_float"],["body","A floating point number that is backed by a long, scaled by a fixed double scaling factor."],["body","\n"],["body","unsigned_long"],["body","An unsigned 64-bit integer with a minimum value of 0 and a maximum value of 264-1."],["body","\n\n\n"],["body","Below is an example of configuring a mapping with numeric fields:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"number_of_bytes\": {\n        \"type\": \"integer\"\n      },\n      \"time_in_seconds\": {\n        \"type\": \"float\"\n      },\n      \"price\": {\n        \"type\": \"scaled_float\",\n        \"scaling_factor\": 100\n      }\n    }\n  }\n}\n"],["body","\n"],["body","Double”、 “float” 和 “half_float” 类型认为 “-0.0” 和 “0.0” 是不同的值。因此，在 “-0.0” 上执行 “术语” 查询将不匹配 “0.0”，反之亦然。对于范围查询也是如此: 如果上限为 “-0.0”，则 “0.0” 将不匹配，如果下限为 “0.0”，则 “-0.0” 将不匹配。"],["body","\n"],["headingLink","which-type-should-i-use"],["heading","Which type should I use?"],["body","\n\n"],["body","就整数类型 ('byte'，'short'，'integer' 和 'long') 而言，您应该选择最小的类型，足以满足您的用例"],["body","\n"],["body","这将有助于索引和搜索更有效。但是请注意，存储是根据存储的实际值进行优化的，因此，选择一种类型而不是另一种类型不会对存储需求产生影响。"],["body","\n"],["body","对于浮点类型，使用缩放因子将浮点数据存储到整数中通常更有效，这就是 'scaled_float '类型在 hood下所做的工作。"],["body","\n"],["body","例如，“价格” 字段可以存储在具有 “100” 的 “缩放因子” 的 “scaled_float” 中。所有api都将工作，就像该字段存储为double一样，但是在 hood 下，Elasticsearch将使用美分的数量，“价格 * 100”，这是一个整数。这主要有助于节省磁盘空间，因为整数比浮点更容易压缩,例如，"],["body","\n"],["body","假设您将cpu利用率跟踪为 “0” 和 “1” 之间的数字。cpu利用率是 “12.7%” 还是 “13%” 通常并不重要，因此您可以使用 “scaled_float” 与 “100” 的 “scaling_factor”，以便将cpu利用率四舍五入到最接近的百分比以节省空间。"],["body","\n"],["body","如果 “scaled_float” 不适合，则应在浮点类型中选择足以满足用例的最小类型"],["body","\n\n"],["body","Type"],["body","Minimum value"],["body","Maximum value"],["body","Significant bits / digits"],["body","\n"],["body","double"],["body","2-1074"],["body","(2-2-52)·21023"],["body","53 / 15.95"],["body","\n"],["body","float"],["body","2-149"],["body","(2-2-23)·2127"],["body","24 / 7.22"],["body","\n"],["body","half_float"],["body","2-24"],["body","65504"],["body","11 / 3.31"],["body","\n\n\n"],["headingLink","mapping-numeric-identifiers"],["heading","Mapping numeric identifiers"],["body","\n\n"],["body","\n"],["body","并非所有数字数据都应映射为 数字 字段数据类型。Elasticsearch为 “范围” 查询优化数字字段，例如 “int” 或 “long”。但是，'keyword' 字段更适合 ['term'](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.13/query-dsl-term-query.html) 和其他 术语级别 查询。"],["body","\n"],["body","\n"],["body","\n"],["body","标识符 (例如ISBN或产品ID) 很少在 “范围” 查询中使用。但是，通常使用术语级查询来检索它们。"],["body","\n"],["body","\n\n"],["body","考虑将数字标识符映射为 “关键词”，如果:"],["body","\n\n"],["body","您不打算使用 范围 查询搜索标识符数据。"],["body","\n"],["body","快速检索很重要。“关键词” 字段上的 “术语” 查询搜索通常比数字字段上的 “术语” 搜索快。"],["body","\n\n"],["body","如果不确定要使用哪个，可以使用 多字段 将数据映射为 “关键字”  和 数字数据类型。"],["body","\n"],["headingLink","parameters-for-numeric-fields"],["heading","Parameters for numeric fields"],["body","\n"],["body","The following parameters are accepted by numeric types:"],["body","\n"],["body","\n"],["body","coerce"],["body","Try to convert strings to numbers and truncate fractions for integers. Accepts true (default) and false. Not applicable for unsigned_long. Note that this cannot be set if the script parameter is used."],["body","\n"],["body","boost"],["body","Mapping field-level query time boosting. Accepts a floating point number, defaults to 1.0."],["body","\n"],["body","doc_values"],["body","Should the field be stored on disk in a column-stride fashion, so that it can later be used for sorting, aggregations, or scripting? Accepts true (default) or false."],["body","\n"],["body","ignore_malformed"],["body","If true, malformed numbers are ignored. If false (default), malformed numbers throw an exception and reject the whole document. Note that this cannot be set if the script parameter is used."],["body","\n"],["body","index"],["body","Should the field be searchable? Accepts true (default) and false."],["body","\n"],["body","null_value"],["body","Accepts a numeric value of the same type as the field which is substituted for any explicit null values. Defaults to null, which means the field is treated as missing. Note that this cannot be set if the script parameter is used."],["body","\n"],["body","on_script_error"],["body","Defines what to do if the script defined by the script parameter throws an error at indexing time. Accepts fail (default), which will cause the entire document to be rejected, and continue, which will register the field in the document’s _ignored metadata field and continue indexing. This parameter can only be set if the script field is also set."],["body","\n"],["body","script"],["body","If this parameter is set, then the field will index values generated by this script, rather than reading the values directly from the source. If a value is set for this field on the input document, then the document will be rejected with an error. Scripts are in the same format as their runtime equivalent. Scripts can only be configured on long and double field types."],["body","\n"],["body","store"],["body","Whether the field value should be stored and retrievable separately from the _source field. Accepts true or false (default)."],["body","\n"],["body","meta"],["body","Metadata about the field."],["body","\n\n\n"],["headingLink","parameters-for-scaled_float"],["heading","Parameters for scaled_float"],["body","\n"],["body","scaled_float accepts an additional parameter:"],["body","\n"],["body","\n"],["body","scaling_factor"],["body","The scaling factor to use when encoding values. Values will be multiplied by this factor at index time and rounded to the closest long value. For instance, a scaled_float with a scaling_factor of 10 would internally store 2.34 as 23 and all search-time operations (queries, aggregations, sorting) will behave as if the document had a value of 2.3. High values of scaling_factor improve accuracy but also increase space requirements. This parameter is required."],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/24.Search-as-you-type.html"],["title","Search-as-you-type.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","search-as-you-type-field-type"],["heading","Search-as-you-type field type"],["body","\n\n"],["body","\n"],["body","'Search_as_you_type '字段类型是一个类似文本的字段。但是针对 开箱即用的 as-you-type 的自动补全 场景优化"],["body","\n"],["body","\n"],["body","\n"],["body","它创建了一系列子字段，这些子字段被分析成terms 。这些terms 可以有效 匹配整个 索引textValue"],["body","\n"],["body","\n"],["body","\n"],["body","支持前缀补全 (即从输入开始的匹配项) 和中缀补全 (即在输入内的任何位置匹配项)。"],["body","\n"],["body","\n\n"],["body","When adding a field of this type to a mapping"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_field\": {\n        \"type\": \"search_as_you_type\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","This creates the following fields"],["body","\n"],["body","\n"],["body","my_field"],["body","Analyzed as configured in the mapping. If an analyzer is not configured, the default analyzer for the index is used"],["body","\n"],["body","my_field._2gram"],["body","Wraps the analyzer of my_field with a shingle token filter of shingle size 2"],["body","\n"],["body","my_field._3gram"],["body","Wraps the analyzer of my_field with a shingle token filter of shingle size 3"],["body","\n"],["body","my_field._index_prefix"],["body","Wraps the analyzer of my_field._3gram with an edge ngram token filter"],["body","\n\n\n\n"],["body","子字段中 shingles 的大小可以使用 “max_shingle_size” 映射参数进行配置。默认3此参数的有效值为2-4 (含) 整数值。"],["body","\n"],["body","Shingle subfields will be created for each shingle size from 2 up to and including the max_shingle_size."],["body","\n"],["body","'my_field._ index_prefix' 子字段在构造自己的分析器时将始终使用带有 'max_shingle_size '设置 的shingle子字段中的分析器。"],["body","\n"],["body","Increasing the max_shingle_size will improve matches for queries with more consecutive terms, at the cost of larger index size. The default max_shingle_size should usually be sufficient."],["body","\n\n"],["body","当索引文档具有根字段 “my_field” 的值时，相同的输入文本会自动索引到这些字段中的每个字段中，并具有不同的分析链。"],["body","\n"],["body","PUT my-index-000001/_doc/1?refresh\n{\n  \"my_field\": \"quick brown fox jump lazy dog\"\n}\n"],["body","\n"],["body","search-as-you-type 场景最有效方式通常是 'multi_match' 类型 ['bool_prefix'](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.13/query-dsl-match-bool-prefix-query.html)，其目标是根 'search_as_you_type '字段及其shingle子字段。"],["body","\n"],["body","这可以以任何顺序匹配查询术语，但是如果文档在shingle子字段中按顺序包含术语，则会给文档评分更高。"],["body","\n"],["body","GET my-index-000001/_search\n{\n  \"query\": {\n    \"multi_match\": {\n      \"query\": \"brown f\",\n      \"type\": \"bool_prefix\",\n      \"fields\": [\n        \"my_field\",\n        \"my_field._2gram\",\n        \"my_field._3gram\"\n      ]\n    }\n  }\n}\n"],["body","\n"],["body","{\n  \"took\" : 44,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 1,\n    \"successful\" : 1,\n    \"skipped\" : 0,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : {\n      \"value\" : 1,\n      \"relation\" : \"eq\"\n    },\n    \"max_score\" : 0.8630463,\n    \"hits\" : [\n      {\n        \"_index\" : \"my-index-000001\",\n        \"_type\" : \"_doc\",\n        \"_id\" : \"1\",\n        \"_score\" : 0.8630463,\n        \"_source\" : {\n          \"my_field\" : \"quick brown fox jump lazy dog\"\n        }\n      }\n    ]\n  }\n}\n"],["body","\n"],["body","要搜索按顺序严格匹配查询术语的文档，或使用短语查询的其他属性进行搜索，请在根字段上使用 'match_phrase_prefix '查询。如果最后一个术语应该精确匹配，而不是作为前缀，也可以使用 “match_phrase” 查询。使用短语查询可能比使用 “match_bool_prefix” 查询效率低。"],["body","\n"],["body","GET my-index-000001/_search\n{\n  \"query\": {\n    \"match_phrase_prefix\": {\n      \"my_field\": \"brown f\"\n    }\n  }\n}\n"],["body","\n"],["headingLink","parameters-specific-to-the-search_as_you_type-field"],["heading","Parameters specific to the search_as_you_type field"],["body","\n"],["body","The following parameters are accepted in a mapping for the search_as_you_type field and are specific to this field type"],["body","\n\n"],["body","\n"],["body","max_shingle_size"],["body","\n"],["body","(Optional, integer) Largest shingle size to create. Valid values are 2 (inclusive) to 4 (inclusive). Defaults to 3.A subfield is created for each integer between 2 and this value. For example, a value of 3 creates two subfields: my_field._2gram and my_field._3gramMore subfields enables more specific queries but increases index size."],["body","\n"],["body","\n\n"],["headingLink","parameters-of-the-field-type-as-a-text-field"],["heading","Parameters of the field type as a text field"],["body","\n"],["body","The following parameters are accepted in a mapping for the search_as_you_type field due to its nature as a text-like field, and behave similarly to their behavior when configuring a field of the text data type. Unless otherwise noted, these options configure the root fields subfields in the same way."],["body","\n"],["body","由于 “search_as_you_type” 字段的性质，因此在 “search_as_you_type” 字段的映射中接受以下参数，并且在配置 text字段时其行为类似于它们的行为。除非另有说明，否则这些选项以相同的方式配置根字段子字段。"],["body","\n\n"],["body","\n"],["body","analyzer"],["body","\n"],["body","The analyzer which should be used for text fields, both at index-time and at search-time (unless overridden by the search_analyzer). Defaults to the default index analyzer, or the standard analyzer."],["body","\n"],["body","\n"],["body","\n"],["body","index"],["body","\n"],["body","Should the field be searchable? Accepts true (default) or false."],["body","\n"],["body","\n"],["body","\n"],["body","index_options"],["body","\n"],["body","What information should be stored in the index, for search and highlighting purposes. Defaults to positions."],["body","\n"],["body","\n"],["body","\n"],["body","norms"],["body","\n"],["body","Whether field-length should be taken into account when scoring queries. Accepts true or false. This option configures the root field and shingle subfields, where its default is true. It does not configure the prefix subfield, where it is false."],["body","\n"],["body","\n"],["body","\n"],["body","store"],["body","\n"],["body","Whether the field value should be stored and retrievable separately from the _source field. Accepts true or false (default). This option only configures the root field, and does not configure any subfields."],["body","\n"],["body","\n"],["body","\n"],["body","search_analyzer"],["body","\n"],["body","The analyzer that should be used at search time on text fields. Defaults to the analyzer setting."],["body","\n"],["body","\n"],["body","\n"],["body","search_quote_analyzer"],["body","\n"],["body","The analyzer that should be used at search time when a phrase is encountered. Defaults to the search_analyzer setting."],["body","\n"],["body","\n"],["body","\n"],["body","similarity"],["body","\n"],["body","Which scoring algorithm or similarity should be used. Defaults to BM25."],["body","\n"],["body","\n"],["body","\n"],["body","term_vector"],["body","\n"],["body","Whether term vectors should be stored for the field. Defaults to no. This option configures the root field and shingle subfields, but not the prefix subfield."],["body","\n"],["body","\n\n"],["headingLink","optimization-of-prefix-queries"],["heading","Optimization of prefix queries"],["body","\n"],["body","When making a prefix query to the root field or any of its subfields, the query will be rewritten to a term query on the ._index_prefix subfield. This matches more efficiently than is typical of prefix queries on text fields, as prefixes up to a certain length of each shingle are indexed directly as terms in the ._index_prefix subfield."],["body","\n"],["body","The analyzer of the ._index_prefix subfield slightly modifies the shingle-building behavior to also index prefixes of the terms at the end of the field’s value that normally would not be produced as shingles. For example, if the value quick brown fox is indexed into a search_as_you_type field with max_shingle_size of 3, prefixes for brown fox and fox are also indexed into the ._index_prefix subfield even though they do not appear as terms in the ._3gram subfield. This allows for completion of all the terms in the field’s input."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/10.GeoPoint.html"],["title","GeoPoint.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","geo-point-field-type"],["heading","Geo-point field type"],["body","\n"],["body","类型为 “geo_point” 的字段接受经纬度 pairs，可以用来:"],["body","\n\n"],["body","在一定边界内 bounding box,  查找 geo-points。例如 指定中心点的 指定距离内 distance  或指定 多边形 polygon  或者指定  geo_shape query."],["body","\n"],["body","to aggregate documents geographically or by distance from a central point."],["body","\n"],["body","to integrate distance into a document’s relevance score."],["body","\n"],["body","to sort documents by distance."],["body","\n\n"],["body","There are five ways that a geo-point may be specified, as demonstrated below:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"location\": {\n        \"type\": \"geo_point\"\n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/1\n{\n  \"text\": \"Geo-point as an object\",// Geo-point expressed as an object, with `lat` and `lon` keys.\n  \"location\": { \n    \"lat\": 41.12,\n    \"lon\": -71.34\n  }\n}\n\nPUT my-index-000001/_doc/2\n{\n  \"text\": \"Geo-point as a string\",\n  \"location\": \"41.12,-71.34\"  //Geo-point expressed as a string with the format: `\"lat,lon\"`.\n}\n\nPUT my-index-000001/_doc/3\n{\n  \"text\": \"Geo-point as a geohash\",\n  \"location\": \"drm3btev3e86\" //Geo-point expressed as a geohash.\n}\n\nPUT my-index-000001/_doc/4\n{\n  \"text\": \"Geo-point as an array\",\n  \"location\": [ -71.34, 41.12 ] //Geo-point expressed as an array with the format: [ `lon`, `lat`]\n}\n\nPUT my-index-000001/_doc/5\n{\n  \"text\": \"Geo-point as a WKT POINT primitive\",\n  \"location\" : \"POINT (-71.34 41.12)\" //Geo-point expressed as a [Well-Known Text](https://docs.opengeospatial.org/is/12-063r5/12-063r5.html) POINT with the format: `\"POINT(lon lat)\"`\n}\n\nGET my-index-000001/_search\n{\n  \"query\": {\n    \"geo_bounding_box\": {  A geo-bounding box query which finds all geo-points that fall inside the box.\n      \"location\": {\n        \"top_left\": {\n          \"lat\": 42,\n          \"lon\": -72\n        },\n        \"bottom_right\": {\n          \"lat\": 40,\n          \"lon\": -74\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","geo-points-expressed-as-an-array-or-string"],["heading","Geo-points expressed as an array or string"],["body","\n\n"],["body","\n"],["body","请注意，字符串地理点排序为 'lat，lon'，而数组地理点排序为反向: 'lon，lat'。"],["body","\n"],["body","\n"],["body","\n"],["body","最初，'lat，lon' 用于数组和字符串，但是数组格式在早期被更改以符合GeoJSON使用的格式。"],["body","\n"],["body","\n\n"],["body","GeoHash"],["body","\n\n"],["body","A point can be expressed as a geohash. Geohashes are base32 encoded strings of the bits of the latitude and longitude interleaved."],["body","\n"],["body","geohash中的每个字符都会增加5位精度。所以哈希越长，它就越精确。"],["body","\n"],["body","对于索引，将地理标记转换为经纬度对。 在此过程中，仅使用前12个字符，因此在geohash中指定超过12个字符不会提高精度。12个字符提供60位，这应该将可能的错误降低到小于2厘米。"],["body","\n\n"],["headingLink","parameters-for-geo_point-fields"],["heading","Parameters for geo_point fields"],["body","\n"],["body","\n"],["body","ignore_malformed"],["body","If true, malformed geo-points are ignored. If false (default), malformed geo-points throw an exception and reject the whole document. A geo-point is considered malformed if its latitude is outside the range -90 ⇐ latitude ⇐ 90, or if its longitude is outside the range -180 ⇐ longitude ⇐ 180. Note that this cannot be set if the script parameter is used."],["body","\n"],["body","ignore_z_value"],["body","If true (default) three dimension points will be accepted (stored in source) but only latitude and longitude values will be indexed; the third dimension is ignored. If false, geo-points containing any more than latitude and longitude (two dimensions) values throw an exception and reject the whole document. Note that this cannot be set if the script parameter is used."],["body","\n"],["body","index"],["body","Should the field be searchable? Accepts true (default) and false."],["body","\n"],["body","null_value"],["body","Accepts an geopoint value which is substituted for any explicit null values. Defaults to null, which means the field is treated as missing. Note that this cannot be set if the script parameter is used."],["body","\n"],["body","on_script_error"],["body","Defines what to do if the script defined by the script parameter throws an error at indexing time. Accepts fail (default), which will cause the entire document to be rejected, and continue, which will register the field in the document’s _ignored metadata field and continue indexing. This parameter can only be set if the script field is also set."],["body","\n"],["body","script"],["body","If this parameter is set, then the field will index values generated by this script, rather than reading the values directly from the source. If a value is set for this field on the input document, then the document will be rejected with an error. Scripts are in the same format as their runtime equivalent, and should emit points as a pair of (lat, lon) double values."],["body","\n\n\n"],["headingLink","using-geo-points-in-scripts"],["heading","Using geo-points in scripts"],["body","\n"],["body","When accessing the value of a geo-point in a script, the value is returned as a GeoPoint object, which allows access to the .lat and .lon values respectively:"],["body","\n"],["body","def geopoint = doc['location'].value;\ndef lat      = geopoint.lat;\ndef lon      = geopoint.lon;\n"],["body","\n"],["body","For performance reasons, it is better to access the lat/lon values directly:"],["body","\n"],["body","def lat      = doc['location'].lat;\ndef lon      = doc['location'].lon;\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/4.Binary.html"],["title","Binary.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","binary-field-type"],["heading","Binary field type"],["body","\n"],["body","The binary type accepts a binary value as a Base64 encoded string. The field is not stored by default and is not searchable:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": {\n        \"type\": \"text\"\n      },\n      \"blob\": {\n        \"type\": \"binary\"\n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/1\n{\n  \"name\": \"Some binary blob\",\n  \"blob\": \"U29tZSBiaW5hcnkgYmxvYg==\" \n}\n"],["body","\n"],["headingLink","parameters-for-binary-fields"],["heading","Parameters for binary fields"],["body","\n"],["body","The following parameters are accepted by binary fields:"],["body","\n"],["body","\n"],["body","doc_values"],["body","Should the field be stored on disk in a column-stride fashion, so that it can later be used for sorting, aggregations, or scripting? Accepts true or false (default)."],["body","\n"],["body","store"],["body","Whether the field value should be stored and retrievable separately from the _source field. Accepts true or false (default)."],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/12.Histogram.html"],["title","Histogram.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","histogram-field-type"],["heading","Histogram field type"],["body","\n"],["body","用于存储表示直方图的预聚合数值数据的字段。此数据使用两个配对数组定义:"],["body","\n\n"],["body","'double' 数值的 values arryay，表示直方图的桶。这些值必须按升序排列."],["body","\n"],["body","'integer' 数值的相应 count array表示每个存储桶中有多少个值。这些数字必须为正或为零。"],["body","\n\n"],["body","两个数组通过 索引对应，所以长度必须一致"],["body","\n\n"],["body","A histogram field can only store a single pair of values and count arrays per document. Nested arrays are not supported."],["body","\n"],["body","histogram fields do not support sorting."],["body","\n\n"],["headingLink","uses"],["heading","Uses"],["body","\n\n"],["body","“直方图” 字段主要用于聚合。"],["body","\n"],["body","为了使聚合更容易访问，“直方图” 字段数据存储为二进制 doc值，而不是索引。"],["body","\n"],["body","它的字节大小最多为 13 * numValues，其中 'numValues' 是提供的数组的长度。"],["body","\n\n"],["body","Because the data is not indexed, you only can use histogram fields for the following aggregations and queries:"],["body","\n\n"],["body","min aggregation"],["body","\n"],["body","max aggregation"],["body","\n"],["body","sum aggregation"],["body","\n"],["body","value_count aggregation"],["body","\n"],["body","avg aggregation"],["body","\n"],["body","percentiles aggregation"],["body","\n"],["body","percentile ranks aggregation"],["body","\n"],["body","boxplot aggregation"],["body","\n"],["body","histogram aggregation"],["body","\n"],["body","exists query"],["body","\n\n"],["headingLink","building-a-histogram"],["heading","Building a histogram"],["body","\n\n"],["body","当使用直方图作为聚合的一部分时,结果的准确性将取决于直方图是如何构建的，重要的是要考虑将用于构建它的 percentiles aggregation mode"],["body","\n\n\n"],["body","For the T-Digest mode, the values array represents the mean centroid positions and the counts array represents the number of values that are attributed to each centroid. If the algorithm has already started to approximate the percentiles, this inaccuracy is carried over in the histogram."],["body","\n"],["body","For the High Dynamic Range (HDR) histogram mode, the values array represents fixed upper limits of each bucket interval, and the counts array represents the number of values that are attributed to each interval. This implementation maintains a fixed worse-case percentage error (specified as a number of significant digits), therefore the value used when generating the histogram would be the maximum accuracy you can achieve at aggregation time."],["body","\n\n"],["body","The histogram field is \"algorithm agnostic\" and does not store data specific to either T-Digest or HDRHistogram. While this means the field can technically be aggregated with either algorithm, in practice the user should chose one algorithm and index data in that manner (e.g. centroids for T-Digest or intervals for HDRHistogram) to ensure best accuracy."],["body","\n"],["headingLink","examples"],["heading","Examples"],["body","\n"],["body","The following create index API request creates a new index with two field mappings:"],["body","\n\n"],["body","my_histogram, a histogram field used to store percentile data"],["body","\n"],["body","my_text, a keyword field used to store a title for the histogram"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\" : {\n    \"properties\" : {\n      \"my_histogram\" : {\n        \"type\" : \"histogram\"\n      },\n      \"my_text\" : {\n        \"type\" : \"keyword\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","The following index API requests store pre-aggregated for two histograms: histogram_1 and histogram_2."],["body","\n"],["body","PUT my-index-000001/_doc/1\n{\n  \"my_text\" : \"histogram_1\",\n  \"my_histogram\" : {\n      \"values\" : [0.1, 0.2, 0.3, 0.4, 0.5], \n      \"counts\" : [3, 7, 23, 12, 6] \n   }\n}\n\nPUT my-index-000001/_doc/2\n{\n  \"my_text\" : \"histogram_2\",\n  \"my_histogram\" : {\n      \"values\" : [0.1, 0.25, 0.35, 0.4, 0.45, 0.5], \n      \"counts\" : [8, 17, 8, 7, 6, 2] \n   }\n}\n"],["body","\n\n"],["body","Values for each bucket. Values in the array are treated as doubles and must be given in increasing order. For T-Digest histograms this value represents the mean value. In case of HDR histograms this represents the value iterated to."],["body","\n"],["body","Count for each bucket. Values in the arrays are treated as integers and must be positive or zero. Negative values will be rejected. The relation between a bucket and a count is given by the position in the array."],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/5.Boolean.html"],["title","Boolean.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","boolean-field-type"],["heading","Boolean field type"],["body","\n"],["body","Boolean fields accept JSON true and false values, but can also accept strings which are interpreted as either true or false:"],["body","\n"],["body","\n"],["body","False values"],["body","false, \"false\", \"\" (empty string)"],["body","\n"],["body","True values"],["body","true, \"true\""],["body","\n\n\n"],["body","For example:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"is_published\": {\n        \"type\": \"boolean\"\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_doc/1?refresh\n{\n  \"is_published\": \"true\" \n}\n\nGET my-index-000001/_search\n{\n  \"query\": {\n    \"term\": {\n      \"is_published\": true \n    }\n  }\n}\n"],["body","\n"],["body","terms聚合中 对于 key 使用 1 和 0. 对于 key_as_string 使用 true和false"],["body","\n"],["body","POST my-index-000001/_doc/1?refresh\n{\n  \"is_published\": true\n}\n\nPOST my-index-000001/_doc/2?refresh\n{\n  \"is_published\": false\n}\n\nGET my-index-000001/_search\n{\n  \"aggs\": {\n    \"publish_state\": {\n      \"terms\": {\n        \"field\": \"is_published\"\n      }\n    }\n  },\n  \"sort\": [ \"is_published\" ],\n  \"fields\": [\n    {\"field\": \"weight\"}\n  ],\n  \"runtime_mappings\": {\n    \"weight\": {\n      \"type\": \"long\",\n      \"script\": \"emit(doc['is_published'].value ? 10 : 0)\"\n    }\n  }\n}\n"],["body","\n"],["headingLink","parameters-for-boolean-fields"],["heading","Parameters for boolean fields"],["body","\n"],["body","The following parameters are accepted by boolean fields:"],["body","\n"],["body","\n"],["body","boost"],["body","Mapping field-level query time boosting. Accepts a floating point number, defaults to 1.0."],["body","\n"],["body","doc_values"],["body","Should the field be stored on disk in a column-stride fashion, so that it can later be used for sorting, aggregations, or scripting? Accepts true (default) or false."],["body","\n"],["body","index"],["body","Should the field be searchable? Accepts true (default) and false."],["body","\n"],["body","null_value"],["body","Accepts any of the true or false values listed above. The value is substituted for any explicit null values. Defaults to null, which means the field is treated as missing. Note that this cannot be set if the script parameter is used."],["body","\n"],["body","on_script_error"],["body","Defines what to do if the script defined by the script parameter throws an error at indexing time. Accepts fail (default), which will cause the entire document to be rejected, and continue, which will register the field in the document’s _ignored metadata field and continue indexing. This parameter can only be set if the script field is also set."],["body","\n"],["body","script"],["body","If this parameter is set, then the field will index values generated by this script, rather than reading the values directly from the source. If a value is set for this field on the input document, then the document will be rejected with an error. Scripts are in the same format as their runtime equivalent."],["body","\n"],["body","store"],["body","Whether the field value should be stored and retrievable separately from the _source field. Accepts true or false (default)."],["body","\n"],["body","meta"],["body","Metadata about the field."],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/29.Version.html"],["title","Version.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","version-field-type"],["heading","Version field type"],["body","\n\n"],["body","\n"],["body","“Version” 字段类型是 “keytword” 字段的专门优化化，用于处理软件版本值并为其支持专门的优先级规则"],["body","\n"],["body","\n"],["body","\n"],["body","Precedence is defined following the rules outlined by Semantic Versioning, which for example means that major, minor and patch version parts are sorted numerically (i.e. \"2.1.0\" < \"2.4.1\" < \"2.11.2\") and pre-release versions are sorted before release versions (i.e. \"1.0.0-alpha\" < \"1.0.0\")."],["body","\n"],["body","\n"],["body","\n"],["body","优先级是按照 语义版本控制 概述的规则定义的，例如，这意味着 major，minor 和  patch 版本部分按数字排序 (即 “2.1.0” <“2.4.1” <“2.11.2”)，并在发布版本之前对预发布版本进行排序 (即 “1.0.0-alpha” <“1.0.0”)。"],["body","\n"],["body","\n\n"],["body","You index a version field as follows"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_version\": {\n        \"type\": \"version\"\n      }\n    }\n  }\n}\n"],["body","\n\n"],["body","\n"],["body","该字段提供与常规关键字字段相同的搜索功能。"],["body","\n"],["body","\n"],["body","\n"],["body","它可以例如使用 “匹配” 或 “术语” 查询搜索精确匹配，并支持前缀和通配符搜索"],["body","\n"],["body","\n"],["body","\n"],["body","主要的好处是 “范围” 查询将 遵守 Semver排序，因此 “1.0.0” 和 “1.5.0” 之间的 “范围” 查询将包括 “1.2.3” 的版本，但不包括 “1.11.2” 的版本。请注意，当使用常规的 “关键字” 字段进行索引时，顺序是字母顺序的情况会有所不同。"],["body","\n"],["body","\n"],["body","\n"],["body","软件版本应遵循 语义版本控制规则 模式和优先规则，但有一个明显的例外，即允许多于或少于三个主版本标识符 (即 “1.2” 或 “1.2.3.4” 符合有效版本，而它们不符合严格的Semver规则)。在Semver定义下无效的版本字符串 (例如 “1.2.alpha.4”) 仍然可以索引和检索为精确匹配，但是它们都将以常规字母顺序出现在任何有效版本之后。空字符串 “” 被视为无效，并在所有有效版本之后排序，但在其他无效版本之前排序。"],["body","\n"],["body","\n\n"],["headingLink","limitations"],["heading","Limitations"],["body","\n"],["body","此字段类型未针对大量通配符，正则表达式或模糊搜索进行优化。虽然这些类型的查询在此字段中工作，但如果您强烈依赖此类查询，则应考虑使用常规的 “关键字” 字段。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/21.Range.html"],["title","Range.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","range-field-types"],["heading","Range field types"],["body","\n\n"],["body","\n"],["body","范围字段类型表示上限和下限之间的值的连续范围."],["body","\n"],["body","\n"],["body","\n"],["body","例如，范围可以表示10月的任何日期 或 任何从0到9的整数"],["body","\n"],["body","\n"],["body","\n"],["body","使用 gt 或者 gte 定义下界。使用lt lte定义上界"],["body","\n"],["body","\n"],["body","\n"],["body","它们可用于查询，并且对聚合的支持有限。The only supported aggregations are histogram, cardinality."],["body","\n"],["body","\n\n"],["body","支持以下范围类型:"],["body","\n"],["body","\n"],["body","integer_range"],["body","A range of signed 32-bit integers with a minimum value of -231 and maximum of 231-1."],["body","\n"],["body","float_range"],["body","A range of single-precision 32-bit IEEE 754 floating point values."],["body","\n"],["body","long_range"],["body","A range of signed 64-bit integers with a minimum value of -263 and maximum of 263-1."],["body","\n"],["body","double_range"],["body","A range of double-precision 64-bit IEEE 754 floating point values."],["body","\n"],["body","date_range"],["body","A range of date values. Date ranges support various date formats through the format mapping parameter. Regardless of the format used, date values are parsed into an unsigned 64-bit integer representing milliseconds since the Unix epoch in UTC. Values containing the now date math expression are not supported."],["body","\n"],["body","ip_range"],["body","A range of ip values supporting either IPv4 or IPv6 (or mixed) addresses."],["body","\n\n\n"],["body","下面是使用各种范围字段配置映射的示例，然后是对几种范围类型进行索引的示例。"],["body","\n"],["body","PUT range_index\n{\n  \"settings\": {\n    \"number_of_shards\": 2\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"expected_attendees\": {\n        \"type\": \"integer_range\"\n      },\n      \"time_frame\": {\n        \"type\": \"date_range\", //`date_range` types accept the same field parameters defined by the [`date`](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/date.html) type.\n        \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\"\n      }\n    }\n  }\n}\n\nPUT range_index/_doc/1?refresh\n{\n  \"expected_attendees\" : { Example indexing a meeting with 10 to 20 attendees, not including 20.\n    \"gte\" : 10,\n    \"lt\" : 20\n  },\n  \"time_frame\" : {Example date range using date time stamp.\n    \"gte\" : \"2015-10-31 12:00:00\", \n    \"lte\" : \"2015-11-01\"\n  }\n}\n"],["body","\n"],["body","GET range_index/_search\n{\n  \"query\" : {\n    \"term\" : {\n      \"expected_attendees\" : {\n        \"value\": 12\n      }\n    }\n  }\n}\n"],["body","\n"],["body","The result produced by the above query."],["body","\n"],["body","{\n  \"took\": 13,\n  \"timed_out\": false,\n  \"_shards\" : {\n    \"total\": 2,\n    \"successful\": 2,\n    \"skipped\" : 0,\n    \"failed\": 0\n  },\n  \"hits\" : {\n    \"total\" : {\n        \"value\": 1,\n        \"relation\": \"eq\"\n    },\n    \"max_score\" : 1.0,\n    \"hits\" : [\n      {\n        \"_index\" : \"range_index\",\n        \"_type\" : \"_doc\",\n        \"_id\" : \"1\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"expected_attendees\" : {\n            \"gte\" : 10, \"lt\" : 20\n          },\n          \"time_frame\" : {\n            \"gte\" : \"2015-10-31 12:00:00\", \"lte\" : \"2015-11-01\"\n          }\n        }\n      }\n    ]\n  }\n}\n"],["body","\n"],["body","The following is an example of a date_range query over the date_range field named \"time_frame\"."],["body","\n"],["body","GET range_index/_search\n{\n  \"query\" : {\n    \"range\" : {\n      \"time_frame\" : { Range queries work the same as described in [range query](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/query-dsl-range-query.html).\n        \"gte\" : \"2015-10-31\",\n        \"lte\" : \"2015-11-01\",\n        \"relation\" : \"within\"  Range queries over range [fields](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/mapping-types.html) support a `relation` parameter which can be one of `WITHIN`, `CONTAINS`, `INTERSECTS` (default).\n      }\n    }\n  }\n}\n"],["body","\n"],["body","This query produces a similar result:"],["body","\n"],["body","{\n  \"took\": 13,\n  \"timed_out\": false,\n  \"_shards\" : {\n    \"total\": 2,\n    \"successful\": 2,\n    \"skipped\" : 0,\n    \"failed\": 0\n  },\n  \"hits\" : {\n    \"total\" : {\n        \"value\": 1,\n        \"relation\": \"eq\"\n    },\n    \"max_score\" : 1.0,\n    \"hits\" : [\n      {\n        \"_index\" : \"range_index\",\n        \"_type\" : \"_doc\",\n        \"_id\" : \"1\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"expected_attendees\" : {\n            \"gte\" : 10, \"lt\" : 20\n          },\n          \"time_frame\" : {\n            \"gte\" : \"2015-10-31 12:00:00\", \"lte\" : \"2015-11-01\"\n          }\n        }\n      }\n    ]\n  }\n}\n"],["body","\n"],["headingLink","ip-range"],["heading","IP Range"],["body","\n"],["body","In addition to the range format above, IP ranges can be provided in CIDR notation:"],["body","\n"],["body","PUT range_index/_mapping\n{\n  \"properties\": {\n    \"ip_allowlist\": {\n      \"type\": \"ip_range\"\n    }\n  }\n}\n\nPUT range_index/_doc/2\n{\n  \"ip_allowlist\" : \"192.168.0.0/16\"\n}\n"],["body","\n"],["headingLink","parameters-for-range-fields"],["heading","Parameters for range fields"],["body","\n"],["body","The following parameters are accepted by range types:"],["body","\n"],["body","\n"],["body","coerce"],["body","Try to convert strings to numbers and truncate fractions for integers. Accepts true (default) and false."],["body","\n"],["body","boost"],["body","Mapping field-level query time boosting. Accepts a floating point number, defaults to 1.0."],["body","\n"],["body","index"],["body","Should the field be searchable? Accepts true (default) and false."],["body","\n"],["body","store"],["body","Whether the field value should be stored and retrievable separately from the _source field. Accepts true or false (default)."],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/17.Nested.html"],["title","Nested.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","nested-field-type"],["heading","Nested field type"],["body","\n\n"],["body","'nested' 类型是 'object' 数据类型的特殊版本，允许对象数组以彼此独立查询的方式进行索引。"],["body","\n"],["body","当索引 一个大的任意键集对键值对时，您可能会考虑使用 “键” 和 “值” 字段将每个键值对建模为自己的嵌套文档。相反，请考虑使用 flattened 数据类型，该类型将整个对象映射为单个字段，并允许对其内容进行简单搜索。"],["body","\n"],["body","嵌套文档和查询通常很昂贵，因此在此用例中使用 “扁平化” 数据类型是更好的选择。"],["body","\n\n"],["headingLink","how-arrays-of-objects-are-flattened"],["heading","How arrays of objects are flattened"],["body","\n\n"],["body","Elasticsearch没有内部对象的概念。"],["body","\n"],["body","因此，它将对象层次结构展平为字段名称和值的简单列表。例如，考虑以下文档:"],["body","\n\n"],["body","PUT my-index-000001/_doc/1\n{\n  \"group\" : \"fans\",\n  \"user\" : [ //The `user` field is dynamically added as a field of type `object`.\n    {\n      \"first\" : \"John\",\n      \"last\" :  \"Smith\"\n    },\n    {\n      \"first\" : \"Alice\",\n      \"last\" :  \"White\"\n    }\n  ]\n}\n"],["body","\n"],["body","以前的文档将在内部转换为看起来更像这样的文档:"],["body","\n"],["body","{\n  \"group\" :        \"fans\",\n  \"user.first\" : [ \"alice\", \"john\" ],\n  \"user.last\" :  [ \"smith\", \"white\" ]\n}\n"],["body","\n\n"],["body","The user.first and user.last fields are flattened into multi-value fields,"],["body","\n"],["body","and the association between alice and white is lost."],["body","\n"],["body","This document would incorrectly match a query for alice AND smith:"],["body","\n\n"],["body","GET my-index-000001/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        { \"match\": { \"user.first\": \"Alice\" }},\n        { \"match\": { \"user.last\":  \"Smith\" }}\n      ]\n    }\n  }\n}\n"],["body","\n"],["headingLink","using-nested-fields-for-arrays-of-objects"],["heading","Using nested fields for arrays of objects"],["body","\n\n"],["body","\n"],["body","如果需要索引对象数组并保持数组中每个对象的独立性，请使用 “嵌套” 数据类型而不是 “对象” 数据类型。"],["body","\n"],["body","\n"],["body","\n"],["body","在内部，嵌套对象将数组中的每个对象索引为单独的隐藏文档，这意味着每个嵌套对象都可以通过 “嵌套” 查询独立于其他对象进行查询:"],["body","\n"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"user\": { //The `user` field is mapped as type `nested` instead of type `object`.\n        \"type\": \"nested\" \n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/1 //\n{\n  \"group\" : \"fans\",\n  \"user\" : [\n    {\n      \"first\" : \"John\",\n      \"last\" :  \"Smith\"\n    },\n    {\n      \"first\" : \"Alice\",\n      \"last\" :  \"White\"\n    }\n  ]\n}\n\nGET my-index-000001/_search // This query doesn’t match because `Alice` and `Smith` are not in the same nested object.\n{\n  \"query\": {\n    \"nested\": {\n      \"path\": \"user\",\n      \"query\": {\n        \"bool\": {\n          \"must\": [\n            { \"match\": { \"user.first\": \"Alice\" }},\n            { \"match\": { \"user.last\":  \"Smith\" }} \n          ]\n        }\n      }\n    }\n  }\n}\n\nGET my-index-000001/_search //This query matches because `Alice` and `White` are in the same nested object.\n{\n  \"query\": {\n    \"nested\": {\n      \"path\": \"user\",\n      \"query\": {\n        \"bool\": {\n          \"must\": [\n            { \"match\": { \"user.first\": \"Alice\" }},\n            { \"match\": { \"user.last\":  \"White\" }} \n          ]\n        }\n      },\n      \"inner_hits\": {\n        \"highlight\": { //`inner_hits` allow us to highlight the matching nested documents.\n          \"fields\": {\n            \"user.first\": {}\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","interacting-with-nested-documents"],["heading","Interacting with nested documents"],["body","\n"],["body","嵌套文档可以:"],["body","\n\n"],["body","queried with the nested query."],["body","\n"],["body","analyzed with the nested and reverse_nested aggregations."],["body","\n"],["body","sorted with nested sorting."],["body","\n"],["body","retrieved and highlighted with nested inner hits."],["body","\n\n\n"],["body","\n"],["body","因为嵌套文档被索引为单独的文档, 只能在 “嵌套” 查询的范围内访问它们, the nested/reverse_nested aggregations, or nested inner hits."],["body","\n"],["body","\n"],["body","\n"],["body","例如，如果嵌套文档中的字符串字段将 “index_options” 设置为 “offsets”，以允许在突出显示期间使用发布，these offsets will not be available during the main highlighting phase."],["body","\n"],["body","\n"],["body","\n"],["body","Instead, highlighting needs to be performed via nested inner hits. The same consideration applies when loading fields during a search through docvalue_fields or stored_fields."],["body","\n"],["body","\n\n"],["headingLink","parameters-for-nested-fields"],["heading","Parameters for nested fields"],["body","\n\n"],["body","\n"],["body","dynamic"],["body","\n"],["body","(Optional, string) Whether or not new properties should be added dynamically to an existing nested object. Accepts true (default), false and strict."],["body","\n"],["body","\n"],["body","\n"],["body","properties"],["body","\n"],["body","(Optional, object) The fields within the nested object, which can be of any data type, including nested. New properties may be added to an existing nested object."],["body","\n"],["body","\n"],["body","\n"],["body","include_in_parent"],["body","\n"],["body","(Optional, Boolean) If true, all fields in the nested object are also added to the parent document as standard (flat) fields. Defaults to false."],["body","\n"],["body","\n"],["body","\n"],["body","include_in_root"],["body","\n"],["body","(Optional, Boolean) If true, all fields in the nested object are also added to the root document as standard (flat) fields. Defaults to false."],["body","\n"],["body","\n\n"],["headingLink","limits-on-nested-mappings-and-objects"],["heading","Limits on nested mappings and objects"],["body","\n"],["body","如前所述，每个嵌套对象都被索引为单独的Lucene文档."],["body","\n"],["body","继续前面的示例，如果我们为包含100 “用户” 对象的单个文档编制索引，则将创建101 Lucene文档: 一个用于父文档，一个用于每个嵌套对象。由于与 “嵌套” 映射相关的消费吗，Elasticsearch设置设置以防止性能问题:"],["body","\n\n"],["body","\n"],["body","index.mapping.nested_fields.limit"],["body","\n"],["body","索引中不同 “嵌套” 映射的最大数量。“嵌套” 类型仅应在特殊情况下使用，当需要相互独立查询对象数组时。为了防止映射设计不良，此设置限制了每个索引唯一的 “嵌套” 类型的数量。默认值为 “50”。"],["body","\n"],["body","\n"],["body","\n"],["body","index.mapping.nested_objects.limit"],["body","\n"],["body","单个文档跨所有 “嵌套” 类型可以包含的嵌套JSON对象的最大数量。当文档包含太多嵌套对象时，此限制有助于防止内存不足错误。默认值为 “10000”。"],["body","\n"],["body","See Settings to prevent mapping explosion regarding additional settings for preventing mappings explosion."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/13.IP.html"],["title","IP.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","ip-field-type"],["heading","IP field type"],["body","\n"],["body","An ip field can index/store either IPv4 or IPv6 addresses."],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"ip_addr\": {\n        \"type\": \"ip\"\n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/1\n{\n  \"ip_addr\": \"192.168.1.1\"\n}\n\nGET my-index-000001/_search\n{\n  \"query\": {\n    \"term\": {\n      \"ip_addr\": \"192.168.0.0/16\"\n    }\n  }\n}\n"],["body","\n"],["body","You can also store ip ranges in a single field using an ip_range data type."],["body","\n"],["headingLink","parameters-for-ip-fields"],["heading","Parameters for ip fields"],["body","\n"],["body","The following parameters are accepted by ip fields:"],["body","\n"],["body","\n"],["body","boost"],["body","Mapping field-level query time boosting. Accepts a floating point number, defaults to 1.0."],["body","\n"],["body","doc_values"],["body","Should the field be stored on disk in a column-stride fashion, so that it can later be used for sorting, aggregations, or scripting? Accepts true (default) or false."],["body","\n"],["body","ignore_malformed"],["body","If true, malformed IP addresses are ignored. If false (default), malformed IP addresses throw an exception and reject the whole document. Note that this cannot be set if the script parameter is used."],["body","\n"],["body","index"],["body","Should the field be searchable? Accepts true (default) and false."],["body","\n"],["body","null_value"],["body","Accepts an IPv4 or IPv6 value which is substituted for any explicit null values. Defaults to null, which means the field is treated as missing. Note that this cannot be set if the script parameter is used."],["body","\n"],["body","on_script_error"],["body","Defines what to do if the script defined by the script parameter throws an error at indexing time. Accepts reject (default), which will cause the entire document to be rejected, and ignore, which will register the field in the document’s _ignored metadata field and continue indexing. This parameter can only be set if the script field is also set."],["body","\n"],["body","script"],["body","If this parameter is set, then the field will index values generated by this script, rather than reading the values directly from the source. If a value is set for this field on the input document, then the document will be rejected with an error. Scripts are in the same format as their runtime equivalent, and should emit strings containing IPv4 or IPv6 formatted addresses."],["body","\n"],["body","store"],["body","Whether the field value should be stored and retrievable separately from the _source field. Accepts true or false (default)."],["body","\n\n\n"],["headingLink","querying-ip-fields"],["heading","Querying ip fields"],["body","\n"],["body","The most common way to query ip addresses is to use the CIDR notation: [ip_address]/[prefix_length]. For instance:"],["body","\n"],["body","GET my-index-000001/_search\n{\n  \"query\": {\n    \"term\": {\n      \"ip_addr\": \"192.168.0.0/16\"\n    }\n  }\n}\n"],["body","\n"],["body","or"],["body","\n"],["body","GET my-index-000001/_search\n{\n  \"query\": {\n    \"term\": {\n      \"ip_addr\": \"2001:db8::/48\"\n    }\n  }\n}\n"],["body","\n"],["body","Also beware that colons are special characters to the query_string query, so ipv6 addresses will need to be escaped. The easiest way to do so is to put quotes around the searched value:"],["body","\n"],["body","GET my-index-000001/_search\n{\n  \"query\": {\n    \"query_string\" : {\n      \"query\": \"ip_addr:\\\"2001:db8::/48\\\"\"\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/11.Geo-shape.html"],["title","Geo-shape.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","geo-shape-field-type"],["heading","Geo-shape field type"],["body","\n\n"],["body","\n"],["body","'geo_shape' 数据类型有助于对任意地理形状 (例如矩形和多边形) 进行索引和搜索。"],["body","\n"],["body","\n"],["body","\n"],["body","You can query documents using this type using geo_shape Query."],["body","\n"],["body","\n"],["body","\n"],["body","Elasticsearch encodes geo_shape values as BKD trees by default. To use BKD encoding, do not specify the following mapping options:"],["body","\n\n"],["body","\n"],["body","distance_error_pct"],["body","\n"],["body","\n"],["body","\n"],["body","points_only"],["body","\n"],["body","\n"],["body","\n"],["body","precision"],["body","\n"],["body","\n"],["body","\n"],["body","strategy"],["body","\n"],["body","\n"],["body","\n"],["body","tree_levels"],["body","\n"],["body","\n"],["body","\n"],["body","tree"],["body","\n"],["body","\n\n"],["body","\n\n"],["body","如果指定这些选项中的一个或多个，则该字段将使用前缀树编码。前缀树编码已弃用。"],["body","\n"],["headingLink","mapping-options"],["heading","Mapping Options"],["body","\n"],["body","The geo_shape mapping maps geo_json geometry objects to the geo_shape type. To enable it, users must explicitly map fields to the geo_shape type."],["body","\n"],["body","Option"],["body","Description"],["body","Default"],["body","\n"],["body","tree"],["body","[6.6] Deprecated in 6.6. PrefixTrees no longer usedName of the PrefixTree implementation to be used: geohash for GeohashPrefixTree and quadtree for QuadPrefixTree. Note: This parameter is only relevant for term and recursive strategies."],["body","quadtree"],["body","\n"],["body","precision"],["body","[6.6] Deprecated in 6.6. PrefixTrees no longer usedThis parameter may be used instead of tree_levels to set an appropriate value for the tree_levels parameter. The value specifies the desired precision and Elasticsearch will calculate the best tree_levels value to honor this precision. The value should be a number followed by an optional distance unit. Valid distance units include: in, inch, yd, yard, mi, miles, km, kilometers, m,meters, cm,centimeters, mm, millimeters. Note: This parameter is only relevant for term and recursive strategies."],["body","50m"],["body","\n"],["body","tree_levels"],["body","[6.6] Deprecated in 6.6. PrefixTrees no longer usedMaximum number of layers to be used by the PrefixTree. This can be used to control the precision of shape representations andtherefore how many terms are indexed. Defaults to the default value of the chosen PrefixTree implementation. Since this parameter requires a certain level of understanding of the underlying implementation, users may use the precision parameter instead. However, Elasticsearch only uses the tree_levels parameter internally and this is what is returned via the mapping API even if you use the precision parameter. Note: This parameter is only relevant for term and recursive strategies."],["body","various"],["body","\n"],["body","strategy"],["body","[6.6] Deprecated in 6.6. PrefixTrees no longer usedThe strategy parameter defines the approach for how to represent shapes at indexing and search time. It also influences the capabilities available so it is recommended to let Elasticsearch set this parameter automatically. There are two strategies available: recursive, and term. Recursive and Term strategies are deprecated and will be removed in a future version. While they are still available, the Term strategy supports point types only (the points_only parameter will be automatically set to true) while Recursive strategy supports all shape types. (IMPORTANT: see Prefix trees for more detailed information about these strategies)"],["body","recursive"],["body","\n"],["body","distance_error_pct"],["body","[6.6] Deprecated in 6.6. PrefixTrees no longer usedUsed as a hint to the PrefixTree about how precise it should be. Defaults to 0.025 (2.5%) with 0.5 as the maximum supported value. PERFORMANCE NOTE: This value will default to 0 if a precision or tree_level definition is explicitly defined. This guarantees spatial precision at the level defined in the mapping. This can lead to significant memory usage for high resolution shapes with low error (e.g., large shapes at 1m with < 0.001 error). To improve indexing performance (at the cost of query accuracy) explicitly define tree_level or precision along with a reasonable distance_error_pct, noting that large shapes will have greater false positives. Note: This parameter is only relevant for term and recursive strategies."],["body","0.025"],["body","\n"],["body","orientation"],["body","Optional. Vertex order for the shape’s coordinates list.This parameter sets and returns only a RIGHT (counterclockwise) or LEFT (clockwise) value. However, you can specify either value in multiple ways.To set RIGHT, use one of the following arguments or its uppercase variant:right``counterclockwise``ccwTo set LEFT, use one of the following arguments or its uppercase variant:left``clockwise``cwDefaults to RIGHT to comply with OGC standards. OGC standards define outer ring vertices in counterclockwise order with inner ring (hole) vertices in clockwise order.Individual GeoJSON or WKT documents can override this parameter."],["body","RIGHT"],["body","\n"],["body","points_only"],["body","[6.6] Deprecated in 6.6. PrefixTrees no longer usedSetting this option to true (defaults to false) configures the geo_shape field type for point shapes only (NOTE: Multi-Points are not yet supported). This optimizes index and search performance for the geohash and quadtree when it is known that only points will be indexed. At present geo_shape queries can not be executed on geo_point field types. This option bridges the gap by improving point performance on a geo_shape field so that geo_shape queries are optimal on a point only field."],["body","false"],["body","\n"],["body","ignore_malformed"],["body","If true, malformed GeoJSON or WKT shapes are ignored. If false (default), malformed GeoJSON and WKT shapes throw an exception and reject the entire document."],["body","false"],["body","\n"],["body","ignore_z_value"],["body","If true (default) three dimension points will be accepted (stored in source) but only latitude and longitude values will be indexed; the third dimension is ignored. If false, geo-points containing any more than latitude and longitude (two dimensions) values throw an exception and reject the whole document."],["body","true"],["body","\n"],["body","coerce"],["body","If true unclosed linear rings in polygons will be automatically closed."],["body","false"],["body","\n\n\n"],["headingLink","indexing-approach"],["heading","Indexing approach"],["body","\n"],["body","GeoShape types are indexed by decomposing the shape into a triangular mesh and indexing each triangle as a 7 dimension point in a BKD tree. This provides near perfect spatial resolution (down to 1e-7 decimal degree precision) since all spatial relations are computed using an encoded vector representation of the original shape instead of a raster-grid representation as used by the Prefix trees indexing approach. Performance of the tessellator primarily depends on the number of vertices that define the polygon/multi-polygon. While this is the default indexing technique prefix trees can still be used by setting the tree or strategy parameters according to the appropriate Mapping Options. Note that these parameters are now deprecated and will be removed in a future version."],["body","\n"],["body","IMPORTANT NOTES"],["body","\n"],["body","CONTAINS relation query - when using the new default vector indexing strategy, geo_shape queries with relation defined as contains are supported for indices created with ElasticSearch 7.5.0 or higher."],["body","\n"],["headingLink","prefix-trees"],["heading","Prefix trees"],["body","\n"],["body","[6.6] Deprecated in 6.6. PrefixTrees no longer usedTo efficiently represent shapes in an inverted index, Shapes are converted into a series of hashes representing grid squares (commonly referred to as \"rasters\") using implementations of a PrefixTree. The tree notion comes from the fact that the PrefixTree uses multiple grid layers, each with an increasing level of precision to represent the Earth. This can be thought of as increasing the level of detail of a map or image at higher zoom levels. Since this approach causes precision issues with indexed shape, it has been deprecated in favor of a vector indexing approach that indexes the shapes as a triangular mesh (see Indexing approach)."],["body","\n"],["body","Multiple PrefixTree implementations are provided:"],["body","\n\n"],["body","GeohashPrefixTree - Uses geohashes for grid squares. Geohashes are base32 encoded strings of the bits of the latitude and longitude interleaved. So the longer the hash, the more precise it is. Each character added to the geohash represents another tree level and adds 5 bits of precision to the geohash. A geohash represents a rectangular area and has 32 sub rectangles. The maximum number of levels in Elasticsearch is 24; the default is 9."],["body","\n"],["body","QuadPrefixTree - Uses a quadtree for grid squares. Similar to geohash, quad trees interleave the bits of the latitude and longitude the resulting hash is a bit set. A tree level in a quad tree represents 2 bits in this bit set, one for each coordinate. The maximum number of levels for the quad trees in Elasticsearch is 29; the default is 21."],["body","\n\n"],["headingLink","spatial-strategies"],["heading","Spatial strategies"],["body","\n"],["body","[6.6] Deprecated in 6.6. PrefixTrees no longer usedThe indexing implementation selected relies on a SpatialStrategy for choosing how to decompose the shapes (either as grid squares or a tessellated triangular mesh). Each strategy answers the following:"],["body","\n\n"],["body","What type of Shapes can be indexed?"],["body","\n"],["body","What types of Query Operations and Shapes can be used?"],["body","\n"],["body","Does it support more than one Shape per field?"],["body","\n\n"],["body","The following Strategy implementations (with corresponding capabilities) are provided:"],["body","\n"],["body","Strategy"],["body","Supported Shapes"],["body","Supported Queries"],["body","Multiple Shapes"],["body","\n"],["body","recursive"],["body","All"],["body","INTERSECTS, DISJOINT, WITHIN, CONTAINS"],["body","Yes"],["body","\n"],["body","term"],["body","Points"],["body","INTERSECTS"],["body","Yes"],["body","\n\n\n"],["headingLink","accuracy"],["heading","Accuracy"],["body","\n"],["body","Recursive and Term strategies do not provide 100% accuracy and depending on how they are configured it may return some false positives for INTERSECTS, WITHIN and CONTAINS queries, and some false negatives for DISJOINT queries. To mitigate this, it is important to select an appropriate value for the tree_levels parameter and to adjust expectations accordingly. For example, a point may be near the border of a particular grid cell and may thus not match a query that only matches the cell right next to it — even though the shape is very close to the point."],["body","\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","PUT /example\n{\n  \"mappings\": {\n    \"properties\": {\n      \"location\": {\n        \"type\": \"geo_shape\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","This mapping definition maps the location field to the geo_shape type using the default vector implementation. It provides approximately 1e-7 decimal degree precision."],["body","\n"],["headingLink","performance-considerations-with-prefix-trees"],["heading","Performance considerations with Prefix Trees"],["body","\n"],["body","[6.6] Deprecated in 6.6. PrefixTrees no longer usedWith prefix trees, Elasticsearch uses the paths in the tree as terms in the inverted index and in queries. The higher the level (and thus the precision), the more terms are generated. Of course, calculating the terms, keeping them in memory, and storing them on disk all have a price. Especially with higher tree levels, indices can become extremely large even with a modest amount of data. Additionally, the size of the features also matters. Big, complex polygons can take up a lot of space at higher tree levels. Which setting is right depends on the use case. Generally one trades off accuracy against index size and query performance."],["body","\n"],["body","The defaults in Elasticsearch for both implementations are a compromise between index size and a reasonable level of precision of 50m at the equator. This allows for indexing tens of millions of shapes without overly bloating the resulting index too much relative to the input size."],["body","\n"],["body","Geo-shape queries on geo-shapes implemented with PrefixTrees will not be executed if search.allow_expensive_queries is set to false."],["body","\n"],["headingLink","input-structure"],["heading","Input Structure"],["body","\n"],["body","Shapes can be represented using either the GeoJSON or Well-Known Text (WKT) format. The following table provides a mapping of GeoJSON and WKT to Elasticsearch types:"],["body","\n"],["body","GeoJSON Type"],["body","WKT Type"],["body","Elasticsearch Type"],["body","Description"],["body","\n"],["body","Point"],["body","POINT"],["body","point"],["body","A single geographic coordinate. Note: Elasticsearch uses WGS-84 coordinates only."],["body","\n"],["body","LineString"],["body","LINESTRING"],["body","linestring"],["body","An arbitrary line given two or more points."],["body","\n"],["body","Polygon"],["body","POLYGON"],["body","polygon"],["body","A closed polygon whose first and last point must match, thus requiring n + 1 vertices to create an n-sided polygon and a minimum of 4 vertices."],["body","\n"],["body","MultiPoint"],["body","MULTIPOINT"],["body","multipoint"],["body","An array of unconnected, but likely related points."],["body","\n"],["body","MultiLineString"],["body","MULTILINESTRING"],["body","multilinestring"],["body","An array of separate linestrings."],["body","\n"],["body","MultiPolygon"],["body","MULTIPOLYGON"],["body","multipolygon"],["body","An array of separate polygons."],["body","\n"],["body","GeometryCollection"],["body","GEOMETRYCOLLECTION"],["body","geometrycollection"],["body","A GeoJSON shape similar to the multi* shapes except that multiple types can coexist (e.g., a Point and a LineString)."],["body","\n"],["body","N/A"],["body","BBOX"],["body","envelope"],["body","A bounding rectangle, or envelope, specified by specifying only the top left and bottom right points."],["body","\n"],["body","N/A"],["body","N/A"],["body","circle"],["body","A circle specified by a center point and radius with units, which default to METERS."],["body","\n\n\n"],["body","For all types, both the inner type and coordinates fields are required."],["body","\n"],["body","In GeoJSON and WKT, and therefore Elasticsearch, the correct coordinate order is longitude, latitude (X, Y) within coordinate arrays. This differs from many Geospatial APIs (e.g., Google Maps) that generally use the colloquial latitude, longitude (Y, X)."],["body","\n"],["headingLink","point"],["heading","Point"],["body","\n"],["body","A point is a single geographic coordinate, such as the location of a building or the current position given by a smartphone’s Geolocation API. The following is an example of a point in GeoJSON."],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : {\n    \"type\" : \"point\",\n    \"coordinates\" : [-77.03653, 38.897676]\n  }\n}\n"],["body","\n"],["body","Copy as curlView in Console"],["body","\n"],["body","The following is an example of a point in WKT:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : \"POINT (-77.03653 38.897676)\"\n}\n"],["body","\n"],["body","Copy as curlView in Console"],["body","\n"],["headingLink","linestring"],["heading","LineString"],["body","\n"],["body","A linestring defined by an array of two or more positions. By specifying only two points, the linestring will represent a straight line. Specifying more than two points creates an arbitrary path. The following is an example of a LineString in GeoJSON."],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : {\n    \"type\" : \"linestring\",\n    \"coordinates\" : [[-77.03653, 38.897676], [-77.009051, 38.889939]]\n  }\n}\n"],["body","\n"],["body","Copy as curlView in Console"],["body","\n"],["body","The following is an example of a LineString in WKT:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : \"LINESTRING (-77.03653 38.897676, -77.009051 38.889939)\"\n}\n"],["body","\n"],["body","Copy as curlView in Console"],["body","\n"],["body","The above linestring would draw a straight line starting at the White House to the US Capitol Building."],["body","\n"],["headingLink","polygon"],["heading","Polygon"],["body","\n"],["body","A polygon is defined by a list of a list of points. The first and last points in each (outer) list must be the same (the polygon must be closed). The following is an example of a Polygon in GeoJSON."],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : {\n    \"type\" : \"polygon\",\n    \"coordinates\" : [\n      [ [100.0, 0.0], [101.0, 0.0], [101.0, 1.0], [100.0, 1.0], [100.0, 0.0] ]\n    ]\n  }\n}\n"],["body","\n"],["body","Copy as curlView in Console"],["body","\n"],["body","The following is an example of a Polygon in WKT:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : \"POLYGON ((100.0 0.0, 101.0 0.0, 101.0 1.0, 100.0 1.0, 100.0 0.0))\"\n}\n"],["body","\n"],["body","Copy as curlView in Console"],["body","\n"],["body","The first array represents the outer boundary of the polygon, the other arrays represent the interior shapes (\"holes\"). The following is a GeoJSON example of a polygon with a hole:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : {\n    \"type\" : \"polygon\",\n    \"coordinates\" : [\n      [ [100.0, 0.0], [101.0, 0.0], [101.0, 1.0], [100.0, 1.0], [100.0, 0.0] ],\n      [ [100.2, 0.2], [100.8, 0.2], [100.8, 0.8], [100.2, 0.8], [100.2, 0.2] ]\n    ]\n  }\n}\n"],["body","\n"],["body","Copy as curlView in Console"],["body","\n"],["body","The following is an example of a Polygon with a hole in WKT:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : \"POLYGON ((100.0 0.0, 101.0 0.0, 101.0 1.0, 100.0 1.0, 100.0 0.0), (100.2 0.2, 100.8 0.2, 100.8 0.8, 100.2 0.8, 100.2 0.2))\"\n}\n"],["body","\n"],["body","Copy as curlView in Console"],["body","\n"],["body","IMPORTANT NOTE: WKT does not enforce a specific order for vertices thus ambiguous polygons around the dateline and poles are possible. GeoJSON mandates that the outer polygon must be counterclockwise and interior shapes must be clockwise, which agrees with the Open Geospatial Consortium (OGC) Simple Feature Access specification for vertex ordering."],["body","\n"],["body","Elasticsearch accepts both clockwise and counterclockwise polygons if they appear not to cross the dateline (i.e. they cross less than 180° of longitude), but for polygons that do cross the dateline (or for other polygons wider than 180°) Elasticsearch requires the vertex ordering to comply with the OGC and GeoJSON specifications. Otherwise, an unintended polygon may be created and unexpected query/filter results will be returned."],["body","\n"],["body","The following provides an example of an ambiguous polygon. Elasticsearch will apply the GeoJSON standard to eliminate ambiguity resulting in a polygon that crosses the dateline."],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : {\n    \"type\" : \"polygon\",\n    \"coordinates\" : [\n      [ [-177.0, 10.0], [176.0, 15.0], [172.0, 0.0], [176.0, -15.0], [-177.0, -10.0], [-177.0, 10.0] ],\n      [ [178.2, 8.2], [-178.8, 8.2], [-180.8, -8.8], [178.2, 8.8] ]\n    ]\n  }\n}\n"],["body","\n"],["body","An orientation parameter can be defined when setting the geo_shape mapping (see Mapping Options). This will define vertex order for the coordinate list on the mapped geo_shape field. It can also be overridden on each document. The following is an example for overriding the orientation on a document:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : {\n    \"type\" : \"polygon\",\n    \"orientation\" : \"clockwise\",\n    \"coordinates\" : [\n      [ [100.0, 0.0], [100.0, 1.0], [101.0, 1.0], [101.0, 0.0], [100.0, 0.0] ]\n    ]\n  }\n}\n"],["body","\n"],["headingLink","multipoint"],["heading","MultiPoint"],["body","\n"],["body","The following is an example of a list of geojson points:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : {\n    \"type\" : \"multipoint\",\n    \"coordinates\" : [\n      [102.0, 2.0], [103.0, 2.0]\n    ]\n  }\n}\n"],["body","\n"],["body","The following is an example of a list of WKT points:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : \"MULTIPOINT (102.0 2.0, 103.0 2.0)\"\n}\n"],["body","\n"],["headingLink","multilinestring"],["heading","MultiLineString"],["body","\n"],["body","The following is an example of a list of geojson linestrings:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : {\n    \"type\" : \"multilinestring\",\n    \"coordinates\" : [\n      [ [102.0, 2.0], [103.0, 2.0], [103.0, 3.0], [102.0, 3.0] ],\n      [ [100.0, 0.0], [101.0, 0.0], [101.0, 1.0], [100.0, 1.0] ],\n      [ [100.2, 0.2], [100.8, 0.2], [100.8, 0.8], [100.2, 0.8] ]\n    ]\n  }\n}\n"],["body","\n"],["body","The following is an example of a list of WKT linestrings:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : \"MULTILINESTRING ((102.0 2.0, 103.0 2.0, 103.0 3.0, 102.0 3.0), (100.0 0.0, 101.0 0.0, 101.0 1.0, 100.0 1.0), (100.2 0.2, 100.8 0.2, 100.8 0.8, 100.2 0.8))\"\n}\n"],["body","\n"],["headingLink","multipolygon"],["heading","MultiPolygon"],["body","\n"],["body","The following is an example of a list of geojson polygons (second polygon contains a hole):"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : {\n    \"type\" : \"multipolygon\",\n    \"coordinates\" : [\n      [ [[102.0, 2.0], [103.0, 2.0], [103.0, 3.0], [102.0, 3.0], [102.0, 2.0]] ],\n      [ [[100.0, 0.0], [101.0, 0.0], [101.0, 1.0], [100.0, 1.0], [100.0, 0.0]],\n        [[100.2, 0.2], [100.8, 0.2], [100.8, 0.8], [100.2, 0.8], [100.2, 0.2]] ]\n    ]\n  }\n}\n"],["body","\n"],["body","Copy as curlView in Console"],["body","\n"],["body","The following is an example of a list of WKT polygons (second polygon contains a hole):"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : \"MULTIPOLYGON (((102.0 2.0, 103.0 2.0, 103.0 3.0, 102.0 3.0, 102.0 2.0)), ((100.0 0.0, 101.0 0.0, 101.0 1.0, 100.0 1.0, 100.0 0.0), (100.2 0.2, 100.8 0.2, 100.8 0.8, 100.2 0.8, 100.2 0.2)))\"\n}\n"],["body","\n"],["headingLink","geometry-collection"],["heading","Geometry Collection"],["body","\n"],["body","The following is an example of a collection of geojson geometry objects:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : {\n    \"type\": \"geometrycollection\",\n    \"geometries\": [\n      {\n        \"type\": \"point\",\n        \"coordinates\": [100.0, 0.0]\n      },\n      {\n        \"type\": \"linestring\",\n        \"coordinates\": [ [101.0, 0.0], [102.0, 1.0] ]\n      }\n    ]\n  }\n}\n"],["body","\n"],["body","The following is an example of a collection of WKT geometry objects:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : \"GEOMETRYCOLLECTION (POINT (100.0 0.0), LINESTRING (101.0 0.0, 102.0 1.0))\"\n}\n"],["body","\n"],["headingLink","envelope"],["heading","Envelope"],["body","\n"],["body","Elasticsearch supports an envelope type, which consists of coordinates for upper left and lower right points of the shape to represent a bounding rectangle in the format [[minLon, maxLat], [maxLon, minLat]]:"],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : {\n    \"type\" : \"envelope\",\n    \"coordinates\" : [ [100.0, 1.0], [101.0, 0.0] ]\n  }\n}\n"],["body","\n"],["body","The following is an example of an envelope using the WKT BBOX format:"],["body","\n"],["body","NOTE: WKT specification expects the following order: minLon, maxLon, maxLat, minLat."],["body","\n"],["body","POST /example/_doc\n{\n  \"location\" : \"BBOX (100.0, 102.0, 2.0, 0.0)\"\n}\n"],["body","\n"],["headingLink","circle"],["heading","Circle"],["body","\n"],["body","Elasticsearch supports a circle type, which consists of a center point with a radius."],["body","\n"],["body","You cannot index the circle type using the default BKD tree indexing approach. Instead, use a circle ingest processor to approximate the circle as a polygon."],["body","\n"],["body","The circle type requires a geo_shape field mapping with the deprecated recursive Prefix Tree strategy."],["body","\n"],["body","PUT /circle-example\n{\n  \"mappings\": {\n    \"properties\": {\n      \"location\": {\n        \"type\": \"geo_shape\",\n        \"strategy\": \"recursive\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","The following request indexes a circle geo-shape."],["body","\n"],["body","POST /circle-example/_doc\n{\n  \"location\" : {\n    \"type\" : \"circle\",\n    \"coordinates\" : [101.0, 1.0],\n    \"radius\" : \"100m\"\n  }\n}\n"],["body","\n"],["body","Note: The inner radius field is required. If not specified, then the units of the radius will default to METERS."],["body","\n"],["body","NOTE: Neither GeoJSON or WKT support a point-radius circle type."],["body","\n"],["headingLink","sorting-and-retrieving-index-shapes"],["heading","Sorting and Retrieving index Shapes"],["body","\n"],["body","Due to the complex input structure and index representation of shapes, it is not currently possible to sort shapes or retrieve their fields directly. The geo_shape value is only retrievable through the _source field."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/1.AggregateMetric.html"],["title","AggregateMetric.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","aggregate-metric-field-type"],["heading","Aggregate metric field type"],["body","\n\n"],["body","\n"],["body","为  metric aggregations.  存储预聚合的数值"],["body","\n"],["body","\n"],["body","\n"],["body","Aggregate_metric_double     字段是一个包含以下一个或多个度量子字段的对象: 'min'，'max'，'sum' 和 'value_count'。"],["body","\n"],["body","\n"],["body","\n"],["body","当您在 aggregate_metric_double 字段上运行某些度量聚合时， 聚合使用相关子字段的值。"],["body","\n"],["body","\n"],["body","\n"],["body","例如，a min aggregation on an aggregate_metric_double field returns the minimum value of all min sub-fields"],["body","\n"],["body","\n"],["body","\n"],["body","对于每个度量子字段，aggregate_metric_double   字段存储单个数字  doc value  ，不支持数组值。'min' 、 'max' 和 'sum' 值是 'double' 数字。“value_count” 是一个正的 “长” 数。"],["body","\n"],["body","\n\n"],["body","PUT my-index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my-agg-metric-field\": {\n        \"type\": \"aggregate_metric_double\",\n        \"metrics\": [ \"min\", \"max\", \"sum\", \"value_count\" ],\n        \"default_metric\": \"max\"\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","parameters-for-aggregate_metric_double-fields"],["heading","Parameters for aggregate_metric_double fields"],["body","\n\n"],["body","\n"],["body","metrics"],["body","\n"],["body","(Required, array of strings) Array of metric sub-fields to store. Each value corresponds to a metric aggregation. Valid values are min, max, sum, and value_count. You must specify at least one value."],["body","\n"],["body","\n"],["body","\n"],["body","default_metric"],["body","\n"],["body","(Required, string) Default metric sub-field to use for queries, scripts, and aggregations that don’t use a sub-field. Must be a value from the metrics array."],["body","\n"],["body","\n\n"],["headingLink","uses"],["heading","Uses"],["body","\n"],["body","We designed aggregate_metric_double fields for use with the following aggregations:"],["body","\n\n"],["body","A min aggregation returns the minimum value of all min sub-fields."],["body","\n"],["body","A max aggregation returns the maximum value of all max sub-fields."],["body","\n"],["body","A sum aggregation returns the sum of the values of all sum sub-fields."],["body","\n"],["body","A value_count aggregation returns the sum of the values of all value_count sub-fields."],["body","\n"],["body","A avg aggregation. There is no avg sub-field; the result of the avg aggregation is computed using the sum and value_count metrics. To run an avg aggregation, the field must contain both sum and value_count metric sub-field"],["body","\n\n"],["body","If you use an aggregate_metric_double field with other aggregations, the field uses the default_metric value, which behaves as a double field. The default_metric is also used in scripts and the following queries:"],["body","\n\n"],["body","exists"],["body","\n"],["body","range"],["body","\n"],["body","term"],["body","\n"],["body","terms"],["body","\n\n"],["headingLink","examples"],["heading","Examples"],["body","\n"],["body","The following create index API request creates an index with an aggregate_metric_double field named agg_metric. The request sets max as the field’s default_metric."],["body","\n"],["body","PUT stats-index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"agg_metric\": {\n        \"type\": \"aggregate_metric_double\",\n        \"metrics\": [ \"min\", \"max\", \"sum\", \"value_count\" ],\n        \"default_metric\": \"max\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","The following index API request adds documents with pre-aggregated data in the agg_metric field."],["body","\n"],["body","PUT stats-index/_doc/1\n{\n  \"agg_metric\": {\n    \"min\": -302.50,\n    \"max\": 702.30,\n    \"sum\": 200.0,\n    \"value_count\": 25\n  }\n}\n\nPUT stats-index/_doc/2\n{\n  \"agg_metric\": {\n    \"min\": -93.00,\n    \"max\": 1702.30,\n    \"sum\": 300.00,\n    \"value_count\": 25\n  }\n}\n"],["body","\n"],["body","You can run min, max, sum, value_count, and avg aggregations on a agg_metric field."],["body","\n"],["body","POST stats-index/_search?size=0\n{\n  \"aggs\": {\n    \"metric_min\": { \"min\": { \"field\": \"agg_metric\" } },\n    \"metric_max\": { \"max\": { \"field\": \"agg_metric\" } },\n    \"metric_value_count\": { \"value_count\": { \"field\": \"agg_metric\" } },\n    \"metric_sum\": { \"sum\": { \"field\": \"agg_metric\" } },\n    \"metric_avg\": { \"avg\": { \"field\": \"agg_metric\" } }\n  }\n}\n"],["body","\n"],["body","The aggregation results are based on related metric sub-field values."],["body","\n"],["body","{\n...\n  \"aggregations\": {\n    \"metric_min\": {\n      \"value\": -302.5\n    },\n    \"metric_max\": {\n      \"value\": 1702.3\n    },\n    \"metric_value_count\": {\n      \"value\": 50\n    },\n    \"metric_sum\": {\n      \"value\": 500.0\n    },\n    \"metric_avg\": {\n      \"value\": 10.0\n    }\n  }\n}\n"],["body","\n"],["body","Queries on a aggregate_metric_double field use the default_metric value."],["body","\n"],["body","GET stats-index/_search\n{\n  \"query\": {\n    \"term\": {\n      \"agg_metric\": {\n        \"value\": 702.30\n      }\n    }\n  }\n}\n"],["body","\n"],["body","The search returns the following hit. The value of the default_metric field, max, matches the query value."],["body","\n"],["body","{\n  ...\n    \"hits\": {\n    \"total\": {\n      \"value\": 1,\n      \"relation\": \"eq\"\n    },\n    \"max_score\": 1.0,\n    \"hits\": [\n      {\n        \"_index\": \"stats-index\",\n        \"_type\": \"_doc\",\n        \"_id\": \"1\",\n        \"_score\": 1.0,\n        \"_source\": {\n          \"agg_metric\": {\n            \"min\": -302.5,\n            \"max\": 702.3,\n            \"sum\": 200.0,\n            \"value_count\": 25\n          }\n        }\n      }\n    ]\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/22.RankFeature.html"],["title","RankFeature.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","rank-feature-field-type"],["heading","Rank feature field type"],["body","\n"],["body","A rank_feature field can index numbers so that they can later be used to boost documents in queries with a rank_feature query."],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"pagerank\": {\n        \"type\": \"rank_feature\" //Rank feature fields must use the `rank_feature` field type\n      },\n      \"url_length\": {\n        \"type\": \"rank_feature\",\n        \"positive_score_impact\": false \n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/1 \n{\n  \"pagerank\": 8,//Rank features that correlate negatively with the score need to declare it\n  \"url_length\": 22\n}\n\nGET my-index-000001/_search\n{\n  \"query\": {\n    \"rank_feature\": {\n      \"field\": \"pagerank\"\n    }\n  }\n}\n"],["body","\n\n"],["body","\n"],["body","rank_feature fields only support single-valued fields and strictly positive values. Multi-valued fields and negative values will be rejected."],["body","\n"],["body","\n"],["body","\n"],["body","rank_feature fields do not support querying, sorting or aggregating. They may only be used within rank_feature queries."],["body","\n"],["body","\n"],["body","\n"],["body","rank_feature fields only preserve 9 significant bits for the precision, which translates to a relative error of about 0.4%."],["body","\n"],["body","\n"],["body","\n"],["body","与得分负相关的排名特征应将 “positive_score_impact” 设置为 “false” (默认为 “true”)。 'Rank_feate' 查询将使用此方法来修改评分公式，以使得分随特征的值而减小而不是增加。例如，在网络搜索中，url长度是与分数负相关的常用功能。"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/27.TokenCount.html"],["title","TokenCount.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","token-count-field-type"],["heading","Token count field type"],["body","\n"],["body","A field of type token_count is really an integer field which accepts string values, analyzes them, then indexes the number of tokens in the string."],["body","\n"],["body","类型为 “token_count” 的字段实际上是一个 “整数” 字段，该字段接受字符串值，对其进行分析，然后索引字符串中token 的数量。"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"name\": { \n        \"type\": \"text\",\n        \"fields\": {\n          \"length\": { \n            \"type\":     \"token_count\",\n            \"analyzer\": \"standard\"\n          }\n        }\n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/1\n{ \"name\": \"John Smith\" }\n\nPUT my-index-000001/_doc/2\n{ \"name\": \"Rachel Alice Williams\" }\n\nGET my-index-000001/_search\n{\n  \"query\": {\n    \"term\": {\n      \"name.length\": 3 \n    }\n  }\n}\n"],["body","\n"],["headingLink","parameters-for-token_count-fields"],["heading","Parameters for token_count fields"],["body","\n"],["body","The following parameters are accepted by token_count fields:"],["body","\n"],["body","\n"],["body","analyzer"],["body","The analyzer which should be used to analyze the string value. Required. For best performance, use an analyzer without token filters."],["body","\n"],["body","enable_position_increments"],["body","Indicates if position increments should be counted. Set to false if you don’t want to count tokens removed by analyzer filters (like stop). Defaults to true."],["body","\n"],["body","boost"],["body","Mapping field-level query time boosting. Accepts a floating point number, defaults to 1.0."],["body","\n"],["body","doc_values"],["body","Should the field be stored on disk in a column-stride fashion, so that it can later be used for sorting, aggregations, or scripting? Accepts true (default) or false."],["body","\n"],["body","index"],["body","Should the field be searchable? Accepts true (default) and false."],["body","\n"],["body","null_value"],["body","Accepts a numeric value of the same type as the field which is substituted for any explicit null values. Defaults to null, which means the field is treated as missing."],["body","\n"],["body","store"],["body","Whether the field value should be stored and retrievable separately from the _source field. Accepts true or false (default)."],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/19.Percolator.html"],["title","Percolator.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","percolator-field-type"],["heading","Percolator field type"],["body","\n\n"],["body","\n"],["body","'percolator' 字段类型将json结构解析为本地查询并存储该查询，"],["body","\n"],["body","\n"],["body","\n"],["body","so that the percolate query can use it to match provided documents."],["body","\n"],["body","\n"],["body","\n"],["body","任何包含json对象的字段都可以配置为 percolator 字段，渗滤器字段类型没有设置，仅配置 “percolator” 字段类型就足以指示Elasticsearch将字段视为查询。"],["body","\n"],["body","\n"],["body","\n"],["body","以下映射为 “查询” 字段配置了 “percolator” 字段类型:"],["body","\n"],["body","\n\n"],["headingLink","什么是percolator查询"],["heading","什么是percolator查询"],["body","\n"],["body","percolator中文含义：渗透器。"],["body","\n"],["body","percolator query等价于渗透查询或者反向查询。"],["body","\n"],["body","将查询存储到索引中，然后通过Percolate API定义文档以检索这些查询。"],["body","\n\n"],["body","传统查询，根据查询语句的查询条件返回文档。query->document。"],["body","\n"],["body","而percolator渗透查询，根据文档返回与之匹配的查询语句。document->query。"],["body","\n\n"],["headingLink","percolator-查询的应用场景"],["heading","percolator 查询的应用场景"],["body","\n"],["body","提供一个存储用户兴趣的平台，以便在每次有新内容进入时将正确的内容（通知警报）发送给正确的用户。"],["body","\n"],["body","用户订阅了特定主题，以便一旦该主题的新文章出现，就会向感兴趣的用户发送通知。"],["body","\n"],["body","应用场景如下："],["body","\n\n"],["body","价格监控"],["body","\n"],["body","新闻警报"],["body","\n"],["body","股票警告"],["body","\n"],["body","日志监控"],["body","\n"],["body","天气预报"],["body","\n"],["body","库存警报"],["body","\n\n"],["headingLink","概念"],["heading","概念"],["body","\n"],["body","Elasticsearch 的正常工作流程是将文档（JSON数据）存储在索引中，然后在执行搜索时通过索引查询这些文档的信息。"],["body","\n"],["body","设想如果反转这种使用流程将如何（即先有查询条件，再有文档），Percolate即可实现这种逆转的流程。"],["body","\n"],["body","其使用流程是先存储search条件，之后使用文档询问是否可命中这些搜索条件。本文接下来将介绍如何构造和使用percolator。"],["body","\n"],["body","Percolation功能围绕percolator字段类型展开。 与其他字段类型一样（先在mappings中定义，再进行写入），不同的是它将搜索条件作为文档进行存储。当存储数据时，索引会将此搜索条件的文档处理为可执行形式，并将其保存以备后用。"],["body","\n"],["body","Percolate query接受一个或多个文档，并返回预先存储的搜索条件文档（该条件至少匹配一个传入的文档）。在执行搜索时，Percolate query的工作原理与其他任何查询模式一样，不同的一些细节将在下文介绍。"],["body","\n"],["headingLink","深入理解"],["heading","深入理解"],["body","\n"],["body","在底层，具有percolate字段的索引将保留于一个隐藏的索引（内存中）"],["body","\n"],["body","查询时，首先将在 percolate query 中列出的文档放入该索引，然后对该索引执行常规查询，看与原始的含 percolate 字段的搜索条件是否匹配。"],["body","\n"],["body","该隐藏索引是从原始 percolator 索引获取其映射的。因此，用于 percolate query 的索引字段需要具有适合原始数据和查询文档数据的mappings配置。"],["body","\n"],["body","这引入了一些索引管理的问题，因为你的索引数据和 percolate query 文档可能以不同的方式使用同一字段。一个简单的方式是使用对象类型（object type) 将 percolate 相关的映射与普通文档映射分开，具体可参考后文给出的例子。"],["body","\n"],["body","假设你使用的查询最初是为另一个索引A中的数据编写的，那么最直接的方法是将数据隔离以避免数据直接写到 percolate 索引中去，并将索引A中根级别的mappings在 percolate索引中进行定义。"],["body","\n"],["body","此外，由于percolate field被解析为搜索条件并在索引时保存，因此在升级ES主版本后可能需要reindex Percolate文档。"],["body","\n"],["headingLink","示例"],["heading","示例"],["body","\n\n"],["body","\n"],["body","在此示例中，我们将建立一个索引，该索引含有保存的玩具名字和玩具价格搜索条件。"],["body","\n"],["body","\n"],["body","\n"],["body","其背后思路是，用户输入搜索词和最高价格，然后在与该玩具名匹配的商品价格低于用户指定价格时立即得到通知"],["body","\n"],["body","\n"],["body","\n"],["body","此外用户还可以打开和关闭这些通知。下面的映射通过percolate索引来支持此功能。与保存的搜索条件本身相关的字段位于search对象中，而与原始玩具相关的字段位于映射的根级别。"],["body","\n"],["body","\n\n"],["body"," PUT toys {\n  \"mappings\": {\n    \"properties\": {\n      \"search\": {\n        \"properties\": {\n          \"query\": {\n            \"type\": \"percolator\"\n          },\n          \"user_id\": {\n            \"type\": \"integer\"\n          },\n          \"enabled\": {\n            \"type\": \"boolean\"\n          }\n        }\n      },\n      \"price\": {\n        \"type\": \"float\"\n      },\n      \"description\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","PUT toys/_doc/1\n{\n  \"search\": {\n    \"user_id\": 5,\n    \"enabled\": true,\n    \"query\": {\n      \"bool\": {\n        \"filter\": [\n          {\n           \"match\": {\n              \"description\": {\n                \"query\": \"nintendo switch\"\n              }\n            }\n          },\n          {\n            \"range\": {\n              \"price\": {\n                \"lte\": 300\n              }\n            }\n          }\n        ] \n      }\n    }\n  }\n}\n"],["body","\n"],["body","检查document文档是否满足 percolate查询"],["body","\n"],["body","\nGET toys/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"filter\": [\n        {\n          \"percolate\": {\n            \"field\": \"search.query\",\n            \"document\": {\n              \"description\": \"Nintendo Switch\",\n              \"price\": 250\n            }\n          }\n        },\n        {\n          \"term\": {\n            \"search.enabled\": true\n          }\n        },\n        {\n          \"term\": {\n            \"search.user_id\": 5\n          }\n        }\n      ]\n    }\n  }\n}\n"],["body","\n"],["body","请注意，此处结合使用了基础字段的查询（search.user_id和search.enabled字段），以及percolator条件字段的查询（search.query），用以对指定的用户ID在启用状态下生效。"],["body","\n"],["body"," {\n  \"took\" : 1,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 1,\n    \"successful\" : 1,\n    \"skipped\" : 0,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : {\n      \"value\" : 1,\n      \"relation\" : \"eq\"\n    },\n    \"max_score\" : 0.0,\n    \"hits\" : [\n      {\n        \"_index\" : \"toys\",\n        \"_type\" : \"_doc\",\n        \"_id\" : \"1\",\n        \"_score\" : 0.0,\n        \"_source\" : {\n          \"search\" : {\n            \"user_id\" : 5,\n            \"enabled\" : true,\n           \"query\" : {\n              \"bool\" : {\n                \"filter\" : [\n                  {\n                    \"match\" : {\n                      \"description\" : {\n                        \"query\" : \"nintendo switch\"\n                      }\n                    }\n                  },\n                  {\n                    \"range\" : {\n                      \"price\" : {\n                        \"lte\" : 300\n                      }\n                    }\n                  }\n                ]\n              }\n            }\n          }\n        },\n        \"fields\" : {\n          \"_percolator_document_slot\" : [\n            0\n          ] \n        }\n      }\n    ] \n  }\n}\n"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"query\": {\n        \"type\": \"percolator\"\n      },\n      \"field\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","Then you can index a query:"],["body","\n"],["body","PUT my-index-000001/_doc/match_value\n{\n  \"query\": {\n    \"match\": {\n      \"field\": \"value\"\n    }\n  }\n}\n"],["body","\n"],["body","percolator 查询中引用的字段必须 已经存在于与用于percolation的索引相关联的映射中"],["body","\n"],["headingLink","reindexing-your-percolator-queries"],["heading","Reindexing your percolator queries"],["body","\n\n"],["body","\n"],["body","有时需要重新索引percolator器查询，才能从新版本中对 “percolator” 字段类型的改进中受益。"],["body","\n"],["body","\n"],["body","\n"],["body","Reindexing percolator queries can be reindexed by using the reindex api. Lets take a look at the following index with a percolator field type:"],["body","\n"],["body","\n\n"],["body","PUT index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"query\" : {\n        \"type\" : \"percolator\"\n      },\n      \"body\" : {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\n\nPOST _aliases\n{\n  \"actions\": [\n    {\n      \"add\": {\n        \"index\": \"index\",\n        \"alias\": \"queries\" \n      }\n    }\n  ]\n}\n\nPUT queries/_doc/1?refresh\n{\n  \"query\" : {\n    \"match\" : {\n      \"body\" : \"quick brown fox\"\n    }\n  }\n}\n"],["body","\n"],["body","始终建议为索引定义别名，以便在重新索引的情况下，系统/应用程序不需要更改即可知道渗滤器查询现在位于不同的索引中。"],["body","\n"],["body","假设您将升级到新的主要版本，并且为了使新的Elasticsearch版本仍然能够读取您的查询，您需要将查询重新索引为当前Elasticsearch版本的新索引:"],["body","\n"],["body","PUT new_index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"query\" : {\n        \"type\" : \"percolator\"\n      },\n      \"body\" : {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\n\nPOST /_reindex?refresh\n{\n  \"source\": {\n    \"index\": \"index\"\n  },\n  \"dest\": {\n    \"index\": \"new_index\"\n  }\n}\n\nPOST _aliases\n{\n  \"actions\": [ \n    {\n      \"remove\": {\n        \"index\" : \"index\",\n        \"alias\": \"queries\"\n      }\n    },\n    {\n      \"add\": {\n        \"index\": \"new_index\",\n        \"alias\": \"queries\"\n      }\n    }\n  ]\n}\n"],["body","\n"],["body","Executing the percolate query via the queries alias:"],["body","\n"],["body","GET /queries/_search\n{\n  \"query\": {\n    \"percolate\" : {\n      \"field\" : \"query\",\n      \"document\" : {\n        \"body\" : \"fox jumps over the lazy dog\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","now returns matches from the new index:"],["body","\n"],["body","{\n  \"took\": 3,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\" : 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\" : {\n        \"value\": 1,\n        \"relation\": \"eq\"\n    },\n    \"max_score\": 0.13076457,\n    \"hits\": [\n      {\n        \"_index\": \"new_index\", \n        \"_type\": \"_doc\",\n        \"_id\": \"1\",\n        \"_score\": 0.13076457,\n        \"_source\": {\n          \"query\": {\n            \"match\": {\n              \"body\": \"quick brown fox\"\n            }\n          }\n        },\n        \"fields\" : {\n          \"_percolator_document_slot\" : [0]\n        }\n      }\n    ]\n  }\n}\n"],["body","\n"],["headingLink","optimizing-query-time-text-analysis"],["heading","Optimizing query time text analysis"],["body","\n\n"],["body","当percolator验证要解析的percolator候选匹配时，执行查询时文本分析，最终 对正在 percolator 的文档执行 percolator query"],["body","\n"],["body","每个候选的 precolate 匹配和每次 'percolate' 查询执行完成的。"],["body","\n"],["body","如果您的查询时文本分析是查询解析中相对昂贵的一部分，那么文本分析可能成为percolate 花费时间的主要因素"],["body","\n"],["body","当percolate 最终验证许多候选percolator 查询匹配时，此查询解析开销可能会变得明显。"],["body","\n"],["body","为了避免在percolator时间进行文本分析中最昂贵的部分 .索引 percolator query时，可以选择进行文本分析的昂贵部分 . 这需要使用两个不同的分析器\n\n"],["body","第一个分析器实际上执行需要执行的文本分析 (昂贵的部分)。"],["body","\n"],["body","第二个分析器 (通常是空白) 只是分割第一个分析器已经产生的生成的标记."],["body","\n"],["body","然后在索引percolator query 之前，analyze api应该是s，以使用更昂贵的分析器分析查询文本。analyze api的结果，tokens，应用于替换percolator查询中的原始查询文本。重要的是，现在应该将查询配置为从映射中覆盖分析器，而仅覆盖第二个分析器。大多数基于文本的查询都支持 'analyzer' 选项 ('match' 、 'query_string '、 'simple_query_string')。使用这种方法，昂贵的文本分析被执行一次，而不是多次。"],["body","\n\n"],["body","\n\n"],["body","让我们通过一个简化的例子来演示这个工作流程。"],["body","\n"],["body","假设我们要索引以下percolator查询:"],["body","\n"],["body","{\n  \"query\" : {\n    \"match\" : {\n      \"body\" : {\n        \"query\" : \"missing bicycles\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","with these settings and mapping:"],["body","\n"],["body","PUT /test_index\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\" : {\n          \"tokenizer\": \"standard\",//For the purpose of this example, this analyzer is considered expensive.\n          \"filter\" : [\"lowercase\", \"porter_stem\"]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"query\" : {\n        \"type\": \"percolator\"\n      },\n      \"body\" : {\n        \"type\": \"text\",\n        \"analyzer\": \"my_analyzer\" \n      }\n    }\n  }\n}\n"],["body","\n"],["body","首先，我们需要使用analyze api在索引之前执行文本分析:"],["body","\n"],["body","POST /test_index/_analyze\n{\n  \"analyzer\" : \"my_analyzer\",\n  \"text\" : \"missing bicycles\"\n}\n"],["body","\n"],["body","This results the following response:"],["body","\n"],["body","{\n  \"tokens\": [\n    {\n      \"token\": \"miss\",\n      \"start_offset\": 0,\n      \"end_offset\": 7,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"bicycl\",\n      \"start_offset\": 8,\n      \"end_offset\": 16,\n      \"type\": \"<ALPHANUM>\",\n      \"position\": 1\n    }\n  ]\n}\n"],["body","\n"],["body","返回的顺序中的所有令牌都需要替换percolator查询中的查询文本:"],["body","\n"],["body","PUT /test_index/_doc/1?refresh\n{\n  \"query\" : {\n    \"match\" : {\n      \"body\" : {\n        \"query\" : \"miss bicycl\",\n        \"analyzer\" : \"whitespace\" //在这里选择空格分析器很重要，否则将使用映射中定义的分析器,这就失去了使用这个工作流程的意义。\n      }\n    }\n  }\n}\n"],["body","\n"],["body","请注意，'whitespace' 是一个内置的分析器，如果需要使用不同的分析器，则需要首先在索引的设置中对其进行配置。"],["body","\n"],["body","对于每个percolator query，应在对percolator工作流进行索引之前进行分析api。"],["body","\n"],["body","在percolate 时没有任何变化，percolate查询可以正常定义:"],["body","\n"],["body","GET /test_index/_search\n{\n  \"query\": {\n    \"percolate\" : {\n      \"field\" : \"query\",\n      \"document\" : {\n        \"body\" : \"Bycicles are missing\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","This results in a response like this:"],["body","\n"],["body","{\n  \"took\": 6,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\" : 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\" : {\n        \"value\": 1,\n        \"relation\": \"eq\"\n    },\n    \"max_score\": 0.13076457,\n    \"hits\": [\n      {\n        \"_index\": \"test_index\",\n        \"_type\": \"_doc\",\n        \"_id\": \"1\",\n        \"_score\": 0.13076457,\n        \"_source\": {\n          \"query\": {\n            \"match\": {\n              \"body\": {\n                \"query\": \"miss bicycl\",\n                \"analyzer\": \"whitespace\"\n              }\n            }\n          }\n        },\n        \"fields\" : {\n          \"_percolator_document_slot\" : [0]\n        }\n      }\n    ]\n  }\n}\n"],["body","\n"],["headingLink","optimizing-wildcard-queries"],["heading","Optimizing wildcard queries."],["body","\n\n"],["body","\n"],["body","通配符查询比percolator 的其他查询更昂贵，,特别是如果通配符表达式很大。"],["body","\n"],["body","\n"],["body","\n"],["body","对于带有前缀通配符表达式的 “通配符” 查询或仅使用 “前缀” 查询的情况，可以使用 “edge_ngram” 令牌过滤器将这些查询替换为配置了 “edge_ngram” 令牌过滤器的字段上的常规 “term” 查询。"],["body","\n"],["body","\n\n"],["body","Creating an index with custom analysis settings:"],["body","\n"],["body","PUT my_queries1\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"wildcard_prefix\": { \n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"lowercase\",\n            \"wildcard_edge_ngram\"\n          ]\n        }\n      },\n      \"filter\": {\n        \"wildcard_edge_ngram\": { \n          \"type\": \"edge_ngram\",//Increase the `min_gram` and decrease `max_gram` settings based on your prefix search needs.\n          \"min_gram\": 1, \n          \"max_gram\": 32\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"query\": {\n        \"type\": \"percolator\"\n      },\n      \"my_field\": { //This multifield should be used to do the prefix search with a `term` or `match` query instead of a `prefix` or `wildcard` query.\n        \"type\": \"text\",\n        \"fields\": {\n          \"prefix\": { \n            \"type\": \"text\",\n            \"analyzer\": \"wildcard_prefix\", //The analyzer that generates the prefix tokens to be used at index time only.\n            \"search_analyzer\": \"standard\"\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","然后，而不是索引以下查询:"],["body","\n"],["body","{\n  \"query\": {\n    \"wildcard\": {\n      \"my_field\": \"abc*\"\n    }\n  }\n}\n"],["body","\n"],["body","下面的这个查询应该被索引:"],["body","\n"],["body","PUT /my_queries1/_doc/1?refresh\n{\n  \"query\": {\n    \"term\": {\n      \"my_field.prefix\": \"abc\"\n    }\n  }\n}\n"],["body","\n"],["body","这种方式可以比第一个查询更有效地处理第二个查询。"],["body","\n"],["body","以下搜索请求将与先前索引的渗滤器查询匹配:"],["body","\n"],["body","GET /my_queries1/_search\n{\n  \"query\": {\n    \"percolate\": {\n      \"field\": \"query\",\n      \"document\": {\n        \"my_field\": \"abcd\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","{\n  \"took\": 6,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\" : {\n        \"value\": 1,\n        \"relation\": \"eq\"\n    },\n    \"max_score\": 0.18864399,\n    \"hits\": [\n      {\n        \"_index\": \"my_queries1\",\n        \"_type\": \"_doc\",\n        \"_id\": \"1\",\n        \"_score\": 0.18864399,\n        \"_source\": {\n          \"query\": {\n            \"term\": {\n              \"my_field.prefix\": \"abc\"\n            }\n          }\n        },\n        \"fields\": {\n          \"_percolator_document_slot\": [\n            0\n          ]\n        }\n      }\n    ]\n  }\n}\n"],["body","\n"],["body","同样的技术也可以用来加速后缀通配符搜索。通过在 “edge_ngram” 令牌过滤器之前使用 “reverse” 令牌过滤器。"],["body","\n"],["body","PUT my_queries2\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"wildcard_suffix\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"lowercase\",\n            \"reverse\",\n            \"wildcard_edge_ngram\"\n          ]\n        },\n        \"wildcard_suffix_search_time\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"lowercase\",\n            \"reverse\"\n          ]\n        }\n      },\n      \"filter\": {\n        \"wildcard_edge_ngram\": {\n          \"type\": \"edge_ngram\",\n          \"min_gram\": 1,\n          \"max_gram\": 32\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"query\": {\n        \"type\": \"percolator\"\n      },\n      \"my_field\": {\n        \"type\": \"text\",\n        \"fields\": {\n          \"suffix\": {\n            \"type\": \"text\",\n            \"analyzer\": \"wildcard_suffix\",\n            \"search_analyzer\": \"wildcard_suffix_search_time\" \n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","A custom analyzer is needed at search time too, because otherwise the query terms are not being reversed and would otherwise not match with the reserved suffix tokens."],["body","\n"],["body","Then instead of indexing the following query:"],["body","\n"],["body","{\n  \"query\": {\n    \"wildcard\": {\n      \"my_field\": \"*xyz\"\n    }\n  }\n}\n"],["body","\n"],["body","PUT /my_queries2/_doc/2?refresh\n{\n  \"query\": { //The `match` query should be used instead of the `term` query, because text analysis needs to reverse the query terms.\n    \"match\": { \n      \"my_field.suffix\": \"xyz\"\n    }\n  }\n}\n"],["body","\n"],["body","以下搜索请求将与先前索引的渗滤器查询匹配:"],["body","\n"],["body","GET /my_queries2/_search\n{\n  \"query\": {\n    \"percolate\": {\n      \"field\": \"query\",\n      \"document\": {\n        \"my_field\": \"wxyz\"\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","dedicated-percolator-index"],["heading","Dedicated Percolator Index"],["body","\n\n"],["body","可以将Percolate查询添加到任何索引中"],["body","\n"],["body","不是将percolate查询添加到数据驻留的索引中，而是添加到专用索引"],["body","\n"],["body","这样做的好处是，这种专用的Percolate 索引可以具有自己的索引设置 (例如，主分片和副本分片的数量)。如果选择使用专用的percolate索引，则需要确保在percolate索引上也可以使用来自普通索引的映射。否则percolate 查询可能会被错误地解析。"],["body","\n\n"],["headingLink","forcing-unmapped-fields-to-be-handled-as-strings"],["heading","Forcing Unmapped Fields to be Handled as Strings"],["body","\n\n"],["body","在某些情况下，不知道会注册哪种渗滤器查询，,并且如果对于由percolator查询引用的字段不存在字段映射，则添加percolator查询失败，这意味着需要更新映射以使字段具有适当的设置"],["body","\n"],["body","但是有时候，如果所有未映射的字段都像默认文本字段一样处理，那就足够了。"],["body","\n"],["body","In those cases one can configure the index.percolator.map_unmapped_fields_as_text setting to true (default to false) and"],["body","\n"],["body","then if a field referred in a percolator query does not exist, it will be handled as a default text field so that adding the percolator query doesn’t fail."],["body","\n\n"],["headingLink","limitations"],["heading","Limitations"],["body","\n"],["headingLink","parentchild"],["heading","Parent/child"],["body","\n"],["body","由于 “percolate” 查询一次处理一个文档，因此它不支持针对子文档 (例如 “has_child” 和 “has_parent”) 运行的查询和过滤器。"],["body","\n"],["headingLink","fetching-queries"],["heading","Fetching queries"],["body","\n\n"],["body","在查询解析过程中，有许多查询通过get调用获取数据。"],["body","\n"],["body","例如，使用term query时的  term lookup"],["body","\n"],["body","template query when using indexed scripts and geo_shape when using pre-indexed shapes."],["body","\n"],["body","当这些查询由 “percolator” 字段类型索引时，get调用将执行一次"],["body","\n"],["body","因此，每次 “percolator” 查询评估这些查询时，将使用获取术语，形状等作为索引时间。"],["body","\n"],["body","每次percolator查询都在主分片和副本分片上建立索引时都会发生，因此，如果源索引在索引时更改了，则在分片副本之间实际索引的术语可能会有所不同。"],["body","\n\n"],["headingLink","script-query"],["heading","Script query"],["body","\n\n"],["body","“脚本” 查询中的脚本只能访问doc值字段。"],["body","\n"],["body","“percolate” 查询将提供的文档索引为内存索引"],["body","\n"],["body","此内存索引不支持存储字段，因此 “_ source” 字段和其他存储字段不存储。这就是为什么在 “脚本” 查询中 “_ source” 和其他存储字段不可用的原因。"],["body","\n\n"],["headingLink","field-aliases"],["heading","Field aliases"],["body","\n"],["body","包含 字段别名 的Percolator 查询可能并不总是如预期的那样运行。特别地，如果注册了包含字段别名的percolator查询，然后在映射中更新该别名以引用不同的字段，则存储的查询仍将引用原始目标字段。要获取对字段别名的更改，必须显式地重新索引percolator查询。"],["body","\n"],["headingLink","percolate-query"],["heading","Percolate query"],["body","\n\n"],["body","\n"],["body","“percolate” 查询可用于匹配存储在索引中的查询。"],["body","\n"],["body","\n"],["body","\n"],["body","percolate  查询本身包含将用作查询以与存储的查询匹配的文档。"],["body","\n"],["body","\n\n"],["headingLink","sample-usage"],["heading","Sample Usage"],["body","\n"],["body","Create an index with two fields:"],["body","\n"],["body","PUT /my-index-00001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"message\": {\n        \"type\": \"text\"\n      },\n      \"query\": {\n        \"type\": \"percolator\"\n      }\n    }\n  }\n}\n"],["body","\n\n"],["body","“message” 字段是用于在 “percolator” 查询中定义的文档被索引为临时索引之前对其进行预处理的字段。"],["body","\n"],["body","“query” 字段用于索引查询文档。 。它将持有一个表示实际Elasticsearch查询的json对象。"],["body","\n"],["body","The query field has been configured to use the percolator field type. 此字段类型可以理解查询dsl并以这样的方式存储查询，以便以后可以使用它来匹配在 “percolator” 查询中定义的文档。"],["body","\n\n"],["body","Register a query in the percolator:"],["body","\n"],["body","PUT /my-index-00001/_doc/1?refresh\n{\n  \"query\": {\n    \"match\": {\n      \"message\": \"bonsai tree\"\n    }\n  }\n}\n"],["body","\n"],["body","Match a document to the registered percolator queries:"],["body","\n"],["body","GET /my-index-00001/_search\n{\n  \"query\": {\n    \"percolate\": {\n      \"field\": \"query\",\n      \"document\": {\n        \"message\": \"A new bonsai tree in the office\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","The above request will yield the following response:"],["body","\n"],["body","{\n  \"took\": 13,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\" : 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\" : {\n        \"value\": 1,\n        \"relation\": \"eq\"\n    },\n    \"max_score\": 0.26152915,\n    \"hits\": [\n      { \n        \"_index\": \"my-index-00001\",\n        \"_type\": \"_doc\",\n        \"_id\": \"1\", The query with id `1` matches our document.\n        \"_score\": 0.26152915,\n        \"_source\": {\n          \"query\": {\n            \"match\": {\n              \"message\": \"bonsai tree\"\n            }\n          }\n        },\n        \"fields\" : {\n          \"_percolator_document_slot\" : [0]//“_ percolator_document_slot” 字段指示哪个文档与此查询匹配。当同时percolator多个文档时很有用。\n        }\n      }\n    ]\n  }\n}\n"],["body","\n\n"],["body","为了提供一个简单的示例，此文档对percolate 查询和文档使用一个索引 “my-index-00001”。"],["body","\n"],["body","当仅注册了几个percolate 查询时，此设置可以很好地工作"],["body","\n"],["body","但是，如果使用率较高，建议将查询和文档存储在单独的索引中。"],["body","\n"],["body","Please see How it Works Under the Hood for more details."],["body","\n\n"],["headingLink","parameters"],["heading","Parameters"],["body","\n"],["body","percolate 文档时需要以下参数:"],["body","\n"],["body","\n"],["body","field"],["body","保存索引查询的类型为 “percolator” 的字段。这是必填参数。"],["body","\n"],["body","name"],["body","如果指定了多个 “percolator_document_slot” 查询，则用于 “_ percolator_document_slot” 字段的后缀。这是一个可选参数。"],["body","\n"],["body","document"],["body","正在percolator的文档的来源。"],["body","\n"],["body","documents"],["body","像 'document' 参数，但通过json数组接受多个文档。"],["body","\n"],["body","document_type"],["body","The type / mapping of the document being percolated. This parameter is deprecated and will be removed in Elasticsearch 8.0."],["body","\n\n\n"],["body","从已经源检索文档"],["body","\n"],["body","也可以从已经存储的文档中检索源，而不是指定要 percolate 的文档的源。然后，“percolate” 查询将在内部执行get请求以获取该文档。"],["body","\n"],["body","在这种情况下，“文档” 参数可以用以下参数代替:"],["body","\n"],["body","\n"],["body","index"],["body","The index the document resides in. This is a required parameter."],["body","\n"],["body","type"],["body","The type of the document to fetch. This parameter is deprecated and will be removed in Elasticsearch 8.0."],["body","\n"],["body","id"],["body","The id of the document to fetch. This is a required parameter."],["body","\n"],["body","routing"],["body","Optionally, routing to be used to fetch document to percolate."],["body","\n"],["body","preference"],["body","Optionally, preference to be used to fetch document to percolate."],["body","\n"],["body","version"],["body","Optionally, the expected version of the document to be fetched."],["body","\n\n\n"],["headingLink","percolating-in-a-filter-context"],["heading","Percolating in a filter context"],["body","\n"],["body","如果您对分数不感兴趣，可以通过将percolator查询包装在 'bool' 查询的filter子句中或 'constant_score '查询中来期望更好的性能:"],["body","\n"],["body","GET /my-index-00001/_search\n{\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"percolate\": {\n          \"field\": \"query\",\n          \"document\": {\n            \"message\": \"A new bonsai tree in the office\"\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","在索引时，从percolator 查询中提取术语，percolator器通常可以仅通过查看提取的术语来确定查询是否匹配。"],["body","\n"],["body","但是，计算分数需要对每个匹配的查询进行反序列化，并针对percolated的文档运行它，这是一个昂贵得多的操作。因此，如果不需要计算分数，则 “percolate” 查询应包装在 “constant_score” 查询或 “bool” 查询的filter子句中。"],["body","\n"],["body","请注意，查询缓存永远不会缓存 “percolate” 查询。"],["body","\n"],["headingLink","percolating-multiple-documents"],["heading","Percolating multiple documents"],["body","\n\n"],["body","\n"],["body","“Percolating” 查询可以与索引的Percolating 查询同时匹配多个文档."],["body","\n"],["body","\n"],["body","\n"],["body","在单个请求中渗透多个文档可以提高性能，因为查询只需要解析和匹配一次，而不是多次。"],["body","\n"],["body","\n"],["body","\n"],["body","当同时percolator 多个文档时，与每个匹配的percolator 查询一起返回的 “_percolator_document_slot” 字段非常重要。它指示哪些文档与特定的percolator查询匹配"],["body","\n"],["body","\n"],["body","\n"],["body","这些数字与 “percolator” 查询中指定的 “文档” 数组中的插槽相关。"],["body","\n"],["body","\n\n"],["body","GET /my-index-00001/_search\n{\n  \"query\": {\n    \"percolate\": {\n      \"field\": \"query\", //The documents array contains 4 documents that are going to be percolated at the same time.\n      \"documents\": [ \n        {\n          \"message\": \"bonsai tree\"\n        },\n        {\n          \"message\": \"new tree\"\n        },\n        {\n          \"message\": \"the office\"\n        },\n        {\n          \"message\": \"office tree\"\n        }\n      ]\n    }\n  }\n}\n"],["body","\n"],["body","{\n  \"took\": 13,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\" : 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\" : {\n        \"value\": 1,\n        \"relation\": \"eq\"\n    },\n    \"max_score\": 0.7093853,\n    \"hits\": [\n      {\n        \"_index\": \"my-index-00001\",\n        \"_type\": \"_doc\",\n        \"_id\": \"1\",\n        \"_score\": 0.7093853,\n        \"_source\": {\n          \"query\": {\n            \"match\": {\n              \"message\": \"bonsai tree\"\n            }\n          }\n        },\n        \"fields\" : { //The `_percolator_document_slot` indicates that the first, second and last documents specified in the `percolate` query are matching with this query.\n          \"_percolator_document_slot\" : [0, 1, 3] \n        }\n      }\n    ]\n  }\n}\n"],["body","\n"],["headingLink","percolating-an-existing-document"],["heading","Percolating an Existing Document"],["body","\n\n"],["body","为了Percolating 新索引的文档，可以使用 “percolate” 查询"],["body","\n"],["body","根据来自索引请求的响应，可以使用 “_ id” 和其他元信息立即percolate新添加的文档。"],["body","\n\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","Based on the previous example."],["body","\n"],["body","Index the document we want to percolate:"],["body","\n"],["body","PUT /my-index-00001/_doc/2\n{\n  \"message\" : \"A new bonsai tree in the office\"\n}\n"],["body","\n"],["body","Index response:"],["body","\n"],["body","{\n  \"_index\": \"my-index-00001\",\n  \"_type\": \"_doc\",\n  \"_id\": \"2\",\n  \"_version\": 1,\n  \"_shards\": {\n    \"total\": 2,\n    \"successful\": 1,\n    \"failed\": 0\n  },\n  \"result\": \"created\",\n  \"_seq_no\" : 1,\n  \"_primary_term\" : 1\n}\n"],["body","\n"],["body","Percolat 现有文档，使用索引响应作为基础来构建新的搜索请求:"],["body","\n"],["body","GET /my-index-00001/_search\n{\n  \"query\": {\n    \"percolate\": {\n      \"field\": \"query\",\n      \"index\": \"my-index-00001\",\n      \"id\": \"2\",\n      \"version\": 1 \n    }\n  }\n}\n"],["body","\n"],["body","该版本是可选的，但在某些情况下很有用。我们可以确保我们正在尝试渗透我们刚刚索引的文档"],["body","\n"],["body","在我们建立索引后可能会进行更改，如果是这种情况，则搜索请求将因版本冲突错误而失败。"],["body","\n"],["body","返回的搜索响应与前面的示例相同。"],["body","\n"],["headingLink","percolate-query-and-highlighting"],["heading","Percolate query and highlighting"],["body","\n"],["body","当涉及到highlighting ，“渗透” 查询以一种特殊的方式处理。查询命中用于highlight “渗透” 查询中提供的文档。而通过常规突出显示，搜索请求中的查询用于突出显示命中的文档"],["body","\n"],["headingLink","example-1"],["heading","Example"],["body","\n"],["body","This example is based on the mapping of the first example."],["body","\n"],["body","Save a query:"],["body","\n"],["body","PUT /my-index-00001/_doc/3?refresh\n{\n  \"query\": {\n    \"match\": {\n      \"message\": \"brown fox\"\n    }\n  }\n}\n"],["body","\n"],["body","Save another query:"],["body","\n"],["body","PUT /my-index-00001/_doc/4?refresh\n{\n  \"query\": {\n    \"match\": {\n      \"message\": \"lazy dog\"\n    }\n  }\n}\n"],["body","\n"],["body","Execute a search request with the percolate query and highlighting enabled:"],["body","\n"],["body","GET /my-index-00001/_search\n{\n  \"query\": {\n    \"percolate\": {\n      \"field\": \"query\",\n      \"document\": {\n        \"message\": \"The quick brown fox jumps over the lazy dog\"\n      }\n    }\n  },\n  \"highlight\": {\n    \"fields\": {\n      \"message\": {}\n    }\n  }\n}\n"],["body","\n"],["body","单文档多渗透查询"],["body","\n"],["body","This will yield the following response."],["body","\n"],["body","{\n  \"took\": 7,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\" : 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\" : {\n        \"value\": 2,\n        \"relation\": \"eq\"\n    },\n    \"max_score\": 0.26152915,\n    \"hits\": [\n      {\n        \"_index\": \"my-index-00001\",\n        \"_type\": \"_doc\",\n        \"_id\": \"3\",\n        \"_score\": 0.26152915,\n        \"_source\": {\n          \"query\": {\n            \"match\": {\n              \"message\": \"brown fox\"\n            }\n          }\n        },\n        \"highlight\": {\n          \"message\": [\n            \"The quick <em>brown</em> <em>fox</em> jumps over the lazy dog\" \n          ]\n        },\n        \"fields\" : {\n          \"_percolator_document_slot\" : [0]\n        }\n      },\n      {\n        \"_index\": \"my-index-00001\",\n        \"_type\": \"_doc\",\n        \"_id\": \"4\",\n        \"_score\": 0.26152915,\n        \"_source\": {\n          \"query\": {\n            \"match\": {\n              \"message\": \"lazy dog\"\n            }\n          }\n        },\n        \"highlight\": {\n          \"message\": [\n            \"The quick brown fox jumps over the <em>lazy</em> <em>dog</em>\" \n          ]\n        },\n        \"fields\" : {\n          \"_percolator_document_slot\" : [0]\n        }\n      }\n    ]\n  }\n}\n"],["body","\n"],["body","单渗透查询多文档"],["body","\n"],["body","渗滤器查询不是突出显示 “渗滤器” 命中的搜索请求中的查询，而是突出显示 “渗滤器” 查询中定义的文档。"],["body","\n"],["body","当像下面的请求一样同时渗透多个文档时，高亮响应是不同的:"],["body","\n"],["body","GET /my-index-00001/_search\n{\n  \"query\": {\n    \"percolate\": {\n      \"field\": \"query\",\n      \"documents\": [\n        {\n          \"message\": \"bonsai tree\"\n        },\n        {\n          \"message\": \"new tree\"\n        },\n        {\n          \"message\": \"the office\"\n        },\n        {\n          \"message\": \"office tree\"\n        }\n      ]\n    }\n  },\n  \"highlight\": {\n    \"fields\": {\n      \"message\": {}\n    }\n  }\n}\n"],["body","\n"],["body","The slightly different response:"],["body","\n"],["body","{\n  \"took\": 13,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\" : 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\" : {\n        \"value\": 1,\n        \"relation\": \"eq\"\n    },\n    \"max_score\": 0.7093853,\n    \"hits\": [\n      {\n        \"_index\": \"my-index-00001\",\n        \"_type\": \"_doc\",\n        \"_id\": \"1\",\n        \"_score\": 0.7093853,\n        \"_source\": {\n          \"query\": {\n            \"match\": {\n              \"message\": \"bonsai tree\"\n            }\n          }\n        },\n        \"fields\" : {\n          \"_percolator_document_slot\" : [0, 1, 3]\n        },\n        \"highlight\" : {  // The highlight fields have been prefixed with the document slot they belong to, in order to know which highlight field belongs to what document.\n          \"0_message\" : [\n              \"<em>bonsai</em> <em>tree</em>\"\n          ],\n          \"3_message\" : [\n              \"office <em>tree</em>\"\n          ],\n          \"1_message\" : [\n              \"new <em>tree</em>\"\n          ]\n        }\n      }\n    ]\n  }\n}\n"],["body","\n"],["headingLink","specifying-multiple-percolate-queries"],["heading","Specifying multiple percolate queries"],["body","\n"],["body","It is possible to specify multiple percolate queries in a single search request:"],["body","\n"],["body","GET /my-index-00001/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        {\n          \"percolate\": {\n            \"field\": \"query\",\n            \"document\": {\n              \"message\": \"bonsai tree\"\n            },\n            \"name\": \"query1\" \n          }\n        },\n        {\n          \"percolate\": {\n            \"field\": \"query\",\n            \"document\": {\n              \"message\": \"tulip flower\"\n            },\n            \"name\": \"query2\" \n          }\n        }\n      ]\n    }\n  }\n}\n"],["body","\n"],["body","The _percolator_document_slot field name will be suffixed with what is specified in the _name parameter. If that isn’t specified then the field parameter will be used, which in this case will result in ambiguity."],["body","\n"],["body","The above search request returns a response similar to this:"],["body","\n"],["body","{\n  \"took\": 13,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\" : 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\" : {\n        \"value\": 1,\n        \"relation\": \"eq\"\n    },\n    \"max_score\": 0.26152915,\n    \"hits\": [\n      {\n        \"_index\": \"my-index-00001\",\n        \"_type\": \"_doc\",\n        \"_id\": \"1\",\n        \"_score\": 0.26152915,\n        \"_source\": {\n          \"query\": {\n            \"match\": {\n              \"message\": \"bonsai tree\"\n            }\n          }\n        },\n        \"fields\" : {\n          \"_percolator_document_slot_query1\" : [0] \n        }\n      }\n    ]\n  }\n}\n"],["body","\n"],["body","'_ Percolator_document_slot _ query1'  percolator slot 字段指示这些匹配的插槽来自 “percolate” 查询，其中 “_ name” 参数设置为 “query1”。"],["body","\n"],["headingLink","how-it-works-under-the-hood"],["heading","How it Works Under the Hood"],["body","\n\n"],["body","\n"],["body","当将文档索引为已配置 percolator字段类型 映射的索引时，文档的查询部分被解析为Lucene查询并存储到Lucene索引中，而且查询的术语也被分析并存储到索引字段中。"],["body","\n"],["body","\n"],["body","\n"],["body","在搜索时，请求中指定的文档被解析为Lucene文档，并存储在内存中的临时Lucene索引中。这个内存索引只能容纳这个文档，并且为此进行了优化。"],["body","\n"],["body","\n"],["body","\n"],["body","在此之后，将根据内存索引中的术语构建特殊查询，该查询根据其索引的查询术语选择候选percolator 查询"],["body","\n"],["body","\n"],["body","\n"],["body","The selecting of candidate percolator queries matches is an important performance optimization during the execution of the percolate query as it can significantly reduce the number of candidate matches the in-memory index needs to evaluate."],["body","\n"],["body","\n"],["body","\n"],["body","The reason the percolate query can do this is because during indexing of the percolator queries the query terms are being extracted and indexed with the percolator query."],["body","\n"],["body","\n"],["body","\n"],["body","Unfortunately the percolator cannot extract terms from all queries (for example the wildcard or geo_shape query) and as a result of that in certain cases the percolator can’t do the selecting optimization (例如，如果在布尔查询的 must 子句中定义了不支持的查询，或者不支持的查询是percolator文档中唯一的查询). These queries are marked by the percolator and can be found by running the following search:"],["body","\n"],["body","\n\n"],["body","GET /_search\n{\n  \"query\": {\n    \"term\" : {\n      \"query.extraction_result\" : \"failed\"\n    }\n  }\n}\n"],["body","\n\n"],["body","上面的示例假设映射中存在类型为 “percolator” 的 “query” 字段。"],["body","\n"],["body","考虑到percolation的设计，通常对percolation query 和要percolation 的文档使用单独的索引是有意义的，而不是像我们在示例中那样使用单个索引。这种方法有一些好处:"],["body","\n\n\n"],["body","因为 percolation 包含与percolation document 不同的一组字段, 使用两个单独的索引可以使字段以更密集，更有效的方式存储。"],["body","\n"],["body","Percolate queries do not scale in the same way as other queries, so percolation performance may benefit from using a different index configuration, like the number of primary shards."],["body","\n\n"],["headingLink","notes"],["heading","Notes"],["body","\n"],["headingLink","allow-expensive-queries"],["heading","Allow expensive queries"],["body","\n"],["body","Percolate queries will not be executed if search.allow_expensive_queries is set to false."],["body","\n"],["headingLink","可参考文章"],["heading","可参考文章"],["body","\n\n"],["body","https://juejin.cn/post/7020585236433993735"],["body","\n"],["body","https://developer.aliyun.com/article/776868"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/26.Text.html"],["title","Text.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","text-field-type"],["heading","Text field type"],["body","\n\n"],["body","全文索引字段，通过  analyzer  将字符串 转换为 terms"],["body","\n"],["body","分析过程允许Elasticsearch在每个全文字段中 搜索单个单词。"],["body","\n"],["body","不用于排序，很少用于聚合 (尽管 重要文本聚合 是一个明显的例外)。"],["body","\n"],["body","“文本” 字段最适合非结构化但人类可读的内容。如果需要对非结构化机器生成的内容进行索引，请参阅 映射非结构化内容。"],["body","\n\n"],["headingLink","use-a-field-as-both-text-and-keyword"],["heading","Use a field as both text and keyword"],["body","\n"],["body","Sometimes it is useful to have both a full text (text) and a keyword (keyword) version of the same field: one for full text search and the other for aggregations and sorting. This can be achieved with multi-fields."],["body","\n"],["headingLink","parameters-for-text-fields"],["heading","Parameters for text fields"],["body","\n"],["body","The following parameters are accepted by text fields:"],["body","\n"],["body","\n"],["body","analyzer"],["body","分析器：The analyzer which should be used for the text field, both at index-time and at search-time (unless overridden by the search_analyzer). Defaults to the default index analyzer, or the standard analyzer."],["body","\n"],["body","boost"],["body","权重：Mapping field-level query time boosting. Accepts a floating point number, defaults to 1.0."],["body","\n"],["body","eager_global_ordinals"],["body","Should global ordinals be loaded eagerly on refresh? Accepts true or false (default). Enabling this is a good idea on fields that are frequently used for (significant) terms aggregations."],["body","\n"],["body","fielddata"],["body","正向索引：Can the field use in-memory fielddata for sorting, aggregations, or scripting? Accepts true or false (default)."],["body","\n"],["body","fielddata_frequency_filter"],["body","Expert settings which allow to decide which values to load in memory when fielddata is enabled. By default all values are loaded."],["body","\n"],["body","fields"],["body","多字段：Multi-fields allow the same string value to be indexed in multiple ways for different purposes, such as one field for search and a multi-field for sorting and aggregations, or the same string value analyzed by different analyzers."],["body","\n"],["body","index"],["body","可搜索：Should the field be searchable? Accepts true (default) or false."],["body","\n"],["body","index_options"],["body","What information should be stored in the index, for search and highlighting purposes. Defaults to positions."],["body","\n"],["body","index_prefixes"],["body","If enabled, term prefixes of between 2 and 5 characters are indexed into a separate field. This allows prefix searches to run more efficiently, at the expense of a larger index."],["body","\n"],["body","index_phrases"],["body","If enabled, two-term word combinations (shingles) are indexed into a separate field. This allows exact phrase queries (no slop) to run more efficiently, at the expense of a larger index. Note that this works best when stopwords are not removed, as phrases containing stopwords will not use the subsidiary field and will fall back to a standard phrase query. Accepts true or false (default)."],["body","\n"],["body","norms"],["body","字段长度是否考虑：Whether field-length should be taken into account when scoring queries. Accepts true (default) or false."],["body","\n"],["body","position_increment_gap"],["body","多值间隔：The number of fake term position which should be inserted between each element of an array of strings. Defaults to the position_increment_gap configured on the analyzer which defaults to 100. 100 was chosen because it prevents phrase queries with reasonably large slops (less than 100) from matching terms across field values."],["body","\n"],["body","store"],["body","Whether the field value should be stored and retrievable separately from the _source field. Accepts true or false (default)."],["body","\n"],["body","search_analyzer"],["body","The analyzer that should be used at search time on the text field. Defaults to the analyzer setting."],["body","\n"],["body","search_quote_analyzer"],["body","The analyzer that should be used at search time when a phrase is encountered. Defaults to the search_analyzer setting."],["body","\n"],["body","similarity"],["body","Which scoring algorithm or similarity should be used. Defaults to BM25."],["body","\n"],["body","term_vector"],["body","Whether term vectors should be stored for the field. Defaults to no."],["body","\n"],["body","meta"],["body","Metadata about the field."],["body","\n\n\n"],["headingLink","fielddata-mapping-parameter"],["heading","fielddata mapping parameter"],["body","\n\n"],["body","\n"],["body","默认情况下，“文本” 字段是可搜索的，但默认情况下，不适用于聚合，排序或脚本。如果您尝试对 “文本” 字段上的脚本进行排序，聚合或访问值，则会看到以下异常:"],["body","\n"],["body","\n"],["body","\n"],["body","默认情况下，在文本字段上禁用Fielddata。在 “your_field_name” 上设置 “fielddata = true”，以便通过取消反转索引将字段数据加载到内存中。"],["body","\n"],["body","\n"],["body","\n"],["body","请注意，这可能会使用大量内存。"],["body","\n"],["body","\n"],["body","\n"],["body","字段数据是从聚合，排序或脚本中的全文字段访问分析的令牌的唯一方法"],["body","\n"],["body","\n"],["body","\n"],["body","例如，像 New York 这样的全文字段将被分析为 “new” 和 “york”。要在这些令牌上聚合，需要  field data."],["body","\n"],["body","\n\n"],["headingLink","before-enabling-fielddata"],["heading","Before enabling fielddata"],["body","\n\n"],["body","\n"],["body","在文本字段上启用fielddata通常没有意义。"],["body","\n"],["body","\n"],["body","\n"],["body","Field data is stored in the heap with the field data cache because it is expensive to calculate."],["body","\n"],["body","\n"],["body","\n"],["body","计算 field data  可能会导致延迟尖峰，而增加堆使用率是导致群集性能问题的原因。"],["body","\n"],["body","\n"],["body","\n"],["body","大多数想要对文本字段做更多事情的用户使用 多字段映射，方法是同时具有用于全文搜索的 “文本” 字段和未分析的 [“关键字”](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.13/keyword.html) 字段的聚合，如下所示:"],["body","\n"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_field\": { \n        \"type\": \"text\",\n        \"fields\": {\n          \"keyword\": { \n            \"type\": \"keyword\"\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","enabling-fielddata-on-text-fields"],["heading","Enabling fielddata on text fields"],["body","\n"],["body","You can enable fielddata on an existing text field using the update mapping API as follows:"],["body","\n"],["body","PUT my-index-000001/_mapping\n{\n  \"properties\": {\n    \"my_field\": { //The mapping that you specify for `my_field` should consist of the existing mapping for that field, plus the `fielddata` parameter.\n      \"type\":     \"text\",\n      \"fielddata\": true\n    }\n  }\n}\n"],["body","\n"],["headingLink","fielddata_frequency_filter-mapping-parameter"],["heading","fielddata_frequency_filter mapping parameter"],["body","\n\n"],["body","\n"],["body","Fielddata过滤可用于减少加载到内存中的项数，从而减少内存使用"],["body","\n"],["body","\n"],["body","\n"],["body","可以通过频率过滤 terms:"],["body","\n"],["body","\n"],["body","\n"],["body","The frequency filter  允许您仅加载文档频率介于 “min” 和 “max” 值之间的terms"],["body","\n"],["body","\n"],["body","\n"],["body","可以表示为绝对数 (当该数大于1.0时) 或表示为百分比 (例如 “0.01” 是 “1%”，“1.0” 是 “100%”)。"],["body","\n"],["body","\n"],["body","\n"],["body","频率计算每段"],["body","\n"],["body","\n"],["body","\n"],["body","百分比基于具有该字段值的文档数量，而不是该 segement 中的所有文档。"],["body","\n"],["body","\n"],["body","\n"],["body","小段可以通过指定段应包含的最小docs数与 'min_segment_size '来完全排除:"],["body","\n"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"tag\": {\n        \"type\": \"text\",\n        \"fielddata\": true,\n        \"fielddata_frequency_filter\": {\n          \"min\": 0.001,\n          \"max\": 0.1,\n          \"min_segment_size\": 500\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/4.FieldDataTypes/16.Object.html"],["title","Object.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","object-field-type"],["heading","Object field type"],["body","\n"],["body","JSON文档本质上是分层的: 文档可能包含内部对象，而内部对象本身可能包含内部对象:"],["body","\n"],["body","PUT my-index-000001/_doc/1\n{ \n  \"region\": \"US\", // The outer document is also a JSON object.\n  \"manager\": { //It contains an inner object called `manager`.\n    \"age\":     30,\n    \"name\": {  //Which in turn contains an inner object called `name`.\n      \"first\": \"John\",\n      \"last\":  \"Smith\"\n    }\n  }\n}\n"],["body","\n"],["body","在内部，此文档被索引为键值对的简单，平面列表，如下:"],["body","\n"],["body","{\n  \"region\":             \"US\",\n  \"manager.age\":        30,\n  \"manager.name.first\": \"John\",\n  \"manager.name.last\":  \"Smith\"\n}\n"],["body","\n"],["body","上述文档的显式映射可能如下所示:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": { ;//Properties in the top-level mappings definition.\n      \"region\": {\n        \"type\": \"keyword\"\n      },\n      \"manager\": { //The `manager` field is an inner `object` field.\n        \"properties\": {\n          \"age\":  { \"type\": \"integer\" },\n          \"name\": {  //The `manager.name` field is an inner `object` field within the `manager` field.\n            \"properties\": {\n              \"first\": { \"type\": \"text\" },\n              \"last\":  { \"type\": \"text\" }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","您不需要显式设置字段 “类型” 为 “对象”，因为这是默认值。"],["body","\n"],["headingLink","parameters-for-object-fields"],["heading","Parameters for object fields"],["body","\n"],["body","The following parameters are accepted by object fields:"],["body","\n"],["body","\n"],["body","dynamic"],["body","Whether or not new properties should be added dynamically to an existing object. Accepts true (default), runtime, false and strict."],["body","\n"],["body","enabled"],["body","Whether the JSON value given for the object field should be parsed and indexed (true, default) or completely ignored (false)."],["body","\n"],["body","properties"],["body","The fields within the object, which can be of any data type, including object. New properties may be added to an existing object."],["body","\n\n\n"],["body","If you need to index arrays of objects instead of single objects, read Nested first."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/7.MappingLimitSettings/README.html"],["title","MappingLimitSettings - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","mapping-limit-settings"],["heading","Mapping limit settings"],["body","\n"],["body","\n"],["body","Refrence"],["body","\n"],["body","\n"],["body","使用以下设置来限制字段映射 (手动或动态创建) 的数量，并防止导致 文档映射爆炸:"],["body","\n\n"],["body","\n"],["body","index.mapping.total_fields.limit"],["body","\n"],["body","The maximum number of fields in an index. Field and object mappings, as well as field aliases count towards this limit. The default value is 1000.The limit is in place to prevent mappings and searches from becoming too large. Higher values can lead to performance degradations and memory issues, especially in clusters with a high load or few resources.If you increase this setting, we recommend you also increase the indices.query.bool.max_clause_count setting, which limits the maximum number of boolean clauses in a query.If your field mappings contain a large, arbitrary set of keys, consider using the flattened data type."],["body","\n"],["body","\n"],["body","\n"],["body","index.mapping.depth.limit"],["body","\n"],["body","The maximum depth for a field, which is measured as the number of inner objects. For instance, if all fields are defined at the root object level, then the depth is 1. If there is one object mapping, then the depth is 2, etc. Default is 20."],["body","\n"],["body","\n"],["body","\n"],["body","index.mapping.nested_fields.limit"],["body","\n"],["body","The maximum number of distinct nested mappings in an index. The nested type should only be used in special cases, when arrays of objects need to be queried independently of each other. To safeguard against poorly designed mappings, this setting limits the number of unique nested types per index. Default is 50."],["body","\n"],["body","\n"],["body","\n"],["body","index.mapping.nested_objects.limit"],["body","\n"],["body","The maximum number of nested JSON objects that a single document can contain across all nested types. This limit helps to prevent out of memory errors when a document contains too many nested objects. Default is 10000."],["body","\n"],["body","\n"],["body","\n"],["body","index.mapping.field_name_length.limit"],["body","\n"],["body","Setting for the maximum length of a field name. This setting isn’t really something that addresses mappings explosion but might still be useful if you want to limit the field length. It usually shouldn’t be necessary to set this setting. The default is okay unless a user starts to add a huge number of fields with really long names. Default is Long.MAX_VALUE (no limit)."],["body","\n"],["body","\n"],["body","\n"],["body","index.mapping.dimension_fields.limit"],["body","\n"],["body","[preview] This functionality is in technical preview and may be changed or removed in a future release. Elastic will apply best effort to fix any issues, but features in technical preview are not subject to the support SLA of official GA features.(Dynamic, integer)For internal use by Elastic only.Maximum number of time series dimensions for the index. Defaults to 16.You can mark a field as a dimension using the time_series_dimension mapping parameter."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/README.html"],["title","Mapping - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","mapping"],["heading","Mapping"],["body","\n\n"],["body","Mapping 是定义文档及其包含的字段的存储和索引方式的过程。"],["body","\n"],["body","每个文档是字段的集合，每个字段都有自己的 数据类型  data type"],["body","\n"],["body","Mapping包含一系列字段的定义集合。同时也包含 源字段 metadata fields 例如 _source（它定义了如何处理文档的元数据）"],["body","\n"],["body","使用 dynamic mapping  explicit mapping 定义 mapping。两种方式各有好处"],["body","\n"],["body","7.0.0之后 type name 被废弃 Removal of mapping types."],["body","\n\n"],["body","Experiment with mapping options"],["body","\n"],["body","Define runtime fields in a search request to experiment with different mapping options, and also fix mistakes in your index mapping values by overriding values in the mapping during the search request."],["body","\n"],["headingLink","dynamic-mapping"],["heading","Dynamic mapping"],["body","\n\n"],["body","Dynamic mapping  开箱即用，自动添加新字段"],["body","\n"],["body","可以在top-level mapping 上添加字段 ，也可以在内部 object and nested 添加字段"],["body","\n"],["body","使用 dynamic templates  定义自定义的映射。这些映射会基于 匹配条件自动的 应用于新字段的添加"],["body","\n\n"],["headingLink","explicit-mapping"],["heading","Explicit mapping"],["body","\n"],["body","Explicit mapping 允许您精确选择如何定义映射定义，例如:"],["body","\n\n"],["body","哪些字符串文本要全文索引"],["body","\n"],["body","哪些字段包含数字、日期或地理位置。"],["body","\n"],["body","The format of date values."],["body","\n"],["body","Custom rules to control the mapping for dynamically added fields."],["body","\n\n\n"],["body","\n"],["body","可以使用  runtime fields 在不重新索引的情况下，进行schema变更"],["body","\n"],["body","\n"],["body","\n"],["body","您可以将 运行时字段与 索引字段结合使用，以平衡资源使用和性能。"],["body","\n"],["body","\n\n"],["headingLink","settings-to-prevent-mapping-explosion"],["heading","Settings to prevent mapping explosion"],["body","\n\n"],["body","\n"],["body","在索引中定义过多的字段可能会导致映射爆炸，这可能会导致内存不足的错误和难以恢复的情况。"],["body","\n"],["body","\n"],["body","\n"],["body","如果每个文档都会引入一些新子段。每个字段都会被添加到 索引映射中，随着映射增长这可能会成一个问题"],["body","\n"],["body","\n"],["body","\n"],["body","使用 mapping limit settings 限制手动或自动 创建的字段数量，这可以避免字段映射爆炸"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/2.ExplicitMapping/README.html"],["title","ExplicitMapping - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","explicit-mapping"],["heading","Explicit mapping"],["body","\n"],["body","You can create field mappings when you create an index and add fields to an existing index."],["body","\n"],["headingLink","create-an-index-with-an-explicit-mapping"],["heading","Create an index with an explicit mapping"],["body","\n"],["body","使用 create index API 显示创建字段映射"],["body","\n"],["body","PUT /my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"age\":    { \"type\": \"integer\" },  \n      \"email\":  { \"type\": \"keyword\"  }, \n      \"name\":   { \"type\": \"text\"  }     \n    }\n  }\n}\n"],["body","\n"],["headingLink","add-a-field-to-an-existing-mapping"],["heading","Add a field to an existing mapping"],["body","\n"],["body","使用 update mapping API 更新映射"],["body","\n"],["body","PUT /my-index-000001/_mapping\n{\n  \"properties\": {\n    \"employee-id\": {\n      \"type\": \"keyword\",\n      \"index\": false\n    }\n  }\n}\n"],["body","\n"],["headingLink","update-the-mapping-of-a-field"],["heading","Update the mapping of a field"],["body","\n\n"],["body","\n"],["body","Except for supported mapping parameters, 您不能更改现有字段的映射或字段类型。"],["body","\n"],["body","\n"],["body","\n"],["body","更改现有字段可能会使已经索引的数据无效。"],["body","\n"],["body","\n"],["body","\n"],["body","想要修改 dataStream的 后备索引，see Change mappings and settings for a data stream."],["body","\n"],["body","\n"],["body","\n"],["body","如果您需要更改其他索引中某个字段的映射，请使用正确的映射创建一个新索引，并将您的数据重新 reindex  到该索引中。"],["body","\n"],["body","\n"],["body","\n"],["body","重命名字段将使已在旧字段名称下索引的数据无效，可以使用 alias 来创建新的字段名"],["body","\n"],["body","\n\n"],["headingLink","view-the-mapping-of-an-index"],["heading","View the mapping of an index"],["body","\n"],["body","您可以使用 get mapping API  查看现有索引的映射。"],["body","\n"],["body","GET /my-index-000001/_mapping\n"],["body","\n"],["body","The API returns the following response:"],["body","\n"],["body","{\n  \"my-index-000001\" : {\n    \"mappings\" : {\n      \"properties\" : {\n        \"age\" : {\n          \"type\" : \"integer\"\n        },\n        \"email\" : {\n          \"type\" : \"keyword\"\n        },\n        \"employee-id\" : {\n          \"type\" : \"keyword\",\n          \"index\" : false\n        },\n        \"name\" : {\n          \"type\" : \"text\"\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","view-the-mapping-of-specific-fields"],["heading","View the mapping of specific fields"],["body","\n\n"],["body","如果您只想查看一个或多个特定字段的映射。使用get field mapping API."],["body","\n"],["body","可以只查看某写字段的映射"],["body","\n\n"],["body","GET /my-index-000001/_mapping/field/employee-id\n"],["body","\n"],["body","The API returns the following response:"],["body","\n"],["body","{\n  \"my-index-000001\" : {\n    \"mappings\" : {\n      \"employee-id\" : {\n        \"full_name\" : \"employee-id\",\n        \"mapping\" : {\n          \"employee-id\" : {\n            \"type\" : \"keyword\",\n            \"index\" : false\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/8.RemovalOfMappingTypes/README.html"],["title","RemovalOfMappingTypes - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","removal-of-mapping-types"],["heading","Removal of mapping types"],["body","\n"],["body","在Elasticsearch 7.0.0或更高版本中创建的索引不再接受  '_ default _' 映射。"],["body","\n"],["body","在6.X中创建的索引将继续像以前在Elasticsearch 6.X中一样起作用。"],["body","\n"],["body","Types 在7.0的api中已经过期，这对于    index creation, put mapping, get mapping, put template, get template and get field mappings APIs. 是 breaking changes"],["body","\n"],["headingLink","what-are-mapping-types"],["heading","What are mapping types?"],["body","\n\n"],["body","\n"],["body","自Elasticsearch的第一个版本以来，每个文档都存储在单个索引中，并分配了单个映射类型。映射类型用于表示要索引的文档或实体的类型，例如，“twitter” 索引可能具有 “用户” 类型和 “tweet” 类型。"],["body","\n"],["body","\n"],["body","\n"],["body","每个映射类型都可以具有自己的字段，因此 “用户” 类型可能具有 “完整名称” 字段、“用户名” 字段和 “电子邮件” 字段，而 “tweet” 类型可以具有 “内容” 字段，“tweet_at” 字段以及 “用户” 类型一样的 “用户名” 字段。"],["body","\n"],["body","\n"],["body","\n"],["body","每个文档都有一个包含类型名称的 “_ type' 元数据字段，通过在URL中指定类型名称，可以将搜索限制为一个或多个类型:"],["body","\n"],["body","\n\n"],["body","GET twitter/user,tweet/_search\n{\n  \"query\": {\n    \"match\": {\n      \"user_name\": \"kimchy\"\n    }\n  }\n}\n"],["body","\n"],["body","'_Type' 字段与文档的 '_id' 组合以生成 '_uid' 字段，因此具有相同 '_id' 的不同类型的文档可以存在于单个索引中。"],["body","\n"],["body","映射类型也用于在文档之间建立 父子关系，因此类型为 “问题” 的文档可以是  类型为 “答案” 的文档的父母"],["body","\n"],["headingLink","why-are-mapping-types-being-removed"],["heading","Why are mapping types being removed?"],["body","\n"],["body","最初，我们谈到 “索引” 类似于sql数据库中的 “数据库”，而 “类型” 等效于 “表”。"],["body","\n"],["body","这是一个不好的类比，导致了不正确的假设"],["body","\n\n"],["body","在sql数据库中，表是相互独立的"],["body","\n"],["body","一个表中的列与另一个表中的同名列没有任何关系"],["body","\n"],["body","映射类型中的字段不是这种情况\n\n"],["body","在Elasticsearch索引中，在不同映射类型中具有相同名称的字段在内部由相同的Lucene字段进行支撑。"],["body","\n"],["body","换句话说，使用上面的示例，'用户' 类型中的 “user_name” 字段与 “tweet” 类型中的 “user_name” 字段存储在完全相同的字段中，并且两个 “user_name” 字段在两种类型中都必须具有相同的映射 (定义)"],["body","\n"],["body","最重要的是，在同一索引中存储很少或没有共同字段的不同实体会导致数据稀疏，并干扰Lucene有效压缩文档的能力"],["body","\n"],["body","由于这些原因，我们决定从Elasticsearch中删除映射类型的概念。"],["body","\n\n"],["body","\n\n"],["headingLink","alternatives-to-mapping-types"],["heading","Alternatives to mapping types"],["body","\n"],["headingLink","index-per-document-type"],["heading","Index per document type"],["body","\n"],["body","第一种选择是每个文档类型有一个索引。您可以将tweets和用户存储在 “tweets” 索引中，而不是将tweets和用户存储在 “用户” 索引中。索引是完全独立的，因此索引之间不会存在字段类型冲突。"],["body","\n"],["body","这种方法有两个好处:"],["body","\n\n"],["body","数据更有可能是密集的，因此受益于Lucene中使用的压缩技术。"],["body","\n"],["body","全文搜索中用于评分的统计一词更可能是准确的，因为同一索引中的所有文档都代表一个实体。"],["body","\n\n"],["body","每个索引可以根据它将包含的文档数量进行适当的调整"],["body","\n\n"],["body","\n"],["body","您可以为 “用户” 使用较少数量的主分片"],["body","\n"],["body","\n"],["body","\n"],["body","为 “tweet” 使用较多数量的主分片"],["body","\n"],["body","\n\n"],["headingLink","custom-type-field"],["heading","Custom type field"],["body","\n"],["body","当然，集群中可以存在多少个主分片是有限制的，因此您可能不想将整个分片浪费在只有几千个文档的集合上。"],["body","\n"],["body","在这种情况下，您可以实现自己的自定义 “类型” 字段，该字段的工作方式与旧的 “_ 类型” 类似。"],["body","\n"],["body","让我们以上面的 'user'/'tweet' 示例。最初，工作流程看起来像这样:"],["body","\n"],["body","PUT twitter\n{\n  \"mappings\": {\n    \"user\": {\n      \"properties\": {\n        \"name\": { \"type\": \"text\" },\n        \"user_name\": { \"type\": \"keyword\" },\n        \"email\": { \"type\": \"keyword\" }\n      }\n    },\n    \"tweet\": {\n      \"properties\": {\n        \"content\": { \"type\": \"text\" },\n        \"user_name\": { \"type\": \"keyword\" },\n        \"tweeted_at\": { \"type\": \"date\" }\n      }\n    }\n  }\n}\n\nPUT twitter/user/kimchy\n{\n  \"name\": \"Shay Banon\",\n  \"user_name\": \"kimchy\",\n  \"email\": \"shay@kimchy.com\"\n}\n\nPUT twitter/tweet/1\n{\n  \"user_name\": \"kimchy\",\n  \"tweeted_at\": \"2017-10-24T09:00:00Z\",\n  \"content\": \"Types are going away\"\n}\n\nGET twitter/tweet/_search\n{\n  \"query\": {\n    \"match\": {\n      \"user_name\": \"kimchy\"\n    }\n  }\n}\n"],["body","\n"],["body","您可以通过添加自定义的 “类型” 字段来实现相同的目的，如下所示:"],["body","\n"],["body","PUT twitter\n{\n  \"mappings\": {\n    \"_doc\": {\n      \"properties\": {\n        \"type\": { \"type\": \"keyword\" }, \n        \"name\": { \"type\": \"text\" },\n        \"user_name\": { \"type\": \"keyword\" },\n        \"email\": { \"type\": \"keyword\" },\n        \"content\": { \"type\": \"text\" },\n        \"tweeted_at\": { \"type\": \"date\" }\n      }\n    }\n  }\n}\n\nPUT twitter/_doc/user-kimchy\n{\n  \"type\": \"user\", \n  \"name\": \"Shay Banon\",\n  \"user_name\": \"kimchy\",\n  \"email\": \"shay@kimchy.com\"\n}\n\nPUT twitter/_doc/tweet-1\n{\n  \"type\": \"tweet\", \n  \"user_name\": \"kimchy\",\n  \"tweeted_at\": \"2017-10-24T09:00:00Z\",\n  \"content\": \"Types are going away\"\n}\n\nGET twitter/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": {\n        \"match\": {\n          \"user_name\": \"kimchy\"\n        }\n      },\n      \"filter\": {\n        \"match\": {\n          \"type\": \"tweet\" \n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","parentchild-without-mapping-types"],["heading","Parent/Child without mapping types"],["body","\n"],["body","以前，通过使一个映射类型为父级，而使一个或多个其他映射类型为子级来表示父子关系。没有类型，我们就不能再使用这个语法了。父子功能将继续像以前一样起作用，只是文档之间关系的表达方式已更改为使用新的 'join' 字段。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/1.DynamicMapping/README.html"],["title","DynamicMapping - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","dynamic-mapping"],["heading","Dynamic mapping"],["body","\n"],["body","Elasticsearch最重要的功能之一开箱即用，索引文档时会自动创建字段映射"],["body","\n"],["body","PUT data/_doc/1 \n{ \"count\": 5 }\n"],["body","\n"],["body","dynamic mapping 自动创建字段功能，可以自定义"],["body","\n\n"],["body","Dynamic field mappings"],["body","\n"],["body","Dynamic templates"],["body","\n\n"],["body","Index templates  可以配置 默认的 mappings、settings、aliaes"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/1.DynamicMapping/1.DynamicFieldMapping.html"],["title","DynamicFieldMapping.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","dynamic-field-mapping"],["heading","Dynamic field mapping"],["body","\n"],["body","\n"],["body","自动添加字段"],["body","\n"],["body","\n\n"],["body","Elasticsearch 自动检测新字段的类型"],["body","\n"],["body","dynamic  控制这个行为"],["body","\n"],["body","可以指定 dynamic 或者 runtime"],["body","\n"],["body","启用动态字段映射后，Elasticsearch使用下表中的规则来确定如何映射每个字段的数据类型。"],["body","\n"],["body","ES只会自动检测以下数据类型，其余的需要手动映射"],["body","\n\n"],["body","JSON data type"],["body","\"dynamic\":\"true\""],["body","\"dynamic\":\"runtime\""],["body","\n"],["body","null"],["body","No field added"],["body","No field added"],["body","\n"],["body","true or false"],["body","boolean"],["body","boolean"],["body","\n"],["body","double"],["body","float"],["body","double"],["body","\n"],["body","integer"],["body","long"],["body","long"],["body","\n"],["body","object"],["body","object"],["body","object"],["body","\n"],["body","array"],["body","Depends on the first non-null value in the array"],["body","Depends on the first non-null value in the array"],["body","\n"],["body","string that passes date detection"],["body","date"],["body","date"],["body","\n"],["body","string that passes numeric detection"],["body","float or long"],["body","double or long"],["body","\n"],["body","string that doesn’t pass date detection or numeric detection"],["body","text with a .keyword sub-field"],["body","keyword"],["body","\n"],["body","Objects are always mapped as part of the properties section, even when the dynamic parameter is set to runtime."],["body","\n\n\n\n"],["body","\n"],["body","可以在 禁用映射，在文档级别以及 对象级别 object"],["body","\n"],["body","\n"],["body","\n"],["body","将  dynamic  设置为FALSE，则 会严格拒绝 新文档中的新字段"],["body","\n"],["body","\n"],["body","\n"],["body","使用   update mapping API  更新  dynamic字段"],["body","\n"],["body","\n"],["body","\n"],["body","可以对 date detection and numeric detection 自定义 动态字段映射规则"],["body","\n"],["body","\n"],["body","\n"],["body","要定义可应用于其他动态字段的自定义映射规则，请使用dynamic_templates."],["body","\n"],["body","\n\n"],["headingLink","date-detection"],["heading","Date detection"],["body","\n\n"],["body","启用 date_detection （默认）"],["body","\n"],["body","会检测 字符串 是否 满足：映射配置 dynamic_date_formats 设置定义的模式"],["body","\n"],["body","默认模式是 [ \"strict_date_optional_time\",\"yyyy/MM/dd HH:mm:ss Z||yyyy/MM/dd Z\"]"],["body","\n\n"],["body","For example:"],["body","\n"],["body","PUT my-index-000001/_doc/1\n{\n  \"create_date\": \"2015/09/02\"\n}\n\nGET my-index-000001/_mapping \n"],["body","\n"],["headingLink","disabling-date-detection"],["heading","Disabling date detection"],["body","\n"],["body","Dynamic date detection can be disabled by setting date_detection to false:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"date_detection\": false\n  }\n}\n\nPUT my-index-000001/_doc/1 \n{\n  \"create\": \"2015/09/02\"\n}\n"],["body","\n"],["headingLink","customizing-detected-date-formats"],["heading","Customizing detected date formats"],["body","\n"],["body","Alternatively, the dynamic_date_formats can be customized to support your own date formats"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"dynamic_date_formats\": [\"MM/dd/yyyy\"]\n  }\n}\n\nPUT my-index-000001/_doc/1\n{\n  \"create_date\": \"09/25/2015\"\n}\n"],["body","\n"],["headingLink","numeric-detection"],["heading","Numeric detection"],["body","\n\n"],["body","虽然JSON支持原生的浮点和整数数据类型，但某些应用程序或语言有时可能会将数字呈现为字符串"],["body","\n"],["body","通常正确的解决方案是显式地映射这些字段，但是数值检测（默认禁用）可以自动进行"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"numeric_detection\": true\n  }\n}\n\nPUT my-index-000001/_doc/1\n{\n  \"my_float\":   \"1.0\", \n  \"my_integer\": \"1\" \n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/1.DynamicMapping/2.DynamicTemplates.html"],["title","DynamicTemplates.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","dynamic-templates"],["heading","Dynamic templates"],["body","\n\n"],["body","\n"],["body","Dynamic Templates 可以 允许 细粒度 的控制 字段映射。"],["body","\n"],["body","\n"],["body","\n"],["body","动态模板映射 可以条件性的映射字段类型"],["body","\n\n"],["body","\n"],["body","match_mapping_type 基于json数据类型的条件筛选"],["body","\n"],["body","\n"],["body","\n"],["body","match and unmatch 基于字段名的正则"],["body","\n"],["body","\n"],["body","\n"],["body","path_match and path_unmatch 完整点路径"],["body","\n"],["body","\n\n"],["body","\n"],["body","\n"],["body","这三个条件可以不同定义 match_mapping_type, match, or path_match, 也就不会匹配任何字段. 您仍然可以在 bulk request. 的 *dynamic_templates 区中按名称引用模板。"],["body","\n"],["body","\n"],["body","\n"],["body","Use the {name} and {dynamic_type} template variables in the mapping specification as placeholders."],["body","\n"],["body","\n"],["body","\n"],["body","动态模板自动映射  只会在 字段有 精确值时，才会映射字段类型"],["body","\n"],["body","\n"],["body","\n"],["body","如果在  dynamic_template 中指定 null_value 选项，自动类型应用 只会在 第一个拥有具体值的文档被索引时应用"],["body","\n"],["body","\n\n"],["body","动态模板被指定为命名对象的数组:"],["body","\n"],["body","  \"dynamic_templates\": [\n    {\n      //匹配条件可以使 `match_mapping_type`, `match`, `match_pattern`, `unmatch`, `path_match`, `path_unmatch`. 中的任何一个\n      \"my_template_name\": { \n        ... match conditions ... \n        //条件匹配后 需要应用的字段类型\n        \"mapping\": { ... } \n      }\n    },\n    ...\n  ]\n"],["body","\n"],["headingLink","validating-dynamic-templates"],["heading","Validating dynamic templates"],["body","\n"],["body","\n"],["body","校验动态模板"],["body","\n"],["body","\n\n"],["body","如果提供的映射包含无效的映射片段，则返回验证错误。"],["body","\n"],["body","索引时在应用 动态模板时 会校验"],["body","\n"],["body","并且在大多数情况下，当动态模板更新时会发生验证"],["body","\n\n\n"],["body","\n"],["body","If no match_mapping_type has been specified but the template is valid for at least one predefined mapping type, the mapping snippet is considered valid. However, a validation error is returned at index time if a field matching the template is indexed as a different type. For example, configuring a dynamic template with no match_mapping_type is considered valid as string type, but if a field matching the dynamic template is indexed as a long, a validation error is returned at index time. It is recommended to configure the match_mapping_type to the expected JSON type or configure the desired type in the mapping snippet."],["body","\n"],["body","\n"],["body","\n"],["body","在 mapping snippet中，如果包含{name} 占位符，则更新时会跳过校验，因为字段名此时还不可知，只会在索引时校验"],["body","\n"],["body","\n"],["body","\n"],["body","模板按顺序匹配，第一个匹配的模板胜出。 update mapping API 可以覆盖模板"],["body","\n"],["body","\n\n"],["headingLink","mapping-runtime-fields-in-a-dynamic-template"],["heading","Mapping runtime fields in a dynamic template"],["body","\n\n"],["body","在索引mappings 中设置 \"dynamic\":\"runtime\" ，则可以映射 runtime fields,它们会在 查询时返回"],["body","\n\n"],["body","Let’s say you have data where each of the fields start with ip_. Based on the dynamic mapping rules, Elasticsearch maps any string that passes numeric detection as a float or long. However, you can create a dynamic template that maps new strings as runtime fields of type ip."],["body","\n"],["body","The following request defines a dynamic template named strings_as_ip. When Elasticsearch detects new string fields matching the ip* pattern, it maps those fields as runtime fields of type ip. Because ip fields aren’t mapped dynamically, you can use this template with either \"dynamic\":\"true\" or \"dynamic\":\"runtime\"."],["body","\n"],["body","PUT my-index-000001/\n{\n  \"mappings\": {\n    \"dynamic_templates\": [\n      {\n        \"strings_as_ip\": {\n          \"match_mapping_type\": \"string\",\n          \"match\": \"ip*\",\n          \"runtime\": {\n            \"type\": \"ip\"\n          }\n        }\n      }\n    ]\n  }\n}\n"],["body","\n"],["body","See this example for how to use dynamic templates to map string fields as either indexed fields or runtime fields."],["body","\n"],["headingLink","match_mapping_type"],["heading","match_mapping_type"],["body","\n"],["body","The match_mapping_type is the data type detected by the JSON parser. Because JSON doesn’t distinguish a long from an integer or a double from a float, it always chooses the wider data type such as long for integers and double for floating-point numbers."],["body","\n"],["body","Elasticsearch automatically detects the following data types:"],["body","\n"],["body","JSON data type"],["body","\"dynamic\":\"true\""],["body","\"dynamic\":\"runtime\""],["body","\n"],["body","null"],["body","No field added"],["body","No field added"],["body","\n"],["body","true or false"],["body","boolean"],["body","boolean"],["body","\n"],["body","double"],["body","float"],["body","double"],["body","\n"],["body","integer"],["body","long"],["body","long"],["body","\n"],["body","object1"],["body","object"],["body","object"],["body","\n"],["body","array"],["body","Depends on the first non-null value in the array"],["body","Depends on the first non-null value in the array"],["body","\n"],["body","string that passes date detection"],["body","date"],["body","date"],["body","\n"],["body","string that passes numeric detection"],["body","float or long"],["body","double or long"],["body","\n"],["body","string that doesn’t pass date detection or numeric detection"],["body","text with a .keyword sub-field"],["body","keyword"],["body","\n"],["body","1Objects are always mapped as part of the properties section, even when the dynamic parameter is set to runtime."],["body","\n\n\n"],["body","Use a wildcard (*) to match all data types."],["body","\n\n"],["body","所有 long类型映射为 integer"],["body","\n"],["body","所有strings映射为 text and keyword"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"dynamic_templates\": [\n      {\n        \"integers\": {\n          \"match_mapping_type\": \"long\",\n          \"mapping\": {\n            \"type\": \"integer\"\n          }\n        }\n      },\n      {\n        \"strings\": {\n          \"match_mapping_type\": \"string\",\n          \"mapping\": {\n            \"type\": \"text\",\n            \"fields\": {\n              \"raw\": {\n                \"type\":  \"keyword\",\n                \"ignore_above\": 256\n              }\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n\nPUT my-index-000001/_doc/1\n{\n  \"my_integer\": 5, \n  \"my_string\": \"Some string\" \n}\n"],["body","\n"],["headingLink","match-and-unmatch"],["heading","match and unmatch"],["body","\n\n"],["body","match 参数使用 模式匹配字段名。 unmatch  排除字段名"],["body","\n"],["body","match_pattern 支持Java的完全正则"],["body","\n\n"],["body","  \"match_pattern\": \"regex\",\n  \"match\": \"^profit_\\d+$\"\n"],["body","\n"],["body","The following example matches all string fields whose name starts with long_ (except for those which end with _text) and maps them as long fields:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"dynamic_templates\": [\n      {\n        \"longs_as_strings\": {\n          \"match_mapping_type\": \"string\",\n          \"match\":   \"long_*\",\n          \"unmatch\": \"*_text\",\n          \"mapping\": {\n            \"type\": \"long\"\n          }\n        }\n      }\n    ]\n  }\n}\n\nPUT my-index-000001/_doc/1\n{\n  \"long_num\": \"5\", \n  \"long_text\": \"foo\" \n}\n"],["body","\n"],["headingLink","path_match-and-path_unmatch"],["heading","path_match and path_unmatch"],["body","\n\n"],["body","path_match and path_unmatch  类似 match unmatch 。但是 可以操作 全带点路径"],["body","\n"],["body","下面的示例，匹配 name object 但是不匹配 middle字段"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"dynamic_templates\": [\n      {\n        \"full_name\": {\n          \"path_match\":   \"name.*\",\n          \"path_unmatch\": \"*.middle\",\n          \"mapping\": {\n            \"type\":       \"text\",\n            \"copy_to\":    \"full_name\"\n          }\n        }\n      }\n    ]\n  }\n}\n\nPUT my-index-000001/_doc/1\n{\n  \"name\": {\n    \"first\":  \"John\",\n    \"middle\": \"Winston\",\n    \"last\":   \"Lennon\"\n  }\n}\n"],["body","\n\n"],["body","请注意，除了叶子字段之外，path_match和path_unmatch参数还匹配对象"],["body","\n"],["body","例如，以下示例将会导致  name.title 匹配为 text。这回引发一个错误"],["body","\n\n"],["body","PUT my-index-000001/_doc/2\n{\n  \"name\": {\n    \"first\":  \"Paul\",\n    \"last\":   \"McCartney\",\n    \"title\": {\n      \"value\": \"Sir\",\n      \"category\": \"order of chivalry\"\n    }\n  }\n}\n"],["body","\n"],["headingLink","template-variables"],["heading","Template variables"],["body","\n\n"],["body","\n"],["body","mapping中 字段名中或者 检测动态类型中的 {name} and {dynamic_type}  占位符  会被替换"],["body","\n"],["body","\n"],["body","\n"],["body","The following example sets all string fields to use an analyzer with the same name as the field, and disables doc_values for all non-string fields:"],["body","\n"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"dynamic_templates\": [\n      {\n        \"named_analyzers\": {\n          \"match_mapping_type\": \"string\",\n          \"match\": \"*\",\n          \"mapping\": {\n            \"type\": \"text\",\n            \"analyzer\": \"{name}\"\n          }\n        }\n      },\n      {\n        \"no_doc_values\": {\n          \"match_mapping_type\":\"*\",\n          \"mapping\": {\n            \"type\": \"{dynamic_type}\",\n            \"doc_values\": false\n          }\n        }\n      }\n    ]\n  }\n}\n\nPUT my-index-000001/_doc/1\n{\n  \"english\": \"Some English text\", \n  \"count\":   5 \n}\n"],["body","\n"],["headingLink","dynamic-template-examples"],["heading","Dynamic template examples"],["body","\n"],["body","以下是一些潜在有用的动态模板的示例:"],["body","\n"],["headingLink","structured-search"],["heading","Structured search"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"dynamic_templates\": [\n      {\n        \"strings_as_keywords\": {\n          \"match_mapping_type\": \"string\",\n          \"mapping\": {\n            \"type\": \"keyword\"\n          }\n        }\n      }\n    ]\n  }\n}\n"],["body","\n"],["headingLink","text-only-mappings-for-strings"],["heading","text-only mappings for strings、"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"dynamic_templates\": [\n      {\n        \"strings_as_text\": {\n          \"match_mapping_type\": \"string\",\n          \"mapping\": {\n            \"type\": \"text\"\n          }\n        }\n      }\n    ]\n  }\n}\n"],["body","\n"],["body","Alternatively, you can create a dynamic template to map your string fields as keyword fields in the runtime section of the mapping."],["body","\n"],["body","When Elasticsearch detects new fields of type string, those fields will be created as runtime fields of type keyword."],["body","\n"],["body","Although your string fields won’t be indexed, their values are stored in _source and can be used in search requests, aggregations, filtering, and sorting."],["body","\n"],["body","For example, the following request creates a dynamic template to map string fields as runtime fields of type keyword. Although the runtime definition is blank, new string fields will be mapped as keyword runtime fields based on the dynamic mapping rules that Elasticsearch uses for adding field types to the mapping. Any string that doesn’t pass date detection or numeric detection is automatically mapped as a keyword:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"dynamic_templates\": [\n      {\n        \"strings_as_keywords\": {\n          \"match_mapping_type\": \"string\",\n          \"runtime\": {}\n        }\n      }\n    ]\n  }\n}\n"],["body","\n"],["body","You index a simple document:"],["body","\n"],["body","PUT my-index-000001/_doc/1\n{\n  \"english\": \"Some English text\",\n  \"count\":   5\n}\n"],["body","\n"],["body","When you view the mapping, you’ll see that the english field is a runtime field of type keyword:"],["body","\n"],["body","GET my-index-000001/_mapping\n"],["body","\n"],["body","{\n  \"my-index-000001\" : {\n    \"mappings\" : {\n      \"dynamic_templates\" : [\n        {\n          \"strings_as_keywords\" : {\n            \"match_mapping_type\" : \"string\",\n            \"runtime\" : { }\n          }\n        }\n      ],\n      \"runtime\" : {\n        \"english\" : {\n          \"type\" : \"keyword\"\n        }\n      },\n      \"properties\" : {\n        \"count\" : {\n          \"type\" : \"long\"\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","disabled-norms"],["heading","Disabled norms"],["body","\n\n"],["body","\n"],["body","Norms are index-time scoring factors."],["body","\n"],["body","\n"],["body","\n"],["body","如果您不关心评分，例如，如果您从未按评分对文档进行排序，则可以禁用这些评分因子在索引中的存储并节省一些空间。"],["body","\n"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"dynamic_templates\": [\n      {\n        \"strings_as_keywords\": {\n          \"match_mapping_type\": \"string\",\n          \"mapping\": {\n            \"type\": \"text\",\n            \"norms\": false,\n            \"fields\": {\n              \"keyword\": {\n                \"type\": \"keyword\",\n                \"ignore_above\": 256\n              }\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n"],["body","\n"],["headingLink","time-series"],["heading","Time series"],["body","\n"],["body","When doing time series analysis with Elasticsearch, it is common to have many numeric fields that you will often aggregate on but never filter on. In such a case, you could disable indexing on those fields to save disk space and also maybe gain some indexing speed:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"dynamic_templates\": [\n      {\n        \"unindexed_longs\": {\n          \"match_mapping_type\": \"long\",\n          \"mapping\": {\n            \"type\": \"long\",\n            \"index\": false\n          }\n        }\n      },\n      {\n        \"unindexed_doubles\": {\n          \"match_mapping_type\": \"double\",\n          \"mapping\": {\n            \"type\": \"float\", \n            \"index\": false\n          }\n        }\n      }\n    ]\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/1.Analyzer.html"],["title","Analyzer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","analyzer"],["heading","analyzer"],["body","\n\n"],["body","只有 'text' 字段支持 “analyzer” 映射参数。"],["body","\n"],["body","The analyzer parameter specifies the analyzer used for text analysis when indexing or searching a text field."],["body","\n"],["body","除非使用 'search_analyzer ' 映射参数覆盖，否则此分析器用于 [索引和搜索分析](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.13/analysis-index-search-time.html)。请参阅 指定分析器。"],["body","\n"],["body","我们建议在生产中使用分析仪之前对其进行测试。请参阅 测试分析器。"],["body","\n"],["body","使用 update mapping API 在现有字段上更新 “分析器” 设置。"],["body","\n\n"],["headingLink","search_quote_analyzer"],["heading","search_quote_analyzer"],["body","\n\n"],["body","\n"],["body","'Search_quote_analyzer '设置允许您为 phrases 指定分析器，这在处理禁用短语查询的停止词时特别有用。"],["body","\n"],["body","\n"],["body","\n"],["body","要禁用短语的停止词，将需要使用三个分析器设置的字段:"],["body","\n\n"],["body","\n"],["body","An analyzer setting for indexing all terms including stop words"],["body","\n"],["body","\n"],["body","\n"],["body","A search_analyzer setting for non-phrase queries that will remove stop words"],["body","\n"],["body","\n"],["body","\n"],["body","A search_quote_analyzer setting for phrase queries that will not remove stop words"],["body","\n"],["body","\n\n"],["body","\n\n"],["body","PUT my-index-000001\n{\n   \"settings\":{\n      \"analysis\":{\n         \"analyzer\":{\n            \"my_analyzer\":{ \n               \"type\":\"custom\",\n               \"tokenizer\":\"standard\",\n               \"filter\":[\n                  \"lowercase\"\n               ]\n            },\n            \"my_stop_analyzer\":{ \n               \"type\":\"custom\",\n               \"tokenizer\":\"standard\",\n               \"filter\":[\n                  \"lowercase\",\n                  \"english_stop\"\n               ]\n            }\n         },\n         \"filter\":{\n            \"english_stop\":{\n               \"type\":\"stop\",\n               \"stopwords\":\"_english_\"\n            }\n         }\n      }\n   },\n   \"mappings\":{\n       \"properties\":{\n          \"title\": {\n             \"type\":\"text\",\n             \"analyzer\":\"my_analyzer\", \n             \"search_analyzer\":\"my_stop_analyzer\", \n             \"search_quote_analyzer\":\"my_analyzer\" \n         }\n      }\n   }\n}\n\nPUT my-index-000001/_doc/1\n{\n   \"title\":\"The Quick Brown Fox\"\n}\n\nPUT my-index-000001/_doc/2\n{\n   \"title\":\"A Quick Brown Fox\"\n}\n\nGET my-index-000001/_search\n{\n   \"query\":{\n      \"query_string\":{\n         \"query\":\"\\\"the quick brown fox\\\"\" \n      }\n   }\n}\n"],["body","\n"],["body","The search_quote_analyzer setting can be updated on existing fields using the update mapping API."],["body","\n"],["body","\n"],["body","my_analyzer analyzer which tokens all terms including stop words"],["body","\n"],["body","my_stop_analyzer analyzer which removes stop words"],["body","\n"],["body","analyzer setting that points to the my_analyzer analyzer which will be used at index time"],["body","\n"],["body","search_analyzer setting that points to the my_stop_analyzer and removes stop words for non-phrase queries"],["body","\n"],["body","search_quote_analyzer setting that points to the my_analyzer analyzer and ensures that stop words are not removed from phrase queries"],["body","\n"],["body","Since the query is wrapped in quotes it is detected as a phrase query therefore the search_quote_analyzer kicks in and ensures the stop words are not removed from the query. The my_analyzer analyzer will then return the following tokens [the, quick, brown, fox] which will match one of the documents. Meanwhile term queries will be analyzed with the my_stop_analyzer analyzer which will filter out stop words. So a search for either The quick brown fox or A quick brown fox will return both documents since both documents contain the following tokens [quick, brown, fox]. Without the search_quote_analyzer it would not be possible to do exact matches for phrase queries as the stop words from phrase queries would be removed resulting in both documents matching."],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/21.positionIncrementGap.html"],["title","positionIncrementGap.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","position_increment_gap"],["heading","position_increment_gap"],["body","\n"],["body","分析的文本字段考虑了术语位置，以便能够支持邻近性或短语查询"],["body","\n"],["body","当索引具有多个值的文本字段时，值之间会添加 假 间隙，以防止大多数短语查询在值之间进行匹配。"],["body","\n"],["body","此间隙的大小使用position_increment_gap配置，默认为100"],["body","\n"],["body","For example:"],["body","\n"],["body","PUT my-index-000001/_doc/1\n{\n  \"names\": [ \"John Abraham\", \"Lincoln Smith\"]\n}\n"],["body","\n"],["body","This phrase query doesn’t match our document which is totally expected."],["body","\n"],["body","GET my-index-000001/_search\n{\n  \"query\": {\n    \"match_phrase\": {\n      \"names\": {\n        \"query\": \"Abraham Lincoln\" \n      }\n    }\n  }\n}\n"],["body","\n"],["body","This phrase query matches our document, even though Abraham and Lincoln are in separate strings, because slop > position_increment_gap."],["body","\n"],["body","GET my-index-000001/_search\n{\n  \"query\": {\n    \"match_phrase\": {\n      \"names\": {\n        \"query\": \"Abraham Lincoln\",\n        \"slop\": 101 \n      }\n    }\n  }\n}\n"],["body","\n"],["body","The position_increment_gap can be specified in the mapping. For instance:"],["body","\n"],["body","//下一个数组元素中的第一个项将与上一个数组元素中的最后一个项相差0个项。\nPUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"names\": {\n        \"type\": \"text\",\n        \"position_increment_gap\": 0 \n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/1\n{\n  \"names\": [ \"John Abraham\", \"Lincoln Smith\"]\n}\n\n//The phrase query matches our document which is weird, but its what we asked for in the mapping.\nGET my-index-000001/_search\n{\n  \"query\": {\n    \"match_phrase\": {\n      \"names\": \"Abraham Lincoln\" \n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/6.dynamic.html"],["title","dynamic.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","dynamic"],["heading","dynamic"],["body","\n"],["body","当您对包含新字段的文档进行索引时，Elasticsearch 将该字段动态添加 到文档或文档中的内部对象。以下文档在 “名称” 对象下添加了字符串字段 “用户名”，对象字段 “名称” 和两个字符串字段:"],["body","\n"],["body","PUT my-index-000001/_doc/1\n{\n  \"username\": \"johnsmith\",\n  \"name\": { \n    \"first\": \"John\",\n    \"last\": \"Smith\"\n  }\n}\n\nGET my-index-000001/_mapping \n"],["body","\n"],["body","Refer to fields under the name object as name.first and name.last."],["body","\n"],["body","The following document adds two string fields: email and name.middle:"],["body","\n"],["body","PUT my-index-000001/_doc/2\n{\n  \"username\": \"marywhite\",\n  \"email\": \"mary@white.com\",\n  \"name\": {\n    \"first\": \"Mary\",\n    \"middle\": \"Alice\",\n    \"last\": \"White\"\n  }\n}\n\nGET my-index-000001/_mapping\n"],["body","\n"],["headingLink","setting-dynamic-on-inner-objects"],["heading","Setting dynamic on inner objects"],["body","\n"],["body","内部对象 从其父对象或映射类型继承 “动态” 设置。在下面的示例中，在类型级别禁用了动态映射，因此不会动态添加新的顶级字段。"],["body","\n"],["body","但是，'user.social_networks '对象启用了动态映射，因此您可以在此内部对象中添加字段。"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"dynamic\": false, \n    \"properties\": {\n      \"user\": { \n        \"properties\": {\n          \"name\": {\n            \"type\": \"text\"\n          },\n          \"social_networks\": {\n            \"dynamic\": true, \n            \"properties\": {}\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","parameters-for-dynamic"],["heading","Parameters for dynamic"],["body","\n"],["body","The dynamic parameter controls whether new fields are added dynamically, and accepts the following parameters:"],["body","\n"],["body","true"],["body","New fields are added to the mapping (default)."],["body","\n"],["body","runtime"],["body","New fields are added to the mapping as runtime fields. These fields are not indexed, and are loaded from _source at query time."],["body","\n"],["body","false"],["body","New fields are ignored. These fields will not be indexed or searchable, but will still appear in the _source field of returned hits. These fields will not be added to the mapping, and new fields must be added explicitly."],["body","\n"],["body","strict"],["body","If new fields are detected, an exception is thrown and the document is rejected. New fields must be explicitly added to the mapping."],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/5.DocValue.html"],["title","DocValue.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","doc_values"],["heading","doc_values"],["body","\n"],["body","默认情况下，大多数字段都是 索引，这使得它们可以搜索，倒排索引使查询可以在唯一排序的术语列表中查找搜索词，并从中立即访问包含该术语的文档列表。"],["body","\n"],["body","排序、聚合和访问脚本中的字段值需要不同的数据访问模式。我们需要能够查找文档 然后找到字段中的术语，而不是查找术语 然后查找文档。"],["body","\n\n"],["body","DocValues 是磁盘上的数据结构，在文档索引时构建，这使得这种数据访问模式成为可能。"],["body","\n"],["body","它们存储与 _ source 相同的值，但以面向列的方式存储，从而更有效地进行排序和聚合。"],["body","\n"],["body","几乎所有字段类型都支持DocValues，值得注意的例外是 text 和 annotated_text字段"],["body","\n\n"],["body","默认情况下，所有支持DocValues 的字段都已启用。如果您确定不需要对字段进行排序或汇总，也不需要从脚本访问字段值，则可以禁用doc值以节省磁盘空间:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"status_code\": { The `status_code` field has `doc_values` enabled by default.\n        \"type\":       \"keyword\"\n      },\n      \"session_id\": { The `session_id` has `doc_values` disabled, but can still be queried.\n        \"type\":       \"keyword\",\n        \"doc_values\": false\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/19.norms.html"],["title","norms.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","norms"],["heading","norms"],["body","\n\n"],["body","Norms是一种值，它们存储在索引中与字段相邻，用于计算得分，在默认的得分算法中，它们结合了lengthNorm（用于给短字段更高的权重）和任何字段级别的提升（boost）"],["body","\n"],["body","Norms 存储各种   normalization factors，这些因子后来在查询时使用，以便计算文档相对于查询的分数。"],["body","\n"],["body","虽然对得分很有用，但是 norms 需要 很多 disk(通常以索引中每个文档的每个字段的一个字节的顺序，即使对于没有此特定字段的文档)"],["body","\n"],["body","因此，如果您不需要在特定字段上评分，则应禁用该字段上的 norms. 特别是，仅用于过滤或聚合的字段就是这种情况。"],["body","\n"],["body","Norms can be disabled on existing fields using the update mapping API."],["body","\n"],["body","Norms can be disabled (but not reenabled after the fact), using the update mapping API like so:"],["body","\n\n"],["body","PUT my-index-000001/_mapping\n{\n  \"properties\": {\n    \"title\": {\n      \"type\": \"text\",\n      \"norms\": false\n    }\n  }\n}\n"],["body","\n\n"],["body","Norms 不会立即被删除，但是当您继续为新文档编制索引时，旧段被合并为新段时，Norms将被删除。"],["body","\n"],["body","删除了Norms 的字段上的任何分数计算都可能返回不一致的结果，因为某些文档不再具有Norms，而其他文档可能仍然具有Norms。"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/15.indexPrefixes.html"],["title","indexPrefixes.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","index_prefixes"],["heading","index_prefixes"],["body","\n"],["body","index_prefixes 参数   启用术语前缀的索引，以加快前缀搜索。它接受以下可选设置:"],["body","\n\n"],["body","\n"],["body","min_chars: The minimum prefix length to index. Must be greater than 0, and defaults to 2. The value is inclusive."],["body","\n"],["body","\n"],["body","\n"],["body","max_chars:The maximum prefix length to index. Must be less than 20, and defaults to 5. The value is inclusive."],["body","\n"],["body","\n\n"],["body","此示例使用默认前缀长度 设置创建文本字段:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"body_text\": {\n        \"type\": \"text\",\n        \"index_prefixes\": { }    \n      }\n    }\n  }\n}\n"],["body","\n"],["body","An empty settings object will use the default min_chars and max_chars settings"],["body","\n"],["body","This example uses custom prefix length settings:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"full_name\": {\n        \"type\": \"text\",\n        \"index_prefixes\": {\n          \"min_chars\" : 1,\n          \"max_chars\" : 10\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/README.html"],["title","MappingParameters - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","mapping-parameters"],["heading","Mapping parameters"],["body","\n\n"],["body","以下页面提供了 字段映射 使用的各种映射参数的详细说明:"],["body","\n"],["body","以下映射参数对于某些或所有字段数据类型都是通用的:"],["body","\n\n\n"],["body","analyzer"],["body","\n"],["body","boost"],["body","\n"],["body","coerce"],["body","\n"],["body","copy_to"],["body","\n"],["body","doc_values"],["body","\n"],["body","dynamic"],["body","\n"],["body","eager_global_ordinals"],["body","\n"],["body","enabled"],["body","\n"],["body","fielddata"],["body","\n"],["body","fields"],["body","\n"],["body","format"],["body","\n"],["body","ignore_above"],["body","\n"],["body","ignore_malformed"],["body","\n"],["body","index_options"],["body","\n"],["body","index_phrases"],["body","\n"],["body","index_prefixes"],["body","\n"],["body","index"],["body","\n"],["body","meta"],["body","\n"],["body","normalizer"],["body","\n"],["body","norms"],["body","\n"],["body","null_value"],["body","\n"],["body","position_increment_gap"],["body","\n"],["body","properties"],["body","\n"],["body","search_analyzer"],["body","\n"],["body","similarity"],["body","\n"],["body","store"],["body","\n"],["body","term_vector"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/26.termVector.html"],["title","termVector.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","term_vector"],["heading","term_vector"],["body","\n"],["body","Term vectors 包含有关 分析 过程产生的术语的信息，包括:"],["body","\n\n"],["body","a list of terms."],["body","\n"],["body","the position (or order) of each term."],["body","\n"],["body","the start and end character offsets mapping the term to its origin in the original string."],["body","\n"],["body","payloads (if they are available) — user-defined binary data associated with each term position."],["body","\n\n"],["body","These term vectors can be stored so that they can be retrieved for a particular document."],["body","\n"],["body","The term_vector setting accepts:"],["body","\n"],["body","\n"],["body","no"],["body","No term vectors are stored. (default)"],["body","\n"],["body","yes"],["body","Just the terms in the field are stored."],["body","\n"],["body","with_positions"],["body","Terms and positions are stored."],["body","\n"],["body","with_offsets"],["body","Terms and character offsets are stored."],["body","\n"],["body","with_positions_offsets"],["body","Terms, positions, and character offsets are stored."],["body","\n"],["body","with_positions_payloads"],["body","Terms, positions, and payloads are stored."],["body","\n"],["body","with_positions_offsets_payloads"],["body","Terms, positions, offsets and payloads are stored."],["body","\n\n\n"],["body","The fast vector highlighter requires with_positions_offsets. The term vectors API can retrieve whatever is stored."],["body","\n"],["body","Setting with_positions_offsets will double the size of a field’s index."],["body","\n"],["body","PUT my-index-000001\n{\n// The fast vector highlighter will be used by default for the `text` field because term vectors are enabled.\n  \"mappings\": {\n    \"properties\": {\n      \"text\": {\n        \"type\":        \"text\",\n        \"term_vector\": \"with_positions_offsets\"\n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/1\n{\n  \"text\": \"Quick brown fox\"\n}\n\nGET my-index-000001/_search\n{\n  \"query\": {\n    \"match\": {\n      \"text\": \"brown fox\"\n    }\n  },\n  \"highlight\": {\n    \"fields\": {\n      \"text\": {} \n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/8.enabled.html"],["title","enabled.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","enabled"],["heading","enabled"],["body","\n"],["body","Elasticsearch尝试对您提供的所有字段进行索引，但有时您只想存储该字段而无需对其进行索引。\n例如，假设您将Elasticsearch用作web会话存储。您可能希望对会话ID和上次更新时间进行索引，但不需要对会话数据本身进行查询或运行聚合。\nenable 设置只能应用于顶级映射定义和 对象 字段，导致elasticsearch完全跳过对字段内容的解析。JSON仍然可以从 _ source 字段中检索，但它不能搜索或以任何其他方式存储:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"user_id\": {\n        \"type\":  \"keyword\"\n      },\n      \"last_updated\": {\n        \"type\": \"date\"\n      },\n      \"session_data\": { \n        \"type\": \"object\",\n        \"enabled\": false\n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/session_1\n{\n  \"user_id\": \"kimchy\",\n  \"session_data\": {\n    \"arbitrary_object\": {\n      \"some_array\": [ \"foo\", \"bar\", { \"baz\": 2 } ]\n    }\n  },\n  \"last_updated\": \"2015-12-06T18:20:22\"\n}\n\nPUT my-index-000001/_doc/session_2\n{\n  \"user_id\": \"jpountz\",\n  \"session_data\": \"none\", \n  \"last_updated\": \"2015-12-06T18:22:13\"\n}\n"],["body","\n"],["body","整个映射也可能被禁用，在这种情况下，文档存储在 _ source 字段中，这意味着它可以被检索，但是它的内容都没有以任何方式索引:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"enabled\": false \n  }\n}\n\nPUT my-index-000001/_doc/session_1\n{\n  \"user_id\": \"kimchy\",\n  \"session_data\": {\n    \"arbitrary_object\": {\n      \"some_array\": [ \"foo\", \"bar\", { \"baz\": 2 } ]\n    }\n  },\n  \"last_updated\": \"2015-12-06T18:20:22\"\n}\nGET my-index-000001/_doc/session_1 \nGET my-index-000001/_mapping \n"],["body","\n"],["body","无法更新现有字段和顶级映射定义的 anable 设置。\n请注意，由于Elasticsearch完全跳过解析字段内容，因此可以将非对象数据添加到禁用的字段中:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"session_data\": {\n        \"type\": \"object\",\n        \"enabled\": false\n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/session_1\n{\n  \"session_data\": \"foo bar\" \n}\n"],["body","\n"],["body","The document is added successfully, even though session_data contains non-object data."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/18.normalizer.html"],["title","normalizer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","normalizer"],["heading","normalizer"],["body","\n\n"],["body","'关键词' 字段的 normalizer 属性类似于 ['分析器'](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.16/analyzer.html)，但它保证分析链 产生单个token。"],["body","\n"],["body","normalizer 过程会应用于 ：keyword字段的索引、以及查询时通过 query parser 对 keyword字段的检索（例如： match 、或者 term query ）"],["body","\n"],["body","elasticsearch 自带 一个小写的 normalizer 叫做：lowercase，。自定义归一化器可以定义为分析设置的一部分，如下所示。"],["body","\n\n"],["body","PUT index\n{\n  \"settings\": {\n    \"analysis\": {\n      \"normalizer\": {\n        \"my_normalizer\": {\n          \"type\": \"custom\",\n          \"char_filter\": [],\n          \"filter\": [\"lowercase\", \"asciifolding\"]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"foo\": {\n        \"type\": \"keyword\",\n        \"normalizer\": \"my_normalizer\"\n      }\n    }\n  }\n}\n\nPUT index/_doc/1\n{\n  \"foo\": \"BÀR\"\n}\n\nPUT index/_doc/2\n{\n  \"foo\": \"bar\"\n}\n\nPUT index/_doc/3\n{\n  \"foo\": \"baz\"\n}\n\nPOST index/_refresh\n\nGET index/_search\n{\n  \"query\": {\n    \"term\": {\n      \"foo\": \"BAR\"\n    }\n  }\n}\n\nGET index/_search\n{\n  \"query\": {\n    \"match\": {\n      \"foo\": \"BAR\"\n    }\n  }\n}\n"],["body","\n"],["body","The above queries match documents 1 and 2 since BÀR is converted to bar at both index and query time."],["body","\n"],["body","{\n  \"took\": $body.took,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\" : 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\" : {\n        \"value\": 2,\n        \"relation\": \"eq\"\n    },\n    \"max_score\": 0.4700036,\n    \"hits\": [\n      {\n        \"_index\": \"index\",\n        \"_type\": \"_doc\",\n        \"_id\": \"1\",\n        \"_score\": 0.4700036,\n        \"_source\": {\n          \"foo\": \"BÀR\"\n        }\n      },\n      {\n        \"_index\": \"index\",\n        \"_type\": \"_doc\",\n        \"_id\": \"2\",\n        \"_score\": 0.4700036,\n        \"_source\": {\n          \"foo\": \"bar\"\n        }\n      }\n    ]\n  }\n}\n"],["body","\n"],["body","此外，关键字 在索引之前被转换的事实也意味着聚合返回归一化值:"],["body","\n"],["body","GET index/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"foo_terms\": {\n      \"terms\": {\n        \"field\": \"foo\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","returns"],["body","\n"],["body","{\n  \"took\": 43,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\" : 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\" : {\n        \"value\": 3,\n        \"relation\": \"eq\"\n    },\n    \"max_score\": null,\n    \"hits\": []\n  },\n  \"aggregations\": {\n    \"foo_terms\": {\n      \"doc_count_error_upper_bound\": 0,\n      \"sum_other_doc_count\": 0,\n      \"buckets\": [\n        {\n          \"key\": \"bar\",\n          \"doc_count\": 2\n        },\n        {\n          \"key\": \"baz\",\n          \"doc_count\": 1\n        }\n      ]\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/24.similarity.html"],["title","similarity.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","similarity"],["heading","similarity"],["body","\n\n"],["body","\n"],["body","Elasticsearch allows you to configure a scoring algorithm or similarity per field."],["body","\n"],["body","\n"],["body","\n"],["body","The similarity setting provides a simple way of choosing a similarity algorithm other than the default BM25, such as TF/IDF."],["body","\n"],["body","\n"],["body","\n"],["body","Similarities are mostly useful for text fields, but can also apply to other field types."],["body","\n"],["body","\n"],["body","\n"],["body","可以通过调整内置相似性的参数来配置自定义相似性：For more details about this expert options, see the similarity module."],["body","\n"],["body","\n\n"],["body","无需任何进一步配置即可开箱即用的唯一相似之处是:"],["body","\n\n"],["body","\n"],["body","BM25"],["body","\n"],["body","The Okapi BM25 algorithm. The algorithm used by default in Elasticsearch and Lucene."],["body","\n"],["body","\n"],["body","\n"],["body","classic"],["body","\n"],["body","[7.0.0] Deprecated in 7.0.0.The TF/IDF algorithm, the former default in Elasticsearch and Lucene."],["body","\n"],["body","\n"],["body","\n"],["body","boolean"],["body","\n"],["body","A simple boolean similarity, which is used when full-text ranking is not needed and the score should only be based on whether the query terms match or not. Boolean similarity gives terms a score equal to their query boost."],["body","\n"],["body","\n\n"],["body","The similarity can be set on the field level when a field is first created, as follows:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"default_field\": { \n        \"type\": \"text\"\n      },\n      \"boolean_sim_field\": {\n        \"type\": \"text\",\n        \"similarity\": \"boolean\" \n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/25.store.html"],["title","store.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","store"],["heading","store"],["body","\n"],["body","\n"],["body","引用"],["body","\n"],["body","\n\n"],["body","默认情况下，字段值是通过可索引以使它们可搜索，但它们不存储"],["body","\n"],["body","这意味着可以查询该字段，但是无法检索到原始字段值。"],["body","\n"],["body","通常这并不重要。字段值已经是 'source'字段 的一部分，默认情况下存储"],["body","\n"],["body","如果您只想检索单个字段或几个字段的值，而不是整个 '_source'，则可以通过 源过滤。"],["body","\n"],["body","在某些情况下，“存储” 一个字段是有意义的。例如，如果您有一个带有 标题，日期 和非常大的 内容 字段的文档，您可能希望仅检索 标题 和 日期，而不必从大的 “_ source” 字段中提取这些字段:"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\",\n        \"store\": true \n      },\n      \"date\": {\n        \"type\": \"date\",\n        \"store\": true \n      },\n      \"content\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\n//The `title` and `date` fields are stored.\nPUT my-index-000001/_doc/1\n{\n  \"title\":   \"Some short title\",\n  \"date\":    \"2015-01-01\",\n  \"content\": \"A very long content field...\"\n}\n\n//This request will retrieve the values of the `title` and `date` fields.\nGET my-index-000001/_search\n{\n  \"stored_fields\": [ \"title\", \"date\" ] \n}\n"],["body","\n"],["headingLink","stored-fields-returned-as-arrays"],["heading","Stored fields returned as arrays"],["body","\n\n"],["body","为了保持一致性，存储的字段总是作为 * array * 返回，因为无法知道原始字段值是单个值，多个值还是空数组。"],["body","\n"],["body","如果您需要原始值，则应该从 “_ source” 字段中检索它。"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/17.multifileds.html"],["title","multifileds.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","fields"],["heading","fields"],["body","\n"],["body","为了不同的目的，以不同的方式对同一个字段进行索引通常是有用的。"],["body","\n"],["body","这就是 multi-fileds"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"city\": {\n        \"type\": \"text\",\n        \"fields\": {\n          \"raw\": { \n            \"type\":  \"keyword\"\n          }\n        }\n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/1\n{\n  \"city\": \"New York\"\n}\n\nPUT my-index-000001/_doc/2\n{\n  \"city\": \"York\"\n}\n\nGET my-index-000001/_search\n{\n  \"query\": {\n    \"match\": {\n      \"city\": \"york\" \n    }\n  },\n  \"sort\": {\n    \"city.raw\": \"asc\" \n  },\n  \"aggs\": {\n    \"Cities\": {\n      \"terms\": {\n        \"field\": \"city.raw\" \n      }\n    }\n  }\n}\n"],["body","\n"],["body","The city.raw field is a keyword version of the city field."],["body","\n"],["body","The city field can be used for full text search."],["body","\n"],["body","The city.raw field can be used for sorting and aggregations"],["body","\n\n"],["body","使用 update mapping API 更新Mapping"],["body","\n"],["body","多字段映射与父字段的映射完全分开"],["body","\n"],["body","多字段不会从其父字段继承任何映射选项。多字段不要改变原来的 '_ source' 字段"],["body","\n\n"],["headingLink","multi-fields-with-multiple-analyzers"],["heading","Multi-fields with multiple analyzers"],["body","\n\n"],["body","多字段的另一个用例是以不同的方式分析同一字段，以获得更好的相关性。"],["body","\n"],["body","For instance we could index a field with the standard analyzer which breaks text up into words, and again with the english analyzer which stems words into their root form:"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"text\": { \n      //The `text` field uses the `standard` analyzer.\n        \"type\": \"text\",\n        \"fields\": {\n          \"english\": { \n            \"type\":     \"text\",\n            // The `text.english` field uses the `english` analyzer.\n            \"analyzer\": \"english\"\n          }\n        }\n      }\n    }\n  }\n}\n\n// Index two documents, one with `fox` and the other with `foxes`.\nPUT my-index-000001/_doc/1\n{ \"text\": \"quick brown fox\" } \n\nPUT my-index-000001/_doc/2\n{ \"text\": \"quick brown foxes\" } \n\n// Query both the `text` and `text.english` fields and combine the scores.\nGET my-index-000001/_search\n{\n  \"query\": {\n    \"multi_match\": {\n      \"query\": \"quick brown foxes\",\n      \"fields\": [ \n        \"text\",\n        \"text.english\"\n      ],\n      \"type\": \"most_fields\" \n    }\n  }\n}\n"],["body","\n\n"],["body","\n"],["body","“文本” 字段在第一个文档中包含术语 “fox”，在第二个文档中包含术语“ foxes ”。“Text.English” 字段包含两个文档的 “fox”，因为“ foxes ”是由“ fox” 表示的。"],["body","\n"],["body","\n"],["body","\n"],["body","查询字符串由 'text' 字段的 'standard '分析器和 'text.English' 字段的 'english' 分析器进行分析。"],["body","\n"],["body","\n"],["body","\n"],["body","词干字段允许对 “foxes” 的查询也与仅包含 “fox” 的文档匹配。这使我们能够匹配尽可能多的文档。通过查询  unstemmed  “text” 字段，我们可以提高与 “foxes” 完全匹配的文档的相关性得分。"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/23.searchAnalyzer.html"],["title","searchAnalyzer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","search_analyzer"],["heading","search_analyzer"],["body","\n\n"],["body","通常，应该在索引时间和搜索时间应用相同的 分析器，以确保查询中的terms 与倒排索引中的 terms 格式相同。"],["body","\n"],["body","但是，有时在搜索时使用不同的分析器是有意义的，例如使用 'edge_ngram ' 自动完成或使用搜索时 同义词时。"],["body","\n\n"],["body","默认情况下，查询将使用在字段映射中定义的 “分析器”，但是可以使用 “search_analyzer” 设置来覆盖:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"filter\": {\n        \"autocomplete_filter\": {\n          \"type\": \"edge_ngram\",\n          \"min_gram\": 1,\n          \"max_gram\": 20\n        }\n      },\n      \"analyzer\": {\n        \"autocomplete\": { //Analysis settings to define the custom `autocomplete` analyzer.\n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"lowercase\",\n            \"autocomplete_filter\"\n          ]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"text\": {\n        \"type\": \"text\", // The `text` field uses the `autocomplete` analyzer at index time, but the `standard` analyzer at search time.\n        \"analyzer\": \"autocomplete\", \n        \"search_analyzer\": \"standard\" \n      }\n    }\n  }\n}\n\n//This field is indexed as the terms: [ `q`, `qu`, `qui`, `quic`, `quick`, `b`, `br`, `bro`, `brow`, `brown`, `f`, `fo`, `fox` ]\nPUT my-index-000001/_doc/1\n{\n  \"text\": \"Quick Brown Fox\" \n}\n\n//The query searches for both of these terms: [ `quick`, `br` ]\nGET my-index-000001/_search\n{\n  \"query\": {\n    \"match\": {\n      \"text\": {\n        \"query\": \"Quick Br\", \n        \"operator\": \"and\"\n      }\n    }\n  }\n}\n"],["body","\n\n"],["body","See Index time search-as-you- type for a full explanation of this example."],["body","\n"],["body","The search_analyzer setting can be updated on existing fields using the update mapping API."],["body","\n"],["body","注意，为了做到这一点，任何现有的 “分析器” 设置和 “类型” 都需要在更新的字段定义中重复。"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/10.IgnoreAbove.html"],["title","IgnoreAbove.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","ignore_above"],["heading","ignore_above"],["body","\n\n"],["body","\n"],["body","超过 ignore_above 设置的字符串将不会被索引或存储。对于字符串数组，ignore_above  将分别应用于每个数组元素，并且长于 'ignore_above' 的字符串元素将不会被索引或存储。"],["body","\n"],["body","\n"],["body","\n"],["body","如果启用了Elasticsearch中的默认值，则所有字符串/数组元素仍将存在于 “_ source” 字段中。"],["body","\n"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"message\": {\n        \"type\": \"keyword\",\n        \"ignore_above\": 20 This field will ignore any string longer than 20 characters.\n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/1  This document is indexed successfully.\n{\n  \"message\": \"Syntax error\"\n}\n\nPUT my-index-000001/_doc/2  This document will be indexed, but without indexing the `message` field.\n{\n  \"message\": \"Syntax error with some long stacktrace\"\n}\n\nGET my-index-000001/_search  Search returns both documents, but only the first is present in the terms aggregation.\n{\n  \"aggs\": {\n    \"messages\": {\n      \"terms\": {\n        \"field\": \"message\"\n      }\n    }\n  }\n}\n"],["body","\n\n"],["body","\n"],["body","The ignore_above setting can be updated on existing fields using the update mapping API."],["body","\n"],["body","\n"],["body","\n"],["body","This option is also useful for protecting against Lucene’s term byte-length limit of 32766."],["body","\n"],["body","\n"],["body","\n"],["body","The value for ignore_above is the character count, but Lucene counts bytes. If you use UTF-8 text with many non-ASCII characters, you may want to set the limit to 32766 / 4 = 8191 since UTF-8 characters may occupy at most 4 bytes."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/14.indexPharse.html"],["title","indexPharse.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","index_phrases"],["heading","index_phrases"],["body","\n\n"],["body","\n"],["body","如果启用，两个术语的单词组合 ( shingles ) 将被索引到一个单独的字段中。这允许精确的短语查询  (no slop) 更有效地运行，但会导致索引更大。"],["body","\n"],["body","\n"],["body","\n"],["body","请注意，当不删除停词时，此方法效果最佳，因为包含停词的短语将不使用辅助字段，并将退回到标准短语查询。接受 “真” 或 “假” (默认)。"],["body","\n"],["body","\n\n"],["headingLink","示例"],["heading","示例"],["body","\n"],["body","如果要包含\"quick brown fox\"这个短语的文档，您可以使用index_phrases参数来加速查询。例如，您可以这样定义字段："],["body","\n"],["body","{\n  \"type\": \"text\",\n  \"index_phrases\": true\n}\n"],["body","\n"],["body","这样，Elasticsearch会在一个单独的字段中索引两个词的组合，比如\"quick brown\"和\"brown fox\"。当您搜索\"quick brown fox\"时，Elasticsearch会先在这个子字段中查找匹配的组合，然后再在原始字段中验证完整的短语。这样可以减少需要扫描的倒排索引项"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/9.format.html"],["title","format.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","format"],["heading","format"],["body","\n\n"],["body","\n"],["body","在JSON文档中，日期表示为字符串。Elasticsearch使用一组预配置的格式将这些字符串识别并解析为表示UTC中的 milliseconds-since-the-epoch  长值。"],["body","\n"],["body","\n"],["body","\n"],["body","Besides the built-in formats, your own custom formats can be specified using the familiar yyyy/MM/dd syntax:"],["body","\n"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"date\": {\n        \"type\":   \"date\",\n        \"format\": \"yyyy-MM-dd\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","Many APIs which support date values also support date math expressions, such as now-1m/d — the current time, minus one month, rounded down to the nearest day."],["body","\n"],["headingLink","custom-date-formats"],["heading","Custom date formats"],["body","\n"],["body","Completely customizable date formats are supported. The syntax for these is explained DateTimeFormatter docs."],["body","\n"],["headingLink","built-in-formats"],["heading","Built In Formats"],["body","\n"],["body","Most of the below formats have a strict companion format, which means that year, month and day parts of the week must use respectively 4, 2 and 2 digits exactly, potentially prepending zeros. For instance a date like 5/11/1 would be considered invalid and would need to be rewritten to 2005/11/01 to be accepted by the date parser."],["body","\n"],["body","To use them, you need to prepend strict_ to the name of the date format, for instance strict_date_optional_time instead of date_optional_time."],["body","\n"],["body","These strict date formats are especially useful when date fields are dynamically mapped in order to make sure to not accidentally map irrelevant strings as dates."],["body","\n"],["body","The following tables lists all the defaults ISO formats supported:"],["body","\n\n"],["body","\n"],["body","epoch_millis"],["body","\n"],["body","A formatter for the number of milliseconds since the epoch. Note, that this timestamp is subject to the limits of a Java Long.MIN_VALUE and Long.MAX_VALUE."],["body","\n"],["body","\n"],["body","\n"],["body","epoch_second"],["body","\n"],["body","A formatter for the number of seconds since the epoch. Note, that this timestamp is subject to the limits of a Java Long.MIN_VALUE and Long. MAX_VALUE divided by 1000 (the number of milliseconds in a second)."],["body","\n"],["body","\n"],["body","\n"],["body","date_optional_time or strict_date_optional_time"],["body","\n"],["body","A generic ISO datetime parser, where the date must include the year at a minimum, and the time (separated by T), is optional. Examples: yyyy-MM-dd'T'HH:mm:ss.SSSZ or yyyy-MM-dd."],["body","\n"],["body","\n"],["body","\n"],["body","strict_date_optional_time_nanos"],["body","\n"],["body","A generic ISO datetime parser, where the date must include the year at a minimum, and the time (separated by T), is optional. The fraction of a second part has a nanosecond resolution. Examples: yyyy-MM-dd'T'HH:mm:ss.SSSSSSZ or yyyy-MM-dd."],["body","\n"],["body","\n"],["body","\n"],["body","basic_date"],["body","\n"],["body","A basic formatter for a full date as four digit year, two digit month of year, and two digit day of month: yyyyMMdd."],["body","\n"],["body","\n"],["body","\n"],["body","basic_date_time"],["body","\n"],["body","A basic formatter that combines a basic date and time, separated by a T: yyyyMMdd'T'HHmmss.SSSZ."],["body","\n"],["body","\n"],["body","\n"],["body","basic_date_time_no_millis"],["body","\n"],["body","A basic formatter that combines a basic date and time without millis, separated by a T: yyyyMMdd'T'HHmmssZ."],["body","\n"],["body","\n"],["body","\n"],["body","basic_ordinal_date"],["body","\n"],["body","A formatter for a full ordinal date, using a four digit year and three digit dayOfYear: yyyyDDD."],["body","\n"],["body","\n"],["body","\n"],["body","basic_ordinal_date_time"],["body","\n"],["body","A formatter for a full ordinal date and time, using a four digit year and three digit dayOfYear: yyyyDDD'T'HHmmss.SSSZ."],["body","\n"],["body","\n"],["body","\n"],["body","basic_ordinal_date_time_no_millis"],["body","\n"],["body","A formatter for a full ordinal date and time without millis, using a four digit year and three digit dayOfYear: yyyyDDD'T'HHmmssZ."],["body","\n"],["body","\n"],["body","\n"],["body","basic_time"],["body","\n"],["body","A basic formatter for a two digit hour of day, two digit minute of hour, two digit second of minute, three digit millis, and time zone offset: HHmmss.SSSZ."],["body","\n"],["body","\n"],["body","\n"],["body","basic_time_no_millis"],["body","\n"],["body","A basic formatter for a two digit hour of day, two digit minute of hour, two digit second of minute, and time zone offset: HHmmssZ."],["body","\n"],["body","\n"],["body","\n"],["body","basic_t_time"],["body","\n"],["body","A basic formatter for a two digit hour of day, two digit minute of hour, two digit second of minute, three digit millis, and time zone off set prefixed by T: 'T'HHmmss.SSSZ."],["body","\n"],["body","\n"],["body","\n"],["body","basic_t_time_no_millis"],["body","\n"],["body","A basic formatter for a two digit hour of day, two digit minute of hour, two digit second of minute, and time zone offset prefixed by T: 'T'HHmmssZ."],["body","\n"],["body","\n"],["body","\n"],["body","basic_week_date or strict_basic_week_date"],["body","\n"],["body","A basic formatter for a full date as four digit weekyear, two digit week of weekyear, and one digit day of week: xxxx'W'wwe."],["body","\n"],["body","\n"],["body","\n"],["body","basic_week_date_time or strict_basic_week_date_time"],["body","\n"],["body","A basic formatter that combines a basic weekyear date and time, separated by a T: xxxx'W'wwe'T'HHmmss.SSSZ."],["body","\n"],["body","\n"],["body","\n"],["body","basic_week_date_time_no_millis or strict_basic_week_date_time_no_millis"],["body","\n"],["body","A basic formatter that combines a basic weekyear date and time without millis, separated by a T: xxxx'W'wwe'T'HHmmssZ."],["body","\n"],["body","\n"],["body","\n"],["body","date or strict_date"],["body","\n"],["body","A formatter for a full date as four digit year, two digit month of year, and two digit day of month: yyyy-MM-dd."],["body","\n"],["body","\n"],["body","\n"],["body","date_hour or strict_date_hour"],["body","\n"],["body","A formatter that combines a full date and two digit hour of day: yyyy-MM-dd'T'HH."],["body","\n"],["body","\n"],["body","\n"],["body","date_hour_minute or strict_date_hour_minute"],["body","\n"],["body","A formatter that combines a full date, two digit hour of day, and two digit minute of hour: yyyy-MM-dd'T'HH:mm."],["body","\n"],["body","\n"],["body","\n"],["body","date_hour_minute_second or strict_date_hour_minute_second"],["body","\n"],["body","A formatter that combines a full date, two digit hour of day, two digit minute of hour, and two digit second of minute: yyyy-MM-dd'T'HH:mm:ss."],["body","\n"],["body","\n"],["body","\n"],["body","date_hour_minute_second_fraction or strict_date_hour_minute_second_fraction"],["body","\n"],["body","A formatter that combines a full date, two digit hour of day, two digit minute of hour, two digit second of minute, and three digit fraction of second: yyyy-MM-dd'T'HH:mm:ss.SSS."],["body","\n"],["body","\n"],["body","\n"],["body","date_hour_minute_second_millis or strict_date_hour_minute_second_millis"],["body","\n"],["body","A formatter that combines a full date, two digit hour of day, two digit minute of hour, two digit second of minute, and three digit fraction of second: yyyy-MM-dd'T'HH:mm:ss.SSS."],["body","\n"],["body","\n"],["body","\n"],["body","date_time or strict_date_time"],["body","\n"],["body","A formatter that combines a full date and time, separated by a T: yyyy-MM-dd'T'HH:mm:ss.SSSZZ."],["body","\n"],["body","\n"],["body","\n"],["body","date_time_no_millis or strict_date_time_no_millis"],["body","\n"],["body","A formatter that combines a full date and time without millis, separated by a T: yyyy-MM-dd'T'HH:mm:ssZZ."],["body","\n"],["body","\n"],["body","\n"],["body","hour or strict_hour"],["body","\n"],["body","A formatter for a two digit hour of day: HH"],["body","\n"],["body","\n"],["body","\n"],["body","hour_minute or strict_hour_minute"],["body","\n"],["body","A formatter for a two digit hour of day and two digit minute of hour: HH:mm."],["body","\n"],["body","\n"],["body","\n"],["body","hour_minute_second or strict_hour_minute_second"],["body","\n"],["body","A formatter for a two digit hour of day, two digit minute of hour, and two digit second of minute: HH:mm:ss."],["body","\n"],["body","\n"],["body","\n"],["body","hour_minute_second_fraction or strict_hour_minute_second_fraction"],["body","\n"],["body","A formatter for a two digit hour of day, two digit minute of hour, two digit second of minute, and three digit fraction of second: HH:mm:ss.SSS."],["body","\n"],["body","\n"],["body","\n"],["body","hour_minute_second_millis or strict_hour_minute_second_millis"],["body","\n"],["body","A formatter for a two digit hour of day, two digit minute of hour, two digit second of minute, and three digit fraction of second: HH:mm:ss.SSS."],["body","\n"],["body","\n"],["body","\n"],["body","ordinal_date or strict_ordinal_date"],["body","\n"],["body","A formatter for a full ordinal date, using a four digit year and three digit dayOfYear: yyyy-DDD."],["body","\n"],["body","\n"],["body","\n"],["body","ordinal_date_time or strict_ordinal_date_time"],["body","\n"],["body","A formatter for a full ordinal date and time, using a four digit year and three digit dayOfYear: yyyy-DDD'T'HH:mm:ss.SSSZZ."],["body","\n"],["body","\n"],["body","\n"],["body","ordinal_date_time_no_millis or strict_ordinal_date_time_no_millis"],["body","\n"],["body","A formatter for a full ordinal date and time without millis, using a four digit year and three digit dayOfYear: yyyy-DDD'T'HH:mm:ssZZ."],["body","\n"],["body","\n"],["body","\n"],["body","time or strict_time"],["body","\n"],["body","A formatter for a two digit hour of day, two digit minute of hour, two digit second of minute, three digit fraction of second, and time zone offset: HH:mm:ss.SSSZZ."],["body","\n"],["body","\n"],["body","\n"],["body","time_no_millis or strict_time_no_millis"],["body","\n"],["body","A formatter for a two digit hour of day, two digit minute of hour, two digit second of minute, and time zone offset: HH:mm:ssZZ."],["body","\n"],["body","\n"],["body","\n"],["body","t_time or strict_t_time"],["body","\n"],["body","A formatter for a two digit hour of day, two digit minute of hour, two digit second of minute, three digit fraction of second, and time zone offset prefixed by T: 'T'HH:mm:ss.SSSZZ."],["body","\n"],["body","\n"],["body","\n"],["body","t_time_no_millis or strict_t_time_no_millis"],["body","\n"],["body","A formatter for a two digit hour of day, two digit minute of hour, two digit second of minute, and time zone offset prefixed by T: 'T'HH:mm:ssZZ."],["body","\n"],["body","\n"],["body","\n"],["body","week_date or strict_week_date"],["body","\n"],["body","A formatter for a full date as four digit weekyear, two digit week of weekyear, and one digit day of week: xxxx-'W'ww-e."],["body","\n"],["body","\n"],["body","\n"],["body","week_date_time or strict_week_date_time"],["body","\n"],["body","A formatter that combines a full weekyear date and time, separated by a T: xxxx-'W'ww-e'T'HH:mm:ss.SSSZZ."],["body","\n"],["body","\n"],["body","\n"],["body","week_date_time_no_millis or strict_week_date_time_no_millis"],["body","\n"],["body","A formatter that combines a full weekyear date and time without millis, separated by a T: xxxx-'W'ww-e'T'HH:mm:ssZZ."],["body","\n"],["body","\n"],["body","\n"],["body","weekyear or strict_weekyear"],["body","\n"],["body","A formatter for a four digit weekyear: xxxx."],["body","\n"],["body","\n"],["body","\n"],["body","weekyear_week or strict_weekyear_week"],["body","\n"],["body","A formatter for a four digit weekyear and two digit week of weekyear: xxxx-'W'ww."],["body","\n"],["body","\n"],["body","\n"],["body","weekyear_week_day or strict_weekyear_week_day"],["body","\n"],["body","A formatter for a four digit weekyear, two digit week of weekyear, and one digit day of week: xxxx-'W'ww-e."],["body","\n"],["body","\n"],["body","\n"],["body","year or strict_year"],["body","\n"],["body","A formatter for a four digit year: yyyy."],["body","\n"],["body","\n"],["body","\n"],["body","year_month or strict_year_month"],["body","\n"],["body","A formatter for a four digit year and two digit month of year: yyyy-MM."],["body","\n"],["body","\n"],["body","\n"],["body","year_month_day or strict_year_month_day"],["body","\n"],["body","A formatter for a four digit year, two digit month of year, and two digit day of month: yyyy-MM-dd."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/4.CopyTo.html"],["title","CopyTo.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","copy_to"],["heading","copy_to"],["body","\n\n"],["body","Copy_to 参数允许您将多个字段的值复制到组字段中，然后可以将其作为单个字段进行查询。"],["body","\n"],["body","如果您经常搜索多个字段，则可以通过使用 copy_to 搜索更少的字段来提高搜索速度。请参阅 [搜索尽可能少的字段](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/tune-for-search-speed.html # 搜索尽可能少的字段)。"],["body","\n"],["body","例如，可以将 first_name 和 last_name 字段复制到 full_name 字段，如下所示:"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"first_name\": {\n        \"type\": \"text\",\n        \"copy_to\": \"full_name\" \n      },\n      \"last_name\": {\n        \"type\": \"text\",\n        \"copy_to\": \"full_name\" \n      },\n      \"full_name\": { //The values of the `first_name` and `last_name` fields are copied to the `full_name` field.\n        \"type\": \"text\"\n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/1\n{\n  \"first_name\": \"John\",\n  \"last_name\": \"Smith\"\n}\n\nGET my-index-000001/_search //仍然可以分别查询 “first_name” 和 “last_name” 字段中的名字和姓氏，但是可以查询 “full_name” 字段中的名字和姓氏。\n{\n  \"query\": {\n    \"match\": {\n      \"full_name\": { \n        \"query\": \"John Smith\",\n        \"operator\": \"and\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","Some important points:"],["body","\n\n"],["body","It is the field value which is copied, not the terms (which result from the analysis process)."],["body","\n"],["body","The original _source field will not be modified to show the copied values."],["body","\n"],["body","The same value can be copied to multiple fields, with \"copy_to\": [ \"field_1\", \"field_2\" ]"],["body","\n"],["body","You cannot copy recursively via intermediary fields such as a copy_to on field_1 to field_2 and copy_to on field_2 to field_3 expecting indexing into field_1 will eventuate in field_3, instead use copy_to directly to multiple fields from the originating field."],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/22.properties.html"],["title","properties.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","properties"],["heading","properties"],["body","\n\n"],["body","\n"],["body","Type mappings, object fields and nested fields contain sub-fields, called properties."],["body","\n"],["body","\n"],["body","\n"],["body","These properties may be of any data type, including object and nested. Properties can be added:"],["body","\n\n"],["body","explicitly by defining them when creating an index."],["body","\n"],["body","explicitly by defining them when adding or updating a mapping type with the update mapping API."],["body","\n"],["body","dynamically just by indexing documents containing new fields."],["body","\n\n"],["body","\n\n"],["body","Below is an example of adding properties to a mapping type, an object field, and a nested field:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": { //Properties in the top-level mappings definition.\n      \"manager\": {\n        \"properties\": { //Properties under the `manager` object field.\n          \"age\":  { \"type\": \"integer\" },\n          \"name\": { \"type\": \"text\"  }\n        }\n      },\n      \"employees\": {\n        \"type\": \"nested\",\n        \"properties\": { //Properties under the `employees` nested field.\n          \"age\":  { \"type\": \"integer\" },\n          \"name\": { \"type\": \"text\"  }\n        }\n      }\n    }\n  }\n}\n\n//An example document which corresponds to the above mapping.\nPUT my-index-000001/_doc/1 \n{\n  \"region\": \"US\",\n  \"manager\": {\n    \"name\": \"Alice White\",\n    \"age\": 30\n  },\n  \"employees\": [\n    {\n      \"name\": \"John Smith\",\n      \"age\": 34\n    },\n    {\n      \"name\": \"Peter Brown\",\n      \"age\": 26\n    }\n  ]\n}\n"],["body","\n"],["headingLink","dot-notation"],["heading","Dot notation"],["body","\n"],["body","内部字段可以在查询，聚合等中引用，使用点符号"],["body","\n"],["body","GET my-index-000001/_search\n{\n  \"query\": {\n    \"match\": {\n      \"manager.name\": \"Alice White\"\n    }\n  },\n  \"aggs\": {\n    \"Employees\": {\n      \"nested\": {\n        \"path\": \"employees\"\n      },\n      \"aggs\": {\n        \"Employee Ages\": {\n          \"histogram\": {\n            \"field\": \"employees.age\",\n            \"interval\": 5\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","The full path to the inner field must be specified."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/12.index.html"],["title","index.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","index"],["heading","index"],["body","\n\n"],["body","\n"],["body","The index option controls whether field values are indexed. It accepts true or false and defaults to true."],["body","\n"],["body","\n"],["body","\n"],["body","Fields that are not indexed are not queryable."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/3.coerce.html"],["title","coerce.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","coerce"],["heading","coerce"],["body","\n"],["body","数据并不总是干净的。根据它是如何产生的，一个数字可能在JSON主体中呈现为真实的JSON数字，例如 “5”，但它也可能呈现为字符串，例如 “5”。替代地，应该是整数的数字可以改为被呈现为浮点，例如 “5.0” 或甚至 “5.0”。"],["body","\n"],["body","强制尝试清理脏值以适合字段的数据类型。例如:"],["body","\n\n"],["body","字符串将被强制使用数字。"],["body","\n"],["body","浮点数将被截断为整数值。"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"number_one\": {\n        \"type\": \"integer\"\n      },\n      \"number_two\": {\n        \"type\": \"integer\",\n        \"coerce\": false\n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/1\n{\n  \"number_one\": \"10\" //The `number_one` field will contain the integer `10`.\n}\n\nPUT my-index-000001/_doc/2\n{\n  \"number_two\": \"10\" //This document will be rejected because coercion is disabled.\n}\n"],["body","\n"],["body","可以使用 update mapping API 在现有字段上更新 “强制” 设置值。"],["body","\n"],["headingLink","index-level-default"],["heading","Index-level default"],["body","\n"],["body","The index.mapping.coerce setting can be set on the index level to disable coercion globally across all mapping types:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"index.mapping.coerce\": false\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"number_one\": {\n        \"type\": \"integer\",\n        \"coerce\": true\n      },\n      \"number_two\": {\n        \"type\": \"integer\"\n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/1\n{ \"number_one\": \"10\" } \n\nPUT my-index-000001/_doc/2\n{ \"number_two\": \"10\" } \n"],["body","\n\n"],["body","\n"],["body","“number_one” 字段覆盖索引级别设置以启用强制。"],["body","\n"],["body","\n"],["body","\n"],["body","该文档将被拒绝，因为 'number_two' 字段继承了索引级强制设置。"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/13.IndexOptions.html"],["title","IndexOptions.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","index_options"],["heading","index_options"],["body","\n\n"],["body","\n"],["body","index_options 参数控制将哪些信息添加到倒排索引中，以进行搜索和突出显示。"],["body","\n"],["body","\n"],["body","\n"],["body","The index_options 主要用于Text字段。避免对其他字段类型使用 index_options"],["body","\n"],["body","\n"],["body","\n"],["body","参数接受以下值之一。每个值从先前列出的值中检索信息。"],["body","\n"],["body","例如, freqs contains docs; positions contains both freqs and docs."],["body","\n\n"],["body","docs\n1. 只有文档编号被索引。"],["body","\n"],["body","freqs\n1. 文档编号和词频会被索引，用于文档排序"],["body","\n"],["body","positions (default)\n\n"],["body","Doc number, term frequencies, and term positions (or order) are indexed"],["body","\n"],["body","Positions can be used for proximity or phrase queries."],["body","\n\n"],["body","\n"],["body","offsets\n\n"],["body","Doc number, term frequencies, positions, and start and end character offsets (which map the term back to the original string) are indexed."],["body","\n"],["body","Offsets are used by the unified highlighter to speed up highlighting. （加速高亮）"],["body","\n\n"],["body","\n\n"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"text\": {\n        \"type\": \"text\",\n        \"index_options\": \"offsets\"\n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/1\n{\n  \"text\": \"Quick brown fox\"\n}\n\nGET my-index-000001/_search\n{\n  \"query\": {\n    \"match\": {\n      \"text\": \"brown fox\"\n    }\n  },\n  \"highlight\": {\n    \"fields\": {\n      \"text\": {} \n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/7.eagerGlobalOrdinals.html"],["title","eagerGlobalOrdinals.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","eager_global_ordinals"],["heading","eager_global_ordinals"],["body","\n"],["headingLink","what-are-global-ordinals"],["heading","What are global ordinals?"],["body","\n\n"],["body","为了支持聚合和其他需要在每个文档的基础上查找字段值的操作，Elasticsearch使用名为 doc values 的数据结构"],["body","\n"],["body","基于term的字段类型 (例如 keyword) 使用 ordinal mapping  来存储其doc值，以实现更紧凑的表示。此映射的工作原理是根据每个术语的词典顺序为其分配一个增量整数或序数。该字段的doc值仅存储每个文档的序数，而不是原始Term，并具有单独的查找结构以在序数和 term 之间进行转换。"],["body","\n"],["body","在聚合过程中使用时，序数可以大大提高性能，例如，term 聚合仅依赖于序数将文档收集到分片级别的存储桶中，然后在跨分片组合结果时将序数转换回其原始术语值。"],["body","\n"],["body","每个索引段都定义了自己的 序数映射，但是聚合会在整个分片中收集数据。因此，为了能够将序数用于诸如聚合之类的分片级操作，Elasticsearch创建了一个统一的映射，称为 全局序数。全局序数映射建立在段序数之上，并且通过为每个段维护从全局序数到局部序数的映射来工作。"],["body","\n\n"],["body","如果搜索包含以下任何组件，则使用全局序数:"],["body","\n\n"],["body","Certain bucket aggregations on keyword, ip, and flattened fields. This includes terms aggregations as mentioned above, as well as composite, diversified_sampler, and significant_terms."],["body","\n"],["body","Bucket aggregations on text fields that require fielddata to be enabled."],["body","\n"],["body","从 join 字段对父文档和子文档进行操作，包括 has_child 查询和 Parent 聚合。"],["body","\n\n"],["body","全局序数映射使用堆内存作为 字段数据缓存的一部分。高基数字段上的聚合可以使用大量内存并触发 [字段数据断路器](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/circuit-breaker.html # fielddata-断路器)"],["body","\n"],["headingLink","loading-global-ordinals"],["heading","Loading global ordinals"],["body","\n"],["body","必须先构建全局序数映射，然后才能在搜索过程中使用序数。默认情况下，映射是在第一次需要全局序数时在搜索期间加载的。如果您正在优化索引速度，这是正确的方法，但是如果搜索性能是优先考虑的，建议急切地将全局序数加载到将在聚合中使用的字段上:"],["body","\n"],["body","PUT my-index-000001/_mapping\n{\n  \"properties\": {\n    \"tags\": {\n      \"type\": \"keyword\",\n      \"eager_global_ordinals\": true\n    }\n  }\n}\n"],["body","\n\n"],["body","启用 eager_global_ordinals 时，分片 refreshed  时将会 构建 global ordinals"],["body","\n"],["body","Elasticsearch 总是在向索引内容公开更改之前加载它们"],["body","\n"],["body","这将构建全局序数的成本从搜索转移到索引时间"],["body","\n"],["body","在创建分片的新副本时，Elasticsearch还将  急切地构建全局序号，这在增加副本数或将分片重新定位到新节点时可能会发生。"],["body","\n\n"],["body","可以随时通过更新 eager_global_ordinals 设置来禁用Eager加载"],["body","\n"],["body","PUT my-index-000001/_mapping\n{\n  \"properties\": {\n    \"tags\": {\n      \"type\": \"keyword\",\n      \"eager_global_ordinals\": false\n    }\n  }\n}\n"],["body","\n"],["body","在 冻结索引 上，每次搜索后都会丢弃全局序号，并在请求时再次重建。这意味着 eager_global_ordinals 不应用于冻结索引: 它将导致每次搜索都重新加载全局ordinals。相反，索引应该在冻结之前强制合并到单个段。这完全避免了构建全局序数 (更多细节可以在下一节中找到)。"],["body","\n"],["headingLink","avoiding-global-ordinal-loading"],["heading","Avoiding global ordinal loading"],["body","\n"],["body","通常，全局序数在加载时间和内存使用方面不会带来很大的开销。但是，对于具有较大分片的索引，或者如果字段包含大量唯一Term value，则加载全局序数可能会很昂贵。因为全局序数为分片上的所有段提供了统一的映射，所以当新段变得可见时，它们也需要完全重建。"],["body","\n"],["body","在某些情况下，可以完全避免全局顺序加载:"],["body","\n\n"],["body","The terms, sampler, and significant_terms aggregations support a parameter execution_hint that helps control how buckets are collected. It defaults to global_ordinals, but can be set to map to instead use the term values directly."],["body","\n"],["body","如果分片已经 强制合并 向下到单个段，则其段序号已经 可以代表全局序号了。在这种情况下，Elasticsearch不需要构建全局序数映射，并且使用全局序数没有额外的开销。请注意，出于性能原因，您应该只强制合并一个您永远不会再写入的索引。"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/20.nullValue.html"],["title","nullValue.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","null_value"],["heading","null_value"],["body","\n\n"],["body","null 值无法索引或搜索"],["body","\n"],["body","当字段设置为 null 时 (或空数组或 null 值数组)，它被视为该字段没有值。"],["body","\n"],["body","Null_value参数允许您将显式 null 值替换为指定的值，以便可以对其进行索引和搜索。例如:"],["body","\n\n"],["body","PUT my-index-000001\n{\n//Replace explicit `null` values with the term `NULL`.\n  \"mappings\": {\n    \"properties\": {\n      \"status_code\": {\n        \"type\":       \"keyword\",\n        \"null_value\": \"NULL\" \n      }\n    }\n  }\n}\n\n\nPUT my-index-000001/_doc/1\n{\n  \"status_code\": null\n}\n//An empty array does not contain an explicit `null`, and so won’t be replaced with the `null_value`.\nPUT my-index-000001/_doc/2\n{\n  \"status_code\": [] \n}\n\n// A query for `NULL` returns document 1, but not document 2.\nGET my-index-000001/_search\n{\n  \"query\": {\n    \"term\": {\n      \"status_code\": \"NULL\" \n    }\n  }\n}\n"],["body","\n\n"],["body","null_value 需要与字段相同的数据类型。例如，long 字段不能具有字符串 null_value。"],["body","\n"],["body","The null_value only influences how data is indexed, it doesn’t modify the _source document"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/11.IgnoreMalformed.html"],["title","IgnoreMalformed.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","ignore_malformed"],["heading","ignore_malformed"],["body","\n\n"],["body","有时你对你收到的数据没有太多的控制权。一个用户可以发送一个 “登录” 字段，该字段是 “日期”，另一个用户可以发送一个 “登录” 字段，该字段是电子邮件地址。"],["body","\n"],["body","默认情况下，试图将错误的数据类型索引到字段中会引发异常，并拒绝整个文档。'ignore_malformed' 参数，如果设置为 'true'，则允许忽略异常。格式错误的字段没有索引，但是文档中的其他字段正常处理。"],["body","\n\n"],["body","For example:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"number_one\": {\n        \"type\": \"integer\",\n        \"ignore_malformed\": true\n      },\n      \"number_two\": {\n        \"type\": \"integer\"\n      }\n    }\n  }\n}\n\nPUT my-index-000001/_doc/1\n{\n  \"text\":       \"Some text value\",\n  \"number_one\": \"foo\" \n}\n\nPUT my-index-000001/_doc/2\n{\n  \"text\":       \"Some text value\",\n  \"number_two\": \"foo\" \n}\n"],["body","\n"],["body","The ignore_malformed setting is currently supported by the following mapping types:"],["body","\n\n"],["body","\n"],["body","Numeric"],["body","\n"],["body","long, integer, short, byte, double, float, half_float, scaled_float"],["body","\n"],["body","\n"],["body","\n"],["body","Date"],["body","\n"],["body","date"],["body","\n"],["body","\n"],["body","\n"],["body","Date nanoseconds"],["body","\n"],["body","date_nanos"],["body","\n"],["body","\n"],["body","\n"],["body","Geo-point"],["body","\n"],["body","geo_point for lat/lon points"],["body","\n"],["body","\n"],["body","\n"],["body","Geo-shape"],["body","\n"],["body","geo_shape for complex shapes like polygons"],["body","\n"],["body","\n"],["body","\n"],["body","IP"],["body","\n"],["body","ip for IPv4 and IPv6 addresses"],["body","\n"],["body","\n\n"],["body","The ignore_malformed setting value can be updated on existing fields using the update mapping API."],["body","\n"],["headingLink","index-level-default"],["heading","Index-level default"],["body","\n"],["body","可以在索引级别设置 “index.mapping.Ignore_mal格式” 设置，以在所有允许的映射类型中全局忽略格式错误的内容。不支持该设置的映射类型将忽略它，如果在索引级别上设置。"],["body","\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"index.mapping.ignore_malformed\": true \n  },\n  \"mappings\": {\n    \"properties\": {\n      \"number_one\": { \n        \"type\": \"byte\"\n      },\n      \"number_two\": {\n        \"type\": \"integer\",\n        \"ignore_malformed\": false \n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","dealing-with-malformed-fields"],["heading","Dealing with malformed fields"],["body","\n\n"],["body","\n"],["body","当打开 “ignore_malformed” 时，格式错误的字段在索引时会被静默忽略。只要有可能，建议保留包含格式错误的字段的文档数量，否则对该字段的查询将变得毫无意义。"],["body","\n"],["body","\n"],["body","\n"],["body","Elasticsearch通过在特殊的 '_ ignored' 字段上使用 'exists' 、 'term' 或 'terms' 查询，可以轻松检查多少文档的字段格式错误。"],["body","\n"],["body","\n\n"],["headingLink","limits-for-json-objects"],["heading","Limits for JSON Objects"],["body","\n"],["body","You can’t use ignore_malformed with the following data types:"],["body","\n\n"],["body","Nested data type"],["body","\n"],["body","Object data type"],["body","\n"],["body","Range data types"],["body","\n\n"],["body","You also can’t use ignore_malformed to ignore JSON objects submitted to fields of the wrong data type. A JSON object is any data surrounded by curly brackets \"{}\" and includes data mapped to the nested, object, and range data types."],["body","\n"],["body","If you submit a JSON object to an unsupported field, Elasticsearch will return an error and reject the entire document regardless of the ignore_malformed setting."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/16.meta.html"],["title","meta.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","meta"],["heading","meta"],["body","\n"],["body","附加到字段的元数据。此元数据对Elasticsearch是不透明的，它仅对在相同索引上工作的多个应用程序有用，以共享有关字段 (例如单位) 的元信息"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"latency\": {\n        \"type\": \"long\",\n        \"meta\": {\n          \"unit\": \"ms\"\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n"],["body","\n"],["body","字段元数据最多强制执行5个条目，键的长度小于或等于20，并且值是长度小于或等于50的字符串"],["body","\n"],["body","\n"],["body","\n"],["body","字段元数据可通过提交映射更新来更新，更新的元数据将覆盖现有字段的元数据。"],["body","\n"],["body","\n"],["body","\n"],["body","Elastic 产品对字段使用以下标准元数据条目，您可以遵循这些相同的元数据约定，以获得更好的数据开箱即用体验。"],["body","\n\n"],["body","\n"],["body","unit"],["body","\n"],["body","The unit associated with a numeric field: \"percent\", \"byte\" or a time unit. By default, a field does not have a unit. Only valid for numeric fields. The convention for percents is to use value 1 to mean 100%."],["body","\n"],["body","\n"],["body","\n"],["body","metric_type"],["body","\n"],["body","数字字段的度量类型: \"gauge\" or \"counter\"."],["body","\n"],["body","A gauge is a single-value measurement that can go up or down over time, such as a temperature."],["body","\n"],["body","A counter is a single-value cumulative counter that only goes up, such as the number of requests processed by a web server. By default, no metric type is associated with a field. Only valid for numeric fields."],["body","\n"],["body","\n\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/6.MappingParameters/2.boost.html"],["title","boost.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","boost"],["heading","boost"],["body","\n"],["body","可以在查询时使用 “boost” 参数，将各个字段 “自动增强”-对相关性分数进行更多计数，如下所示:"],["body","\n"],["body","# Matches on the `title` field will have twice the weight as those on the `content` field, which has the default `boost` of `1.0`.\nPUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\",\n        \"boost\": 2 \n      },\n      \"content\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\n"],["body","\n\n"],["body","\n"],["body","The boost is applied only for term queries (prefix, range and fuzzy queries are not boosted)"],["body","\n"],["body","\n"],["body","\n"],["body","您可以通过直接在查询中使用boost参数来达到相同的效果，例如以下查询 (带有field time boost):"],["body","\n"],["body","\n\n"],["body","POST _search\n{\n  \"query\": {\n    \"match\": {\n      \"title\": {\n        \"query\": \"quick brown fox\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","is equivalent to:"],["body","\n"],["body","POST _search\n{\n  \"query\": {\n    \"match\": {\n      \"title\": {\n        \"query\": \"quick brown fox\",\n        \"boost\": 2\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","deprecated-in-500"],["heading","Deprecated in 5.0.0."],["body","\n"],["body","索引时 boost 已弃用。而是在查询时应用字段映射增强。对于在5.0.0之前创建的索引，boost仍将在索引时应用。"],["body","\n"],["headingLink","why-index-time-boosting-is-a-bad-idea"],["heading","Why index time boosting is a bad idea"],["body","\n"],["body","出于以下原因，我们建议不要使用 index time boost:"],["body","\n\n"],["body","如果不重新索引所有文档，就不能更改 index time “boost” 值。"],["body","\n"],["body","每个查询都支持 query time boost，达到相同的效果。不同之处在于，您可以调整 'boost' 值，而不必重新索引。"],["body","\n"],["body","index time boost 存储为 'norm' 的一部分，该值仅为一个字节。这降低了 field length 归一化因子的分辨率，这可能导致较低的质量相关性计算。"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/3.RuntimeFields/4.RetrieveARuntimeField.html"],["title","RetrieveARuntimeField.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","retrieve-a-runtime-field"],["heading","Retrieve a runtime field"],["body","\n"],["body","Use the fields parameter on the _search API to retrieve the values of runtime fields. Runtime fields won’t display in _source, but the fields API works for all fields, even those that were not sent as part of the original _source."],["body","\n"],["headingLink","define-a-runtime-field-to-calculate-the-day-of-week"],["heading","Define a runtime field to calculate the day of week"],["body","\n"],["body","For example, the following request adds a runtime field called day_of_week. The runtime field includes a script that calculates the day of the week based on the value of the @timestamp field. We’ll include \"dynamic\":\"runtime\" in the request so that new fields are added to the mapping as runtime fields."],["body","\n"],["body","PUT my-index-000001/\n{\n  \"mappings\": {\n    \"dynamic\": \"runtime\",\n    \"runtime\": {\n      \"day_of_week\": {\n        \"type\": \"keyword\",\n        \"script\": {\n          \"source\": \"emit(doc['@timestamp'].value.dayOfWeekEnum.getDisplayName(TextStyle.FULL, Locale.ROOT))\"\n        }\n      }\n    },\n    \"properties\": {\n      \"@timestamp\": {\"type\": \"date\"}\n    }\n  }\n}\n"],["body","\n"],["headingLink","ingest-some-data"],["heading","Ingest some data"],["body","\n"],["body","Let’s ingest some sample data, which will result in two indexed fields: @timestamp and message."],["body","\n"],["body","POST /my-index-000001/_bulk?refresh\n{ \"index\": {}}\n{ \"@timestamp\": \"2020-06-21T15:00:01-05:00\", \"message\" : \"211.11.9.0 - - [2020-06-21T15:00:01-05:00] \\\"GET /english/index.html HTTP/1.0\\\" 304 0\"}\n{ \"index\": {}}\n{ \"@timestamp\": \"2020-06-21T15:00:01-05:00\", \"message\" : \"211.11.9.0 - - [2020-06-21T15:00:01-05:00] \\\"GET /english/index.html HTTP/1.0\\\" 304 0\"}\n{ \"index\": {}}\n{ \"@timestamp\": \"2020-04-30T14:30:17-05:00\", \"message\" : \"40.135.0.0 - - [2020-04-30T14:30:17-05:00] \\\"GET /images/hm_bg.jpg HTTP/1.0\\\" 200 24736\"}\n{ \"index\": {}}\n{ \"@timestamp\": \"2020-04-30T14:30:53-05:00\", \"message\" : \"232.0.0.0 - - [2020-04-30T14:30:53-05:00] \\\"GET /images/hm_bg.jpg HTTP/1.0\\\" 200 24736\"}\n{ \"index\": {}}\n{ \"@timestamp\": \"2020-04-30T14:31:12-05:00\", \"message\" : \"26.1.0.0 - - [2020-04-30T14:31:12-05:00] \\\"GET /images/hm_bg.jpg HTTP/1.0\\\" 200 24736\"}\n{ \"index\": {}}\n{ \"@timestamp\": \"2020-04-30T14:31:19-05:00\", \"message\" : \"247.37.0.0 - - [2020-04-30T14:31:19-05:00] \\\"GET /french/splash_inet.html HTTP/1.0\\\" 200 3781\"}\n{ \"index\": {}}\n{ \"@timestamp\": \"2020-04-30T14:31:27-05:00\", \"message\" : \"252.0.0.0 - - [2020-04-30T14:31:27-05:00] \\\"GET /images/hm_bg.jpg HTTP/1.0\\\" 200 24736\"}\n{ \"index\": {}}\n{ \"@timestamp\": \"2020-04-30T14:31:29-05:00\", \"message\" : \"247.37.0.0 - - [2020-04-30T14:31:29-05:00] \\\"GET /images/hm_brdl.gif HTTP/1.0\\\" 304 0\"}\n{ \"index\": {}}\n{ \"@timestamp\": \"2020-04-30T14:31:29-05:00\", \"message\" : \"247.37.0.0 - - [2020-04-30T14:31:29-05:00] \\\"GET /images/hm_arw.gif HTTP/1.0\\\" 304 0\"}\n{ \"index\": {}}\n{ \"@timestamp\": \"2020-04-30T14:31:32-05:00\", \"message\" : \"247.37.0.0 - - [2020-04-30T14:31:32-05:00] \\\"GET /images/nav_bg_top.gif HTTP/1.0\\\" 200 929\"}\n{ \"index\": {}}\n{ \"@timestamp\": \"2020-04-30T14:31:43-05:00\", \"message\" : \"247.37.0.0 - - [2020-04-30T14:31:43-05:00] \\\"GET /french/images/nav_venue_off.gif HTTP/1.0\\\" 304 0\"}\n"],["body","\n"],["headingLink","search-for-the-calculated-day-of-week"],["heading","Search for the calculated day of week"],["body","\n"],["body","The following request uses the search API to retrieve the day_of_week field that the original request defined as a runtime field in the mapping. The value for this field is calculated dynamically at query time without reindexing documents or indexing the day_of_week field. This flexibility allows you to modify the mapping without changing any field values."],["body","\n"],["body","GET my-index-000001/_search\n{\n  \"fields\": [\n    \"@timestamp\",\n    \"day_of_week\"\n  ],\n  \"_source\": false\n}\n"],["body","\n"],["body","The previous request returns the day_of_week field for all matching documents. We can define another runtime field called client_ip that also operates on the message field and will further refine the query:"],["body","\n"],["body","PUT /my-index-000001/_mapping\n{\n  \"runtime\": {\n    \"client_ip\": {\n      \"type\": \"ip\",\n      \"script\" : {\n      \"source\" : \"String m = doc[\\\"message\\\"].value; int end = m.indexOf(\\\" \\\"); emit(m.substring(0, end));\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","Run another query, but search for a specific IP address using the client_ip runtime field:"],["body","\n"],["body","GET my-index-000001/_search\n{\n  \"size\": 1,\n  \"query\": {\n    \"match\": {\n      \"client_ip\": \"211.11.9.0\"\n    }\n  },\n  \"fields\" : [\"*\"]\n}\n"],["body","\n"],["body","This time, the response includes only two hits. The value for day_of_week (Sunday) was calculated at query time using the runtime script defined in the mapping, and the result includes only documents matching the 211.11.9.0 IP address."],["body","\n"],["body","{\n  ...\n  \"hits\" : {\n    \"total\" : {\n      \"value\" : 2,\n      \"relation\" : \"eq\"\n    },\n    \"max_score\" : 1.0,\n    \"hits\" : [\n      {\n        \"_index\" : \"my-index-000001\",\n        \"_type\" : \"_doc\",\n        \"_id\" : \"oWs5KXYB-XyJbifr9mrz\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"@timestamp\" : \"2020-06-21T15:00:01-05:00\",\n          \"message\" : \"211.11.9.0 - - [2020-06-21T15:00:01-05:00] \\\"GET /english/index.html HTTP/1.0\\\" 304 0\"\n        },\n        \"fields\" : {\n          \"@timestamp\" : [\n            \"2020-06-21T20:00:01.000Z\"\n          ],\n          \"client_ip\" : [\n            \"211.11.9.0\"\n          ],\n          \"message\" : [\n            \"211.11.9.0 - - [2020-06-21T15:00:01-05:00] \\\"GET /english/index.html HTTP/1.0\\\" 304 0\"\n          ],\n          \"day_of_week\" : [\n            \"Sunday\"\n          ]\n        }\n      }\n    ]\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/3.RuntimeFields/README.html"],["title","RuntimeFields - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","runtime-fields"],["heading","Runtime fields"],["body","\n"],["body","运行时字段是在查询时评估的字段。运行时字段使您能够:"],["body","\n\n"],["body","将字段添加到现有文档，而无需重新索引数据"],["body","\n"],["body","在不了解数据结构的情况下开始处理数据"],["body","\n"],["body","在查询时覆盖从索引字段返回的值"],["body","\n"],["body","在不修改基础架构的情况下为特定用途定义字段"],["body","\n\n\n"],["body","您可以像其他字段一样从 searchAPI 访问运行时字段，Elasticsearch看到的运行时字段没有不同"],["body","\n"],["body","可以在  index mapping or in the search request.  中定义 运行时字段"],["body","\n"],["body","在 _search API 中 使用  fields  参数  retrieve the values of runtime fields,Runtime fields  不会保存在 _source中。但是  fields  API 对所有字段都生效，即时字段不属于 _source的一部分"],["body","\n"],["body","Runtime fields对 日志数据场景很有用。尤其是当您不确定数据结构时，检索速率降低了，但是索引大小减少了"],["body","\n\n"],["headingLink","benefits"],["heading","Benefits"],["body","\n"],["body","\n"],["body","好处"],["body","\n"],["body","\n\n"],["body","因为运行时字段没有索引，所以添加运行时字段不会增加索引大小。"],["body","\n"],["body","可以直接在 index mapping 中 定义运行时字段。节省存储成本，提高摄入速度，您可以更快地将数据引入Elastic Stack 并立即访问它"],["body","\n"],["body","定义运行时字段时，您可以立即在搜索请求、聚合、过滤和排序中使用它。"],["body","\n"],["body","如果将运行时字段设置为索引字段，则无需修改引用运行时字段的任何查询。"],["body","\n"],["body","更好的是，您可以引用  某些索引 (其中该字段是运行时字段)，以及其他索引 (其中该字段是索引字段)"],["body","\n"],["body","从本质上讲，运行时字段最重要的好处是可以在摄入后将字段添加到文档中"],["body","\n"],["body","此功能简化了映射决策，因为您不必预先决定如何解析数据，并且可以随时使用运行时字段来修改映射。"],["body","\n"],["body","使用运行时字段可以实现更小的索引和更快的摄取时间，这结合起来使用更少的资源并降低运营成本。"],["body","\n\n"],["headingLink","incentives"],["heading","Incentives"],["body","\n"],["body","\n"],["body","激励措施"],["body","\n"],["body","\n\n"],["body","运行时字段可以替换您可以使用 _search API使用script的许多方式。"],["body","\n"],["body","使用运行时字段的方式受   文档数量的影响，这些文档包含脚本"],["body","\n"],["body","For example, if you’re using the fields parameter on the _search API to retrieve the values of a runtime field, the script runs only against the top hits just like script fields do"],["body","\n"],["body","可以使用  script query  过滤文档。 Runtime fields 更加具有弹性。可以编写 script 创建 字段，创建之后 到处可用。 such as fields, all queries, and aggregations. ？"],["body","\n"],["body","您也可以使用脚本对搜索结果 进行排序（ sort search results, ），但是相同的脚本在运行时字段中的工作方式完全相同？"],["body","\n"],["body","script fields 跟 runtime fields 的性能一致。性能主要取决于 脚本的计算量以及文档的数目"],["body","\n\n"],["headingLink","compromises"],["heading","Compromises"],["body","\n"],["body","\n"],["body","妥协"],["body","\n"],["body","\n\n"],["body","Runtime fields 可以减小磁盘空间占用。但是影响性能"],["body","\n"],["body","为了平衡 搜索性能与空间使用。索引你经常过滤与聚合的字段。Elasticsearch在运行查询时会自动首先使用这些索引字段，从而获得快速的响应时间。然后，您可以使用运行时字段来限制Elasticsearch计算值所需的字段数量"],["body","\n"],["body","使用 asynchronous search API  运行包含运行时字段的搜索。这种搜索方法有助于抵消  在每个包含该字段的文档中  计算运行时字段值的性能影响。如果查询无法同步返回结果集，则当结果可用时，您将异步获得结果。"],["body","\n"],["body","针对运行时字段的查询被认为是昂贵的，如果 search.allow_expensive_queries is set to false,不允许进行昂贵的查询，Elasticsearch将拒绝针对运行时字段的任何查询。"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/3.RuntimeFields/2.DefineRuntimeFieldsInASearchRequest.html"],["title","DefineRuntimeFieldsInASearchRequest.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","define-runtime-fields-in-a-search-request"],["heading","Define runtime fields in a search request"],["body","\n\n"],["body","在 索引API中的 runtime_mapping 区块中。添加 runtime_mapping"],["body","\n"],["body","查询时定义 runtime_mapping 与 索引时定义 的格式一样"],["body","\n\n"],["body","GET my-index-000001/_search\n{\n  \"runtime_mappings\": {\n    \"day_of_week\": {\n      \"type\": \"keyword\",\n      \"script\": {\n        \"source\": \"emit(doc['@timestamp'].value.dayOfWeekEnum.getDisplayName(TextStyle.FULL, Locale.ROOT))\"\n      }\n    }\n  },\n  \"aggs\": {\n    \"day_of_week\": {\n      \"terms\": {\n        \"field\": \"day_of_week\"\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","create-runtime-fields-that-use-other-runtime-fields"],["heading","Create runtime fields that use other runtime fields"],["body","\n"],["body","You can even define runtime fields in a search request that return values from other runtime fields. For example, let’s say you bulk index some sensor data:"],["body","\n"],["body","您甚至可以在搜索请求中定义从其他运行时字段返回值的运行时字段。例如，假设你批量索引一些传感器数据:"],["body","\n"],["body","POST my-index-000001/_bulk?refresh=true\n{\"index\":{}}\n{\"@timestamp\":1516729294000,\"model_number\":\"QVKC92Q\",\"measures\":{\"voltage\":\"5.2\",\"start\": \"300\",\"end\":\"8675309\"}}\n{\"index\":{}}\n{\"@timestamp\":1516642894000,\"model_number\":\"QVKC92Q\",\"measures\":{\"voltage\":\"5.8\",\"start\": \"300\",\"end\":\"8675309\"}}\n{\"index\":{}}\n{\"@timestamp\":1516556494000,\"model_number\":\"QVKC92Q\",\"measures\":{\"voltage\":\"5.1\",\"start\": \"300\",\"end\":\"8675309\"}}\n{\"index\":{}}\n{\"@timestamp\":1516470094000,\"model_number\":\"QVKC92Q\",\"measures\":{\"voltage\":\"5.6\",\"start\": \"300\",\"end\":\"8675309\"}}\n{\"index\":{}}\n{\"@timestamp\":1516383694000,\"model_number\":\"HG537PU\",\"measures\":{\"voltage\":\"4.2\",\"start\": \"400\",\"end\":\"8625309\"}}\n{\"index\":{}}\n{\"@timestamp\":1516297294000,\"model_number\":\"HG537PU\",\"measures\":{\"voltage\":\"4.0\",\"start\": \"400\",\"end\":\"8625309\"}}\n"],["body","\n\n"],["body","想 基于measures.start measures.end 做聚合"],["body","\n"],["body","但是定义成了 text格式。无法聚合"],["body","\n"],["body","可以通过 runtime mapping重定义"],["body","\n\n"],["body","PUT my-index-000001/_mapping\n{\n  \"runtime\": {\n    \"measures.start\": {\n      \"type\": \"long\"\n    },\n    \"measures.end\": {\n      \"type\": \"long\"\n    }\n  }\n}\n"],["body","\n\n"],["body","Runtime fields 可以取代原有的重名字段"],["body","\n"],["body","这种能力使得你 可以 遮蔽现有字段"],["body","\n\n"],["body","GET my-index-000001/_search\n{\n  \"aggs\": {\n    \"avg_start\": {\n      \"avg\": {\n        \"field\": \"measures.start\"\n      }\n    },\n    \"avg_end\": {\n      \"avg\": {\n        \"field\": \"measures.end\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","The response includes the aggregation results without changing the values for the underlying data:"],["body","\n"],["body","{\n  \"aggregations\" : {\n    \"avg_start\" : {\n      \"value\" : 333.3333333333333\n    },\n    \"avg_end\" : {\n      \"value\" : 8658642.333333334\n    }\n  }\n}\n"],["body","\n\n"],["body","可以在搜索中 定义 runtime fields 然后运行  stats aggregation"],["body","\n"],["body","通过 end - start 定义 duration"],["body","\n"],["body","然后通过duration 进行stat统计"],["body","\n\n"],["body","GET my-index-000001/_search\n{\n  \"runtime_mappings\": {\n    \"duration\": {\n      \"type\": \"long\",\n      \"script\": {\n        \"source\": \"\"\"\n          emit(doc['measures.end'].value - doc['measures.start'].value);\n          \"\"\"\n      }\n    }\n  },\n  \"aggs\": {\n    \"duration_stats\": {\n      \"stats\": {\n        \"field\": \"duration\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","{\n  \"aggregations\" : {\n    \"duration_stats\" : {\n      \"count\" : 6,\n      \"min\" : 8624909.0,\n      \"max\" : 8675009.0,\n      \"avg\" : 8658309.0,\n      \"sum\" : 5.1949854E7\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/3.RuntimeFields/6.ExploreYourDataWithRuntimeFields.html"],["title","ExploreYourDataWithRuntimeFields.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","explore-your-data-with-runtime-fields"],["heading","Explore your data with runtime fields"],["body","\n\n"],["body","考虑要从中提取字段的大量日志数据。对数据进行索引非常耗时，并且会占用大量磁盘空间，您只想探索数据结构，而无需预先提交架构。"],["body","\n"],["body","您知道您的日志数据包含要提取的特定字段。在这种情况下，我们希望专注于 “@ timestamp” 和“ message ”字段。通过使用运行时字段，您可以定义脚本以在搜索时计算这些字段的值。"],["body","\n\n"],["headingLink","define-indexed-fields-as-a-starting-point"],["heading","Define indexed fields as a starting point"],["body","\n"],["body","You can start with a simple example by adding the @timestamp and message fields to the my-index-000001 mapping as indexed fields. To remain flexible, use wildcard as the field type for message:"],["body","\n"],["body","PUT /my-index-000001/\n{\n  \"mappings\": {\n    \"properties\": {\n      \"@timestamp\": {\n        \"format\": \"strict_date_optional_time||epoch_second\",\n        \"type\": \"date\"\n      },\n      \"message\": {\n        \"type\": \"wildcard\"\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","ingest-some-data"],["heading","Ingest some data"],["body","\n"],["body","The final document is not a valid Apache log format, but we can account for that scenario in our script."],["body","\n"],["body","POST /my-index-000001/_bulk?refresh\n{\"index\":{}}\n{\"timestamp\":\"2020-04-30T14:30:17-05:00\",\"message\":\"40.135.0.0 - - [30/Apr/2020:14:30:17 -0500] \\\"GET /images/hm_bg.jpg HTTP/1.0\\\" 200 24736\"}\n{\"index\":{}}\n{\"timestamp\":\"2020-04-30T14:30:53-05:00\",\"message\":\"232.0.0.0 - - [30/Apr/2020:14:30:53 -0500] \\\"GET /images/hm_bg.jpg HTTP/1.0\\\" 200 24736\"}\n{\"index\":{}}\n{\"timestamp\":\"2020-04-30T14:31:12-05:00\",\"message\":\"26.1.0.0 - - [30/Apr/2020:14:31:12 -0500] \\\"GET /images/hm_bg.jpg HTTP/1.0\\\" 200 24736\"}\n{\"index\":{}}\n{\"timestamp\":\"2020-04-30T14:31:19-05:00\",\"message\":\"247.37.0.0 - - [30/Apr/2020:14:31:19 -0500] \\\"GET /french/splash_inet.html HTTP/1.0\\\" 200 3781\"}\n{\"index\":{}}\n{\"timestamp\":\"2020-04-30T14:31:22-05:00\",\"message\":\"247.37.0.0 - - [30/Apr/2020:14:31:22 -0500] \\\"GET /images/hm_nbg.jpg HTTP/1.0\\\" 304 0\"}\n{\"index\":{}}\n{\"timestamp\":\"2020-04-30T14:31:27-05:00\",\"message\":\"252.0.0.0 - - [30/Apr/2020:14:31:27 -0500] \\\"GET /images/hm_bg.jpg HTTP/1.0\\\" 200 24736\"}\n{\"index\":{}}\n{\"timestamp\":\"2020-04-30T14:31:28-05:00\",\"message\":\"not a valid apache log\"}\n"],["body","\n"],["body","At this point, you can view how Elasticsearch stores your raw data."],["body","\n"],["body","GET /my-index-000001\n"],["body","\n"],["body","The mapping contains two fields: @timestamp and message."],["body","\n"],["body","{\n  \"my-index-000001\" : {\n    \"aliases\" : { },\n    \"mappings\" : {\n      \"properties\" : {\n        \"@timestamp\" : {\n          \"type\" : \"date\",\n          \"format\" : \"strict_date_optional_time||epoch_second\"\n        },\n        \"message\" : {\n          \"type\" : \"wildcard\"\n        },\n        \"timestamp\" : {\n          \"type\" : \"date\"\n        }\n      }\n    },\n    ...\n  }\n}\n"],["body","\n"],["headingLink","define-a-runtime-field-with-a-grok-pattern"],["heading","Define a runtime field with a grok pattern"],["body","\n\n"],["body","如果要检索包含 “clientip” 的结果，可以将该字段添加为映射中的运行时字段"],["body","\n"],["body","以下 grok pattern  从 text fields 字段中抽取 结构化字段"],["body","\n"],["body","grok表达式 是可重用的 命名 正则表达式"],["body","\n\n"],["body","The script matches on the %{COMMONAPACHELOG} log pattern, which understands the structure of Apache logs. If the pattern matches, the script emits the value of the matching IP address. If the pattern doesn’t match (clientip != null), the script just returns the field value without crashing."],["body","\n"],["body","PUT my-index-000001/_mappings\n{\n  \"runtime\": {\n    \"http.clientip\": {\n      \"type\": \"ip\",\n      \"script\": \"\"\"\n        String clientip=grok('%{COMMONAPACHELOG}').extract(doc[\"message\"].value)?.clientip;\n        if (clientip != null) emit(clientip);  //This condition ensures that the script doesn’t crash even if the pattern of the message doesn’t match.\n      \"\"\"\n    }\n  }\n}\n"],["body","\n\n"],["body","或者，您可以在搜索请求的上下文中定义相同的运行时字段"],["body","\n\n"],["body","GET my-index-000001/_search\n{\n  \"runtime_mappings\": {\n    \"http.clientip\": {\n      \"type\": \"ip\",\n      \"script\": \"\"\"\n        String clientip=grok('%{COMMONAPACHELOG}').extract(doc[\"message\"].value)?.clientip;\n        if (clientip != null) emit(clientip);\n      \"\"\"\n    }\n  },\n  \"query\": {\n    \"match\": {\n      \"http.clientip\": \"40.135.0.0\"\n    }\n  },\n  \"fields\" : [\"http.clientip\"]\n}\n"],["body","\n"],["headingLink","search-for-a-specific-ip-address"],["heading","Search for a specific IP address"],["body","\n"],["body","Using the http.clientip runtime field, you can define a simple query to run a search for a specific IP address and return all related fields."],["body","\n"],["body","GET my-index-000001/_search\n{\n  \"query\": {\n    \"match\": {\n      \"http.clientip\": \"40.135.0.0\"\n    }\n  },\n  \"fields\" : [\"*\"]\n}\n"],["body","\n"],["body","The API returns the following result. Without building your data structure in advance, you can search and explore your data in meaningful ways to experiment and determine which fields to index."],["body","\n"],["body","Also, remember that if statement in the script?"],["body","\n"],["body","if (clientip != null) emit(clientip);\n"],["body","\n"],["body","If the script didn’t include this condition, the query would fail on any shard that doesn’t match the pattern. By including this condition, the query skips data that doesn’t match the grok pattern."],["body","\n"],["body","{\n  ...\n  \"hits\" : {\n    \"total\" : {\n      \"value\" : 1,\n      \"relation\" : \"eq\"\n    },\n    \"max_score\" : 1.0,\n    \"hits\" : [\n      {\n        \"_index\" : \"my-index-000001\",\n        \"_type\" : \"_doc\",\n        \"_id\" : \"FdLqu3cBhqheMnFKd0gK\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"timestamp\" : \"2020-04-30T14:30:17-05:00\",\n          \"message\" : \"40.135.0.0 - - [30/Apr/2020:14:30:17 -0500] \\\"GET /images/hm_bg.jpg HTTP/1.0\\\" 200 24736\"\n        },\n        \"fields\" : {\n          \"http.clientip\" : [\n            \"40.135.0.0\"\n          ],\n          \"message\" : [\n            \"40.135.0.0 - - [30/Apr/2020:14:30:17 -0500] \\\"GET /images/hm_bg.jpg HTTP/1.0\\\" 200 24736\"\n          ],\n          \"timestamp\" : [\n            \"2020-04-30T19:30:17.000Z\"\n          ]\n        }\n      }\n    ]\n  }\n}\n"],["body","\n"],["headingLink","search-for-documents-in-a-specific-range"],["heading","Search for documents in a specific range"],["body","\n"],["body","You can also run a range query that operates on the timestamp field. The following query returns any documents where the timestamp is greater than or equal to 2020-04-30T14:31:27-05:00:"],["body","\n"],["body","GET my-index-000001/_search\n{\n  \"query\": {\n    \"range\": {\n      \"timestamp\": {\n        \"gte\": \"2020-04-30T14:31:27-05:00\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","The response includes the document where the log format doesn’t match, but the timestamp falls within the defined range."],["body","\n"],["body","{\n  ...\n  \"hits\" : {\n    \"total\" : {\n      \"value\" : 2,\n      \"relation\" : \"eq\"\n    },\n    \"max_score\" : 1.0,\n    \"hits\" : [\n      {\n        \"_index\" : \"my-index-000001\",\n        \"_type\" : \"_doc\",\n        \"_id\" : \"hdEhyncBRSB6iD-PoBqe\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"timestamp\" : \"2020-04-30T14:31:27-05:00\",\n          \"message\" : \"252.0.0.0 - - [30/Apr/2020:14:31:27 -0500] \\\"GET /images/hm_bg.jpg HTTP/1.0\\\" 200 24736\"\n        }\n      },\n      {\n        \"_index\" : \"my-index-000001\",\n        \"_type\" : \"_doc\",\n        \"_id\" : \"htEhyncBRSB6iD-PoBqe\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"timestamp\" : \"2020-04-30T14:31:28-05:00\",\n          \"message\" : \"not a valid apache log\"\n        }\n      }\n    ]\n  }\n}\n"],["body","\n"],["headingLink","define-a-runtime-field-with-a-dissect-pattern"],["heading","Define a runtime field with a dissect pattern"],["body","\n\n"],["body","可以使用  dissect patterns  而不是 使用 grok patterns。解剖模式在固定分隔符上匹配，但通常比grok更快。"],["body","\n"],["body","您可以使用剖析来获得与使用 grok模式解析Apache日志相同的结果  grok pattern  不是使用模式匹配日志，而是指定要丢弃的字符串部分。特别注意要丢弃的字符串部分将有助于建立成功的解剖模式。"],["body","\n\n"],["body","PUT my-index-000001/_mappings\n{\n  \"runtime\": {\n    \"http.client.ip\": {\n      \"type\": \"ip\",\n      \"script\": \"\"\"\n        String clientip=dissect('%{clientip} %{ident} %{auth} [%{@timestamp}] \"%{verb} %{request} HTTP/%{httpversion}\" %{status} %{size}').extract(doc[\"message\"].value)?.clientip;\n        if (clientip != null) emit(clientip);\n      \"\"\"\n    }\n  }\n}\n"],["body","\n"],["body","Similarly, you can define a dissect pattern to extract the HTTP response code:"],["body","\n"],["body","PUT my-index-000001/_mappings\n{\n  \"runtime\": {\n    \"http.response\": {\n      \"type\": \"long\",\n      \"script\": \"\"\"\n        String response=dissect('%{clientip} %{ident} %{auth} [%{@timestamp}] \"%{verb} %{request} HTTP/%{httpversion}\" %{response} %{size}').extract(doc[\"message\"].value)?.response;\n        if (response != null) emit(Integer.parseInt(response));\n      \"\"\"\n    }\n  }\n}\n"],["body","\n"],["body","You can then run a query to retrieve a specific HTTP response using the http.response runtime field:"],["body","\n"],["body","GET my-index-000001/_search\n{\n  \"query\": {\n    \"match\": {\n      \"http.response\": \"304\"\n    }\n  },\n  \"fields\" : [\"*\"]\n}\n"],["body","\n"],["body","The response includes a single document where the HTTP response is 304:"],["body","\n"],["body","{\n  ...\n  \"hits\" : {\n    \"total\" : {\n      \"value\" : 1,\n      \"relation\" : \"eq\"\n    },\n    \"max_score\" : 1.0,\n    \"hits\" : [\n      {\n        \"_index\" : \"my-index-000001\",\n        \"_type\" : \"_doc\",\n        \"_id\" : \"A2qDy3cBWRMvVAuI7F8M\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"timestamp\" : \"2020-04-30T14:31:22-05:00\",\n          \"message\" : \"247.37.0.0 - - [30/Apr/2020:14:31:22 -0500] \\\"GET /images/hm_nbg.jpg HTTP/1.0\\\" 304 0\"\n        },\n        \"fields\" : {\n          \"http.clientip\" : [\n            \"247.37.0.0\"\n          ],\n          \"http.response\" : [\n            304\n          ],\n          \"message\" : [\n            \"247.37.0.0 - - [30/Apr/2020:14:31:22 -0500] \\\"GET /images/hm_nbg.jpg HTTP/1.0\\\" 304 0\"\n          ],\n          \"http.client.ip\" : [\n            \"247.37.0.0\"\n          ],\n          \"timestamp\" : [\n            \"2020-04-30T19:31:22.000Z\"\n          ]\n        }\n      }\n    ]\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/3.RuntimeFields/3.OverrideFieldValuesAtQueryTime.html"],["title","OverrideFieldValuesAtQueryTime.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","override-field-values-at-query-time"],["heading","Override field values at query time"],["body","\n"],["body","For example, let’s say you indexed the following documents into my-index-000001:"],["body","\n"],["body","POST my-index-000001/_bulk?refresh=true\n{\"index\":{}}\n{\"@timestamp\":1516729294000,\"model_number\":\"QVKC92Q\",\"measures\":{\"voltage\":5.2}}\n{\"index\":{}}\n{\"@timestamp\":1516642894000,\"model_number\":\"QVKC92Q\",\"measures\":{\"voltage\":5.8}}\n{\"index\":{}}\n{\"@timestamp\":1516556494000,\"model_number\":\"QVKC92Q\",\"measures\":{\"voltage\":5.1}}\n{\"index\":{}}\n{\"@timestamp\":1516470094000,\"model_number\":\"QVKC92Q\",\"measures\":{\"voltage\":5.6}}\n{\"index\":{}}\n{\"@timestamp\":1516383694000,\"model_number\":\"HG537PU\",\"measures\":{\"voltage\":4.2}}\n{\"index\":{}}\n{\"@timestamp\":1516297294000,\"model_number\":\"HG537PU\",\"measures\":{\"voltage\":4.0}}\n"],["body","\n"],["body","You later realize that the HG537PU sensors aren’t reporting their true voltage. The indexed values are supposed to be 1.7 times higher than the reported values! Instead of reindexing your data, you can define a script in the runtime_mappings section of the _search r\tequest to shadow the voltage field and calculate a new value at query time."],["body","\n"],["body","If you search for documents where the model number matches HG537PU:"],["body","\n"],["body","GET my-index-000001/_search\n{\n  \"query\": {\n    \"match\": {\n      \"model_number\": \"HG537PU\"\n    }\n  }\n}\n"],["body","\n"],["body","The response includes indexed values for documents matching model number HG537PU:"],["body","\n"],["body","{\n  ...\n  \"hits\" : {\n    \"total\" : {\n      \"value\" : 2,\n      \"relation\" : \"eq\"\n    },\n    \"max_score\" : 1.0296195,\n    \"hits\" : [\n      {\n        \"_index\" : \"my-index-000001\",\n        \"_type\" : \"_doc\",\n        \"_id\" : \"F1BeSXYBg_szTodcYCmk\",\n        \"_score\" : 1.0296195,\n        \"_source\" : {\n          \"@timestamp\" : 1516383694000,\n          \"model_number\" : \"HG537PU\",\n          \"measures\" : {\n            \"voltage\" : 4.2\n          }\n        }\n      },\n      {\n        \"_index\" : \"my-index-000001\",\n        \"_type\" : \"_doc\",\n        \"_id\" : \"l02aSXYBkpNf6QRDO62Q\",\n        \"_score\" : 1.0296195,\n        \"_source\" : {\n          \"@timestamp\" : 1516297294000,\n          \"model_number\" : \"HG537PU\",\n          \"measures\" : {\n            \"voltage\" : 4.0\n          }\n        }\n      }\n    ]\n  }\n}\n"],["body","\n"],["body","The following request defines a runtime field where the script evaluates the model_number field where the value is HG537PU. For each match, the script multiplies the value for the voltage field by 1.7."],["body","\n"],["body","Using the fields parameter on the _search API, you can retrieve the value that the script calculates for the measures.voltage field for documents matching the search request:"],["body","\n"],["body","POST my-index-000001/_search\n{\n  \"runtime_mappings\": {\n    \"measures.voltage\": {\n      \"type\": \"double\",\n      \"script\": {\n        \"source\":\n        \"\"\"if (doc['model_number.keyword'].value.equals('HG537PU'))\n        {emit(1.7 * params._source['measures']['voltage']);}\n        else{emit(params._source['measures']['voltage']);}\"\"\"\n      }\n    }\n  },\n  \"query\": {\n    \"match\": {\n      \"model_number\": \"HG537PU\"\n    }\n  },\n  \"fields\": [\"measures.voltage\"]\n}\n"],["body","\n"],["body","Looking at the response, the calculated values for measures.voltage on each result are 7.14 and 6.8. That’s more like it! The runtime field calculated this value as part of the search request without modifying the mapped value, which still returns in the response:"],["body","\n"],["body","{\n  ...\n  \"hits\" : {\n    \"total\" : {\n      \"value\" : 2,\n      \"relation\" : \"eq\"\n    },\n    \"max_score\" : 1.0296195,\n    \"hits\" : [\n      {\n        \"_index\" : \"my-index-000001\",\n        \"_type\" : \"_doc\",\n        \"_id\" : \"F1BeSXYBg_szTodcYCmk\",\n        \"_score\" : 1.0296195,\n        \"_source\" : {\n          \"@timestamp\" : 1516383694000,\n          \"model_number\" : \"HG537PU\",\n          \"measures\" : {\n            \"voltage\" : 4.2\n          }\n        },\n        \"fields\" : {\n          \"measures.voltage\" : [\n            7.14\n          ]\n        }\n      },\n      {\n        \"_index\" : \"my-index-000001\",\n        \"_type\" : \"_doc\",\n        \"_id\" : \"l02aSXYBkpNf6QRDO62Q\",\n        \"_score\" : 1.0296195,\n        \"_source\" : {\n          \"@timestamp\" : 1516297294000,\n          \"model_number\" : \"HG537PU\",\n          \"measures\" : {\n            \"voltage\" : 4.0\n          }\n        },\n        \"fields\" : {\n          \"measures.voltage\" : [\n            6.8\n          ]\n        }\n      }\n    ]\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/3.RuntimeFields/5.IndexARuntimeField.html"],["title","IndexARuntimeField.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","index-a-runtime-field"],["heading","Index a runtime field"],["body","\n"],["body","Runtime fields are defined by the context where they run. For example, you can define runtime fields in the context of a search query or within the runtime section of an index mapping."],["body","\n\n"],["body","如果您决定索引  运行时字段以获得更高的性能，只需将完整的运行时字段定义 (包括脚本) 移动到索引映射的上下文中。此功能意味着您只需编写一次脚本，并将其应用于支持运行时字段的任何上下文。"],["body","\n"],["body","索引运行时字段后，您无法更新包含的脚本。如果需要更改脚本，请使用更新的脚本创建一个新字段。"],["body","\n\n"],["body","For example, let’s say your company wants to replace some old pressure valves. The connected sensors are only capable of reporting a fraction of the true readings. Rather than outfit the pressure valves with new sensors, you decide to calculate the values based on reported readings. Based on the reported data, you define the following fields in your mapping for my-index-000001:"],["body","\n"],["body","PUT my-index-000001/\n{\n  \"mappings\": {\n    \"properties\": {\n      \"timestamp\": {\n        \"type\": \"date\"\n      },\n      \"temperature\": {\n        \"type\": \"long\"\n      },\n      \"voltage\": {\n        \"type\": \"double\"\n      },\n      \"node\": {\n        \"type\": \"keyword\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","You then bulk index some sample data from your sensors. This data includes voltage readings for each sensor:"],["body","\n"],["body","POST my-index-000001/_bulk?refresh=true\n{\"index\":{}}\n{\"timestamp\": 1516729294000, \"temperature\": 200, \"voltage\": 5.2, \"node\": \"a\"}\n{\"index\":{}}\n{\"timestamp\": 1516642894000, \"temperature\": 201, \"voltage\": 5.8, \"node\": \"b\"}\n{\"index\":{}}\n{\"timestamp\": 1516556494000, \"temperature\": 202, \"voltage\": 5.1, \"node\": \"a\"}\n{\"index\":{}}\n{\"timestamp\": 1516470094000, \"temperature\": 198, \"voltage\": 5.6, \"node\": \"b\"}\n{\"index\":{}}\n{\"timestamp\": 1516383694000, \"temperature\": 200, \"voltage\": 4.2, \"node\": \"c\"}\n{\"index\":{}}\n{\"timestamp\": 1516297294000, \"temperature\": 202, \"voltage\": 4.0, \"node\": \"c\"}\n"],["body","\n"],["body","After talking to a few site engineers, you realize that the sensors should be reporting at least double the current values, but potentially higher. You create a runtime field named voltage_corrected that retrieves the current voltage and multiplies it by 2:"],["body","\n"],["body","PUT my-index-000001/_mapping\n{\n  \"runtime\": {\n    \"voltage_corrected\": {\n      \"type\": \"double\",\n      \"script\": {\n        \"source\": \"\"\"\n        emit(doc['voltage'].value * params['multiplier'])\n        \"\"\",\n        \"params\": {\n          \"multiplier\": 2\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","You retrieve the calculated values using the fields parameter on the _search API:"],["body","\n"],["body","GET my-index-000001/_search\n{\n  \"fields\": [\n    \"voltage_corrected\",\n    \"node\"\n  ],\n  \"size\": 2\n}\n"],["body","\n"],["body","After reviewing the sensor data and running some tests, you determine that the multiplier for reported sensor data should be 4. To gain greater performance, you decide to index the voltage_corrected runtime field with the new multiplier parameter."],["body","\n"],["body","In a new index named my-index-000001, copy the voltage_corrected runtime field definition into the mappings of the new index. It’s that simple! You can add an optional parameter named on_script_error that determines whether to reject the entire document if the script throws an error at index time (default)."],["body","\n"],["body","PUT my-index-000001/\n{\n  \"mappings\": {\n    \"properties\": {\n      \"timestamp\": {\n        \"type\": \"date\"\n      },\n      \"temperature\": {\n        \"type\": \"long\"\n      },\n      \"voltage\": {\n        \"type\": \"double\"\n      },\n      \"node\": {\n        \"type\": \"keyword\"\n      },\n      \"voltage_corrected\": {\n        \"type\": \"double\",\n        \"on_script_error\": \"fail\", //Causes the entire document to be rejected if the script throws an error at index time. Setting the value to `ignore` will register the field in the document’s `_ignored` metadata field and continue indexing.\n        \"script\": {\n          \"source\": \"\"\"\n        emit(doc['voltage'].value * params['multiplier'])\n        \"\"\",\n          \"params\": {\n            \"multiplier\": 4\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","Bulk index some sample data from your sensors into the my-index-000001 index:"],["body","\n"],["body","POST my-index-000001/_bulk?refresh=true\n{ \"index\": {}}\n{ \"timestamp\": 1516729294000, \"temperature\": 200, \"voltage\": 5.2, \"node\": \"a\"}\n{ \"index\": {}}\n{ \"timestamp\": 1516642894000, \"temperature\": 201, \"voltage\": 5.8, \"node\": \"b\"}\n{ \"index\": {}}\n{ \"timestamp\": 1516556494000, \"temperature\": 202, \"voltage\": 5.1, \"node\": \"a\"}\n{ \"index\": {}}\n{ \"timestamp\": 1516470094000, \"temperature\": 198, \"voltage\": 5.6, \"node\": \"b\"}\n{ \"index\": {}}\n{ \"timestamp\": 1516383694000, \"temperature\": 200, \"voltage\": 4.2, \"node\": \"c\"}\n{ \"index\": {}}\n{ \"timestamp\": 1516297294000, \"temperature\": 202, \"voltage\": 4.0, \"node\": \"c\"}\n"],["body","\n"],["body","You can now retrieve calculated values in a search query, and find documents based on precise values. The following range query returns all documents where the calculated voltage_corrected is greater than or equal to 16, but less than or equal to 20. Again, use the fields parameter on the _search API to retrieve the fields you want:"],["body","\n"],["body","POST my-index-000001/_search\n{\n  \"query\": {\n    \"range\": {\n      \"voltage_corrected\": {\n        \"gte\": 16,\n        \"lte\": 20,\n        \"boost\": 1.0\n      }\n    }\n  },\n  \"fields\": [\n    \"voltage_corrected\", \"node\"]\n}\n"],["body","\n"],["body","The response includes the voltage_corrected field for the documents that match the range query, based on the calculated value of the included script:"],["body","\n"],["body","{\n  \"hits\" : {\n    \"total\" : {\n      \"value\" : 2,\n      \"relation\" : \"eq\"\n    },\n    \"max_score\" : 1.0,\n    \"hits\" : [\n      {\n        \"_index\" : \"my-index-000001\",\n        \"_type\" : \"_doc\",\n        \"_id\" : \"yoSLrHgBdg9xpPrUZz_P\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"timestamp\" : 1516383694000,\n          \"temperature\" : 200,\n          \"voltage\" : 4.2,\n          \"node\" : \"c\"\n        },\n        \"fields\" : {\n          \"voltage_corrected\" : [\n            16.8\n          ],\n          \"node\" : [\n            \"c\"\n          ]\n        }\n      },\n      {\n        \"_index\" : \"my-index-000001\",\n        \"_type\" : \"_doc\",\n        \"_id\" : \"y4SLrHgBdg9xpPrUZz_P\",\n        \"_score\" : 1.0,\n        \"_source\" : {\n          \"timestamp\" : 1516297294000,\n          \"temperature\" : 202,\n          \"voltage\" : 4.0,\n          \"node\" : \"c\"\n        },\n        \"fields\" : {\n          \"voltage_corrected\" : [\n            16.0\n          ],\n          \"node\" : [\n            \"c\"\n          ]\n        }\n      }\n    ]\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/3.RuntimeFields/1.MapARuntimeField.html"],["title","MapARuntimeField.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","map-a-runtime-field"],["heading","Map a runtime field"],["body","\n\n"],["body","在 runtime 区块中 定义  a Painless script.  来添加一个 runtime fields"],["body","\n"],["body","该脚本可以访问文档的整个上下文，包括：_source 任何映射的字段加上它们的值。"],["body","\n"],["body","在查询时，脚本运行并为查询所需的每个脚本字段生成值。"],["body","\n\n"],["body","Emitting runtime field values"],["body","\n\n"],["body","必须使用  emit method to emit calculated values."],["body","\n"],["body","实例"],["body","\n\n"],["body","PUT my-index-000001/\n{\n  \"mappings\": {\n    \"runtime\": {\n      \"day_of_week\": {\n        \"type\": \"keyword\",\n        \"script\": {\n          \"source\": \"emit(doc['@timestamp'].value.dayOfWeekEnum.getDisplayName(TextStyle.FULL, Locale.ROOT))\"\n        }\n      }\n    },\n    \"properties\": {\n      \"@timestamp\": {\"type\": \"date\"}\n    }\n  }\n}\n"],["body","\n"],["body","The runtime section can be any of these data types:"],["body","\n\n"],["body","boolean"],["body","\n"],["body","date"],["body","\n"],["body","double"],["body","\n"],["body","geo_point"],["body","\n"],["body","ip"],["body","\n"],["body","keyword"],["body","\n"],["body","long"],["body","\n\n\n"],["body","\n"],["body","Runtime fields with a type of date can accept the format parameter exactly as the date field type."],["body","\n"],["body","\n"],["body","\n"],["body","If dynamic field mapping is enabled where the dynamic parameter is set to runtime, new fields are automatically added to the index mapping as runtime fields:"],["body","\n"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"dynamic\": \"runtime\",\n    \"properties\": {\n      \"@timestamp\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","define-runtime-fields-without-a-script"],["heading","Define runtime fields without a script"],["body","\n"],["body","可以不使用脚本。直接从  _source 中取同名字段"],["body","\n"],["body","PUT my-index-000001/\n{\n  \"mappings\": {\n    \"runtime\": {\n      \"day_of_week\": {\n        \"type\": \"keyword\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","尽可能从  doc_values 取 fields values，因为这比 _source 快"],["body","\n"],["headingLink","updating-and-removing-runtime-fields"],["heading","Updating and removing runtime fields"],["body","\n\n"],["body","\n"],["body","要替换现有的运行时字段，请将新的运行时字段添加到具有相同名称的映射中。"],["body","\n"],["body","\n"],["body","\n"],["body","要从映射中删除运行时字段，请将运行时字段的值设置为null:"],["body","\n"],["body","\n\n"],["body","PUT my-index-000001/_mapping\n{\n \"runtime\": {\n   \"day_of_week\": null\n }\n}\n"],["body","\n"],["body","Downstream impacts"],["body","\n\n"],["body","\n"],["body","在依赖查询运行时更新或删除运行时字段可能会返回不一致的结果"],["body","\n"],["body","\n"],["body","\n"],["body","每个分片可能有权访问脚本的不同版本，具体取决于映射更改何时生效。"],["body","\n"],["body","\n"],["body","\n"],["body","Existing queries or visualizations in Kibana that rely on runtime fields can fail if you remove or update the field. For example, a bar chart visualization that uses a runtime field of type ip will fail if the type is changed to boolean, or if the runtime field is removed."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/Mapping.mm.html"],["title","Mapping.mm.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["body","\n"],["body","markmap:\ncolorFreezeLevel: 2\ninitialExpandLevel: 2"],["body","\n"],["headingLink","maxwidth-300"],["heading","maxWidth: 300"],["body","\n"],["body","\n"],["headingLink","索引映射"],["heading","索引映射"],["body","\n"],["headingLink","mapping数据类型映射"],["heading","(Mapping)数据类型映射"],["body","\n"],["headingLink","动态类型映射"],["heading","动态类型映射"],["body","\n\n"],["body","Dynamic field mapping\n自动根据字段的数据类型映射"],["body","\n"],["body","Dynamic templates\n根据模板设置的规则来映射数据类型"],["body","\n\n"],["headingLink","explicit-mapping显示类型映射"],["heading","Explicit mapping（显示类型映射）"],["body","\n"],["body","  \"mappings\": {\n    \"properties\": {\n      \"age\":    { \"type\": \"integer\" },  \n      \"email\":  { \"type\": \"keyword\"  }, \n      \"name\":   { \"type\": \"text\"  }     \n    }\n  }\n"],["body","\n"],["headingLink","runtimefields运行时映射字段"],["heading","RuntimeFields(运行时映射字段)"],["body","\n"],["headingLink","mapping中定义字段"],["heading","Mapping中定义字段"],["body","\n"],["body","PUT my-index-000001/\n{\n  \"mappings\": {\n    \"runtime\": {\n      \"day_of_week\": {\n        \"type\": \"keyword\",\n        \"script\": {\n          \"source\": \"emit(doc['@timestamp'].value.dayOfWeekEnum.getDisplayName(TextStyle.FULL, Locale.ROOT))\"\n        }\n      }\n    },\n    \"properties\": {\n      \"@timestamp\": {\"type\": \"date\"}\n    }\n  }\n}\n"],["body","\n"],["headingLink","search中定义runtime-字段"],["heading","Search中定义Runtime 字段"],["body","\n"],["body","GET my-index-000001/_search\n{\n  \"runtime_mappings\": {\n    \"day_of_week\": {\n      \"type\": \"keyword\",\n      \"script\": {\n        \"source\": \"emit(doc['@timestamp'].value.dayOfWeekEnum.getDisplayName(TextStyle.FULL, Locale.ROOT))\"\n      }\n    }\n  },\n  \"aggs\": {\n    \"day_of_week\": {\n      \"terms\": {\n        \"field\": \"day_of_week\"\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","将runtime字段-索引"],["heading","将runtime字段 索引"],["body","\n"],["body","PUT my-index-000001/\n{\n  \"mappings\": {\n    \"properties\": {\n      \"timestamp\": {\n        \"type\": \"date\"\n      },\n      \"temperature\": {\n        \"type\": \"long\"\n      },\n      \"voltage\": {\n        \"type\": \"double\"\n      },\n      \"node\": {\n        \"type\": \"keyword\"\n      },\n      \"voltage_corrected\": {\n        \"type\": \"double\",\n        \"on_script_error\": \"fail\", //Causes the entire document to be rejected if the script throws an error at index time. Setting the value to `ignore` will register the field in the document’s `_ignored` metadata field and continue indexing.\n        \"script\": {\n          \"source\": \"\"\"\n        emit(doc['voltage'].value * params['multiplier'])\n        \"\"\",\n          \"params\": {\n            \"multiplier\": 4\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","解构数据"],["heading","解构数据"],["body","\n"],["headingLink","grok"],["heading","GROK"],["body","\n"],["body","PUT my-index-000001/_mappings\n{\n  \"runtime\": {\n    \"http.clientip\": {\n      \"type\": \"ip\",\n      \"script\": \"\"\"\n        String clientip=grok('%{COMMONAPACHELOG}').extract(doc[\"message\"].value)?.clientip;\n        if (clientip != null) emit(clientip);  //This condition ensures that the script doesn’t crash even if the pattern of the message doesn’t match.\n      \"\"\"\n    }\n  }\n}\n"],["body","\n"],["headingLink","dissect"],["heading","Dissect"],["body","\n"],["body","PUT my-index-000001/_mappings\n{\n  \"runtime\": {\n    \"http.client.ip\": {\n      \"type\": \"ip\",\n      \"script\": \"\"\"\n        String clientip=dissect('%{clientip} %{ident} %{auth} [%{@timestamp}] \"%{verb} %{request} HTTP/%{httpversion}\" %{status} %{size}').extract(doc[\"message\"].value)?.clientip;\n        if (clientip != null) emit(clientip);\n      \"\"\"\n    }\n  }\n}\n"],["body","\n"],["headingLink","data-type字段数据类型"],["heading","Data Type(字段数据类型)"],["body","\n"],["headingLink","common-types常见数据类型"],["heading","Common types（常见数据类型）"],["body","\n\n"],["body","Binary\nBase64编码的二进制数据"],["body","\n"],["body","Boolean\ntrue、false"],["body","\n"],["body","Keyword\n关键字数据类型、keyword、constant_keyword、wild_keyword"],["body","\n"],["body","Numbers\n数值类型、long、double"],["body","\n"],["body","Dates\n日期类型、date、date_nanos"],["body","\n"],["body","alias\n定义一个已经存在的字段别名"],["body","\n\n"],["headingLink","objects-and-relational-types对象和关联类型"],["heading","Objects and relational types（对象和关联类型)"],["body","\n\n"],["body","Object\n对象类型"],["body","\n"],["body","flattened\n扁平化类型"],["body","\n"],["body","netsed\n嵌套类型"],["body","\n"],["body","join\n父子类型"],["body","\n\n"],["headingLink","structured-data-types结构化数据类型"],["heading","Structured Data Types(结构化数据类型)"],["body","\n\n"],["body","Range\nRange Types, such as long_range, double_range, date_range, and ip_range."],["body","\n"],["body","IP\nIPv4 and IPv6 addresses."],["body","\n"],["body","Version\n软件版本，SemanticVesioning"],["body","\n"],["body","murmur3\n计算value的Hash值"],["body","\n\n"],["headingLink","aggregate-data-types适用于聚合的数据类型"],["heading","Aggregate data types(适用于聚合的数据类型)"],["body","\n\n"],["body","aggregate_metric_double\n预聚合的数据"],["body","\n"],["body","histogram\n直方图预聚合"],["body","\n\n"],["headingLink","text-search-types-为全文检索优化的数据类型"],["heading","Text search types (为全文检索优化的数据类型)"],["body","\n\n"],["body","\n"],["body","Text"],["body","\n非结构化文本"],["body","\n"],["body","\n"],["body","\n"],["body","annotated-text\n带有特殊标记的Text"],["body","\n"],["body","\n"],["body","\n"],["body","completion\n自动完成的数据结构"],["body","\n"],["body","\n"],["body","\n"],["body","search_as_you_type\n输入即搜索"],["body","\n"],["body","\n\n"],["headingLink","document-ranking-types适用于文档排序的数据类型"],["heading","Document ranking types(适用于文档排序的数据类型)"],["body","\n\n"],["body","\n"],["body","dense_vector\n稠密向量字段主要用于k-近邻（kNN）搜索"],["body","\n"],["body","\n"],["body","\n"],["body","sparse_vector\n存储 图像或文本等稀疏数据"],["body","\n"],["body","\n"],["body","\n"],["body","rank_feature 、rank_features\n在查询中提升文档相关性"],["body","\n"],["body","\n\n"],["headingLink","spatial-data-types专有数据类型"],["heading","Spatial data types(专有数据类型)"],["body","\n\n"],["body","\n"],["body","GeoPoint\n经纬度"],["body","\n"],["body","\n"],["body","\n"],["body","GeoShape\n多边形"],["body","\n"],["body","\n"],["body","\n"],["body","Point\n任意平面坐标点"],["body","\n"],["body","\n"],["body","\n"],["body","Shape\n任意平面形状"],["body","\n"],["body","\n\n"],["headingLink","multi-fields多字段"],["heading","Multi-fields(多字段)"],["body","\n\n"],["body","为了不同的目的，以不同的方式对同一个字段进行索引"],["body","\n\n"],["headingLink","arrays"],["heading","Arrays"],["body","\n\n"],["body","默认情况下，任何字段都可以包含零个或多个值"],["body","\n\n"],["headingLink","metadata-fields元数据字段"],["heading","Metadata Fields(元数据字段)"],["body","\n\n"],["body","\n"],["body","Identity metadata fields\n身份元数据字段"],["body","\n\n"],["body","_index: 文档索引"],["body","\n"],["body","_type： 文档Type。已过期"],["body","\n"],["body","_id：文档ID"],["body","\n\n"],["body","\n"],["body","\n"],["body","Indexing metadata fields\n索引源数据"],["body","\n\n"],["body","field_names: 文档包含所有非空的字段名。"],["body","\n"],["body","_ignored： 由于 ignore_malformed 导致的被忽略的字段"],["body","\n\n"],["body","\n"],["body","\n"],["body","Document source metadata fields\n文档源元数据"],["body","\n\n"],["body","_source ： 文档的源JSON"],["body","\n"],["body","_size： 文档JSON的比特大小"],["body","\n\n"],["body","\n"],["body","\n"],["body","Doc count metadata field\n文档数量源数据，用于聚合"],["body","\n\n"],["body","_doc_count： 聚合桶中的文档数"],["body","\n\n"],["body","\n"],["body","\n"],["body","Routing MetaData Field\n分片路由元字段"],["body","\n\n"],["body","_routing： 自定义分片路由值"],["body","\n\n"],["body","\n"],["body","\n"],["body","Other Metadata Field"],["body","\n\n"],["body","_meta： 应用程序指定的元数据"],["body","\n"],["body","_tier： 当前文档的 data tier preference"],["body","\n\n"],["body","\n\n"],["headingLink","mapping-parameters索引映射参数"],["heading","(Mapping Parameters)索引映射参数"],["body","\n"],["headingLink","字段索引相关"],["heading","字段索引相关"],["body","\n\n"],["body","store\n默认情况下，只索引不存储。可以通过\"stored_fields\": [\"title\"]来取"],["body","\n"],["body","index\n字段是否被索引"],["body","\n"],["body","index_options\n控制将哪些信息添加到倒排索引中，以进行搜索和高亮显示"],["body","\n"],["body","index_prefixes\n启用术语前缀的索引，以加快前缀搜索"],["body","\n"],["body","index_phrases\n将双terms的组合索引到新的字段"],["body","\n"],["body","copy\nCopy其他字段合成一个新字段\n"],["body","{\n  \"mappings\": {\n    \"properties\": {\n      \"first_name\": {\n        \"type\": \"text\",\n        \"copy_to\": \"full_name\" \n      },\n      \"last_name\": {\n        \"type\": \"text\",\n        \"copy_to\": \"full_name\" \n      },\n      \"full_name\": { //The values of the `first_name` and `last_name` fields are copied to the `full_name` field.\n        \"type\": \"text\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","\n"],["body","fields\n对同一个源字段应用不同的索引方式"],["body","\n"],["body","null_value\n索引空值，便于检索，不会修改源数据"],["body","\n"],["body","doc_values\n是否保存doc_values，加快聚合"],["body","\n"],["body","meta\n字段相关的元数据"],["body","\n"],["body","enabled\n如果禁用，则字段不可检索。只能从_source中返回"],["body","\n"],["body","dynamic\n控制新字段添加到映射中的方式"],["body","\n"],["body","properties\n字段映射的定义"],["body","\n\n"],["headingLink","数据类型相关"],["heading","数据类型相关"],["body","\n\n"],["body","coerce\n强制数据类型转换。默认True"],["body","\n"],["body","format\n日期类型的格式化"],["body","\n"],["body","ignore_above\n如果字符串超长，则会被截断"],["body","\n"],["body","ignore_malformed\n是否忽略类型不合法的数据"],["body","\n\n"],["headingLink","文本分析相关"],["heading","文本分析相关"],["body","\n\n"],["body","\n"],["body","analyzer\n指定字段 查询时、索引时使用的分析器"],["body","\n"],["body","\n"],["body","\n"],["body","boost\n索引时权重增强。已废弃"],["body","\n"],["body","\n"],["body","\n"],["body","normalizer\n指定文本字段的归一化器，用于规整词项"],["body","\n"],["body","\n"],["body","\n"],["body","norms\n预存储评分信息"],["body","\n"],["body","\n"],["body","\n"],["body","position_increment_gap\n多值之间的Gap、默认为100，用于match_phrase的Slop匹配"],["body","\n"],["body","\n"],["body","\n"],["body","search_analyzer\n查询时使用的分析器"],["body","\n"],["body","\n"],["body","\n"],["body","term_vector\n词向量保存的粒度"],["body","\n"],["body","\n"],["body","\n"],["body","similarity\n相似度算法"],["body","\n"],["body","\n"],["body","\n"],["body","eager_global_ordinals\n是否立即更新全局序数"],["body","\n"],["body","\n\n"],["headingLink","mapping-limtsettings映射的限制设置"],["heading","Mapping LimtSettings(映射的限制设置)"],["body","\n\n"],["body","index.mapping.total_fields.limit\n索引的总字段数限制，默认1000"],["body","\n"],["body","index.mapping.depth.limit\n索引字段嵌套的深度，默认20"],["body","\n"],["body","index.mapping.nested_fields.limit\n每个索引的  限制 nested字段的个数，默认50"],["body","\n"],["body","index.mapping.nested_objects.limit\n每个索引 包含的 嵌套对象的 个数。默认10000"],["body","\n"],["body","index.mapping.field_name_length.limit\n字段名的长度限制。默认 Long.MAX_VALUE"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/5.MetadataFields/8.Source.html"],["title","Source.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","_source-field"],["heading","_source field"],["body","\n\n"],["body","“_ Source” 字段包含在索引时传递的原始JSON文档正文。'_Source' 字段本身没有索引 (因此不可搜索)，"],["body","\n"],["body","但是它被存储为可以在执行 * fetch * 请求时返回，例如 [get](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.13/docs-get.html) 或 search。"],["body","\n\n"],["headingLink","disabling-the-_source-field"],["heading","Disabling the _source field"],["body","\n"],["body","尽管非常方便，但源字段确实会在索引内产生存储开销。由于这个原因，它可以被禁用如下:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"_source\": {\n      \"enabled\": false\n    }\n  }\n}\n"],["body","\n"],["headingLink","think-before-disabling-the-_source-field"],["heading","Think before disabling the _source field"],["body","\n"],["body","用户经常在不考虑后果的情况下禁用 'source' 字段，then live to regret it.。如果 “ source” 字段不可用，则不支持许多功能:"],["body","\n\n"],["body","The update, update_by_query, and reindex APIs."],["body","\n"],["body","On the fly highlighting."],["body","\n"],["body","The ability to reindex from one Elasticsearch index to another, either to change mappings or analysis, or to upgrade an index to a new major version."],["body","\n"],["body","通过查看索引时使用的原始文档来调试查询或聚合的能力。"],["body","\n"],["body","在未来，可能会自动修复索引损坏的能力。"],["body","\n\n"],["body","如果需要考虑磁盘空间，请增加 [压缩级别](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules.html # index-codec)，而不是禁用 “_ source”。"],["body","\n"],["headingLink","including--excluding-fields-from-_source"],["heading","Including / Excluding fields from _source"],["body","\n\n"],["body","\n"],["body","只有专家功能的功能是在文档被索引之后但在存储 “_ source” 字段之前修剪 “_ source” 字段的内容。\n从 “_ source” 中删除字段与禁用 “_ source” 具有类似的缺点，尤其是您无法将文档从一个Elasticsearch索引重新索引到另一个。"],["body","\n"],["body","\n"],["body","\n"],["body","Consider using source filtering instead."],["body","\n"],["body","\n\n"],["body","'includes'/'excludes' 参数 (也接受通配符) 可以如下使用:"],["body","\n"],["body","PUT logs\n{\n  \"mappings\": {\n    \"_source\": {\n      \"includes\": [\n        \"*.count\",\n        \"meta.*\"\n      ],\n      \"excludes\": [\n        \"meta.description\",\n        \"meta.other.*\"\n      ]\n    }\n  }\n}\n\nPUT logs/_doc/1\n{\n  \"requests\": {\n    \"count\": 10,\n    \"foo\": \"bar\" \n  },\n  \"meta\": {\n    \"name\": \"Some metric\",\n    \"description\": \"Some metric description\", \n    \"other\": {\n      \"foo\": \"one\", \n      \"baz\": \"two\" \n    }\n  }\n}\n\nGET logs/_search\n{\n  \"query\": {\n    \"match\": {\n      \"meta.other.foo\": \"one\" \n    }\n  }\n}\n"],["body","\n"],["body","这些字段将从存储的 “_ source” 字段中删除。我们仍然可以在这个字段上搜索，即使它不在存储的 '_source' 中。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/5.MetadataFields/README.html"],["title","MetadataFields - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","metadata-fields"],["heading","Metadata fields"],["body","\n\n"],["body","每个文档都有关联的元数据。例如 _index、mapping type、_id"],["body","\n"],["body","创建 mapping tpye时 可以自定义元数据"],["body","\n\n"],["headingLink","identity-metadata-fields"],["heading","Identity metadata fields"],["body","\n"],["body","\n"],["body","_index"],["body","The index to which the document belongs."],["body","\n"],["body","_type"],["body","The document’s mapping type."],["body","\n"],["body","_id"],["body","The document’s ID."],["body","\n\n\n"],["headingLink","document-source-metadata-fields"],["heading","Document source metadata fields"],["body","\n\n"],["body","\n"],["body","_source"],["body","\n"],["body","The original JSON representing the body of the document."],["body","\n"],["body","\n"],["body","\n"],["body","_size"],["body","\n"],["body","The size of the _source field in bytes, provided by the mapper-size plugin."],["body","\n"],["body","\n\n"],["headingLink","doc-count-metadata-field"],["heading","Doc count metadata field"],["body","\n\n"],["body","\n"],["body","_doc_count"],["body","\n"],["body","A custom field used for storing doc counts when a document represents pre-aggregated data."],["body","\n"],["body","\n\n"],["headingLink","indexing-metadata-fields"],["heading","Indexing metadata fields"],["body","\n\n"],["body","\n"],["body","_field_names"],["body","\n"],["body","All fields in the document which contain non-null values."],["body","\n"],["body","\n"],["body","\n"],["body","_ignored"],["body","\n"],["body","All fields in the document that have been ignored at index time because of ignore_malformed."],["body","\n"],["body","\n\n"],["headingLink","routing-metadata-field"],["heading","Routing metadata field"],["body","\n\n"],["body","\n"],["body","_routing"],["body","\n"],["body","A custom routing value which routes a document to a particular shard."],["body","\n"],["body","\n\n"],["headingLink","other-metadata-field"],["heading","Other metadata field"],["body","\n\n"],["body","\n"],["body","_meta"],["body","\n"],["body","Application specific metadata."],["body","\n"],["body","\n"],["body","\n"],["body","_tier"],["body","\n"],["body","The current data tier preference of the   to which the document belongs."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/5.MetadataFields/7.Routing.html"],["title","Routing.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","_routing-field"],["heading","_routing field"],["body","\n"],["body","使用以下公式将文档路由到索引中的特定分片:"],["body","\n"],["body","shard_num = hash(_routing) % num_primary_shards\n"],["body","\n\n"],["body","The default value used for _routing is the document’s _id."],["body","\n"],["body","可以通过为每个文档指定自定义的 “路由” 值来实现自定义路由模式。例如:"],["body","\n\n"],["body","PUT my-index-000001/_doc/1?routing=user1&refresh=true \n{\n  \"title\": \"This is a document\"\n}\n//This document uses `user1` as its routing value, instead of its ID.\nGET my-index-000001/_doc/1?routing=user1 \n"],["body","\n"],["body","The same routing value needs to be provided when getting, deleting, or updating the document."],["body","\n"],["body","\nGET my-index-000001/_search\n{\n  \"query\": {\n    \"terms\": {\n      \"_routing\": [ \"user1\" ] \n    }\n  }\n}\n"],["body","\n"],["body","数据流不支持自定义路由。相反，以流的适当支持索引为目标。"],["body","\n"],["headingLink","searching-with-custom-routing"],["heading","Searching with custom routing"],["body","\n\n"],["body","自定义路由可以减少搜索的影响。不必将搜索请求分裂到索引中的所有分片，而是可以将请求发送到与特定路由值 (或多个值) 匹配的分片:"],["body","\n\n"],["body","This search request will only be executed on the shards associated with the user1 and user2 routing values."],["body","\n"],["body","GET my-index-000001/_search?routing=user1,user2 \n{\n  \"query\": {\n    \"match\": {\n      \"title\": \"document\"\n    }\n  }\n}\n"],["body","\n"],["headingLink","making-a-routing-value-required"],["heading","Making a routing value required"],["body","\n\n"],["body","使用自定义路由时，重要的是在 [索引](https:// www.elastic.co/guide/en/elasticsearch/reference/7.13/docs-index _.html)，[获取](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.13/docs-get.html)，删除，或 [更新](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.13/docs-update.html) 文档。"],["body","\n"],["body","忘记路由值可能会导致文档在多个分片上建立索引。作为保护措施，可以将 “_ routing” 字段配置为使所有CRUD操作所需的自定义 “routing” 值:"],["body","\n\n"],["body","PUT my-index-000002\n{\n  \"mappings\": {\n    \"_routing\": {\n      \"required\": true \n    }\n  }\n}\n\nPUT my-index-000002/_doc/1 \n{\n  \"text\": \"No routing value provided\"\n}\n"],["body","\n"],["body","This index request throws a routing_missing_exception."],["body","\n"],["headingLink","unique-ids-with-custom-routing"],["heading","Unique IDs with custom routing"],["body","\n\n"],["body","\n"],["body","当索引指定自定义 “_ routing” 的文档时  不能保证在索引中的所有分片中“_ id” 的唯一性、"],["body","\n"],["body","\n"],["body","\n"],["body","实际上，如果使用不同的 “_ 路由” 值进行索引，则具有相同 “_ id” 的文档可能会以不同的分片结束。"],["body","\n"],["body","\n"],["body","\n"],["body","由用户来确保id在整个索引中是唯一的。"],["body","\n"],["body","\n\n"],["headingLink","routing-to-an-index-partition"],["heading","Routing to an index partition"],["body","\n\n"],["body","\n"],["body","可以配置索引，以使自定义路由值将转到分片的子集，而不是单个分片。这有助于降低最终导致不平衡的集群，同时仍然减少搜索的影响。"],["body","\n"],["body","\n"],["body","\n"],["body","这是通过在索引创建时提供索引级别设置 ['index.routing_partition_size '](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules.html # routing-partition-size) 来完成的。随着分区大小的增加，数据分布越均匀，作为代价，每个请求检索的分片就越多。"],["body","\n"],["body","\n"],["body","\n"],["body","当存在此设置时，用于计算分片的公式变为:"],["body","\n"],["body","\n\n"],["body","shard_num = (hash(_routing) + hash(_id) % routing_partition_size) % num_primary_shards\n"],["body","\n\n"],["body","也就是说，'_routing' 字段用于计算索引内的一组分片，然后 '_id' 用于选择该集合内的一个分片。"],["body","\n"],["body","要启用此功能，'index.routing_partition_size '的值应大于1且小于 'index.number_of_shards'。"],["body","\n\n"],["body","启用后，分区索引将具有以下限制:"],["body","\n\n"],["body","无法在其中创建具有 'join' 字段 关系的映射。"],["body","\n"],["body","索引中的所有映射都必须具有标记为 “_ routing” 字段。"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/5.MetadataFields/1.DocCount.html"],["title","DocCount.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","_doc_count-field"],["heading","_doc_count field"],["body","\n"],["body","\n"],["body","手动指定文档数"],["body","\n"],["body","\n\n"],["body","\n"],["body","存储桶聚合总是返回一个名为 “doc_count” 的字段，该字段显示在每个存储桶中聚合和分区的文档数。"],["body","\n"],["body","\n"],["body","\n"],["body","“doc_count” 的值的计算非常简单。对于每个存储桶中收集的每个文档，“doc_count” 增加1。"],["body","\n"],["body","\n"],["body","\n"],["body","虽然这种简单的方法在计算单个文档的聚合时是有效的，它无法准确表示存储预聚合数据的文档 (例如 'histogram '或 'aggregate_metric_double' 字段)，因为一个 summary 字段可能表示多个文档。"],["body","\n"],["body","\n"],["body","\n"],["body","为了在处理预聚合数据时允许正确计算文档数量，我们引入了一个名为 “_ doc_count” 的元数据字段类型。“_ doc_count” 必须始终是一个正整数，表示在单个 summary 字段中聚合的文档数。"],["body","\n"],["body","\n"],["body","\n"],["body","当将字段 “_ doc_count” 添加到文档中时，所有存储桶聚合都将尊重其值，并将存储桶 “doc_count” 增加该字段的值。如果文档不包含任何 “_ doc_count” 字段，则默认情况下“_ doc_count = 1”。"],["body","\n\n"],["body","\n"],["body","“_ doc_count” 字段每个文档只能存储一个正整数。不允许嵌套数组。"],["body","\n"],["body","\n"],["body","\n"],["body","如果文档不包含 “_ doc_count” 字段，则聚合器将增加1，这是默认行为。"],["body","\n"],["body","\n\n"],["body","\n\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","The following create index API request creates a new index with the following field mappings:"],["body","\n\n"],["body","“my_histogram”，用于存储百分位数据的 “histogram” 字段"],["body","\n"],["body","my_text, a keyword field used to store a title for the histogram"],["body","\n\n"],["body","PUT my_index\n{\n  \"mappings\" : {\n    \"properties\" : {\n      \"my_histogram\" : {\n        \"type\" : \"histogram\"\n      },\n      \"my_text\" : {\n        \"type\" : \"keyword\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","The following index API requests store pre-aggregated data for two histograms: histogram_1 and histogram_2."],["body","\n"],["body","下面的 [索引](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/docs-index _.html) API请求存储两个直方图的预聚合数据"],["body","\n\n"],["body","直方图 _ 1"],["body","\n"],["body","直方图 _ 2"],["body","\n\n"],["body","PUT my_index/_doc/1\n{\n  \"my_text\" : \"histogram_1\",\n  \"my_histogram\" : {\n      \"values\" : [0.1, 0.2, 0.3, 0.4, 0.5],\n      \"counts\" : [3, 7, 23, 12, 6]\n   },\n  \"_doc_count\": 45 \n}\n\nPUT my_index/_doc/2\n{\n  \"my_text\" : \"histogram_2\",\n  \"my_histogram\" : {\n      \"values\" : [0.1, 0.25, 0.35, 0.4, 0.45, 0.5],\n      \"counts\" : [8, 17, 8, 7, 6, 2]\n   },\n  \"_doc_count\": 62 \n}\n"],["body","\n"],["body","字段 “_ doc_count” 必须是一个正整数，用于存储聚合生成每个直方图的文档数。"],["body","\n"],["body","If we run the following terms aggregation on my_index:"],["body","\n"],["body","GET /_search\n{\n    \"aggs\" : {\n        \"histogram_titles\" : {\n            \"terms\" : { \"field\" : \"my_text\" }\n        }\n    }\n}\n"],["body","\n"],["body","{\n    ...\n    \"aggregations\" : {\n        \"histogram_titles\" : {\n            \"doc_count_error_upper_bound\": 0,\n            \"sum_other_doc_count\": 0,\n            \"buckets\" : [\n                {\n                    \"key\" : \"histogram_2\",\n                    \"doc_count\" : 62\n                },\n                {\n                    \"key\" : \"histogram_1\",\n                    \"doc_count\" : 45\n                }\n            ]\n        }\n    }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/5.MetadataFields/4.Id.html"],["title","Id.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","_id-field"],["heading","_id field"],["body","\n\n"],["body","每个文档都有一个唯一标识它的 “id”，它被索引，以便可以使用 GETAPI 或 [IDS](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.13/query-dsl-ids-query.html)。"],["body","\n"],["body","可以在索引时分配 “ id”，也可以由Elasticsearch生成唯一的“ _ id”。"],["body","\n"],["body","该字段在映射中不可配置"],["body","\n\n"],["body","'_Id' 字段的值可在诸如 'term' 、 'terms' 、 'match' 和 'query_string '之类的查询中访问。"],["body","\n"],["body","# Example documents\nPUT my-index-000001/_doc/1\n{\n  \"text\": \"Document with ID 1\"\n}\n\nPUT my-index-000001/_doc/2?refresh=true\n{\n  \"text\": \"Document with ID 2\"\n}\n\nGET my-index-000001/_search\n{\n  \"query\": {\n    \"terms\": {\n      \"_id\": [ \"1\", \"2\" ] \n    }\n  }\n}\n"],["body","\n\n"],["body","\n"],["body","“_ Id” 字段被限制在聚合、排序和脚本中使用。如果需要对 “_ id” 字段进行排序或聚合，建议将 “_ id” 字段的内容复制到启用了 “doc_values” 的另一个字段中。"],["body","\n"],["body","\n"],["body","\n"],["body","_id is limited to 512 bytes in size and larger values will be rejected."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/5.MetadataFields/2.FiledNames.html"],["title","FiledNames.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","_field_names-field"],["heading","_field_names field"],["body","\n\n"],["body","“_ field_names” 字段用于索引文档中包含除 “null” 以外的任何值的每个字段的名称。"],["body","\n"],["body","'exists' 查询使用此字段来查找具有或不具有特定字段的任何非 “null” 值的文档。"],["body","\n"],["body","现在，“_ field_names” 字段 只索引了字段名称。禁用了 doc_values 和 norms"],["body","\n"],["body","对于启用了 “doc_values” 或 “norm” 的字段，“exists” 查询将仍然可用，但不会使用 “_ field_names” 字段。"],["body","\n\n"],["headingLink","disabling-_field_names"],["heading","Disabling _field_names"],["body","\n\n"],["body","禁用 “_ field_names” 已被弃用，并将在以后的主要版本中删除。"],["body","\n"],["body","通常不需要禁用 “_ field_names”，因为它不再承载曾经的索引开销。"],["body","\n"],["body","如果您有很多字段禁用了 “doc_values” 和 “norms'，并且您不需要使用这些字段执行“ exists ”查询，则可能希望通过将以下内容添加到映射中来禁用“ _ field_names ”:"],["body","\n\n"],["body","PUT tweets\n{\n  \"mappings\": {\n    \"_field_names\": {\n      \"enabled\": false\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/5.MetadataFields/6.Meta.html"],["title","Meta.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","_meta-field"],["heading","_meta field"],["body","\n"],["body","映射类型可以具有与之关联的自定义元数据。这些根本不被Elasticsearch使用，但可以用于存储特定于应用程序的元数据，例如文档所属的类:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"_meta\": { \n      \"class\": \"MyApp::User\",\n      \"version\": {\n        \"min\": \"1.0\",\n        \"max\": \"1.3\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","This _meta info can be retrieved with the GET mapping API."],["body","\n"],["body","The _meta field can be updated on an existing type using the update mapping API:"],["body","\n"],["body","PUT my-index-000001/_mapping\n{\n  \"_meta\": {\n    \"class\": \"MyApp2::User3\",\n    \"version\": {\n      \"min\": \"1.3\",\n      \"max\": \"1.5\"\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/5.MetadataFields/9.Tier.html"],["title","Tier.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","_tier-field"],["heading","_tier field"],["body","\n"],["body","当跨多个索引执行查询时，有时希望目标是在给定数据层 (“data_hot” 、 “data_warm” 、 “data_cold” 或 “data_frozen”) 的节点上保留的索引。“_ Tier” 字段允许匹配文档索引的 “tier_preference” 设置。在某些查询中可以访问首选值:"],["body","\n"],["body","PUT index_1/_doc/1\n{\n  \"text\": \"Document in index 1\"\n}\n\nPUT index_2/_doc/2?refresh=true\n{\n  \"text\": \"Document in index 2\"\n}\n\nGET index_1,index_2/_search\n{\n  \"query\": {\n    \"terms\": {\n      \"_tier\": [\"data_hot\", \"data_warm\"] \n    }\n  }\n}\n"],["body","\n\n"],["body","通常，查询将使用 “terms” 查询来列出感兴趣的层，但是您可以在重写为 “terms” 查询的任何查询中使用 “_tier” 字段，例如 “match”，“query_string”，“terms” 或 “simple_query_string” 查询，以及 “前缀” 和 “通配符” 查询。但是，它不支持 'regexp' 和 'fuzzy' 查询。"],["body","\n"],["body","索引的 “tier_preference” 设置是按优先顺序的层名称的逗号分隔列表，即首先列出用于托管索引的首选层，然后是潜在的许多 fall-back options 。查询匹配仅考虑第一个首选项 (列表的第一个值)。"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/5.MetadataFields/5.Index.html"],["title","Index.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","_index-field"],["heading","_index field"],["body","\n\n"],["body","\n"],["body","在跨多个索引执行查询时，有时希望添加与仅某些索引的文档相关联的查询子句。"],["body","\n"],["body","\n"],["body","\n"],["body","“_ Index” 字段允许在索引中匹配文档。它的值可在某些查询和聚合中访问，并且在排序或脚本时:"],["body","\n"],["body","\n\n"],["body","PUT index_1/_doc/1   \n{\n  \"text\": \"Document in index 1\"\n}\n\nPUT index_2/_doc/2?refresh=true\n{\n  \"text\": \"Document in index 2\"\n}\n\nGET index_1,index_2/_search \n{ //Querying on the `_index` field\n  \"query\": {\n    \"terms\": {\n      \"_index\": [\"index_1\", \"index_2\"] \n    }\n  },\n  \"aggs\": {\n    \"indices\": {  \n      \"terms\": {\n        \"field\": \"_index\", \n        \"size\": 10\n      }\n    }\n  },\n  \"sort\": [\n    {  //Sorting on the `_index` field\n      \"_index\": { \n        \"order\": \"asc\"\n      }\n    }\n  ],\n  \"script_fields\": {\n    \"index_name\": {\n      \"script\": {\n        \"lang\": \"painless\",Accessing the `_index` field in scripts\n        \"source\": \"doc['_index']\" \n      }\n    }\n  }\n}\n"],["body","\n\n"],["body","'_Index' 字段是虚拟公开的-它没有作为真实字段添加到Lucene索引中"],["body","\n"],["body","这意味着您可以在 “term” 或“ terms ”查询 (或重写为“ terms ”查询的任何查询，例如“ match ”，“ query_string ”或“ simple_query_string ”查询) 中使用“ _ index” 字段，以及 “prefix” 和 “通配符” 查询。但是，它不支持 'regexp' 和 'fuzzy' 查询。"],["body","\n"],["body","除了具体的索引名称之外，对 “_ index” 字段的查询还接受索引别名。"],["body","\n"],["body","当指定远程索引名如 'cluster_1:index_3 '时，查询必须包含分隔符':'。例如，对 “cluster _ *:index_3” 的 “通配符” 查询将匹配远程索引中的文档。"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/3.Mapping/5.MetadataFields/3.Ignored.html"],["title","Ignored.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","_ignored-field"],["heading","_ignored field"],["body","\n\n"],["body","“_ ignored” 字段索引并存储文档中由于格式错误而被忽略的每个字段的名称，并且 “ignore_mal格式” 已打开。"],["body","\n"],["body","此字段可使用 “术语” 搜索，和 'exists' 查询，并作为搜索命中的一部分返回。"],["body","\n\n"],["body","例如，以下查询匹配具有一个或多个被忽略的字段的所有文档:"],["body","\n"],["body","GET _search\n{\n  \"query\": {\n    \"exists\": {\n      \"field\": \"_ignored\"\n    }\n  }\n}\n"],["body","\n"],["body","同样，下面的查询会找到在索引时忽略 “@ timestamp” 字段的所有文档:"],["body","\n"],["body","GET _search\n{\n  \"query\": {\n    \"term\": {\n      \"_ignored\": \"@timestamp\"\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/慢日志.html"],["title","慢日志.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["body","PUT /_all/_settings\n{\n    \"index.search.slowlog.threshold.query.warn\":\"5s\",\n    \"index.search.slowlog.threshold.query.info\":\"2s\",\n    \"index.search.slowlog.threshold.query.debug\":\"1s\",\n    \"index.search.slowlog.threshold.query.trace\":\"400ms\",\n    \n    \n    \"index.search.slowlog.threshold.fetch.warn\":\"1s\",\n    \"index.search.slowlog.threshold.fetch.info\":\"800ms\",\n    \"index.search.slowlog.threshold.fetch.debug\":\"500ms\",\n    \"index.search.slowlog.threshold.fetch.trace\":\"200ms\",\n    \n    \n    \"index.indexing.slowlog.threshold.index.warn\":\"5s\",\n    \"index.indexing.slowlog.threshold.index.info\":\"2s\",\n    \"index.indexing.slowlog.threshold.index.debug\":\"1s\",\n    \"index.indexing.slowlog.threshold.index.trace\":\"400ms\"\n}\n"],["body","\n"],["body","查询慢于 5 秒输出一个 WARN 日志\n索引慢于 2 秒输出一个 INFO 日志\n获取慢于 1 秒输出一个 DEBUG 日志"],["body","\n"],["body","PUT /_all/_settings\n{\n    \"index.search.slowlog.threshold.query.warn\":\"5s\",\n\n    \"index.search.slowlog.threshold.fetch.warn\":\"5s\",\n    \"index.indexing.slowlog.threshold.index.warn\":\"5s\",\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/queryDSL/2.CompoundQueries.html"],["title","CompoundQueries.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","compound-queries"],["heading","Compound queries"],["body","\n"],["body","复核查询的作用是包装复核查询或者叶子查询"],["body","\n\n"],["body","以组合其结果和分数"],["body","\n"],["body","更改其行为 例如更改分数评分规则"],["body","\n"],["body","或者从查询上下文切换到过滤上下文"],["body","\n\n"],["body","The queries in this group are:"],["body","\n\n"],["body","\n"],["body","bool query"],["body","\n"],["body","The default query for combining multiple leaf or compound query clauses, as must, should, must_not, or filter clauses. The must and should clauses have their scores combined — the more matching clauses, the better — while the must_not and filter clauses are executed in filter context."],["body","\n"],["body","组合多个叶子或复合查询子句的默认查询， must、should、must_not、filter子句。must和should子句的分数组合在一起-匹配子句越多评分越好，而must_not和filter子句在filter上下文中执行。"],["body","\n"],["body","\n"],["body","\n"],["body","boosting query"],["body","\n"],["body","Return documents which match a positive query, but reduce the score of documents which also match a negative query."],["body","\n"],["body","返回与肯定查询匹配的文档，但减少也与否定查询匹配的文档的分数。"],["body","\n"],["body","\n"],["body","\n"],["body","constant_score query"],["body","\n"],["body","A query which wraps another query, but executes it in filter context. All matching documents are given the same “constant” _score."],["body","\n"],["body","在过滤上下文中执行，所有文档都会被给定 常量分数"],["body","\n"],["body","\n"],["body","\n"],["body","dis_max query"],["body","\n"],["body","A query which accepts multiple queries, and returns any documents which match any of the query clauses. While the bool query combines the scores from all matching queries, the dis_max query uses the score of the single best- matching query clause."],["body","\n"],["body","接受多个查询并返回与任何查询子句匹配的任何文档的查询。虽然bool查询合并了所有匹配查询的分数，但dis_max查询使用了单个最佳匹配查询子句的分数。"],["body","\n"],["body","\n"],["body","\n"],["body","function_score query"],["body","\n"],["body","\n\n"],["body","​\t\t使用函数修改主查询返回的分数，以考虑诸如流行度（popularity），新近度（recency），距离（distance）或使用脚本实现的自定义算法等因素。"],["body","\n"],["headingLink","boolean-query"],["heading","Boolean query"],["body","\n"],["body","A query that matches documents matching boolean combinations of other queries. The bool query maps to Lucene BooleanQuery. It is built using one or more boolean clauses, each clause with a typed occurrence. The occurrence types are:"],["body","\n"],["body","与其他查询的布尔组合匹配的文档匹配的查询。布尔查询映射到Lucene BooleanQuery。"],["body","\n"],["body","它是使用一个或多个布尔子句构建的，每个子句都有一个occurrence types  。occurrence types 为:"],["body","\n"],["body","Occur"],["body","Description"],["body","\n"],["body","must"],["body","子句 (查询) 必须出现在匹配的文档中，并将有助于得分。"],["body","\n"],["body","filter"],["body","子句 (查询) 必须出现在匹配的文档中 "],["body","然而，与must不同的是，查询的分数将被忽略 "],["body","在过滤上下文中执行，意味着忽略计分，并考虑子句进行缓存。"],["body","\n"],["body","should"],["body","子句 (查询) 应该出现在匹配文档中。"],["body","\n"],["body","must_not"],["body","子句 (查询) 不得出现在匹配的文档中"],["body","在过滤上下文中执行，意味着忽略计分，并考虑子句进行缓存。"],["body","由于忽略了评分，因此返回所有文档的评分为0。"],["body","\n\n\n"],["body","bool查询采用的是 匹配的条件越多越好  more-matches-is-better 策略，因此每个匹配 must 或 should 子句中的分数将被添加在一起，以提供每个文档的最终评分"],["body","\n"],["body","POST _search\n{\n  \"query\": {\n    \"bool\" : {\n      \"must\" : {\n        \"term\" : { \"user.id\" : \"kimchy\" }\n      },\n      \"filter\": {\n        \"term\" : { \"tags\" : \"production\" }\n      },\n      \"must_not\" : {\n        \"range\" : {\n          \"age\" : { \"gte\" : 10, \"lte\" : 20 }\n        }\n      },\n      \"should\" : [\n        { \"term\" : { \"tags\" : \"env1\" } },\n        { \"term\" : { \"tags\" : \"deployed\" } }\n      ],\n      \"minimum_should_match\" : 1,\n      \"boost\" : 1.0\n    }\n  }\n}\n"],["body","\n"],["headingLink","using-minimum_should_match"],["heading","Using minimum_should_match"],["body","\n"],["body","您可以使用 minimum_should_match 参数来指定返回的文档必须匹配的should子句的数量或百分比。"],["body","\n"],["body","如果 bool查询 只包含一个 should子句 而且没有 must 或者 filter 则 minimum_should_match 默认值为1，否则默认值为0"],["body","\n"],["body","详见 minimum_should_match parameter."],["body","\n"],["headingLink","scoring-with-boolfilter"],["heading","Scoring with bool.filter"],["body","\n"],["body","在filter元素下指定的查询对评分没有影响分数返回为0。"],["body","\n"],["body","分数仅受已指定的query 影响。例如，以下所有三个查询都返回状态字段包含 “活动” 一词的所有文档。"],["body","\n"],["body","第一个查询为所有文档分配0分，因为没有指定评分查询:"],["body","\n"],["body","GET _search\n{\n  \"query\": {\n    \"bool\": {\n      \"filter\": {\n        \"term\": {\n          \"status\": \"active\"\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","此bool查询具有match_all查询，该查询将分数分配给所有文档1.0分。"],["body","\n"],["body","GET _search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": {\n        \"match_all\": {}\n      },\n      \"filter\": {\n        \"term\": {\n          \"status\": \"active\"\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","这个constant_score查询的行为方式与上面的第二个示例完全相同。constant_score查询为过滤器匹配的所有文档分配1.0的分数。"],["body","\n"],["body","GET _search\n{\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"term\": {\n          \"status\": \"active\"\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","named-queries"],["heading","Named queries"],["body","\n"],["body","每个查询在其顶层定义中接受一个 _name。您可以使用命名查询来跟踪与返回的文档匹配的查询。"],["body","\n"],["body","如果使用命名查询，则响应会为每个命中  包含 matched_queries 属性。"],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        { \"match\": { \"name.first\": { \"query\": \"shay\", \"_name\": \"first\" } } },\n        { \"match\": { \"name.last\": { \"query\": \"banon\", \"_name\": \"last\" } } }\n      ],\n      \"filter\": {\n        \"terms\": {\n          \"name.last\": [ \"banon\", \"kimchy\" ],\n          \"_name\": \"test\"\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","boosting-query"],["heading","Boosting query"],["body","\n"],["body","Returns documents matching a positive query while reducing the relevance score of documents that also match a negative query."],["body","\n"],["body","You can use the boosting query to demote certain documents without excluding them from the search results."],["body","\n"],["body","返回与 positive  查询匹配的文档，同时降低  与否定查询匹配的文档的 relevance score 。\n您可以使用 boosting query 查询来降低某些文档的评分，但不会将它们从搜索结果中排除。"],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"boosting\": {\n      \"positive\": {\n        \"term\": {\n          \"text\": \"apple\"\n        }\n      },\n      \"negative\": {\n        \"term\": {\n          \"text\": \"pie tart fruit crumble tree\"\n        }\n      },\n      \"negative_boost\": 0.5\n    }\n  }\n}\n"],["body","\n"],["headingLink","top-level-parameters-for-boosting"],["heading","Top-level parameters for boosting"],["body","\n\n"],["body","\n"],["body","positive"],["body","\n"],["body","(Required, query object) Query you wish to run. Any returned documents must match this query."],["body","\n"],["body","\n"],["body","\n"],["body","negative"],["body","\n"],["body","(Required, query object) Query used to decrease the relevance score of matching documents.If a returned document matches the positive query and this query, the boosting query calculates the final relevance score for the document as follows:Take the original relevance score from the positive query.Multiply the score by the negative_boost value."],["body","\n"],["body","\n"],["body","\n"],["body","negative_boost"],["body","\n"],["body","(Required, float) Floating point number between 0 and 1.0 used to decrease the relevance scores of documents matching the negative query."],["body","\n"],["body","\n\n"],["headingLink","constant-score-query"],["heading","Constant score query"],["body","\n"],["body","Wraps a filter query and returns every matching document with a relevance score equal to the boost parameter value."],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"term\": { \"user.id\": \"kimchy\" }\n      },\n      \"boost\": 1.2\n    }\n  }\n}\n"],["body","\n"],["headingLink","top-level-parameters-for-constant_score"],["heading","Top-level parameters for constant_score"],["body","\n\n"],["body","\n"],["body","filter"],["body","\n"],["body","(Required, query object) Filter query you wish to run. Any returned documents must match this query.Filter queries do not calculate relevance scores. To speed up performance, Elasticsearch automatically caches frequently used filter queries."],["body","\n"],["body","\n"],["body","\n"],["body","boost"],["body","\n"],["body","(Optional, float) Floating point number used as the constant relevance score for every document matching the filter query. Defaults to 1.0."],["body","\n"],["body","\n\n"],["body","指定文档评分分数返回"],["body","\n"],["headingLink","disjunction-max-query"],["heading","Disjunction max query"],["body","\n"],["body","Returns documents matching one or more wrapped queries, called query clauses or clauses."],["body","\n"],["body","返回与一个或多个包装查询匹配的文档，称为查询子句或子句。"],["body","\n"],["body","If a returned document matches multiple query clauses, the dis_max query assigns the document the highest relevance score from any matching clause, plus a tie breaking increment for any additional matching subqueries."],["body","\n"],["body","如果返回的文档与多个查询子句匹配，则dis_max查询将从匹配的子查询中 选出分数最高的子查询，作为评分。"],["body","\n"],["body",",plus a tie breaking increment for any additional matching subqueries."],["body","\n"],["body","You can use the dis_max to search for a term in fields mapped with different boost factors."],["body","\n"],["headingLink","example-request"],["heading","Example request"],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"dis_max\": {\n      \"queries\": [\n        { \"term\": { \"title\": \"Quick pets\" } },\n        { \"term\": { \"body\": \"Quick pets\" } }\n      ],\n      \"tie_breaker\": 0.7\n    }\n  }\n}\n"],["body","\n"],["headingLink","top-level-parameters-for-dis_max"],["heading","Top-level parameters for dis_max"],["body","\n\n"],["body","\n"],["body","queries"],["body","\n"],["body","(Required, array of query objects) Contains one or more query clauses. Returned documents must match one or more of these queries. If a document matches multiple queries, Elasticsearch uses the highest relevance score."],["body","\n"],["body","\n"],["body","\n"],["body","tie_breaker"],["body","\n"],["body","(可选，浮点数) 0到1.0之间的浮点数，用于增加与多个查询子句匹配的文档的相关性分数。默认为0.0. 可以使用 tie_breaker 值 为 在多个字段中包含相同术语的文档中，选出最好的一个字段"],["body","\n"],["body","如果文档匹配多个子句，dis_max查询将计算文档的相关性得分如下"],["body","\n\n"],["body","从具有最高分数的匹配子句中获取相关性分数"],["body","\n"],["body","将其他匹配子句的分数乘以tie_breaker值，将最高分数加到相乘的分数中"],["body","\n"],["body","如果tie_breaker值大于0.0，则所有匹配子句都会计数，但得分最高的子句会   counts most."],["body","\n\n"],["body","\n\n"],["headingLink","function-score-query"],["heading","Function score query"],["body","\n"],["body","The function_score allows you to modify the score of documents that are retrieved by a query. This can be useful if, for example, a score function is computationally expensive and it is sufficient to compute the score on a filtered set of documents."],["body","\n"],["body","function_score允许您修改查询检索文档的分数。例如，如果得分函数在计算上是昂贵的，并且足以在过滤的文档集上计算得分，则这可能是有用的。"],["body","\n"],["body","要使用function_score，用户必须定义一个查询和一个或多个函数，这些函数为查询返回的每个文档计算新的分数。"],["body","\n"],["body","Function_score 只能与这样的一个函数一起使用:"],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"function_score\": {\n      \"query\": { \"match_all\": {} },\n      \"boost\": \"5\",\n      \"random_score\": {}, \n      \"boost_mode\": \"multiply\"\n    }\n  }\n}\n"],["body","\n"],["body","此外，可以组合几种功能。"],["body","\n"],["body","在这种情况下，只有当文档与给定的过滤查询匹配时，才可以选择应用该函数"],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"function_score\": {\n      \"query\": { \"match_all\": {} },\n      \"boost\": \"5\", \n      \"functions\": [\n        {\n          \"filter\": { \"match\": { \"test\": \"bar\" } },\n          \"random_score\": {}, \n          \"weight\": 23\n        },\n        {\n          \"filter\": { \"match\": { \"test\": \"cat\" } },\n          \"weight\": 42\n        }\n      ],\n      \"max_boost\": 42,\n      \"score_mode\": \"max\",\n      \"boost_mode\": \"multiply\",\n      \"min_score\": 42\n    }\n  }\n}\n"],["body","\n"],["body","每个函数的过滤查询产生的分数无关紧要。\n如果没有使用函数给出过滤器，则等效于指定 “match_all”: {}\n首先，每个文档都由定义的函数评分。参数score_mode指定如何组合计算的分数:"],["body","\n"],["body","score_mode"],["body","description"],["body","\n"],["body","multiply"],["body","scores are multiplied (default)"],["body","\n"],["body","sum"],["body","scores are summed"],["body","\n"],["body","avg"],["body","scores are averaged"],["body","\n"],["body","first"],["body","the first function that has a matching filter is applied"],["body","\n"],["body","max"],["body","maximum score is used"],["body","\n"],["body","min"],["body","minimum score is used"],["body","\n\n\n"],["body","Because scores can be on different scales (for example, between 0 and 1 for decay functions but arbitrary for field_value_factor) and also because sometimes a different impact of functions on the score is desirable, the score of each function can be adjusted with a user defined weight. The weight can be defined per function in the functions array (example above) and is multiplied with the score computed by the respective function. If weight is given without any other function declaration, weight acts as a function that simply returns the weight."],["body","\n"],["body","因为分数可以在不同的尺度上 (例如，衰减函数在0到1之间，但对于field_value_factor是任意的)，而且因为有时函数对分数的不同影响是可取的，所以每个函数的分数可以用用户定义的权重来调整。可以在函数数组 (上面的示例) 中为每个函数定义权重，并将其与相应函数计算的分数相乘。如果在没有任何其他函数声明的情况下给出了weight，则weight充当简单地返回权重的函数。"],["body","\n"],["body","如果score_mode设置为avg，则各个分数将通过加权平均值合并。例如，如果两个函数返回分数1和2，并且它们各自的权重为3和4，那么它们的分数将合并为 (1*3+2*4)/(3+4) ，而不是 (1*3+2*4)/2."],["body","\n"],["body","可以通过设置max_boost参数将新分数限制为不超过某个限制。max_boost的默认值是FLT_MAX。"],["body","\n"],["body","新计算的分数与查询的分数相结合。参数boost_mode定义如何:"],["body","\n"],["body","boost_mode"],["body","description"],["body","\n"],["body","multiply"],["body","查询分数和函数分数相乘 (默认)"],["body","\n"],["body","replace"],["body","只使用函数分数，查询分数被忽略"],["body","\n"],["body","sum"],["body","查询分数和函数分数相加"],["body","\n"],["body","avg"],["body","取平均值"],["body","\n"],["body","max"],["body","max of query score and function score"],["body","\n"],["body","min"],["body","min of query score and function score"],["body","\n\n\n"],["body","By default, modifying the score does not change which documents match. To exclude documents that do not meet a certain score threshold the min_score parameter can be set to the desired score threshold."],["body","\n"],["body","For min_score to work, all documents returned by the query need to be scored and then filtered out one by one."],["body","\n"],["body","The function_score query provides several types of score functions."],["body","\n\n"],["body","script_score"],["body","\n"],["body","weight"],["body","\n"],["body","random_score"],["body","\n"],["body","field_value_factor"],["body","\n"],["body","decay functions: gauss, linear, exp"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/queryDSL/FullTextQueries/README.html"],["title","FullTextQueries - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","全文索引"],["heading","全文索引"],["body","\n"],["body","The full text queries enable you to search analyzed text fields such as the body of an email"],["body","\n"],["body","索引的分析器与 查询的分析器必须相同"],["body","\n"],["body","The queries in this group are:"],["body","\n\n"],["body","\n"],["body","intervals query"],["body","\n"],["body","A full text query that allows fine-grained control of the ordering and proximity of matching terms."],["body","\n"],["body","\n"],["body","\n"],["body","match query"],["body","\n"],["body","用于执行全文查询的标准查询，包括模糊匹配和短语或邻近查询。"],["body","\n"],["body","\n"],["body","\n"],["body","match_bool_prefix query"],["body","\n"],["body","Creates a bool query that matches each term as a term query, except for the last term, which is matched as a prefix query"],["body","\n"],["body","创建一个bool查询，该查询将每个term match为术语查询，但最后一个术语除外，该术语被匹配为前缀查询"],["body","\n"],["body","\n"],["body","\n"],["body","match_phrase query"],["body","\n"],["body","Like the match query but used for matching exact phrases or word proximity matches."],["body","\n"],["body","\n"],["body","\n"],["body","match_phrase_prefix query"],["body","\n"],["body","Like the match_phrase query, but does a wildcard search on the final word."],["body","\n"],["body","\n"],["body","\n"],["body","multi_match query"],["body","\n"],["body","The multi-field version of the match query."],["body","\n"],["body","\n"],["body","\n"],["body","combined_fields query"],["body","\n"],["body","Matches over multiple fields as if they had been indexed into one combined field."],["body","\n"],["body","\n"],["body","\n"],["body","query_string query"],["body","\n"],["body","Supports the compact Lucene query string syntax, allowing you to specify AND|OR|NOT conditions and multi-field search within a single query string. For expert users only."],["body","\n"],["body","\n"],["body","\n"],["body","simple_query_string query"],["body","\n"],["body","A simpler, more robust version of the query_string syntax suitable for exposing directly to users."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/queryDSL/FullTextQueries/1.IntervalsQuery.html"],["title","IntervalsQuery.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","intervals-query"],["heading","Intervals query"],["body","\n"],["body","Returns documents based on the order and proximity of matching terms."],["body","\n"],["body","The intervals query uses matching rules, constructed from a small set of definitions."],["body","\n"],["body","These rules are then applied to terms from a specified field."],["body","\n"],["body","The definitions produce sequences of minimal intervals that span terms in a body of text. These intervals can be further combined and filtered by parent sources."],["body","\n"],["headingLink","example-request"],["heading","Example request"],["body","\n"],["body","The following intervals search returns documents containing my favorite food without any gap, followed by hot water or cold porridge in the my_text field."],["body","\n"],["body","This search would match a my_text value of my favorite food is cold porridge but not when it's cold my favorite food is porridge."],["body","\n"],["body","POST _search\n{\n  \"query\": { \n    \"intervals\" : {\n      \"my_text\" : {\n        \"all_of\" : {\n          \"ordered\" : true,\n          \"intervals\" : [\n            {\n              \"match\" : {\n                \"query\" : \"my favorite food\",\n                \"max_gaps\" : 0,\n                \"ordered\" : true\n              }\n            },\n            {\n              \"any_of\" : {\n                \"intervals\" : [\n                  { \"match\" : { \"query\" : \"hot water\" } },\n                  { \"match\" : { \"query\" : \"cold porridge\" } }\n                ]\n              }\n            }\n          ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","top-level-parameters-for-intervals"],["heading","Top-level parameters for intervals"],["body","\n\n"],["body","\n"],["body","<field>"],["body","\n"],["body","(必填，规则对象) 您希望搜索的字段。此参数的值是一个规则对象，用于根据匹配的术语、顺序和接近度来匹配文档。Valid rules include:matchprefixwildcardfuzzyall_ofany_of"],["body","\n"],["body","\n\n"],["headingLink","match-rule-parameters"],["heading","match rule parameters"],["body","\n"],["body","The match rule matches analyzed text."],["body","\n\n"],["body","\n"],["body","query"],["body","\n"],["body","(Required, string) Text you wish to find in the provided <field>."],["body","\n"],["body","\n"],["body","\n"],["body","max_gaps"],["body","\n"],["body","(Optional, integer) Maximum number of positions between the matching terms. Terms further apart than this are not considered matches. Defaults to -1.If unspecified or set to -1, there is no width restriction on the match. If set to 0, the terms must appear next to each other."],["body","\n"],["body","\n"],["body","\n"],["body","ordered"],["body","\n"],["body","(Optional, Boolean) If true, matching terms must appear in their specified order. Defaults to false."],["body","\n"],["body","\n"],["body","\n"],["body","analyzer"],["body","\n"],["body","(Optional, string) analyzer used to analyze terms in the query. Defaults to the top-level <field>'s analyzer."],["body","\n"],["body","\n"],["body","\n"],["body","filter"],["body","\n"],["body","(Optional, interval filter rule object) An optional interval filter."],["body","\n"],["body","\n"],["body","\n"],["body","use_field"],["body","\n"],["body","(Optional, string) If specified, then match intervals from this field rather than the top-level <field>. Terms are analyzed using the search analyzer from this field. This allows you to search across multiple fields as if they were all the same field; for example, you could index the same text into stemmed and unstemmed fields, and search for stemmed tokens near unstemmed ones."],["body","\n"],["body","\n\n"],["headingLink","prefix-rule-parameters"],["heading","prefix rule parameters"],["body","\n"],["body","The prefix rule matches terms that start with a specified set of characters. This prefix can expand to match at most 128 terms. If the prefix matches more than 128 terms, Elasticsearch returns an error. You can use the index-prefixes option in the field mapping to avoid this limit."],["body","\n\n"],["body","\n"],["body","prefix"],["body","\n"],["body","(Required, string) Beginning characters of terms you wish to find in the top-level <field>."],["body","\n"],["body","\n"],["body","\n"],["body","analyzer"],["body","\n"],["body","(Optional, string) analyzer used to normalize the prefix. Defaults to the top-level <field>'s analyzer."],["body","\n"],["body","\n"],["body","\n"],["body","use_field"],["body","\n"],["body","(Optional, string) If specified, then match intervals from this field rather than the top-level <field>.The prefix is normalized using the search analyzer from this field, unless a separate analyzer is specified."],["body","\n"],["body","\n\n"],["headingLink","wildcard-rule-parameters"],["heading","wildcard rule parameters"],["body","\n"],["body","The wildcard rule matches terms using a wildcard pattern. This pattern can expand to match at most 128 terms. If the pattern matches more than 128 terms, Elasticsearch returns an error."],["body","\n\n"],["body","\n"],["body","pattern"],["body","\n"],["body","(Required, string) Wildcard pattern used to find matching terms.This parameter supports two wildcard operators:?, which matches any single character*, which can match zero or more characters, including an empty oneAvoid beginning patterns with * or ?. This can increase the iterations needed to find matching terms and slow search performance."],["body","\n"],["body","\n"],["body","\n"],["body","analyzer"],["body","\n"],["body","(Optional, string) analyzer used to normalize the pattern. Defaults to the top-level <field>'s analyzer."],["body","\n"],["body","\n"],["body","\n"],["body","use_field"],["body","\n"],["body","(Optional, string) If specified, match intervals from this field rather than the top-level <field>.The pattern is normalized using the search analyzer from this field, unless analyzer is specified separately."],["body","\n"],["body","\n\n"],["headingLink","fuzzy-rule-parameters"],["heading","fuzzy rule parameters"],["body","\n"],["body","The fuzzy rule matches terms that are similar to the provided term, within an edit distance defined by Fuzziness. If the fuzzy expansion matches more than 128 terms, Elasticsearch returns an error."],["body","\n\n"],["body","\n"],["body","term"],["body","\n"],["body","(Required, string) The term to match"],["body","\n"],["body","\n"],["body","\n"],["body","prefix_length"],["body","\n"],["body","(Optional, integer) Number of beginning characters left unchanged when creating expansions. Defaults to 0."],["body","\n"],["body","\n"],["body","\n"],["body","transpositions"],["body","\n"],["body","(Optional, Boolean) Indicates whether edits include transpositions of two adjacent characters (ab → ba). Defaults to true."],["body","\n"],["body","\n"],["body","\n"],["body","fuzziness"],["body","\n"],["body","(Optional, string) Maximum edit distance allowed for matching. See Fuzziness for valid values and more information. Defaults to auto."],["body","\n"],["body","\n"],["body","\n"],["body","analyzer"],["body","\n"],["body","(Optional, string) analyzer used to normalize the term. Defaults to the top-level <field> 's analyzer."],["body","\n"],["body","\n"],["body","\n"],["body","use_field"],["body","\n"],["body","(Optional, string) If specified, match intervals from this field rather than the top-level <field>.The term is normalized using the search analyzer from this field, unless analyzer is specified separately."],["body","\n"],["body","\n\n"],["headingLink","all_of-rule-parameters"],["heading","all_of rule parameters"],["body","\n"],["body","The all_of rule returns matches that span a combination of other rules."],["body","\n\n"],["body","\n"],["body","intervals"],["body","\n"],["body","(Required, array of rule objects) An array of rules to combine. All rules must produce a match in a document for the overall source to match."],["body","\n"],["body","\n"],["body","\n"],["body","max_gaps"],["body","\n"],["body","(Optional, integer) Maximum number of positions between the matching terms. Intervals produced by the rules further apart than this are not considered matches. Defaults to -1.If unspecified or set to -1, there is no width restriction on the match. If set to 0, the terms must appear next to each other."],["body","\n"],["body","(可选，整数) 匹配项之间的最大位置数。比这更远的规则产生的间隔不被视为匹配。默认值为-1。如果未指定或设置为-1，则匹配项没有宽度限制。如果设置为0，则术语必须彼此相邻出现。"],["body","\n"],["body","\n"],["body","\n"],["body","ordered"],["body","\n"],["body","(Optional, Boolean) If true, intervals produced by the rules should appear in the order in which they are specified. Defaults to false."],["body","\n"],["body","\n"],["body","\n"],["body","filter"],["body","\n"],["body","(Optional, interval filter rule object) Rule used to filter returned intervals."],["body","\n"],["body","\n\n"],["headingLink","any_of-rule-parameters"],["heading","any_of rule parameters"],["body","\n"],["body","The any_of rule returns intervals produced by any of its sub-rules."],["body","\n\n"],["body","\n"],["body","intervals"],["body","\n"],["body","(Required, array of rule objects) An array of rules to match."],["body","\n"],["body","\n"],["body","\n"],["body","filter"],["body","\n"],["body","(Optional, interval filter rule object) Rule used to filter returned intervals."],["body","\n"],["body","\n\n"],["headingLink","filter-rule-parameters"],["heading","filter rule parameters"],["body","\n"],["body","The filter rule returns intervals based on a query. See Filter example for an example."],["body","\n\n"],["body","\n"],["body","after"],["body","\n"],["body","(Optional, query object) Query used to return intervals that follow an interval from the filter rule."],["body","\n"],["body","\n"],["body","\n"],["body","before"],["body","\n"],["body","(Optional, query object) Query used to return intervals that occur before an interval from the filter rule."],["body","\n"],["body","\n"],["body","\n"],["body","contained_by"],["body","\n"],["body","(Optional, query object) Query used to return intervals contained by an interval from the filter rule."],["body","\n"],["body","\n"],["body","\n"],["body","containing"],["body","\n"],["body","(Optional, query object) Query used to return intervals that contain an interval from the filter rule."],["body","\n"],["body","\n"],["body","\n"],["body","not_contained_by"],["body","\n"],["body","(Optional, query object) Query used to return intervals that are not contained by an interval from the filter rule."],["body","\n"],["body","\n"],["body","\n"],["body","not_containing"],["body","\n"],["body","(Optional, query object) Query used to return intervals that do not contain an interval from the filter rule."],["body","\n"],["body","\n"],["body","\n"],["body","not_overlapping"],["body","\n"],["body","(Optional, query object) Query used to return intervals that do not overlap with an interval from the filter rule."],["body","\n"],["body","\n"],["body","\n"],["body","overlapping"],["body","\n"],["body","(Optional, query object) Query used to return intervals that overlap with an interval from the filter rule."],["body","\n"],["body","\n"],["body","\n"],["body","script"],["body","\n"],["body","(Optional, script object) Script used to return matching documents. This script must return a boolean value, true or false. See Script filters for an example."],["body","\n"],["body","\n\n"],["headingLink","example"],["heading","Example"],["body","\n"],["headingLink","filter-example"],["heading","Filter example"],["body","\n\n"],["body","hot and porridge  之间不超过10个位置"],["body","\n"],["body","中间不能出现 salty 单词"],["body","\n\n"],["body","POST _search\n{\n  \"query\": {\n    \"intervals\" : {\n      \"my_text\" : {\n        \"match\" : {\n          \"query\" : \"hot porridge\",\n          \"max_gaps\" : 10,\n          \"filter\" : {\n            \"not_containing\" : {\n              \"match\" : {\n                \"query\" : \"salty\"\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","script-filters"],["heading","Script filters"],["body","\n"],["body","You can use a script to filter intervals based on their start position, end position, and internal gap count. The following filter script uses the interval variable with the start, end, and gaps methods:"],["body","\n"],["body","POST _search\n{\n  \"query\": {\n    \"intervals\" : {\n      \"my_text\" : {\n        \"match\" : {\n          \"query\" : \"hot porridge\",\n          \"filter\" : {\n            \"script\" : {\n              \"source\" : \"interval.start > 10 && interval.end < 20 && interval.gaps == 0\"\n            }\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","minimization"],["heading","Minimization"],["body","\n"],["body","间隙查询优先选择最小化间隔以确保查询在线性时间执行完毕"],["body","\n"],["body","This can sometimes cause surprising results, particularly when using max_gaps restrictions or filters. For example, take the following query, searching for salty contained within the phrase hot porridge:"],["body","\n"],["body","POST _search\n{\n  \"query\": {\n    \"intervals\" : {\n      \"my_text\" : {\n        \"match\" : {\n          \"query\" : \"salty\",\n          \"filter\" : {\n            \"contained_by\" : {\n              \"match\" : {\n                \"query\" : \"hot porridge\"\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","This query does not match a document containing the phrase hot porridge is salty porridge, because the intervals returned by the match query for hot porridge only cover the initial two terms in this document, and these do not overlap the intervals covering salty."],["body","\n"],["headingLink","anyof"],["heading","Anyof"],["body","\n"],["body","Another restriction to be aware of is the case of any_of rules that contain sub-rules which overlap. In particular, if one of the rules is a strict prefix of the other, then the longer rule can never match, which can cause surprises when used in combination with max_gaps. Consider the following query, searching for the immediately followed by big or big bad, immediately followed by wolf:"],["body","\n"],["body","POST _search\n{\n  \"query\": {\n    \"intervals\" : {\n      \"my_text\" : {\n        \"all_of\" : {\n          \"intervals\" : [\n            { \"match\" : { \"query\" : \"the\" } },\n            { \"any_of\" : {\n                \"intervals\" : [\n                    { \"match\" : { \"query\" : \"big\" } },\n                    { \"match\" : { \"query\" : \"big bad\" } }\n                ] } },\n            { \"match\" : { \"query\" : \"wolf\" } }\n          ],\n          \"max_gaps\" : 0,\n          \"ordered\" : true\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","Counter-intuitively（与直觉相反，）,不会匹配 the big bad wolf  因为any_of 规则 优先匹配 big 间隔查询，而不会匹配  big bad 间隔查询，因为是基于最小化匹配原则"],["body","\n"],["body","POST _search\n{\n  \"query\": {\n    \"intervals\" : {\n      \"my_text\" : {\n        \"any_of\" : {\n          \"intervals\" : [\n            { \"match\" : {\n                \"query\" : \"the big bad wolf\",\n                \"ordered\" : true,\n                \"max_gaps\" : 0 } },\n            { \"match\" : {\n                \"query\" : \"the big wolf\",\n                \"ordered\" : true,\n                \"max_gaps\" : 0 } }\n           ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/queryDSL/FullTextQueries/2.MatchQuery.html"],["title","MatchQuery.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","match-query"],["heading","Match query"],["body","\n"],["body","返回与提供的文本、数字、日期或布尔值匹配的文档。提供的文本在匹配前进行分析。"],["body","\n"],["body","匹配查询是用于执行全文搜索的标准查询，包括用于模糊匹配的选项。"],["body","\n"],["headingLink","example-request"],["heading","Example request"],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"match\": {\n      \"message\": {\n        \"query\": \"this is a test\"\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","top-level-parameters-for-match"],["heading","Top-level parameters for match"],["body","\n\n"],["body","\n"],["body","<field>"],["body","\n"],["body","(Required, object) Field you wish to search."],["body","\n"],["body","\n\n"],["headingLink","parameters-for-field"],["heading","Parameters for <field>"],["body","\n\n"],["body","\n"],["body","query"],["body","\n"],["body","(Required) Text, number, boolean value or date you wish to find in the provided <field>.The match query analyzes any provided text before performing a search. This means the match query can search text fields for analyzed tokens rather than an exact term."],["body","\n"],["body","\n"],["body","\n"],["body","analyzer"],["body","\n"],["body","(Optional, string) Analyzer used to convert the text in the query value into tokens. Defaults to the index-time analyzer mapped for the <field>. If no analyzer is mapped, the index’s default analyzer is used."],["body","\n"],["body","\n"],["body","\n"],["body","auto_generate_synonyms_phrase_query"],["body","\n"],["body","(Optional, Boolean) If true, match phrase queries are automatically created for multi-term synonyms. Defaults to true.See Use synonyms with match query for an example."],["body","\n"],["body","\n"],["body","\n"],["body","fuzziness"],["body","\n"],["body","(Optional, string) Maximum edit distance allowed for matching. See Fuzziness for valid values and more information. See Fuzziness in the match query for an example."],["body","\n"],["body","\n"],["body","\n"],["body","max_expansions"],["body","\n"],["body","(Optional, integer) Maximum number of terms to which the query will expand. Defaults to 50."],["body","\n"],["body","\n"],["body","\n"],["body","prefix_length"],["body","\n"],["body","(Optional, integer) Number of beginning characters left unchanged for fuzzy matching. Defaults to 0."],["body","\n"],["body","\n"],["body","\n"],["body","fuzzy_transpositions"],["body","\n"],["body","(Optional, Boolean) If true, edits for fuzzy matching include transpositions of two adjacent characters (ab → ba). Defaults to true."],["body","\n"],["body","\n"],["body","\n"],["body","fuzzy_rewrite"],["body","\n"],["body","(Optional, string) Method used to rewrite the query. See the rewrite parameter for valid values and more information.If the fuzziness parameter is not 0, the match query uses a fuzzy_rewrite method of top_terms_blended_freqs_${max_expansions} by default."],["body","\n"],["body","\n"],["body","\n"],["body","lenient"],["body","\n"],["body","(Optional, Boolean) If true, format-based errors, such as providing a text query value for a numeric field, are ignored. Defaults to false."],["body","\n"],["body","\n"],["body","\n"],["body","operator"],["body","\n"],["body","(Optional, string) Boolean logic used to interpret text in the query value. Valid values are:**OR (Default)For example, a query value of capital of Hungary is interpreted as capital OR of OR Hungary.AND**For example, a query value of capital of Hungary is interpreted as capital AND of AND Hungary."],["body","\n"],["body","\n"],["body","\n"],["body","minimum_should_match"],["body","\n"],["body","(Optional, string) Minimum number of clauses that must match for a document to be returned. See the minimum_should_match parameter for valid values and more information."],["body","\n"],["body","\n"],["body","\n"],["body","zero_terms_query"],["body","\n"],["body","(Optional, string) Indicates whether no documents are returned if the analyzer removes all tokens, such as when using a stop filter. Valid values are:**none (Default)No documents are returned if the analyzer removes all tokens.all**Returns all documents, similar to a match_all query.See Zero terms query for an example."],["body","\n"],["body","\n\n"],["headingLink","notes"],["heading","Notes"],["body","\n"],["headingLink","short-request-example"],["heading","Short request example"],["body","\n"],["body","You can simplify the match query syntax by combining the <field> and query parameters. For example:"],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"match\": {\n      \"message\": \"this is a test\"\n    }\n  }\n}\n"],["body","\n"],["headingLink","how-the-match-query-works"],["heading","How the match query works"],["body","\n"],["body","The match query is of type boolean."],["body","\n\n"],["body","为boolean query"],["body","\n"],["body","这意味着对提供的文本进行分析，分析过程从提供的文本中构造一个布尔查询。"],["body","\n"],["body","operator 参数 可以为 or and  控制boolean 子句，默认是 or"],["body","\n"],["body","可以使用minimum_should_match参数设置要匹配的可选应该子句的最小数量。"],["body","\n\n"],["body","Here is an example with the operator parameter:"],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"match\": {\n      \"message\": {\n        \"query\": \"this is a test\",\n        \"operator\": \"and\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","可以设置分析器来控制哪个分析器将对文本执行分析过程。"],["body","\n"],["body","它默认为字段显式映射定义或默认搜索分析器。"],["body","\n"],["body","可以将lenient参数设置为true，以忽略由数据类型不匹配引起的异常，例如尝试使用文本查询字符串查询数字字段。默认为false。"],["body","\n"],["headingLink","fuzziness-in-the-match-query"],["heading","Fuzziness in the match query"],["body","\n"],["body","fuzziness allows fuzzy matching based on the type of field being queried. See Fuzziness for allowed settings."],["body","\n"],["body","The prefix_length and max_expansions can be set in this case to control the fuzzy process."],["body","\n"],["body","If the fuzzy option is set the query will use top_terms_blended_freqs_${max_expansions} as its rewrite method the fuzzy_rewrite parameter allows to control how the query will get rewritten."],["body","\n"],["body","Fuzzy transpositions (ab → ba) are allowed by default but can be disabled by setting fuzzy_transpositions to false."],["body","\n"],["body","Fuzzy matching is not applied to terms with synonyms or in cases where the analysis process produces multiple tokens at the same position. Under the hood these terms are expanded to a special synonym query that blends term frequencies, which does not support fuzzy expansion."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/queryDSL/FullTextQueries/3.MultiMatch.html"],["title","MultiMatch.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","多字段联合检索"],["heading","多字段联合检索"],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"multi_match\" : {\n      \"query\":    \"this is a test\", \n      \"fields\": [ \"subject\", \"message\" ] \n    }\n  }\n}\n"],["body","\n"],["headingLink","模糊检索多字段检索"],["heading","模糊检索多字段检索"],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"multi_match\" : {\n      \"query\":    \"Will Smith\",\n      \"fields\": [ \"title\", \"*_name\" ] \n    }\n  }\n}\n"],["body","\n"],["headingLink","带权重多字段"],["heading","带权重多字段"],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"multi_match\" : {\n      \"query\" : \"this is a test\",\n      \"fields\" : [ \"subject^3\", \"message\" ] \n    }\n  }\n}\n"],["body","\n"],["headingLink","无字段时的默认行为"],["heading","无字段时的默认行为"],["body","\n"],["body","If no fields are provided, the multi_match query defaults to the index.query.default_field index settings, which in turn defaults to *. * extracts all fields in the mapping that are eligible to term queries and filters the metadata fields. All extracted fields are then combined to build a query."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/queryDSL/README.html"],["title","queryDSL - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","query-dsl"],["heading","Query DSL"],["body","\n"],["body","Elasticsearch提供了一个基于JSON的完整查询DSL (域特定语言) 来定义查询。将查询DSL视为查询的AST (抽象语法树)，由两种类型的子句组成:"],["body","\n"],["headingLink","leaf-query-clauses"],["heading","Leaf query clauses"],["body","\n"],["body","叶查询子句在特定字段中查找特定值，例如 match、term 、range。这些查询可以自己使用。"],["body","\n"],["headingLink","compound-query-clauses"],["heading","Compound query clauses"],["body","\n"],["body","复合查询子句包装其他叶子或复合查询，用于以逻辑方式组合多个查询 例如：bool、dis_max"],["body","\n"],["body","或者 修改默认行为  constant_score"],["body","\n"],["body","Query clauses behave differently depending on whether they are used in query context or filter context."],["body","\n"],["body","查询子句的行为 具体取决于它们是在查询上下文中还是在过滤器上下文中使用（ query context or filter context.）"],["body","\n"],["headingLink","allow-expensive-queries"],["heading","Allow expensive queries"],["body","\n"],["body","某些类型的查询通常会由于其实现方式而执行慢，这会影响群集的稳定性。这些查询可以分类如下:"],["body","\n\n"],["body","\n"],["body","需要进行线性扫描以识别匹配的查询:"],["body","\n\n"],["body","script queries"],["body","\n\n"],["body","\n"],["body","\n"],["body","具有较高前期成本的查询:"],["body","\n\n"],["body","fuzzy queries (except on wildcard fields)"],["body","\n"],["body","regexp queries (except on wildcard fields)"],["body","\n"],["body","prefix queries (except on wildcard fields or those without index_prefixes)"],["body","\n"],["body","wildcard queries (except on wildcard fields)"],["body","\n"],["body","range queries>> on < and [keyword` fields"],["body","\n\n"],["body","\n"],["body","\n"],["body","Joining queries"],["body","\n"],["body","\n"],["body","\n"],["body","Queries on deprecated geo shapes"],["body","\n"],["body","\n"],["body","\n"],["body","每个文档成本可能很高的查询:"],["body","\n\n"],["body","script score queries"],["body","\n"],["body","percolate queries"],["body","\n\n"],["body","\n\n"],["body","可以通过将search.allow_expensive_queries设置的值设置为false (默认为true) 来防止此类查询的执行。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/queryDSL/Match中参数解析.html"],["title","Match中参数解析.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","fuzziness"],["heading","fuzziness"],["body","\n"],["body","模糊程度"],["body","\n\n"],["body","用于指定模糊匹配的程度。它允许在搜索关键词中允许一定程度的拼写错误或变体。"],["body","\n"],["body","它可以自动纠正拼写错误、处理单词的缺失或添加、以及处理字母的交换等情况。"],["body","\n"],["body","fuzziness 参数的值可以是一个具体的数字，表示最大允许的编辑距离（即两个词之间最多可以有多少个不同的字符）。默认值为 \"AUTO\"，表示 Elasticsearch 将根据搜索词的长度自动选择合适的编辑距离。"],["body","\n"],["body","通过使用 fuzziness 参数，您可以使 match 查询更加灵活，从而提高搜索结果的准确性和完整性。它可以帮助您找到与搜索关键词相似的结果，即使搜索关键词存在一些拼写错误或变体。"],["body","\n\n"],["headingLink","fuzzy_transpositions"],["heading","fuzzy_transpositions"],["body","\n\n"],["body","用于控制模糊匹配时是否考虑字符交换的情况。"],["body","\n"],["body","Elasticsearch 将考虑字符交换的情况来进行模糊匹配。这意味着，如果搜索词与目标词之间只有相邻字符的位置发生了交换，仍然可以被认为是匹配的。"],["body","\n\n"],["headingLink","lenient"],["heading","lenient"],["body","\n"],["body","数据类型匹配的宽容度"],["body","\n\n"],["body","当在索引文档时，Elasticsearch 会自动根据字段的数据类型进行字段映射。例如，如果一个字段被映射为整数类型，但你尝试索引一个包含非整数值的文档，Elasticsearch 默认会抛出一个错误。"],["body","\n"],["body","但是，当你将 lenient 参数设置为 true 时，Elasticsearch 将会宽容地接受非法值，并尝试将其转换为合适的数据类型。这意味着即使某些字段的值不符合字段映射的数据类型，Elasticsearch 也不会抛出错误，而是尽可能地将其转换为合适的类型。"],["body","\n"],["body","通过使用 lenient 参数，你可以在索引文档时更加宽容，避免由于数据类型不匹配而导致的错误。然而，需要注意的是，宽容度可能会导致一些数据丢失或转换错误，因此在使用 lenient 参数时需要谨慎考虑。"],["body","\n\n"],["headingLink","operator"],["heading","operator"],["body","\n\n"],["body","ElasticSearch 会对 Query 做 搜索词的 分词。得到多个 Term 关键词"],["body","\n"],["body","默认情况下，operator 参数的值为 \"OR\"，表示多个搜索词之间是逻辑或的关系。这意味着只要文档中包含任何一个搜索词，就会被匹配到。"],["body","\n"],["body","如果将 operator 参数设置为 \"AND\"：文章中要包含 所有的搜索词"],["body","\n\n"],["headingLink","max_expansions"],["heading","max_expansions"],["body","\n\n"],["body","\n"],["body","当执行前缀查询或模糊查询时，Elasticsearch 会尝试根据搜索词的前缀或相似性来匹配文档"],["body","\n"],["body","\n"],["body","\n"],["body","为了实现这一点，Elasticsearch 会根据搜索词进行扩展，生成更多可能的搜索词，然后再与文档进行匹配。"],["body","\n"],["body","\n"],["body","\n"],["body","max_expansions 参数用于限制扩展的搜索词的数量。"],["body","\n"],["body","\n"],["body","\n"],["body","如果设置了 max_expansions 参数，Elasticsearch 将在扩展搜索词时最多考虑指定数量的扩展。超过指定数量的扩展将被忽略。"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/queryDSL/SearchAfter.html"],["title","SearchAfter.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","search-after"],["heading","Search After"],["body","\n"],["body","结果的分页可以通过使用 from 和 size 来完成，但是当达到深度分页时成本变得禁止。 index.max_result_window 默认为 10,000 是一种保护，"],["body","\n"],["body","搜索请求占用堆内存和时间与 from + size 成比例。"],["body","\n"],["body","建议使用 Scroll api 进行高效的深层滚动，但滚动上下文是昂贵的，不建议将其用于实时用户请求。"],["body","\n"],["body","search_after 参数通过提供活动光标来规避此问题。"],["body","\n"],["body","这个想法是使用前一页的结果来帮助检索下一页。"],["body","\n"],["body","假设检索第一页的查询如下所示："],["body","\n"],["body","GET twitter/tweet/_search\n{\n    \"size\": 10,\n    \"query\": {\n        \"match\" : {\n            \"title\" : \"elasticsearch\"\n        }\n    },\n    \"sort\": [\n        {\"date\": \"asc\"},\n        {\"_uid\": \"desc\"}\n    ]\n}\n"],["body","\n"],["body","注意"],["body","\n"],["body","每个文档具有一个唯一值的字段应用作排序规范的仲裁。"],["body","\n"],["body","否则，具有相同排序值的文档的排序顺序将是未定义的。 建议的方法是使用字段 _uid，它确保每个文档包含一个唯一值。"],["body","\n"],["body","上述请求的结果包括每个文档的排序值数组。 这些排序值可以与 search_after 参数结合使用，以便在结果列表中的任何文档之后“返回”结果。"],["body","\n"],["body","例如，我们可以使用最后一个文档的排序值，并将其传递给search_after 以检索下一页结果："],["body","\n"],["body","GET twitter/tweet/_search\n{\n    \"size\": 10,\n    \"query\": {\n        \"match\" : {\n            \"title\" : \"elasticsearch\"\n        }\n    },\n    \"search_after\": [1463538857, \"tweet#654323\"],\n    \"sort\": [\n        {\"date\": \"asc\"},\n        {\"_uid\": \"desc\"}\n    ]\n}\n"],["body","\n"],["body","当使用 search_after 时，参数 from 必须设置为 0（或 -1 ）。"],["body","\n"],["body","search_after 不是一种自由地跳到随机页面的解决方案，而是一种并行地滚动许多查询的解决方案。 它非常类似于滚动 API，"],["body","\n"],["body","但不同的是，search_after 参数是无状态的，它总是解决对搜索器的最新版本。 因此，排序顺序可能会在步行期间更改，具体取决于您的索引的更新和删除。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/queryDSL/CompoundQueries/README.html"],["title","CompoundQueries - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","复合查询"],["heading","复合查询"],["body","\n"],["body","复合查询会包装其他复合或叶查询，以组合其结果和分数，更改其行为或从查询切换到过滤器上下文。"],["body","\n"],["body","包括"],["body","\n"],["body","bool query"],["body","\n"],["body","用于组合多个叶或复合查询子句的默认查询，如 'must' 、 'should' 、 'must_not' 或 'filter' 子句。'must' 和 'should' 子句将它们的分数组合在一起-匹配的子句越多越好"],["body","\n"],["body","而 'must_not' 和 'filter' 子句在过滤器上下文中执行。"],["body","\n"],["body","boosting query"],["body","\n"],["body","返回与 positive 查询匹配的文档，但会降低同样 匹配 negative 查询匹配的文档的分数"],["body","\n"],["body","constant_score query"],["body","\n"],["body","包装另一个查询的查询，但在筛选器上下文中执行它的查询。所有匹配的文档都被赋予相同的 “常量” “_score”。"],["body","\n"],["body","dis_max query"],["body","\n"],["body","接受多个查询并返回与任何查询子句匹配的任何文档的查询。当 'bool' 查询组合来自所有匹配查询的分数时，dis_max'查询使用单个最佳匹配查询子句的分数。( single best-matching query clause)"],["body","\n"],["body","function_score query"],["body","\n"],["body","使用函数修改主查询返回的分数，以考虑流行度（popularity）、新近度（recency）、距离（distance）或使用脚本实现的自定义算法等因素。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/queryDSL/CompoundQueries/2.FunctionScore.html"],["title","FunctionScore.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","function-score-query"],["heading","Function score query"],["body","\n"],["body","function_score允许您修改 查询检索的文档的分数。例如，如果分数函数在计算上是昂贵的，并且足以在过滤的文档集合上计算分数，则这可以是有用的。"],["body","\n"],["body","要使用function_score，用户必须定义一个查询 和 一个或多个函数，为查询返回的每个文档计算新的分数。\nfunction_score只能与这样的一个函数一起使用:"],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"function_score\": {\n      \"query\": { \"match_all\": {} },\n      \"boost\": \"5\",\n      \"random_score\": {}, \n      \"boost_mode\": \"multiply\"\n    }\n  }\n}\n"],["body","\n"],["body","此外，可以组合若干函数。在这种情况下，可以选择仅在文档与给定的过滤查询匹配时应用函数"],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"function_score\": {\n      \"query\": { \"match_all\": {} },\n      \"boost\": \"5\", \n      \"functions\": [\n        {\n          \"filter\": { \"match\": { \"test\": \"bar\" } },\n          \"random_score\": {}, \n          \"weight\": 23\n        },\n        {\n          \"filter\": { \"match\": { \"test\": \"cat\" } },\n          \"weight\": 42\n        }\n      ],\n      \"max_boost\": 42,\n      \"score_mode\": \"max\",\n      \"boost_mode\": \"multiply\",\n      \"min_score\": 42\n    }\n  }\n}\n"],["body","\n"],["headingLink","多个function组合时的分数计算"],["heading","多个Function组合时的分数计算"],["body","\n"],["body","\n"],["body","multiply"],["body","scores are multiplied (default)"],["body","\n"],["body","sum"],["body","scores are summed"],["body","\n"],["body","avg"],["body","scores are averaged"],["body","\n"],["body","first"],["body","the first function that has a matching filter is applied"],["body","\n"],["body","max"],["body","maximum score is used"],["body","\n"],["body","min"],["body","minimum score is used"],["body","\n\n\n"],["body","因为分数可以在不同的尺度上 (例如，对于衰减函数（decay functions），在0和1之间，但对于field_value_factor是任意的)，并且还因为有时函数对分数的不同影响是期望的，所以可以用用户定义的权重（weight）来调整每个函数的分数。可以在函数数组上 (上面的示例) 中定义每个函数的权重，并将其乘以由相应函数计算的分数。"],["body","\n"],["body","如果在没有任何其他函数声明的情况下给出权重，则权重充当仅返回权重的函数。"],["body","\n"],["headingLink","加权平均"],["heading","加权平均"],["body","\n"],["body","如果 score_mode 设置为 avg，则各个分数将通过 ** 加权 ** 平均值进行组合。例如，如果两个函数返回得分1和2，并且它们各自的权重是3和4，那么它们的得分将被合并为 '(1*3 2*4 )/(3+4)'"],["body","\n"],["body","通过设置max_boost参数，可以将新分数限制为不超过某个限制。max_boost的默认值为FLT_MAX。"],["body","\n"],["headingLink","查询分数与function分数组合"],["heading","查询分数与function分数组合"],["body","\n"],["body","新计算的分数与查询的分数组合。参数boost_mode定义如下"],["body","\n"],["body","\n"],["body","multiply"],["body","query score and function score is multiplied (default)"],["body","\n"],["body","replace"],["body","only function score is used, the query score is ignored"],["body","\n"],["body","sum"],["body","query score and function score are added"],["body","\n"],["body","avg"],["body","average"],["body","\n"],["body","max"],["body","max of query score and function score"],["body","\n"],["body","min"],["body","min of query score and function score"],["body","\n\n\n"],["body","默认情况下，修改分数不会更改匹配的文档。为了排除不满足某个分数阈值的文档，可以将min_score参数设置为所需的分数阈值。"],["body","\n"],["body","为了使min_score起作用，需要对查询返回的所有文档进行评分，然后逐一过滤掉。"],["body","\n"],["headingLink","分数函数"],["heading","分数函数"],["body","\n"],["body","function_score查询提供了几种类型的分数函数。"],["body","\n\n"],["body","script_score  ： 基于自定义脚本的分数"],["body","\n"],["body","weight ： 基于权重的分数"],["body","\n"],["body","random_score ： 随机分数"],["body","\n"],["body","field_value_factor ： 基于字段值的分数"],["body","\n"],["body","decay functions: gauss, linear, exp ： 衰减函数"],["body","\n\n"],["headingLink","script-score"],["heading","Script score"],["body","\n"],["body","script_score 函数允许您包装另一个查询，并使用脚本表达式从文档中的其他 数字 字段值派生的计算 (可选) 自定义该查询的评分。这里是一个简单的示例:"],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"function_score\": {\n      \"query\": {\n        \"match\": { \"message\": \"elasticsearch\" }\n      },\n      \"script_score\": {\n        \"script\": {\n          \"source\": \"Math.log(2 + doc['my-int'].value)\"\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","注意"],["heading","注意"],["body","\n"],["body","在Elasticsearch中，所有文档分数都是正的32位浮点数。"],["body","\n"],["body","如果script_score函数产生的分数精度更高，则会将其转换为最接近的32位浮点。\n同样，分数必须是非负的。否则，Elasticsearch将返回错误。"],["body","\n"],["body","除了不同的脚本字段值和表达式之外，还可以使用 _score 脚本参数来检索基于包装查询的分数。"],["body","\n"],["body","脚本编译被缓存以加快执行速度。如果脚本具有需要考虑的参数，则最好重用相同的脚本，并为其提供参数:"],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"function_score\": {\n      \"query\": {\n        \"match\": { \"message\": \"elasticsearch\" }\n      },\n      \"script_score\": {\n        \"script\": {\n          \"params\": {\n            \"a\": 5,\n            \"b\": 1.2\n          },\n          \"source\": \"params.a / Math.pow(params.b, doc['my-int'].value)\"\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","weight"],["heading","Weight"],["body","\n"],["body","重量分数允许您将分数乘以提供的重量。这有时可能是需要的，因为在特定查询上设置的提升值被规范化，而对于这个分数函数，它不是。number值的类型为float。"],["body","\n"],["body","\"weight\" : number\n"],["body","\n"],["headingLink","random"],["heading","Random"],["body","\n"],["body","random_score生成从0到但不包括1均匀分布的分数。默认情况下，它使用内部Lucene doc id作为随机性的来源，这是非常有效的，但不幸的是不可重现（reproducible）的，因为文档可能被合并重新编号。"],["body","\n"],["body","如果你想分数是可重复的，可以提供一个种子（seed）和字段(field)。然后，将基于该种子、所考虑的文档的字段的最小值以及  基于索引名称 和 分片id  计算的盐来计算最终得分，使得具有相同值但存储在不同索引中的文档获得不同的得分。"],["body","\n"],["body","请注意，位于同一分片内且字段值相同的文档将获得相同的分数，因此通常希望使用对所有文档具有唯一值的字段。"],["body","\n"],["body","一个好的默认选择可能是使用_seq_no字段，其唯一的缺点是如果文档被更新，分数将会改变，因为更新操作也会更新_seq_no字段的值。"],["body","\n"],["body","可以在不设置字段的情况下设置种子，但这已被弃用，因为这需要在 _id字段上加载fielddata，这会占用大量内存。"],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"function_score\": {\n      \"random_score\": {\n        \"seed\": 10,\n        \"field\": \"_seq_no\"\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","field-value-factor"],["heading","Field Value factor"],["body","\n"],["body","field_value_factor 函数允许您使用文档中的字段来影响分数。它类似于使用script_score函数，但是，它避免了脚本的开销。如果在多值字段上使用，则在计算中仅使用该字段的第一个值。\n例如，假设您有一个使用数值类型 my-int 字段索引的文档，并希望使用此字段影响文档的分数，这样做的示例如下所示:"],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"function_score\": {\n      \"field_value_factor\": {\n        \"field\": \"my-int\",\n        \"factor\": 1.2,\n        \"modifier\": \"sqrt\",\n        \"missing\": 1\n      }\n    }\n  }\n}\n"],["body","\n"],["body","这将转化为以下评分公式:"],["body","\n"],["body","sqrt(1.2 * doc['my-int'].value)\n"],["body","\n"],["headingLink","field_value_factor-函数选项"],["heading","field_value_factor 函数选项"],["body","\n"],["body","\n"],["body","field"],["body","Field to be extracted from the document."],["body","\n"],["body","factor"],["body","Optional factor to multiply the field value with, defaults to 1."],["body","\n"],["body","modifier"],["body","Modifier to apply to the field value, can be one of: none, log, log1p, log2p, ln, ln1p, ln2p, square, sqrt, or reciprocal. Defaults to none."],["body","\n\n\n"],["headingLink","字段值函数"],["heading","字段值函数"],["body","\n"],["body","Modifier"],["body","Meaning"],["body","\n"],["body","none"],["body","Do not apply any multiplier to the field value。 不应用任何函数"],["body","\n"],["body","log"],["body","Take the common logarithm of the field value. 由于此函数将返回负值，并且如果在0和1之间的值上使用时会导致错误，因此建议改用 “log1p”。"],["body","\n"],["body","log1p"],["body","Add 1 to the field value and take the common logarithm。 字段值+1然后取常用对数（10为底的对数）"],["body","\n"],["body","log2p"],["body","Add 2 to the field value and take the common logarithm。 字段值+2然后取常用对数（10为底的对数）"],["body","\n"],["body","ln"],["body","Take the natural logarithm of the field value. Because this function will return a negative value and cause an error if used on values between 0 and 1, it is recommended to use ln1p instead.  由于此函数将返回负值，并且如果在0和1之间的值上使用时会导致错误，"],["body","\n"],["body","ln1p"],["body","Add 1 to the field value and take the natural logarithm。"],["body","\n"],["body","ln2p"],["body","Add 2 to the field value and take the natural logarithm"],["body","\n"],["body","square"],["body","Square the field value (multiply it by itself)。取字段值的平方"],["body","\n"],["body","sqrt"],["body","Take the square root of the field value \"Square roots\"（平方根）是数学术语，指的是一个数的平方等于另一个数时，这个数就是另一个数的平方根。例如，4的平方根是2，因为2的平方等于4。在数学符号中，平方根通常用 \"√\" 表示，例如，√4 = 2。"],["body","\n"],["body","reciprocal"],["body","Reciprocate the field value, same as 1/x where x is the field’s value。\"Reciprocal\"（倒数）是数学术语，指的是一个数的倒数等于1除以这个数。例如，5的倒数是1/5，因为1除以5等于1/5。在数学符号中，倒数通常用 \"1/x\" 表示，其中x是你想要找到倒数的数。"],["body","\n\n\n"],["headingLink","missing"],["heading","missing"],["body","\n\n"],["body","如果文档没有该字段，则使用该值。修饰符和因子仍然应用于它，就像从文档中读取一样。"],["body","\n\n"],["headingLink","注意-1"],["heading","注意"],["body","\n\n"],["body","field_value_score函数产生的分数必须为非负数，否则将引发错误。如果在0和1之间的值上使用，log和ln修饰符将产生负值。请确保使用范围过滤器限制字段的值以避免这种情况，或者使用log1p和ln1p。"],["body","\n"],["body","请记住，取0的log() 或负数的平方根是非法操作，并且会引发异常。请确保使用范围过滤器限制字段的值以避免这种情况，或者使用log1p和ln1p。"],["body","\n\n"],["headingLink","decay-functions"],["heading","Decay functions"],["body","\n"],["body","衰减函数使用根据文档的数字字段值与用户给定原点的距离而衰减的函数对文档进行评分。这类似于范围查询，但使用平滑的边缘而不是框。\n要对具有数值字段的查询使用距离评分，用户必须为每个字段定义原点和刻度。需要原点来定义计算距离的 “中心点”，以及定义衰减率的比例。衰减函数指定为"],["body","\n"],["body","\"DECAY_FUNCTION\": { \n    \"FIELD_NAME\": { \n          \"origin\": \"11, 12\",\n          \"scale\": \"2km\",\n          \"offset\": \"0km\",\n          \"decay\": 0.33\n    }\n}\n"],["body","\n\n"],["body","The DECAY_FUNCTION should be one of linear,exp, or gauss."],["body","\n"],["body","指定的字段必须是数字、日期或地理点字段。numeric, date, or geo-point"],["body","\n\n"],["body","在以上示例中，字段是geo_point，并且原点可以以geo格式提供。在这种情况下，比例和偏移必须以单位给出。如果您的字段是日期字段，则可以将 “缩放” 和 “偏移” 设置为 “天” 、 “周” 等。示例:"],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"function_score\": {\n      \"gauss\": {\n        \"@timestamp\": {\n          \"origin\": \"2013-09-17\", \n          \"scale\": \"10d\",\n          \"offset\": \"5d\",         \n          \"decay\": 0.5            \n        }\n      }\n    }\n  }\n}\n"],["body","\n\n"],["body","The date format of the origin depends on the format defined in your mapping. If you do not define the origin, the current time is used."],["body","\n"],["body","The offset and decay parameters are optional."],["body","\n\n"],["body","\n"],["body","origin"],["body","The point of origin used for calculating distance. Must be given as a number for numeric field, date for date fields and geo point for geo fields. Required for geo and numeric field. For date fields the default is now. Date math (for example now-1h) is supported for origin.  衰减函数的起点，定义了不受衰减影响的理想点。在地理场景中，它通常表示一个经纬度坐标，如\"11, 12\"。对于日期字段，它可能是一个特定的时间戳或日期。  用于计算距离的原点。必须以数字字段的数字、日期字段的日期和地理字段的地理点的形式给出。对于地理和数字字段是必需的。对于日期字段，默认值为 “现在”。起源支持日期数学 (例如 “now-1h”)。"],["body","\n"],["body","scale"],["body","对于所有类型都是必需的。定义与原点偏移的距离，在该距离处计算的分数将等于 decay 参数 。对于geo字段: 可以定义为数字单位 (1千米，12m，…) 。默认单位为米。对于日期字段: 可以定义为数字单位 (“1h”，“10d”，…)。默认单位是毫秒。 定义了得分将从最大值下降到衰减阈值（由\"decay\"参数指定）的距离。例如，\"2km\"表示在距离起点2公里的地方，得分会衰减到最大得分的\"decay\"所指定的比例。该参数与字段类型相关，在地理位置中以距离表示，在日期字段中以时间表示。"],["body","\n"],["body","offset"],["body","指定了在计算得分之前应该忽略的初始距离或时间。在距离或时间小于或等于\"offset\"值的情况下，得分不会受到衰减影响，即得分保持最大。在上面的例子中，\"0km\"意味着没有偏移量，也就是从起点即开始衰减。"],["body","\n"],["body","decay"],["body","指定了在\"scale\"指定的位置时得分的衰减比例。在这个例子中，0.33表示在2公里之外，得分会衰减到最大得分的33%，而在更远距离上得分会进一步衰减。"],["body","\n\n\n"],["headingLink","达到衰减-的-scale-距离后-是如何衰减的"],["heading","达到衰减 的 scale 距离后 是如何衰减的"],["body","\n"],["body","超过这个距离后，得分会根据所选的衰减函数类型继续减少，但具体的衰减方式依赖于该函数的数学性质。我们来看看不同类型的衰减函数是如何在超过 \"scale\" 距离后进一步衰减得分的。"],["body","\n"],["headingLink","线性衰减linear-decay"],["heading","线性衰减（Linear Decay）"],["body","\n"],["body","衰减速率恒定"],["body","\n"],["body","线性衰减函数在超过\"scale\"参数定义的距离后继续以线性的速度降低得分。这意味着得分的降低速率是恒定的。一旦距离超过 \"scale\" 加 \"offset\" 后，分数会沿着一条直线减少，直到达到下限或继续无限接近于0（但实际应用中通常会设置一个下限）。"],["body","\n"],["headingLink","指数衰减exponential-decay"],["heading","指数衰减（Exponential Decay）"],["body","\n"],["body","指数衰减函数在超过\"scale\"距离后的得分降低速率将会加快。具体来说，得分的降低遵循指数函数，这意味着每个\"scale\"单位的距离，得分将进一步减少为原来的某个比例（\"decay\"参数指定的比例）。相比线性衰减，指数衰减在一开始下降较慢，但随着距离的增加，得分下降的速度会逐渐加快。"],["body","\n"],["headingLink","高斯衰减gaussian-decay"],["heading","高斯衰减（Gaussian Decay）"],["body","\n"],["body","高斯衰减（又称为正态衰减）提供了一种在超过\"scale\" 参数后得分下降的方式，其速率先加快后减慢，形状类似于正态分布的钟形曲线。在达到某个点之后，分数下降的速度开始减慢，因此与指数衰减相比，高斯衰减在距离非常远的时候对得分的影响较小。"],["body","\n"],["body","衰减示意图"],["body","\n"],["body","\n"],["headingLink","multi-values-fields"],["heading","Multi-values fields"],["body","\n"],["body","如果用于计算衰减的字段包含多个值，则根据默认值，选择最接近原点的值来确定距离。这可以通过设置multi_value_mode来更改。"],["body","\n"],["body","\n"],["body","min"],["body","Distance is the minimum distance"],["body","\n"],["body","max"],["body","Distance is the maximum distance"],["body","\n"],["body","avg"],["body","Distance is the average distance"],["body","\n"],["body","sum"],["body","Distance is the sum of all distances"],["body","\n\n\n"],["body","    \"DECAY_FUNCTION\": {\n        \"FIELD_NAME\": {\n              \"origin\": ...,\n              \"scale\": ...\n        },\n        \"multi_value_mode\": \"avg\"\n    }\n"],["body","\n"],["headingLink","案例"],["heading","案例"],["body","\n\n"],["body","\n"],["body","假设你正在寻找一个酒店在某个城镇。你的预算是有限的。此外，您希望酒店靠近市中心，因此酒店距离所需位置越远，入住的可能性就越小。"],["body","\n"],["body","\n"],["body","\n"],["body","您希望根据到市中心的距离以及价格对符合您的标准 (例如，“酒店，南希，非吸烟者”) 的查询结果进行评分。"],["body","\n"],["body","\n"],["body","\n"],["body","直观地，您想将市中心定义为原点，也许您愿意从酒店2千米步行到市中心。\n在这种情况下，您的位置字段的原点是镇中心，比例为〜2千米。\n如果你的预算很低，你可能更喜欢便宜的东西而不是昂贵的东西。对于价格字段，origin 为0欧元，scale 取决于您愿意支付的金额，例如20欧元。\n在该示例中，对于酒店的价格，字段可以被称为 price，对于该酒店的坐标，字段可以被称为 “location”。\n在这种情况下，"],["body","\n"],["body","\n\n"],["body","价格的函数是"],["body","\n"],["body","\"gauss\": { \n    \"price\": {\n          \"origin\": \"0\",\n          \"scale\": \"20\"\n    }\n}\n"],["body","\n"],["body","距离的函数是"],["body","\n"],["body","\"gauss\": { \n    \"location\": {\n          \"origin\": \"11, 12\",\n          \"scale\": \"2km\"\n    }\n}\n"],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"function_score\": {\n      \"functions\": [\n        {\n          \"gauss\": {\n            \"price\": {\n              \"origin\": \"0\",\n              \"scale\": \"20\"\n            }\n          }\n        },\n        {\n          \"gauss\": {\n            \"location\": {\n              \"origin\": \"11, 12\",\n              \"scale\": \"2km\"\n            }\n          }\n        }\n      ],\n      \"query\": {\n        \"match\": {\n          \"properties\": \"balcony\"\n        }\n      },\n      \"score_mode\": \"multiply\"\n    }\n  }\n}\n"],["body","\n"],["headingLink","高斯衰减图"],["heading","高斯衰减图"],["body","\n"],["body","当在上面的例子中选择高斯作为衰减函数时，乘数的等高线和曲面图如下所示:"],["body","\n"],["body","\n"],["body","\n"],["headingLink","线性衰减图"],["heading","线性衰减图"],["body","\n"],["body","\n"],["body","\n"],["headingLink","what-if-a-field-is-missing"],["heading","What if a field is missing?"],["body","\n"],["body","如果文档中缺少数字字段，则函数将返回1。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/queryDSL/CompoundQueries/1.BooleanQuery.html"],["title","BooleanQuery.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","boolean-query"],["heading","Boolean query"],["body","\n"],["body","与其他查询的布尔组合匹配的文档匹配的查询。bool查询映射到 Lucene BooleanQuery。它是使用一个或多个布尔子句构建的，每个子句都有一个  typed occurrence （查询条件（子句）在文档中出现的情况）。"],["body","\n"],["body","Occur"],["body","Description"],["body","\n"],["body","must"],["body","The clause (query) must appear in matching documents and will contribute to the score.  子句 (查询) 必须出现在匹配的文档中，并将有助于得分。"],["body","\n"],["body","filter"],["body","The clause (query) must appear in matching documents. However unlike must the score of the query will be ignored. Filter clauses are executed in filter context, meaning that scoring is ignored and clauses are considered for caching.  子句 (查询) 必须出现在匹配的文档中。然而，与 “must” 不同，查询的分数将被忽略。过滤子句在 [过滤上下文](https:// www.elastic.co/guide/en/elasticsearch/reference/8.13/query-filter-context.html) 中执行，这意味着忽略评分，并考虑缓存子句。"],["body","\n"],["body","should"],["body","The clause (query) should appear in the matching document。子句 (查询) 应出现在匹配的文档中。"],["body","\n"],["body","must_not"],["body","The clause (query) must not appear in the matching documents. Clauses are executed in filter context meaning that scoring is ignored and clauses are considered for caching. Because scoring is ignored, a score of 0 for all documents is returned. 子句 (查询) 不得出现在匹配的文档中，子句在 [filter context](https:// www.elastic.co/guide/en/elasticsearch/reference/8.13/query-filter-context.html) 中执行，这意味着忽略评分并考虑缓存子句 。由于忽略了评分，因此将返回所有文档的分数 “0”。"],["body","\n\n\n"],["body","bool查询采用  more-matches-is-best 方法，因此将每个匹配的 must或should子句的分数相加，以提供每个文档的最终 _score。"],["body","\n"],["body","POST _search\n{\n  \"query\": {\n    \"bool\" : {\n      \"must\" : {\n        \"term\" : { \"user.id\" : \"kimchy\" }\n      },\n      \"filter\": {\n        \"term\" : { \"tags\" : \"production\" }\n      },\n      \"must_not\" : {\n        \"range\" : {\n          \"age\" : { \"gte\" : 10, \"lte\" : 20 }\n        }\n      },\n      \"should\" : [\n        { \"term\" : { \"tags\" : \"env1\" } },\n        { \"term\" : { \"tags\" : \"deployed\" } }\n      ],\n      \"minimum_should_match\" : 1,\n      \"boost\" : 1.0\n    }\n  }\n}\n"],["body","\n"],["headingLink","using-minimum_should_match"],["heading","Using minimum_should_match"],["body","\n"],["body","在filter元素下指定的查询对scoring没有影响。scores返回为0。分数仅受已指定的查询影响。例如，以下所有三个查询都返回状态字段包含术语 “活动” 的所有文档。"],["body","\n"],["body","分数仅受已指定的查询影响。例如，以下所有三个查询都返回状态字段包含term  活动 的所有文档。"],["body","\n"],["body","GET _search\n{\n  \"query\": {\n    \"bool\": {\n      \"filter\": {\n        \"term\": {\n          \"status\": \"active\"\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","这个bool查询有一个match_all查询，它为所有文档分配1.0分。"],["body","\n"],["body","GET _search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": {\n        \"match_all\": {}\n      },\n      \"filter\": {\n        \"term\": {\n          \"status\": \"active\"\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","此constant_score 查询的行为方式与上面的第二个示例完全相同。constant_score查询将1.0的分数分配给由过滤器匹配的所有文档。"],["body","\n"],["body","GET _search\n{\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"term\": {\n          \"status\": \"active\"\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","named-queries"],["heading","Named queries"],["body","\n"],["body","每个查询在其顶级定义中接受一个 _name。您可以使用命名查询来跟踪与返回的文档匹配的查询。如果使用命名查询，则响应包括每个命中的matched_queries属性。"],["body","\n"],["body","在同一请求中提供重复的 _name值会导致未定义的行为。具有重复名称的查询可能会相互覆盖。假定查询名称在单个请求中是唯一的。"],["body","\n"],["body","GET /_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        { \"match\": { \"name.first\": { \"query\": \"shay\", \"_name\": \"first\" } } },\n        { \"match\": { \"name.last\": { \"query\": \"banon\", \"_name\": \"last\" } } }\n      ],\n      \"filter\": {\n        \"terms\": {\n          \"name.last\": [ \"banon\", \"kimchy\" ],\n          \"_name\": \"test\"\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","名为include_named_queries_score的请求参数控制是否返回与匹配查询关联的分数。设置后，响应将包含一个matched_queries映射，其中包含作为键匹配的查询的名称及其关联的分数作为值。"],["body","\n"],["body","注意："],["body","\n"],["body","请注意，分数可能对文档的最终分数没有贡献，例如出现在filter或must_not上下文中的命名查询，或者出现在忽略或修改分数的子句 (如constant_score或function_score_query) 内。"],["body","\n"],["body","GET /_search?include_named_queries_score\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        { \"match\": { \"name.first\": { \"query\": \"shay\", \"_name\": \"first\" } } },\n        { \"match\": { \"name.last\": { \"query\": \"banon\", \"_name\": \"last\" } } }\n      ],\n      \"filter\": {\n        \"terms\": {\n          \"name.last\": [ \"banon\", \"kimchy\" ],\n          \"_name\": \"test\"\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","此功能会在搜索响应中的每次命中时重新运行每个命名查询。通常，这会给请求增加少量开销。"],["body","\n"],["body","然而，对大量命中使用计算上昂贵的命名查询可能会增加显著的开销。例如，命名查询与许多存储桶上的top_hits聚合相结合可能会导致更长的响应时间。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/queryDSL/1.QueryAndFilterContext.html"],["title","QueryAndFilterContext.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","query-and-filter-contextquery-and-filter-context"],["heading","Query and filter contextQuery and filter context"],["body","\n"],["headingLink","relevance-scores"],["heading","Relevance scores"],["body","\n"],["body","默认情况下，Elasticsearch按相关性得分对匹配的搜索结果进行排序，该得分衡量每个文档与查询的匹配程度。\n相关性分数是一个正的浮点数，在搜索API的 score元数据字段中返回。 分数越高，文档越相关。虽然每种查询类型可以不同地计算相关性分数，但分数计算还取决于查询子句是在  查询还是过滤器  上下文中运行。"],["body","\n"],["headingLink","query-context"],["heading","Query context"],["body","\n"],["body","在 Query context  中，查询子句回答以下问题: “此文档与该查询子句匹配的程度如何？”"],["body","\n"],["body","除了确定文档是否匹配外，查询子句还计算 _score元数据字段中的相关性分数。"],["body","\n"],["body","只要将 查询子句  传递给  查询 参数 (例如搜索API中( search API)的查询参数)，查询上下文就会生效。"],["body","\n"],["headingLink","filter-context"],["heading","Filter context"],["body","\n"],["body","在过滤器上下文中，查询子句回答以下问题: “此文档是否与此查询子句匹配？”"],["body","\n"],["body","答案是简单的是或否，不会计算分数。过滤器上下文主要用于过滤结构化数据，例如"],["body","\n\n"],["body","Does this timestamp fall into the range 2015 to 2016?"],["body","\n"],["body","Is the status field set to \"published\"?"],["body","\n\n"],["body","常用的过滤器将由Elasticsearch自动缓存，以加快性能。"],["body","\n"],["body","每当将查询子句传递给 filter参数 (例如bool查询中的  filter or must_not ， constant_score  查询中的过滤器参数或 filter aggregation ) 时，过滤器上下文就会生效。"],["body","\n"],["headingLink","example-of-query-and-filter-contexts"],["heading","Example of query and filter contexts"],["body","\n"],["body","Below is an example of query clauses being used in query and filter context in the search API. This query will match documents where all of the following conditions are met:"],["body","\n"],["body","下面是在搜索API中的查询和过滤上下文中使用的查询子句的示例。此查询将匹配满足以下所有条件的文档:"],["body","\n\n"],["body","The title field contains the word search."],["body","\n"],["body","The content field contains the word elasticsearch."],["body","\n"],["body","The status field contains the exact word published."],["body","\n"],["body","The publish_date field contains a date from 1 Jan 2015 onwards."],["body","\n\n"],["body","GET /_search\n{\n  \"query\": { //The query parameter indicates query context.\n    \"bool\": { //The `bool` and two `match` clauses are used in query context, which means that they are used to score how well each document matches.\n      \"must\": [\n        { \"match\": { \"title\":   \"Search\"        }},\n        { \"match\": { \"content\": \"Elasticsearch\" }}\n      ], // The `filter` parameter indicates filter context. Its `term` and `range` clauses are used in filter context. They will filter out documents which do not match, but they will not affect the score for matching documents.\n      \"filter\": [ \n        { \"term\":  { \"status\": \"published\" }},\n        { \"range\": { \"publish_date\": { \"gte\": \"2015-01-01\" }}}\n      ]\n    }\n  }\n}\n"],["body","\n"],["body","在查询上下文中为查询计算的分数表示为单精度浮点数; 对于有效度，它们只有24位。超过显着性精度的分数计算将被转换为精度损失的浮点数。"],["body","\n"],["body","在过滤上下文使用 精确匹配以此获取 精确匹配的值"],["body","\n"],["body","在查询上下文使用的条件应为 影响文档得分的条件"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/深入搜索.html"],["title","深入搜索 - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/搜索引擎中的概念杂项.html"],["title","搜索引擎中的概念杂项.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","分词算法设计中的几个基本原则"],["heading","分词算法设计中的几个基本原则"],["body","\n"],["headingLink","颗粒度越大越好"],["heading","颗粒度越大越好"],["body","\n\n"],["body","\n"],["body","用于进行语义分析的文本分词，要求分词结果的颗粒度越大，即单词的字数越多，所能表示的含义越确切如：“公安局长”可以分为“公安 局长”、“公安局 长”、“公安局长”都算对"],["body","\n"],["body","\n"],["body","\n"],["body","但是要用于语义分析，则“公安局长”的分词结果最好（当然前提是所使用的词典中有这个词）"],["body","\n"],["body","\n\n"],["headingLink","切分结果中非词典词越少越好单字字典词数越少越好"],["heading","切分结果中非词典词越少越好单字字典词数越少越好"],["body","\n\n"],["body","\n"],["body","这里的“非词典词”就是不包含在词典中的单字，而“单字字典词”指的是可以独立运用的单字，如“的”、“了”、“和”、“你”、“我”、“他”。"],["body","\n"],["body","\n"],["body","\n"],["body","例如：“技术和服务”，可以分为“技术 和服 务”以及“技术 和 服务”，但“务”字无法独立成词（即词典中没有），"],["body","\n"],["body","\n"],["body","\n"],["body","但“和”字可以单独成词（词典中要包含），因此“技术 和服 务”有1个非词典词，而“技术 和 服务”有0个非词典词，因此选用后者。"],["body","\n"],["body","\n"],["body","\n"],["body","总体词数越少越好，在相同字数的情况下，总词数越少，说明语义单元越少，那么相对的单个语义单元的权重会越大，因此准确性会越高。"],["body","\n"],["body","\n\n"],["body","下面详细说说正向最大匹配法、逆向最大匹配法和双向最大匹配法具体是如何进行的："],["body","\n"],["headingLink","最大匹配法"],["heading","最大匹配法"],["body","\n"],["headingLink","正向"],["heading","正向"],["body","\n\n"],["body","\n"],["body","最大匹配是指以词典为依据，取词典中最长单词为第一个次取字数量的扫描串，在词典中进行扫描"],["body","\n"],["body","\n"],["body","\n"],["body","例如：词典中最长词为\"今天是个星期日\"共7个汉字，则最大匹配起始字数为7个汉字。然后逐字递减，在对应的词典中进行查找。"],["body","\n"],["body","\n\n"],["body","下面以“我们在野生动物园玩”详细说明一下这几种匹配方法："],["body","\n"],["body","正向即从前往后取词，从7->1，每次减一个字，直到词典命中或剩下1个单字。\n第1次：“我们在野生动物”，扫描7字词典，无\n第2次：“我们在野生动”，扫描6字词典，无\n。。。\n第6次：“我们”，扫描2字词典，有\n扫描中止，输出第1个词为“我们”，去除第1个词后开始第2轮扫描，即：\n\n第2轮扫描\n第1次：“在野生动物园玩”，扫描7字词典，无\n第2次：“在野生动物园”，扫描6字词典，无\n。。。。\n第6次：“在野”，扫描2字词典，有\n扫描中止，输出第2个词为“在野”，去除第2个词后开始第3轮扫描，即：\n\n第3轮扫描：\n第1次：“生动物园玩”，扫描5字词典，无\n第2次：“生动物园”，扫描4字词典，无\n第3次：“生动物”，扫描3字词典，无\n第4次：“生动”，扫描2字词典，有\n扫描中止，输出第3个词为“生动”，第4轮扫描，即：\n\n第4轮扫描：\n第1次：“物园玩”，扫描3字词典，无\n第2次：“物园”，扫描2字词典，无\n第3次：“物”，扫描1字词典，无\n扫描中止，输出第4个词为“物”，非字典词数加1，开始第5轮扫描，即：\n\n第5轮扫描：\n第1次：“园玩”，扫描2字词典，无\n第2次：“园”，扫描1字词典，有\n扫描中止，输出第5个词为“园”，单字字典词数加1，开始第6轮扫描，即：\n\n第6轮扫描：\n第1次：“玩”，扫描1字字典词，有\n扫描中止，输出第6个词为“玩”，单字字典词数加1，整体扫描结束。\n\n正向最大匹配法，最终切分结果为：“我们/在野/生动/物/园/玩”，其中，单字字典词为2，非词典词为1。\n"],["body","\n"],["body","逆向"],["body","\n"],["body","逆向即从后往前取词，其他逻辑和正向相同。即：\n第1轮扫描：“在野生动物园玩”\n第1次：“在野生动物园玩”，扫描7字词典，无\n第2次：“野生动物园玩”，扫描6字词典，无\n。。。。\n第7次：“玩”，扫描1字词典，有\n扫描中止，输出“玩”，单字字典词加1，开始第2轮扫描\n\n第2轮扫描：“们在野生动物园”\n第1次：“们在野生动物园”，扫描7字词典，无\n第2次：“在野生动物园”，扫描6字词典，无\n第3次：“野生动物园”，扫描5字词典，有\n扫描中止，输出“野生动物园”，开始第3轮扫描\n\n第3轮扫描：“我们在”\n第1次：“我们在”，扫描3字词典，无\n第2次：“们在”，扫描2字词典，无\n第3次：“在”，扫描1字词典，有\n扫描中止，输出“在”，单字字典词加1，开始第4轮扫描\n\n第4轮扫描：“我们”\n第1次：“我们”，扫描2字词典，有\n扫描中止，输出“我们”，整体扫描结束。\n\n逆向最大匹配法，最终切分结果为：“我们/在/野生动物园/玩”，其中，单字字典词为2，非词典词为0。\n"],["body","\n"],["headingLink","双向最大匹配法"],["heading","双向最大匹配法"],["body","\n"],["body","正向最大匹配法和逆向最大匹配法，都有其局限性，我举得例子是正向最大匹配法局限性的例子，逆向也同样存在（如：长春药店，逆向切分为“长/春药店”），因此有人又提出了双向最大匹配法，双向最大匹配法。即，两种算法都切一遍，然后根据大颗粒度词越多越好，非词典词和单字词越少越好的原则，选取其中一种分词结果输出。"],["body","\n"],["body","如：“我们在野生动物园玩”\n\n正向最大匹配法，最终切分结果为：“我们/在野/生动/物/园/玩”，其中，两字词3个，单字字典词为2，非词典词为1。\n\n逆向最大匹配法，最终切分结果为：“我们/在/野生动物园/玩”，其中，五字词1个，两字词1个，单字字典词为2，非词典词为0。\n\n非字典词：正向(1)>逆向(0)（越少越好）\n单字字典词：正向(2)=逆向(2)（越少越好）\n总词数：正向(6)>逆向(4)（越少越好）\n\n因此最终输出为逆向结果。\n"],["body","\n"],["headingLink","语言模型中unigrambigramtrigram的概念中unigrambigramtrigram的概念"],["heading","语言模型中unigram、bigram、trigram的概念中unigram、bigram、trigram的概念"],["body","\n"],["body","unigram 一元分词，把句子分成一个一个的汉字\nbigram 二元分词，把句子从头到尾每两个字组成一个词语\ntrigram 三元分词，把句子从头到尾每三个字组成一个词语."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/scroll查询.html"],["title","scroll查询.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","游标查询-scroll"],["heading","游标查询 Scroll"],["body","\n"],["body","scroll 查询 可以用来对 Elasticsearch 有效地执行大批量的文档查询，而又不用付出深度分页那种代价。"],["body","\n"],["body","游标查询允许我们 先做查询初始化，然后再批量地拉取结果。 这有点儿像传统数据库中的 cursor 。"],["body","\n"],["body","游标查询会取某个时间点的快照数据。 查询初始化之后索引上的任何变化会被它忽略。 它通过保存旧的数据文件来实现这个特性，结果就像保留初始化时的索引 视图 一样。"],["body","\n"],["body","深度分页的代价根源是结果集全局排序，如果去掉全局排序的特性的话查询结果的成本就会很低。 游标查询用字段 _doc 来排序。 这个指令让 Elasticsearch 仅仅从还有结果的分片返回下一批结果。"],["body","\n"],["body","启用游标查询可以通过在查询的时候设置参数 scroll 的值为我们期望的游标查询的过期时间。"],["body","\n"],["body","游标查询的过期时间会在每次做查询的时候刷新，所以这个时间只需要足够处理当前批的结果就可以了，而不是处理查询结果的所有文档的所需时间"],["body","\n"],["body","这个过期时间的参数很重要，因为保持这个游标查询窗口需要消耗资源，所以我们期望如果不再需要维护这种资源就该早点儿释放掉。"],["body","\n"],["body","设置这个超时能够让 Elasticsearch 在稍后空闲的时候自动释放这部分资源。"],["body","\n"],["body","GET /old_index/_search?scroll=1m \n{\n    \"query\": { \"match_all\": {}},\n    \"sort\" : [\"_doc\"], \n    \"size\":  1000\n}\n"],["body","\n"],["body","保持游标查询窗口一分钟。"],["body","\n"],["body","关键字 _doc 是最有效的排序顺序。"],["body","\n"],["body","这个查询的返回结果包括一个字段 _scroll_id， 它是一个base64编码的长字符串 。 现在我们能传递字段 _scroll_id 到 _search/scroll 查询接口获取下一批结果："],["body","\n"],["body","GET /_search/scroll\n{\n    \"scroll\": \"1m\", \n    \"scroll_id\" : \"cXVlcnlUaGVuRmV0Y2g7NTsxMDk5NDpkUmpiR2FjOFNhNnlCM1ZDMWpWYnRROzEwOTk1OmRSamJHYWM4U2E2eUIzVkMxalZidFE7MTA5OTM6ZFJqYkdhYzhTYTZ5QjNWQzFqVmJ0UTsxMTE5MDpBVUtwN2lxc1FLZV8yRGVjWlI2QUVBOzEwOTk2OmRSamJHYWM4U2E2eUIzVkMxalZidFE7MDs=\"\n}\n"],["body","\n"],["body","这个游标查询返回的下一批结果。 尽管我们指定字段 size 的值为1000，我们有可能取到超过这个值数量的文档。 当查询的时候， 字段 size 作用于单个分片，所以每个批次实际返回的文档数量最大为 size * number_of_primary_shards 。"],["body","\n"],["body","注意游标查询每次返回一个新字段 _scroll_id。每次我们做下一次游标查询， 我们必须把前一次查询返回的字段 _scroll_id 传递进去。 当没有更多的结果返回的时候，我们就处理完所有匹配的文档了。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/README.html"],["title","elasticSearch - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","索引"],["heading","索引"],["body","\n"],["body","key value key是某种属性，value是文档列表"],["body","\n"],["body","正排索引"],["body","\n"],["body","根据文档相关的属性形成的key。例如 文档ID、文档创建时间等等"],["body","\n"],["body","倒排索引"],["body","\n"],["body","key为 文档内容通过分词形成的单词"],["body","\n"],["body","倒排列表"],["body","\n"],["body","包含出现过某个单词的所有文档列表、出现的位置信息、以及出现的词频"],["body","\n"],["headingLink","相关度"],["heading","相关度"],["body","\n"],["body","衡量某个文档与查询的匹配程度"],["body","\n"],["body","TF-IDF"],["body","\n"],["body","词频：某个单词在某篇文章的出现次数，词频与相关度正相关"],["body","\n"],["body","逆文档频率：某个单词在整个 文档库出现的次数，逆文档频率与相关度逆相关"],["body","\n"],["body","BM-25"],["body","\n"],["body","基于"],["body","\n"],["headingLink","搜索引擎的三个核心指标"],["heading","搜索引擎的三个核心指标"],["body","\n"],["headingLink","更全"],["heading","更全"],["body","\n"],["headingLink","更准"],["heading","更准"],["body","\n"],["headingLink","更块"],["heading","更块"],["body","\n"],["headingLink","搜索引擎的三个核心问题"],["heading","搜索引擎的三个核心问题"],["body","\n"],["body","用户的真正需求是什么"],["body","\n"],["body","哪些信息是和用户的需求真正相关的"],["body","\n"],["body","哪些信息是用户可以信赖的"],["body","\n"],["headingLink","评价搜索引擎的指标"],["heading","评价搜索引擎的指标"],["body","\n"],["body","在搜索结果中"],["body","不在搜索结果中"],["body","\n"],["body","相关"],["body","N"],["body","K"],["body","\n"],["body","不相关"],["body","M"],["body","L"],["body","\n\n\n"],["body","精准率 = N/(N+M)"],["body","\n"],["body","召回率=N/(N+K)"],["body","\n"],["body","针对搜索引擎的业务场景：精准率更为重要"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/2.Concepts/README.html"],["title","Concepts - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","ilm-concepts"],["heading","ILM concepts"],["body","\n\n"],["body","Index lifecycle"],["body","\n"],["body","Rollover"],["body","\n"],["body","Policy updates"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/2.Concepts/3.LifeCyclePolicyUpdates.html"],["title","LifeCyclePolicyUpdates.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","lifecycle-policy-updates"],["heading","Lifecycle policy updates"],["body","\n"],["body","您可以通过修改当前策略或切换到其他策略来更改索引或滚动索引集合的生命周期管理方式。"],["body","\n"],["body","为了确保策略更新不会将索引置于无法退出当前阶段的状态，阶段定义在进入阶段时会缓存在索引元数据中。如果可以安全地应用更改，则ILM会更新缓存的阶段定义。如果不能，则使用缓存的定义继续执行阶段。"],["body","\n"],["body","当索引前进到下一阶段时，它将使用更新策略中的阶段定义。"],["body","\n"],["headingLink","how-changes-are-applied"],["heading","How changes are applied"],["body","\n"],["body","当策略最初应用于索引时，索引将获取策略的最新版本。如果更新策略，则策略版本会被碰撞，并且ILM可以检测到索引使用的是需要更新的早期版本。"],["body","\n"],["body","对min_age的更改不会传播到缓存的定义。更改阶段的min_age不会影响当前正在执行该阶段的索引。"],["body","\n"],["body","例如，如果您创建的策略具有未指定min_age的热阶段，则在应用该策略时，索引立即进入热阶段。如果然后更新策略以为热阶段指定1天的min_age，则对已经处于热阶段的索引没有影响。策略更新后创建的索引在一天之前不会进入热阶段。"],["body","\n"],["headingLink","how-new-policies-are-applied"],["heading","How new policies are applied"],["body","\n"],["body","当您将不同的策略应用于托管索引时，索引将使用先前策略中的缓存定义完成当前阶段。当索引进入下一阶段时，它开始使用新策略。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/2.Concepts/1.IndexLifecycle.html"],["title","IndexLifecycle.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","index-lifecycle"],["heading","Index lifecycle"],["body","\n"],["body","ILM定义了五个索引生命周期阶段"],["body","\n\n"],["body","Hot: 该索引正在积极更新和查询。."],["body","\n"],["body","Warm: 索引不再更新，但仍在查询中."],["body","\n"],["body","Cold: 该索引不再被更新，并且很少被查询。信息仍然需要搜索，但是如果这些查询速度较慢，也可以。."],["body","\n"],["body","Frozen: 该索引不再被更新，并且很少被查询。信息仍然需要搜索，但是如果这些查询非常慢也可以."],["body","\n"],["body","Delete: 该索引不再需要，可以安全地删除。"],["body","\n\n"],["body","索引的生命周期策略指定了哪些阶段适用，在每个阶段中执行了哪些操作以及何时在阶段之间过渡。"],["body","\n"],["body","您可以在创建索引时手动应用生命周期策略"],["body","\n"],["body","对于时间序列索引，您需要将生命周期策略与用于在序列中创建新索引的索引模板相关联。"],["body","\n"],["body","当索引滚动时，手动应用的策略不会自动应用于新索引"],["body","\n"],["body","如果您使用Elasticsearch的安全功能，ILM仅具有在上次策略更新时分配给用户的 roles 。"],["body","\n"],["headingLink","phase-transitions"],["heading","Phase transitions"],["body","\n"],["body","\n"],["body","阶段过渡"],["body","\n"],["body","\n"],["body","ILM根据索引在整个生命周期中的年龄 移动索引，为了控制这些转换的时间，您可以为每个阶段设置一个最小年龄。"],["body","\n"],["body","为了使索引移至下一阶段，当前阶段中的所有操作必须完成，并且索引必须晚于下一阶段的最小年龄。"],["body","\n"],["body","配置的最小年龄必须在后续阶段之间递增，例如，最小年龄为10天的 warm 阶段之后只能是最小年龄为未设置或> = 10天的 cold 阶段。"],["body","\n"],["body","最小年龄默认为零，这会导致ILM在当前阶段的所有操作完成后立即将索引移至下一阶段"],["body","\n"],["body","如果索引具有未分配的分片，并且群集运行状况( cluster health status i)为黄色，则该索引仍可以根据其索引生命周期管理策略过渡到下一阶段"],["body","\n"],["body","但是，由于Elasticsearch只能在绿色集群上执行某些清理任务，因此可能会产生意外的副作用，为了避免增加磁盘使用率和可靠性问题，请及时解决任何群集运行状况问题。"],["body","\n"],["headingLink","phase-execution"],["heading","Phase execution"],["body","\n"],["body","ILM控制执行一个阶段中的动作的顺序以及执行哪些步骤来执行每个动作的必要索引操作。"],["body","\n"],["body","当索引进入阶段时，ILM会在索引元数据中缓存阶段定义，这样可以确保策略更新不会将索引置于永远无法退出阶段的状态。"],["body","\n"],["body","如果可以安全地应用更改，则ILM会更新缓存的阶段定义。如果不能，则使用缓存的定义继续执行阶段。"],["body","\n"],["body","ILM定期运行，检查索引是否符合策略标准，并执行所需的任何步骤。"],["body","\n"],["body","为了避免竞争条件，ILM可能需要运行多次才能执行完成操作所需的所有步骤。"],["body","\n"],["body","例如，如果ILM确定索引已满足rollover标准，则它开始执行完成展期操作所需的步骤。"],["body","\n"],["body","如果它达到了  前进到下一步不安全 的地步，则执行停止"],["body","\n"],["body","下次ILM运行时，ILM在停止执行的地方开始执行，这意味着，即使将indices.lifecycle.poll_interval设置为10分钟，并且索引满足翻转标准，也可能需要20分钟才能完成翻转。"],["body","\n"],["headingLink","phase-actions"],["heading","Phase actions"],["body","\n"],["body","ILM在每个阶段都支持以下操作。ILM按列出的顺序执行操作。"],["body","\n\n"],["body","Hot\n\n"],["body","Set Priority"],["body","\n"],["body","Unfollow"],["body","\n"],["body","Rollover"],["body","\n"],["body","Read-Only"],["body","\n"],["body","Shrink"],["body","\n"],["body","Force Merge"],["body","\n"],["body","Searchable Snapshot"],["body","\n\n"],["body","\n"],["body","Warm\n\n"],["body","Set Priority"],["body","\n"],["body","Unfollow"],["body","\n"],["body","Read-Only"],["body","\n"],["body","Allocate"],["body","\n"],["body","Migrate"],["body","\n"],["body","Shrink"],["body","\n"],["body","Force Merge"],["body","\n\n"],["body","\n"],["body","Cold\n\n"],["body","Set Priority"],["body","\n"],["body","Unfollow"],["body","\n"],["body","Read-Only"],["body","\n"],["body","Searchable Snapshot"],["body","\n"],["body","Allocate"],["body","\n"],["body","Migrate"],["body","\n"],["body","Freeze"],["body","\n\n"],["body","\n"],["body","Frozen\n\n"],["body","Searchable Snapshot"],["body","\n\n"],["body","\n"],["body","Delete\n\n"],["body","Wait For Snapshot"],["body","\n"],["body","Delete"],["body","\n\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/2.Concepts/2.Rollover.html"],["title","Rollover.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","rollover"],["heading","Rollover"],["body","\n"],["body","在索引日志或指标等时间序列数据时，您不能无限期地写入单个索引。为了满足索引和搜索性能要求并管理资源使用情况，您可以写入索引，直到满足某个阈值，然后创建一个新索引并开始写入索引。使用滚动索引使您能够:"],["body","\n\n"],["body","优化高性能热节点高摄取率的活跃指数"],["body","\n"],["body","优化暖节点上的搜索性能"],["body","\n"],["body","将较旧、访问频率较低的数据转移到较便宜的冷节点，"],["body","\n"],["body","通过删除整个索引，根据您的保留策略删除数据。"],["body","\n\n"],["body","我们建议使用数据流（ data streams ）来管理时间序列数据。数据流自动跟踪写索引，同时保持配置最小化"],["body","\n"],["body","每个数据流都需要一个索引模板（ index template ），其中包含:"],["body","\n\n"],["body","数据流的名称或通配符 (*) 模式。"],["body","\n"],["body","数据流的时间戳字段。此字段必须映射为 date or date_nanos  数据类型，并且必须包含在索引到数据流的每个文档中。"],["body","\n"],["body","创建每个后备索引时应用的映射和设置。"],["body","\n\n"],["body","数据流是为仅追加数据而设计的，其中数据流名称可以用作操作 (读，写，翻转，收缩等) 的目标。如果您的用例需要适当地更新数据，则可以使用索引别名（ index aliases.）来管理时间序列数据。但是，还有一些配置步骤和概念:"],["body","\n\n"],["body","一个索引模板，用于指定系列中每个新索引的设置。您可以优化此配置以进行摄取，通常使用与热节点一样多的分片。"],["body","\n"],["body","引用整个索引集的索引别名。"],["body","\n"],["body","指定为写入索引的单个索引。这是处理所有写请求的活动索引。在每次rollover时，新索引成为写索引。"],["body","\n\n"],["headingLink","automatic-rollover"],["heading","Automatic rollover"],["body","\n"],["body","ILM使您能够根据索引大小、文档计数或使用年限自动滚动到新索引。"],["body","\n"],["body","触发翻转时，将创建一个新索引，更新写别名以指向新索引，并将所有后续更新写入新索引。"],["body","\n"],["body","根据大小，文档数量或年龄将其滚动到新索引，比基于时间的滚动更可取。在任意时间滚动通常会导致许多小索引，这可能会对性能和资源使用产生负面影响。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/1.Overview/README.html"],["title","Overview - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","ilm-overview"],["heading","ILM overview"],["body","\n"],["body","您可以创建和应用索引生命周期管理 (ILM) 策略，以根据您的性能、弹性和保留要求自动管理索引。\n索引生命周期策略可以触发以下操作:"],["body","\n\n"],["body","Rollover: 当当前索引达到一定大小，文档数量或年龄时，创建一个新的写入索引。."],["body","\n"],["body","Shrink: 减少索引中的主分片数量。"],["body","\n"],["body","Force merge: 触发 force merge 减少分片中段的数量."],["body","\n"],["body","Freeze: Freezes an index and makes it read-only."],["body","\n"],["body","Delete: Permanently remove an index, including all of its data and metadata."],["body","\n\n"],["body","ILM使用 hot-warm-cold架构管理索引，这在处理日志和指标等时间序列数据时很常见。"],["body","\n"],["body","您可以指定:"],["body","\n\n"],["body","要滚动到新索引的最大分片大小，文档数量或年龄。"],["body","\n"],["body","索引不再更新的点，并且可以减少主分片的数量。"],["body","\n"],["body","何时强制合并以永久删除标记为删除的文档。"],["body","\n"],["body","可以将索引移动到性能较低的硬件的点。"],["body","\n"],["body","可用性不是那么关键，并且可以减少副本数量的点。"],["body","\n"],["body","当可以安全地删除索引时。"],["body","\n\n"],["body","例如，如果要将atm机组中的指标数据索引到Elasticsearch中，则可能会定义一个策略，该策略显示:"],["body","\n\n"],["body","当索引的主分片的总大小达到50gb时，滚动到一个新的索引。"],["body","\n"],["body","将旧索引移至暖阶段，将其标记为只读，然后将其缩小为单个分片。"],["body","\n"],["body","7天后，将索引移至冷阶段，然后将其移至较便宜的硬件。"],["body","\n"],["body","一旦达到所需的30天保留期，请删除索引。"],["body","\n\n"],["headingLink","注意"],["heading","注意"],["body","\n"],["body","要使用ILM，集群中的所有节点都必须运行相同的版本。尽管可能可以在混合版本的集群中创建和应用策略，但不能保证它们能够按预期工作。"],["body","\n"],["body","尝试使用包含群集中所有节点都不支持的操作的策略将导致错误。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/README.html"],["title","ILM - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","ilm-manage-the-index-lifecycle"],["heading","ILM: Manage the index lifecycle"],["body","\n"],["body","您可以配置索引生命周期管理 (ILM) 策略，根据您的性能、弹性和保留要求自动管理索引。例如，您可以使用ILM来:"],["body","\n\n"],["body","当索引达到一定大小或文档数量时，启动新索引"],["body","\n"],["body","每天、每周或每月创建一个新索引，并存档以前的索引"],["body","\n"],["body","删除陈旧索引以强制执行数据保留标准"],["body","\n\n\n"],["body","\n"],["body","您可以通过Kibana Management或ILM api创建和管理索引生命周期策略。为Beats或Logstash Elasticsearch输出插件启用索引生命周期管理时，会自动配置默认策略。"],["body","\n"],["body","\n"],["body","\n"],["body","要自动备份索引和管理快照， use snapshot lifecycle policies."],["body","\n"],["body","\n\n\n"],["body","Overview"],["body","\n"],["body","Concepts"],["body","\n"],["body","Automate rollover"],["body","\n"],["body","Customize built-in ILM policies"],["body","\n"],["body","Configure a lifecycle policy"],["body","\n"],["body","Migrate index allocation filters to node roles"],["body","\n"],["body","Troubleshooting index lifecycle management errors"],["body","\n"],["body","Start and stop index lifecycle management"],["body","\n"],["body","Manage existing indices"],["body","\n"],["body","Skip rollover"],["body","\n"],["body","Restore a managed data stream or index"],["body","\n"],["body","Index lifecycle management APIs"],["body","\n"],["body","Index lifecycle actions"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/8.TroubleshootingIndexLifecycleManagementErrors/README.html"],["title","TroubleshootingIndexLifecycleManagementErrors - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","troubleshooting-index-lifecycle-management-errors"],["heading","Troubleshooting index lifecycle management errors"],["body","\n"],["body","当ILM执行生命周期策略时，在执行步骤的必要索引操作时可能会发生错误。发生这种情况时，ILM将索引移至 ERROR 步骤。如果ILM无法自动解决错误，则将停止执行，直到您解决策略，索引或群集的基本问题为止。"],["body","\n"],["body","例如，您可能有一个 shrink-index 策略，一旦索引至少存在五天，它就会将索引缩减为四个分片:"],["body","\n"],["body","PUT _ilm/policy/shrink-index\n{\n  \"policy\": {\n    \"phases\": {\n      \"warm\": {\n        \"min_age\": \"5d\",\n        \"actions\": {\n          \"shrink\": {\n            \"number_of_shards\": 4\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","没有什么可以阻止 shrink-index 策略应用于只有两个分片的新索引:"],["body","\n"],["body","PUT /my-index-000001\n{\n  \"settings\": {\n    \"index.number_of_shards\": 2,\n    \"index.lifecycle.name\": \"shrink-index\"\n  }\n}\n"],["body","\n"],["body","五天后，ILM试图将 my-index-000001 从两个碎片缩小到四个碎片。"],["body","\n"],["body","因为收缩动作不能增加分片的数量，所以此操作失败，并且ILM将 “my-index-000001” 移动到 “ERROR” 步骤"],["body","\n"],["body","使用 ILM Explain API 要获取有关哪里出了问题的信息:"],["body","\n"],["body","GET /my-index-000001/_ilm/explain\n"],["body","\n"],["body","{\n  \"indices\" : {\n    \"my-index-000001\" : {\n      \"index\" : \"my-index-000001\",\n      \"managed\" : true,\n      //策略\n      \"policy\" : \"shrink-index\",                \n      \n      \"lifecycle_date_millis\" : 1541717265865,\n      //索引年龄\n      \"age\": \"5.1d\",     \n      //索引阶段                       \n      \"phase\" : \"warm\",                         \n      \"phase_time_millis\" : 1541717272601,\n\t  //当前Action\n      \"action\" : \"shrink\",                      \n      \"action_time_millis\" : 1541717272601,\n      //当前步骤\n      \"step\" : \"ERROR\",                         \n      \"step_time_millis\" : 1541717272688,\n      //失败的步骤\n      \"failed_step\" : \"shrink\",                 \n      \"step_info\" : {\n      //error类型\n        \"type\" : \"illegal_argument_exception\",  \n        \"reason\" : \"the number of target shards [4] must be less that the number of source shards [2]\"\n      },\n      //shrink-index的当前阶段的定义\n      \"phase_execution\" : {\n        \"policy\" : \"shrink-index\",\n        \"phase_definition\" : {                  \n          \"min_age\" : \"5d\",\n          \"actions\" : {\n            \"shrink\" : {\n              \"number_of_shards\" : 4\n            }\n          }\n        },\n        \"version\" : 1,\n        \"modified_date_in_millis\" : 1541717264230\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","retrying-failed-lifecycle-policy-steps"],["heading","Retrying failed lifecycle policy steps"],["body","\n"],["body","一旦你解决了在 “ERROR” 步骤中放置索引的问题，你可能需要明确地告诉ILM重试该步骤:"],["body","\n"],["body","POST /my-index-000001/_ilm/retry\n"],["body","\n"],["body","ILM随后尝试重新运行失败的步骤. 使用 ILM Explain API 监控该过程"],["body","\n"],["headingLink","common-ilm-errors"],["heading","Common ILM errors"],["body","\n"],["body","以下是解决 “ERROR” 步骤中报告的最常见错误的方法。"],["body","\n"],["body","翻转别名的问题是错误的常见原因\n使用  data streams 来自动翻转别名"],["body","\n"],["headingLink","rollover-alias-x-can-point-to-multiple-indices-found-duplicated-alias-x-in-index-template-z"],["heading","Rollover alias [x] can point to multiple indices, found duplicated alias [x] in index template [z]"],["body","\n\n"],["body","目标翻转别名在索引模板的 index.lifecycle.rollover_alias 设置中指定。"],["body","\n"],["body","当你  bootstrap the initial index  需要显示配置这个别名"],["body","\n"],["body","然后，翻转操作管理将别名设置 将别名 翻转到每个后续索引。"],["body","\n"],["body","不要在索引模板的别名部分中显式配置相同的别名。"],["body","\n\n"],["headingLink","indexlifecyclerollover_alias-x-does-not-point-to-index-y"],["heading","index.lifecycle.rollover_alias [x] does not point to index [y]"],["body","\n\n"],["body","要么索引使用了错误的别名，要么别名不存在。"],["body","\n"],["body","Check the index.lifecycle.rollover_alias index setting. To see what aliases are configured, use _cat/aliases."],["body","\n\n"],["headingLink","setting-indexlifecyclerollover_alias-for-index-y-is-empty-or-not-defined"],["heading","Setting [index.lifecycle.rollover_alias] for index [y] is empty or not defined"],["body","\n\n"],["body","The index.lifecycle.rollover_alias setting must be configured for the rollover action to work."],["body","\n"],["body","Update the index settings to set index.lifecycle.rollover_alias."],["body","\n\n"],["headingLink","alias-x-has-more-than-one-write-index-yz"],["heading","Alias [x] has more than one write index [y,z]"],["body","\n\n"],["body","只能将一个索引指定为特定别名的写索引。"],["body","\n"],["body","Use the aliases API to set is_write_index:false for all but one index."],["body","\n\n"],["headingLink","index-name-x-does-not-match-pattern--d"],["heading","index name [x] does not match pattern ^.*-\\d+"],["body","\n\n"],["body","索引名称必须与regex模式匹配 ^.*-\\d+ 让翻转行动发挥作用."],["body","\n"],["body","最常见的问题是索引名称不包含尾数字."],["body","\n\n"],["headingLink","circuitbreakingexception-x-data-too-large-data-for-y"],["heading","CircuitBreakingException: [x] data too large, data for [y]"],["body","\n\n"],["body","这表明集群正在达到资源限制。在继续设置ILM之前，您需要采取措施缓解资源问题，see Circuit breaker errors."],["body","\n\n"],["headingLink","high-disk-watermark-x-exceeded-on-y"],["heading","High disk watermark [x] exceeded on [y]"],["body","\n\n"],["body","这表明集群的磁盘空间不足."],["body","\n"],["body","当您没有设置索引生命周期管理来从热节点滚动到暖节点时，可能会发生这种情况."],["body","\n"],["body","考虑添加节点、升级硬件或删除不需要的索引。"],["body","\n\n"],["headingLink","security_exception-action--is-unauthorized-for-user--with-roles--this-action-is-granted-by-the-index-privileges-manage_follow_indexmanageall"],["heading","security_exception: action [] is unauthorized for user [] with roles [], this action is granted by the index privileges [manage_follow_index,manage,all]"],["body","\n\n"],["body","这表示无法执行ILM操作，因为ILM用来执行操作的用户没有适当的权限"],["body","\n"],["body","当更新ILM策略后用户的权限被删除时，可能会发生这种情况"],["body","\n"],["body","ILM操作的运行就像它们是由最后一个修改策略的用户执行的一样"],["body","\n"],["body","用于从中创建或修改策略的帐户应具有执行该策略的所有操作的权限."],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/4.CustomizeBuiltInILMPolicy/README.html"],["title","CustomizeBuiltInILMPolicy - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","tutorial-customize-built-in-ilm-policies"],["heading","Tutorial: Customize built-in ILM policies"],["body","\n"],["body","Elasticsearch包含以下内置ILM策略:"],["body","\n\n"],["body","logs"],["body","\n"],["body","metrics"],["body","\n"],["body","synthetics"],["body","\n\n\n"],["body","\n"],["body","Elastic Agent 使用这些策略来管理其数据流的后备索引。"],["body","\n"],["body","\n"],["body","\n"],["body","本教程向您展示了如何使用Kibana的 **索引生命周期策略 ** 根据应用程序的性能、弹性和保留要求自定义这些策略。"],["body","\n"],["body","\n\n"],["headingLink","scenario"],["heading","Scenario"],["body","\n"],["body","您希望将日志文件发送到Elasticsearch集群，以便您可以可视化和分析数据. 此数据具有以下保留要求:"],["body","\n\n"],["body","当写入索引达到50GB或30天之前，滚动到新索引。"],["body","\n"],["body","翻转后，将索引保留在热数据层中30天。"],["body","\n"],["body","翻转后30天:\n\n"],["body","Move indices to the warm data tier."],["body","\n"],["body","Set replica shards to 1."],["body","\n"],["body","Force merge multiple index segments to free up the space used by deleted documents."],["body","\n\n"],["body","\n"],["body","Delete indices 90 days after rollover."],["body","\n\n"],["headingLink","prerequisites"],["heading","Prerequisites"],["body","\n"],["body","要完成本教程，您需要:"],["body","\n\n"],["body","\n"],["body","An Elasticsearch cluster with hot and warm data tiers."],["body","\n\n"],["body","\n"],["body","Elasticsearch Service: Elastic Stack deployments on Elasticsearch Service include a hot tier by default. To add a warm tier, edit your deployment and click Add capacity for the warm data tier."],["body","\n"],["body","\n"],["body","\n"],["body","\n"],["body","Self-managed cluster: Assign data_hot and data_warm roles to nodes as described in Data tiers."],["body","\n"],["body","For example, include the data_warm node role in the elasticsearch.yml file of each node in the warm tier:"],["body","\n"],["body","node.roles: [ data_warm ]\n"],["body","\n"],["body","\n\n"],["body","\n"],["body","\n"],["body","A host with Elastic Agent installed and configured to send logs to your Elasticsearch cluster."],["body","\n"],["body","\n\n"],["headingLink","view-the-policy"],["heading","View the policy"],["body","\n\n"],["body","Elastic Agent 使用index pattern  为 'logs-* '的数据流来存储日志监控数据。"],["body","\n"],["body","内置的 logs ILM策略会自动管理这些数据流的后备索引。"],["body","\n\n"],["body","要查看Kibana中的 “日志” 策略:"],["body","\n\n"],["body","Open the menu and go to Stack Management > Index Lifecycle Policies."],["body","\n"],["body","Select the logs policy."],["body","\n\n"],["body","“日志” 策略使用建议的 rollover 默认值:"],["body","\n\n"],["body","Start writing to a new index when the current write index reaches 50GB or becomes 30 days old."],["body","\n"],["body","要查看或更改rollOver设置，请单击热阶段的 高级设置 。然后禁用  使用推荐默认值  显示rollover设置。"],["body","\n\n"],["body","\n"],["headingLink","modify-the-policy"],["heading","Modify the policy"],["body","\n"],["body","默认的 “logs” 策略旨在防止创建许多微小的每日索引。您可以修改策略以满足您的性能要求并管理资源使用情况。"],["body","\n\n"],["body","\n"],["body","Activate the warm phase and click Advanced settings."],["body","\n\n"],["body","Set Move data into phase when to 30 days old. This moves indices to the warm tier 30 days after rollover."],["body","\n"],["body","Enable Set replicas and change Number of replicas to 1."],["body","\n"],["body","Enable Force merge data and set Number of segments to 1."],["body","\n\n"],["body","\n"],["body","\n"],["body","\n"],["body","In the warm phase, click the trash icon to enable the delete phase."],["body","\n"],["body","\n"],["body","In the delete phase, set Move data into phase when to 90 days old. This deletes indices 90 days after rollover."],["body","\n"],["body","\n"],["body","\n"],["body","\n"],["body","Click Save Policy."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/11.SkipRollover/README.html"],["title","SkipRollover - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","skip-rollover"],["heading","Skip rollover"],["body","\n"],["body","当 index.lifecycle.indexing_complete设置为 'true' 时，ILM不会对索引执行翻转操作，即使它满足翻转标准。当翻转动作成功完成时，它会由ILM自动设置。"],["body","\n"],["body","如果您需要对您的正常生命周期策略进行例外，并更新别名以强制进行翻转，但希望ILM继续管理索引，您可以手动将其设置为跳过翻转。如果您使用翻转API。无需手动配置此设置。"],["body","\n"],["body","如果删除了索引的生命周期策略，则此设置也将被删除。"],["body","\n"],["body","When index.lifecycle.indexing_complete is true, ILM verifies that the index is no longer the write index for the alias specified by index.lifecycle.rollover_alias. If the index is still the write index or the rollover alias is not set, the index is moved to the ERROR step."],["body","\n"],["body","For example, if you need to change the name of new indices in a series while retaining previously-indexed data in accordance with your configured policy, you can:"],["body","\n\n"],["body","Create a template for the new index pattern that uses the same policy."],["body","\n"],["body","Bootstrap the initial index."],["body","\n"],["body","Change the write index for the alias to the bootstrapped index using the aliases API."],["body","\n"],["body","Set index.lifecycle.indexing_complete to true on the old index to indicate that it does not need to be rolled over."],["body","\n\n"],["body","ILM continues to manage the old index in accordance with your existing policy. New indices are named according to the new template and managed according to the same policy without interruption."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/6.ConfigureALifecyclePolicy/README.html"],["title","ConfigureALifecyclePolicy - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","configure-a-lifecycle-policy"],["heading","Configure a lifecycle policy"],["body","\n\n"],["body","为了使ILM管理索引，必须在 index.lifecycle.name index setting配置 策略名"],["body","\n"],["body","为了让声明周期策略 可以滚动索引 rolling indices，可以将床架你的Policy 加入到 索引模板中 index template"],["body","\n"],["body","若要使用策略来管理不滚动的索引，可以在创建索引时指定生命周期策略，或者将策略直接应用于现有索引。"],["body","\n"],["body","ILM策略存储在全局集群状态中，当您 take the snapshot时，可以通过将 “include_global_state” 设置为 “true” 来包含在快照中的策略，恢复快照后，全局状态下的所有策略都将恢复，并且所有具有相同名称的本地策略都将被覆盖。"],["body","\n"],["body","为Beats或Logstash Elasticsearch输出插件启用索引生命周期管理时，将自动应用必要的策略和配置更改。您可以修改默认策略，但不需要显式配置策略或引导初始索引。"],["body","\n\n"],["headingLink","create-lifecycle-policy"],["heading","Create lifecycle policy"],["body","\n"],["body","PUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"actions\": {\n          \"rollover\": {\n            \"max_primary_shard_size\": \"25GB\" \n          }\n        }\n      },\n      \"delete\": {\n        \"min_age\": \"30d\",\n        \"actions\": {\n          \"delete\": {} \n        }\n      }\n    }\n  }\n}\n\n"],["body","\n"],["headingLink","apply-lifecycle-policy-with-an-index-template"],["heading","Apply lifecycle policy with an index template"],["body","\n\n"],["body","要使得策略触发翻转操作，需要在索引模板中配置策略。您可以指定策略的名称和用于引用滚动索引的别名。"],["body","\n"],["body","可以使用 Kibana Stack Management > Index Management >  Index Templates"],["body","\n\n"],["body","PUT _index_template/my_template\n{\n  \"index_patterns\": [\"test-*\"], [](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/set-up-lifecycle-policy.html#CO486-1)\n  \"template\": {\n    \"settings\": {\n      \"number_of_shards\": 1,\n      \"number_of_replicas\": 1,\n      \"index.lifecycle.name\": \"my_policy\", [](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/set-up-lifecycle-policy.html#CO486-2)\n      \"index.lifecycle.rollover_alias\": \"test-alias\" [](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/set-up-lifecycle-policy.html#CO486-3)\n    }\n  }\n}\n"],["body","\n"],["headingLink","create-an-initial-managed-index"],["heading","Create an initial managed index"],["body","\n\n"],["body","当您为自己的滚动索引设置策略时，您需要手动创建由策略管理的第一个索引，并将其指定为写入索引。"],["body","\n"],["body","索引的名称必须与索引模板中定义的模式匹配，并以数字结尾。该数字递增以生成由 “翻转” 操作创建的索引的名称。"],["body","\n"],["body","例如，下面的请求创建 “test-00001” 索引。由于它与 “my_template” 中指定的索引模式匹配，因此Elasticsearch会自动应用该模板中的设置。"],["body","\n\n"],["body","PUT test-000001\n{\n  \"aliases\": {\n    \"test-alias\":{\n      \"is_write_index\": true [](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/set-up-lifecycle-policy.html#CO487-1)\n    }\n  }\n}\n"],["body","\n"],["headingLink","apply-a-policy-to-multiple-indices"],["heading","Apply a policy to multiple indices"],["body","\n\n"],["body","通过 update settings API 给多个索引应用同一个策略，通过使用 索引名称的通配符"],["body","\n"],["body","请注意，不要无意中匹配不想修改的索引。"],["body","\n\n"],["body","PUT mylogs-pre-ilm*/_settings [](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/set-up-lifecycle-policy.html#CO489-1)\n{\n  \"index\": {\n    \"lifecycle\": {\n      \"name\": \"mylogs_policy_existing\"\n    }\n  }\n}\n"],["body","\n"],["headingLink","switch-lifecycle-policies"],["heading","Switch lifecycle policies"],["body","\n"],["body","要切换索引的生命周期策略，请执行以下步骤:"],["body","\n\n"],["body","使用 remove policy API移除现有的策略"],["body","\n\n"],["body","POST logs-my_app-default/_ilm/remove\n"],["body","\n\n"],["body","\n"],["body","删除策略API从索引中删除所有ILM元数据，并且不考虑索引的生命周期状态。这可能会使索引处于不可预知的状态。"],["body","\n"],["body","例如 ,  forcemerge action 会临时关闭索引. 在 forcemerge 过程中，移除索引ILM的ILM策略 可能会使得索引 无限期的 处于关闭状态"],["body","\n"],["body","策略删除后, 使用 get index API 检查索引策略"],["body","\n"],["body","GET logs-my_app-default"],["body","\n"],["body","然后，您可以根据需要更改索引 ，使用  open index API 重新打开关闭的索引"],["body","\n"],["body","POST logs-my_app-default/_open"],["body","\n"],["body","\n"],["body","\n"],["body","Assign a new policy using the update settings API. Target a data stream or alias to assign a policy to all its indices."],["body","\n"],["body","\n\n"],["body","在没有先删除现有策略的情况下，不要分配新策略。 his can cause phase execution to silently fail."],["body","\n"],["body","    PUT logs-my_app-default/_settings\n    {\n      \"index\": {\n        \"lifecycle\": {\n          \"name\": \"new-lifecycle-policy\"\n        }\n      }\n    }\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/5.IndexLifecycleActions/2.Delete.html"],["title","Delete.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","delete"],["heading","Delete"],["body","\n"],["body","Phases allowed: delete."],["body","\n"],["body","Permanently removes the index."],["body","\n"],["headingLink","options"],["heading","Options"],["body","\n\n"],["body","\n"],["body","delete_searchable_snapshot"],["body","\n"],["body","(可选，布尔值) 删除上一阶段创建的可搜索快照。默认为 “true”。当在任何先前阶段使用 可搜索快照 操作时，此选项适用。"],["body","\n"],["body","\n\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","PUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"delete\": {\n        \"actions\": {\n          \"delete\" : { }\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/5.IndexLifecycleActions/8.SearchableSnapshot.html"],["title","SearchableSnapshot.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","searchable-snapshot"],["heading","Searchable snapshot"],["body","\n"],["body","Phases allowed: hot, cold, frozen."],["body","\n\n"],["body","\n"],["body","在配置的存储库中 对受管索引拍摄快照 然后 挂载为  searchable snapshot，并将其作为 可搜索快照 挂载。如果索引是 数据流则 的一部分 挂载的索引将替换流中的原始索引。"],["body","\n"],["body","\n"],["body","\n"],["body","The searchable_snapshot action requires data tiers. 该操作使用 index.routing.allocation.include._tier_preference setting to mount the index directly to the phase’s corresponding data tier."],["body","\n"],["body","\n"],["body","\n"],["body","In the frozen phase, the action mounts a partially mounted index prefixed with partial- to the frozen tier."],["body","\n"],["body","\n"],["body","\n"],["body","In other phases, the action mounts a fully mounted index prefixed with restored- to the corresponding data tier."],["body","\n"],["body","\n"],["body","\n"],["body","Don’t include the searchable_snapshot action in both the hot and cold phases. This can result in indices failing to automatically migrate to the cold tier during the cold phase."],["body","\n"],["body","\n"],["body","\n"],["body","If the searchable_snapshot action is used in the hot phase the subsequent phases cannot include the shrink, forcemerge, or freeze actions."],["body","\n"],["body","\n"],["body","\n"],["body","不能对数据流的写索引执行此操作，尝试这样做会失败，要将索引转换为可搜索的快照，请首先 [手动翻转](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/use-a-data-stream.html # 手动翻转数据流) 数据流。这将创建一个新的写入索引，因为索引不再是流的写索引，所以该操作可以将其转换为可搜索的快照，在热阶段使用 RollOver 操作的策略将避免这种情况，并避免未来托管索引的手动RollOver。"],["body","\n"],["body","\n"],["body","\n"],["body","Mounting 和Relocating 可搜索快照索引的分片 涉及从快照存储库中复制分片内容，这可能会因常规索引发生的节点之间的复制而产生不同的成本，。这些成本通常较低，但在某些环境中可能较高。有关更多详细信息，请参阅 [使用可搜索快照降低成本](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/searchable-snapshots.html # 可搜索快照-成本)。"],["body","\n"],["body","\n"],["body","\n"],["body","默认情况下，此快照将在删除阶段由 删除操作 删除。要保留快照，请在删除操作中将 “delete_searchable_snapshot” 设置为 “false”。"],["body","\n"],["body","\n\n"],["headingLink","options"],["heading","Options"],["body","\n\n"],["body","\n"],["body","snapshot_repository"],["body","\n"],["body","(Required, string) Repository used to store the snapshot."],["body","\n"],["body","\n"],["body","\n"],["body","force_merge_index"],["body","\n"],["body","(可选，布尔) 强制将托管索引合并到一个段。默认为 “true”。如果托管索引已经使用 强制合并操作 在先前操作中被强制合并，则 “可搜索快照” 操作强制合并步骤将是无操作。"],["body","\n"],["body","\n\n"],["body","“forcemerge” 操作 是最大的努力。可能会发生某些分片正在重新定位，在这种情况下，它们将不会合并。即使并非所有分片都被强制合并，searchable_snapshot 操作也将继续执行。"],["body","\n"],["body","此强制合并发生在索引处于 searchable_snapshot 操作之前的阶段。"],["body","\n"],["body","例如，如果在 “hot” 阶段使用 searchable_snapshot 操作，则将在Hot节点上执行强制合并。"],["body","\n"],["body","如果在 “Cold” 阶段使用 “searchable_snapshot” 操作，则将在 “冷” 阶段之前 索引的任何层上执行强制合并 (“Hot” 或 “Warm”)。"],["body","\n"],["headingLink","examples"],["heading","Examples"],["body","\n"],["body","PUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"cold\": {\n        \"actions\": {\n          \"searchable_snapshot\" : {\n            \"snapshot_repository\" : \"backing_repo\"\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/5.IndexLifecycleActions/6.ReadOnly.html"],["title","ReadOnly.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","read-only"],["heading","Read only"],["body","\n"],["body","Phases allowed: hot, warm, cold."],["body","\n\n"],["body","\n"],["body","Makes the index read-only."],["body","\n"],["body","\n"],["body","\n"],["body","To use the readonly action in the hot phase, the rollover action must be present. If no rollover action is configured, ILM will reject the policy."],["body","\n"],["body","\n"],["body","\n"],["body","要在 hot 阶段使用 readonly 操作，必须存在 rollover 操作。如果未配置任何 RollOver操作，ILM将拒绝该策略。"],["body","\n"],["body","\n\n"],["headingLink","options"],["heading","Options"],["body","\n"],["body","None."],["body","\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","PUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"warm\": {\n        \"actions\": {\n          \"readonly\" : { }\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/5.IndexLifecycleActions/10.Shrink.html"],["title","Shrink.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","shrink"],["heading","Shrink"],["body","\n"],["body","Phases allowed: hot, warm."],["body","\n\n"],["body","将源索引设置为 [只读](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/index-modules-blocks.html # index-blocks-read-only)，并将其缩小为一个具有较少主分片的新索引"],["body","\n"],["body","The name of the resulting index is shrink-<random-uuid>-<original-index-name>."],["body","\n"],["body","此操作对应于 shrink API"],["body","\n"],["body","在 shrink 操作之后，指向源索引的任何别名都指向新的收缩索引。如果ILM对数据流的后备索引执行 Shrink 操作，则收缩索引将替换流中的源索引。您不能对写入索引执行 Shrink 操作。"],["body","\n"],["body","要在hot阶段使用 shrink 操作，rollover action 必须存在，如果不在，则拒绝此策略"],["body","\n"],["body","The shrink action  将会 unset 索引的  index.routing.allocation.total_shards_per_node 设置，意味着没有限制。这是为了确保索引的所有分片都可以复制到单个节点，即使步骤完成后，此设置更改也将持续存在于索引上。"],["body","\n"],["body","If the shrink action is used on a follower index, policy execution waits until the leader index rolls over (or is otherwise marked complete), then converts the follower index into a regular index with the unfollow action before performing the shrink operation."],["body","\n\n"],["headingLink","shrink-options"],["heading","Shrink options"],["body","\n\n"],["body","\n"],["body","number_of_shards"],["body","\n"],["body","(可选，整数) 要缩小到的分片数。必须是源索引中分片数量的一个因素。此参数与 “max_primary_shard_size” 冲突，只能设置其中一个。"],["body","\n"],["body","\n"],["body","\n"],["body","max_primary_shard_size"],["body","\n"],["body","(Optional, byte units)"],["body","\n\n"],["body","目标索引的最大主分片大小。用于寻找目标索引的最佳分片数. 设置此参数时，每个分片在目标索引中的存储不会大于该参数."],["body","\n"],["body","目标索引的分片计数仍将是源索引的分片计数的一个因子，但是如果参数小于源索引中的单个分片大小，则目标索引的分片计数将等于源索引的分片计数。例如，当该参数设置为50gb时，如果源索引有60个主分片，总计100gb，那么目标索引将有2个主分片，每个分片大小为50gb; 如果源索引有60个主分片，总计1000gb，那么目标索引将有20个主分片; 如果源索引有60个主分片，总共4000gb，那么目标索引仍将有60个主分片。此参数与 “设置” 中的 “number_of_shards” 冲突，只能设置其中一个。"],["body","\n\n"],["body","\n\n"],["headingLink","example"],["heading","Example"],["body","\n"],["headingLink","set-the-number-of-shards-of-the-new-shrunken-index-explicitly"],["heading","Set the number of shards of the new shrunken index explicitly"],["body","\n"],["body","PUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"warm\": {\n        \"actions\": {\n          \"shrink\" : {\n            \"number_of_shards\": 1\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","calculate-the-optimal-number-of-primary-shards-for-a-shrunken-index"],["heading","Calculate the optimal number of primary shards for a shrunken index"],["body","\n"],["body","The following policy uses the max_primary_shard_size parameter to automatically calculate the new shrunken index’s primary shard count based on the source index’s storage size."],["body","\n"],["body","PUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"warm\": {\n        \"actions\": {\n          \"shrink\" : {\n            \"max_primary_shard_size\": \"50gb\"\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","shard-allocation-for-shrink"],["heading","Shard allocation for shrink"],["body","\n"],["body","在 “收缩” 操作期间，ILM将源索引的主分片分配给一个节点。缩小索引后，ILM会根据您的分配规则将缩小的索引的分片重新分配给适当的节点。"],["body","\n"],["body","这些分配步骤可能会由于以下几个原因而失败，包括:"],["body","\n\n"],["body","A node is removed during the shrink action."],["body","\n"],["body","没有节点有足够的磁盘空间来承载源索引的分片。"],["body","\n"],["body","由于分配规则冲突，Elasticsearch无法重新分配缩小的索引。"],["body","\n\n"],["body","当其中一个分配步骤失败时，ILM等待   index.lifecycle.step.wait_time_threshold,，默认为12小时。此阈值时段可让群集解决导致分配失败的所有问题。"],["body","\n\n"],["body","\n"],["body","如果阈值周期过去并且ILM尚未Shrink索引，则ILM尝试将源索引的主分片分配给另一个节点。"],["body","\n"],["body","\n"],["body","\n"],["body","如果ILM缩小了索引，但在阈值期间无法重新分配缩小的索引的分片，则ILM会删除缩小的索引并重新尝试整个 “缩小” 操作。"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/5.IndexLifecycleActions/README.html"],["title","IndexLifecycleActions - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","index-lifecycle-actions"],["heading","Index lifecycle actions"],["body","\n\n"],["body","\n"],["body","Allocate"],["body","\n"],["body","将分片移动到具有不同性能特征的节点，并减少副本数量。"],["body","\n"],["body","\n"],["body","\n"],["body","Delete"],["body","\n"],["body","永久删除索引。"],["body","\n"],["body","\n"],["body","\n"],["body","Force merge"],["body","\n"],["body","减少索引段的数量并清除已删除的文档。使索引为只读。"],["body","\n"],["body","\n"],["body","\n"],["body","Freeze"],["body","\n"],["body","冻结索引以最大程度地减少其内存占用。"],["body","\n"],["body","\n"],["body","\n"],["body","Migrate"],["body","\n"],["body","将索引分片移动到对应于当前ILM阶段的 数据层。"],["body","\n"],["body","\n"],["body","\n"],["body","Read only"],["body","\n"],["body","块对索引的写操作。"],["body","\n"],["body","\n"],["body","\n"],["body","Rollover"],["body","\n"],["body","Remove the index as the write index for the rollover alias and start indexing to a new index."],["body","\n"],["body","删除索引作为rollover 别名的写索引，并开始索引到新索引。"],["body","\n"],["body","\n"],["body","\n"],["body","Searchable snapshot"],["body","\n"],["body","在配置的存储库中获取托管索引的快照，并将其作为可搜索的快照挂载。"],["body","\n"],["body","\n"],["body","\n"],["body","Set priority"],["body","\n"],["body","在整个生命周期中，降低索引的优先级，以确保首先恢复热索引。"],["body","\n"],["body","\n"],["body","\n"],["body","Shrink"],["body","\n"],["body","通过将索引缩小为新索引来减少主分片的数量。"],["body","\n"],["body","\n"],["body","\n"],["body","Unfollow"],["body","\n"],["body","将跟随索引转换为常规索引。在翻转、收缩或可搜索快照操作之前自动执行。"],["body","\n"],["body","\n"],["body","\n"],["body","Wait for snapshot"],["body","\n"],["body","在删除索引之前，请确保存在快照。"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/5.IndexLifecycleActions/12.WaitForSnapshot.html"],["title","WaitForSnapshot.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","wait-for-snapshot"],["heading","Wait for snapshot"],["body","\n"],["body","Phases allowed: delete."],["body","\n"],["body","在删除索引之前，等待指定的SLM策略执行。这样可以确保删除索引的快照可用。"],["body","\n"],["headingLink","options"],["heading","Options"],["body","\n\n"],["body","\n"],["body","policy"],["body","\n"],["body","(Required, string) Name of the SLM policy that the delete action should wait for."],["body","\n"],["body","\n\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","PUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"delete\": {\n        \"actions\": {\n          \"wait_for_snapshot\" : {\n            \"policy\": \"slm-policy-name\"\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/5.IndexLifecycleActions/7.Rollover.html"],["title","Rollover.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","rollover"],["heading","Rollover"],["body","\n"],["body","Phases allowed: hot."],["body","\n\n"],["body","\n"],["body","当现有索引满足一个或多个 rollover 条件时，将目标滚动到新索引。"],["body","\n"],["body","\n"],["body","\n"],["body","如果在 follower index 上使用了 RollOver 操作，则策略执行将等待直到 LeaderIndex RollOver (或者 [标记为完成](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.17/skipping-rollover.html))，然后使用 Unfollow action 将FollowIndex转换为常规索引"],["body","\n"],["body","\n"],["body","\n"],["body","RollOverTarget 可以是 数据流 或 [索引别名](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.17/aliases.html)。当以数据流为目标时，新索引成为数据流的写索引，并且其generation递增。"],["body","\n"],["body","\n"],["body","\n"],["body","要滚动索引别名，别名及其写索引必须满足以下条件:"],["body","\n\n"],["body","\n"],["body","索引名称必须与模式  ^.*-\\d+$ 匹配，例如 ('my-index-000001 ')。"],["body","\n"],["body","\n"],["body","\n"],["body","The index.lifecycle.rollover_alias 必须配置为别名才能滚动."],["body","\n"],["body","\n"],["body","\n"],["body","The index must be the write index for the alias."],["body","\n"],["body","\n\n"],["body","\n\n"],["body","For example, if my-index-000001 has the alias my_data, the following settings must be configured."],["body","\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"index.lifecycle.name\": \"my_policy\",\n    \"index.lifecycle.rollover_alias\": \"my_data\"\n  },\n  \"aliases\": {\n    \"my_data\": {\n      \"is_write_index\": true\n    }\n  }\n}\n"],["body","\n"],["headingLink","options"],["heading","Options"],["body","\n"],["body","您必须指定至少一个翻转条件。空的翻转动作无效。"],["body","\n\n"],["body","\n"],["body","max_age"],["body","\n"],["body","(Optional, time units)"],["body","\n\n"],["body","在达到索引创建的最大经过时间后触发翻转."],["body","\n"],["body","时长总是从索引创建时开始算起，即便索引原始日期配置为自定义日期, 例如使用 index.lifecycle.parse_origination_date or index.lifecycle.origination_date settings."],["body","\n\n"],["body","\n"],["body","\n"],["body","max_docs"],["body","\n"],["body","(Optional, integer)"],["body","\n\n"],["body","达到指定的最大文档数后触发翻转."],["body","\n"],["body","自上次刷新以来添加的文档不包括在文档计数中。"],["body","\n"],["body","文档计数不包含副本分片中的文档。"],["body","\n\n"],["body","\n"],["body","\n"],["body","max_size"],["body","\n"],["body","(Optional, byte units)"],["body","\n\n"],["body","当索引达到一定大小时触发展期. 这是索引中所有主分片的总大小."],["body","\n"],["body","副本不计入最大索引大小."],["body","\n"],["body","To see the current index size, use the _cat indices API. The pri.store.size value shows the combined size of all primary shards."],["body","\n\n"],["body","\n"],["body","\n"],["body","max_primary_shard_size"],["body","\n"],["body","(Optional, byte units)"],["body","\n\n"],["body","当索引中最大的主分片达到一定大小时触发Rollover"],["body","\n"],["body","这是索引中主分片的最大大小. 跟 max_size一样,副本被忽略."],["body","\n"],["body","To see the current shard size, use the _cat shards API. The store value shows the size each shard, and prirep indicates whether a shard is a primary (p) or a replica (r)."],["body","\n\n"],["body","\n\n"],["headingLink","example"],["heading","Example"],["body","\n"],["headingLink","roll-over-based-on-largest-primary-shard-size"],["heading","Roll over based on largest primary shard size"],["body","\n"],["body","当其最大的主分片至少为50 gb时，此示例将索引滚动。"],["body","\n"],["body","PUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"actions\": {\n          \"rollover\" : {\n            \"max_primary_shard_size\": \"50GB\"\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","roll-over-based-on-index-size"],["heading","Roll over based on index size"],["body","\n"],["body","This example rolls the index over when it is at least 100 gigabytes."],["body","\n"],["body","PUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"actions\": {\n          \"rollover\" : {\n            \"max_size\": \"100GB\"\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","Copy as curlView in Console"],["body","\n"],["headingLink","roll-over-based-on-document-countedit"],["heading","Roll over based on document countedit"],["body","\n"],["body","This example rolls the index over when it contains at least one hundred million documents."],["body","\n"],["body","PUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"actions\": {\n          \"rollover\" : {\n            \"max_docs\": 100000000\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","roll-over-based-on-index-age"],["heading","Roll over based on index age"],["body","\n"],["body","如果索引是在至少7天前创建的，则此示例将其滚动。"],["body","\n"],["body","PUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"actions\": {\n          \"rollover\" : {\n            \"max_age\": \"7d\"\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","roll-over-using-multiple-conditions"],["heading","Roll over using multiple conditions"],["body","\n"],["body","当您指定多个翻转条件时，当满足任何个条件时，索引将被翻转。如果索引存在至少7天或至少100 gb，则此示例将其滚动。"],["body","\n"],["body","PUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"actions\": {\n          \"rollover\" : {\n            \"max_age\": \"7d\",\n            \"max_size\": \"100GB\"\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","rollover-condition-blocks-phase-transition"],["heading","Rollover condition blocks phase transition"],["body","\n"],["body","仅在满足其条件之一的情况下，才完成翻转操作。这意味着任何后续阶段都将被阻止，直到翻滚成功。"],["body","\n"],["body","例如，以下策略会在索引滚动后的一天删除该索引。它不会在创建索引后的一天删除它。"],["body","\n"],["body","PUT /_ilm/policy/rollover_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"actions\": {\n          \"rollover\": {\n            \"max_size\": \"50GB\"\n          }\n        }\n      },\n      \"delete\": {\n        \"min_age\": \"1d\",\n        \"actions\": {\n          \"delete\": {}\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/5.IndexLifecycleActions/1.Allocate.html"],["title","Allocate.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","allocate"],["heading","Allocate"],["body","\n"],["body","Phases allowed: warm, cold."],["body","\n\n"],["body","\n"],["body","更新索引设置以更改允许哪些节点托管索引分片并更改副本数。"],["body","\n"],["body","\n"],["body","\n"],["body","在热阶段不允许执行 allocate 操作. 索引的初始分配必须手动完成 or via index templates."],["body","\n"],["body","\n"],["body","\n"],["body","您可以将此操作配置为修改分配规则和副本数，仅修改分配规则或仅修改副本数"],["body","\n"],["body","\n"],["body","\n"],["body","有关Elasticsearch如何使用副本进行缩放的更多信息，请参阅 可扩展性和弹性。有关控制elasticsearch分配特定索引的分片的位置的更多信息，请参阅 索引级分片分配过滤。"],["body","\n"],["body","\n\n"],["headingLink","options"],["heading","Options"],["body","\n\n"],["body","您必须指定副本的数量或至少一个 include 、 exlude 或require 选项。空分配操作无效。"],["body","\n"],["body","有关使用自定义属性进行分片分配的详细信息，请参阅 索引级分片分配筛选。"],["body","\n\n\n"],["body","\n"],["body","number_of_replicas"],["body","\n"],["body","(Optional, integer) Number of replicas to assign to the index."],["body","\n"],["body","\n"],["body","\n"],["body","total_shards_per_node"],["body","\n"],["body","(Optional, integer) The maximum number of shards for the index on a single Elasticsearch node. A value of -1 is interpreted as unlimited. See total shards."],["body","\n"],["body","\n"],["body","\n"],["body","include"],["body","\n"],["body","(Optional, object) Assigns an index to nodes that have at least one of the specified custom attributes."],["body","\n"],["body","\n"],["body","\n"],["body","exclude"],["body","\n"],["body","(Optional, object) Assigns an index to nodes that have none of the specified custom attributes."],["body","\n"],["body","\n"],["body","\n"],["body","require"],["body","\n"],["body","(Optional, object) Assigns an index to nodes that have all of the specified custom attributes."],["body","\n"],["body","\n\n"],["headingLink","example"],["heading","Example"],["body","\n\n"],["body","\n"],["body","以下策略中的分配操作将索引的副本数更改为 “2”"],["body","\n"],["body","\n"],["body","\n"],["body","不超过200个索引分片将被放置在任何单个节点上。否则索引分配规则不会更改。"],["body","\n"],["body","\n\n"],["body","PUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"warm\": {\n        \"actions\": {\n          \"allocate\" : {\n            \"number_of_replicas\" : 2,\n            \"total_shards_per_node\" : 200\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","assign-index-to-nodes-using-a-custom-attribute"],["heading","Assign index to nodes using a custom attribute"],["body","\n"],["body","\n"],["body","使用自定义属性分配索引到节点"],["body","\n"],["body","\n\n"],["body","以下策略中的分配操作将索引分配给具有 hot 或 warm 的 box_type 的节点。"],["body","\n"],["body","要指定节点的 box_type，请在节点配置中设置自定义属性"],["body","\n"],["body","For example, set node.attr.box_type: hot in elasticsearch.yml. For more information, see Enabling index-level shard allocation filtering."],["body","\n\n"],["body","PUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"warm\": {\n        \"actions\": {\n          \"allocate\" : {\n            \"include\" : {\n              \"box_type\": \"hot,warm\"\n            }\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","assign-index-to-nodes-based-on-multiple-attributes"],["heading","Assign index to nodes based on multiple attributes"],["body","\n"],["body","分配操作还可以基于多个节点属性将索引分配给节点。以下操作根据 “box_type” 和 “storage” 节点属性分配索引。"],["body","\n"],["body","PUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"cold\": {\n        \"actions\": {\n          \"allocate\" : {\n            \"require\" : {\n              \"box_type\": \"cold\",\n              \"storage\": \"high\"\n            }\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","assign-index-to-a-specific-node-and-update-replica-settings"],["heading","Assign index to a specific node and update replica settings"],["body","\n\n"],["body","\n"],["body","以下策略中的分配操作将索引更新为每个分片具有一个副本，并将其分配给具有 cold 的 box_type 的节点。"],["body","\n"],["body","\n"],["body","\n"],["body","To designate a node’s box_type, you set a custom attribute in the node configuration. For example, set node.attr.box_type: cold in elasticsearch.yml. For more information, see Enabling index-level shard allocation filtering."],["body","\n"],["body","\n\n"],["body","PUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"warm\": {\n        \"actions\": {\n          \"allocate\" : {\n            \"number_of_replicas\": 1,\n            \"require\" : {\n              \"box_type\": \"cold\"\n            }\n        }\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/5.IndexLifecycleActions/3.ForceMerge.html"],["title","ForceMerge.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","force-merge"],["heading","Force merge"],["body","\n"],["body","Phases allowed: hot, warm."],["body","\n\n"],["body","[Force merges](https://www. elastic。co/guide/en/elasticsearch/reference/7.17/indices-forcemerge.html) 将索引 合并成 指定 最大数量的 segments 此操作使索引 [只读](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/index-modules.html # dynamic-index-settings)。"],["body","\n"],["body","forcemerge 行动是最大的努力，可能会发生某些分片正在重新定位，在这种情况下，它们将不会合并。"],["body","\n"],["body","要在 “hot” 阶段使用 “forcemerge” 动作，必须存在 “rollover” 动作。如果未配置任何翻转操作，ILM将拒绝该策略。如果未配置任何翻转操作，ILM将拒绝该策略。"],["body","\n\n"],["body","Performance considerations"],["body","\n\n"],["body","\n"],["body","部队合并是一种资源密集型行动，如果一次触发太多的力合并，可能会对您的集群产生负面影响，当您将包含强制合并操作的ILM策略应用于现有索引时，可能会发生这种情况"],["body","\n"],["body","\n"],["body","\n"],["body","如果它们满足 min_age 标准，则可以立即进行多个阶段。您可以通过增加 min_age 或设置 index.lifecycle.origination_date 来更改索引年龄的计算方式来防止这种情况。"],["body","\n"],["body","\n"],["body","\n"],["body","如果遇到强制合并任务队列积压，则可能需要增加强制合并线程池的大小，以便可以并行强制合并索引。为此，请配置 thread_pool.force_merge.size 群集设置。"],["body","\n"],["body","\n"],["body","\n"],["body","这可能会产生级联的性能影响 .监控集群性能并缓慢增加线程池的大小以减少积压。"],["body","\n"],["body","\n"],["body","\n"],["body","强制合并将由索引当前阶段内的节点执行。"],["body","\n"],["body","\n"],["body","\n"],["body","hot 阶段的forcemerge将使用具有潜在更快节点的热节点，同时对摄取的影响更大。warm 阶段的forcemerge 出现将使用warm node，并且可能需要更长的时间来执行，但不会影响 hot 层的摄入。"],["body","\n"],["body","\n\n"],["headingLink","options"],["heading","Options"],["body","\n\n"],["body","\n"],["body","max_num_segments"],["body","\n"],["body","(必填，整数) 要合并到的段数。要完全合并索引，请设置为 “1”。"],["body","\n"],["body","\n"],["body","\n"],["body","index_codec"],["body","\n"],["body","(可选，字符串) 用于压缩文档存储的编解码器。唯一接受的值是 best_compression ，它使用 DEFLATE 来获得较高的压缩比，但存储字段性能较慢。要使用默认的LZ4编解码器，请省略此参数。如果使用 best_compression，ILM将 关闭，然后 [重新打开](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.17/indices-open-close.html) 力合并之前的索引。关闭时，索引将不可用于读取或写入操作。"],["body","\n"],["body","\n\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","PUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"warm\": {\n        \"actions\": {\n          \"forcemerge\" : {\n            \"max_num_segments\": 1\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/5.IndexLifecycleActions/11.Unfollow.html"],["title","Unfollow.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","unfollow"],["heading","Unfollow"],["body","\n"],["body","Phases allowed: hot, warm, cold, frozen."],["body","\n\n"],["body","\n"],["body","将 CCR Follow Index 转换为常规索引。这使得收缩、翻转和可搜索的快照操作能够安全地对FollowIndex执行。"],["body","\n"],["body","\n"],["body","\n"],["body","在整个生命周期中移动 follow index 时，也可以直接使用 unfollow。对不是 follower 的索引没有影响，阶段执行只是移动到下一个动作。"],["body","\n"],["body","\n"],["body","\n"],["body","This action is triggered automatically by the rollover, shrink, and searchable snapshot actions when they are applied to follower indices."],["body","\n"],["body","\n\n"],["body","此操作一直等到将follower index 转换为常规索引是否安全。必须满足以下条件:"],["body","\n\n"],["body","The leader index must have index.lifecycle.indexing_complete set to true. This happens automatically if the leader index is rolled over using the rollover action, and can be set manually using the index settings API."],["body","\n"],["body","对leader索引执行的所有操作都已复制到follower索引。这样可以确保在转换索引时不会丢失任何操作。"],["body","\n\n"],["body","一旦满足这些条件，unfollow将执行以下操作:"],["body","\n\n"],["body","Pauses indexing following for the follower index."],["body","\n"],["body","Closes the follower index."],["body","\n"],["body","Unfollows the leader index."],["body","\n"],["body","Opens the follower index (which is at this point is a regular index)."],["body","\n\n"],["headingLink","options"],["heading","Options"],["body","\n"],["body","None."],["body","\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","PUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"actions\": {\n          \"unfollow\" : {}\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/5.IndexLifecycleActions/5.Migrate.html"],["title","Migrate.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","migrate"],["heading","Migrate"],["body","\n"],["body","Phases allowed: warm, cold."],["body","\n\n"],["body","Moves the index to the data tier that corresponds to the current phase by updating the index.routing.allocation.include._tier_preference index setting."],["body","\n"],["body","如果没有使用 allocate 操作指定分配选项，ILM会在warm和cold阶段自动注入迁移操作"],["body","\n"],["body","如果指定仅修改索引副本数的分配操作，则ILM会在迁移索引之前减少副本数"],["body","\n"],["body","要防止自动迁移而不指定 allocate 选项，可以显式包括迁移操作，并将启用的选项设置为 false。"],["body","\n\n"],["body","各阶段对应的Migrate操作"],["body","\n\n"],["body","\n"],["body","如果 cold 阶段定义了 [可搜索快照操作](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-searchable-snapshot.html) 在 cold 阶段不会自动注入 migrate 操作，因为托管索引将被直接挂载到目标 tier，且使用migrate 操作配置的[_ tier_preference](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.17/data-tier-shard-filtering.html # 层-首选项-分配-过滤器) 基础结构"],["body","\n"],["body","\n"],["body","\n"],["body","在 warm 暖阶段，“migrate” 操作将 index.routing.allocation.include._tier_preference to data_warm,data_hot. 。这会将索引移动到 warm tier中的节点。如果暖层中没有节点，则它会退回到 [热层](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/data-tiers.html # 热层)。"],["body","\n"],["body","\n"],["body","\n"],["body","在Cold阶段， the migrate action sets index.routing.allocation.include._tier_preference to data_cold,data_warm,data_hot. 。这将索引移动到 cold tier中的节点。如果冷层中没有节点，则它会退回到 [暖层](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/data-tiers.html # 暖层) 层，或 [热](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.17/data-tiers.html # hot-tier) 层 (如果没有可用的热节点)。"],["body","\n"],["body","\n"],["body","\n"],["body","在Frozen结阶段不允许migrate操作，这将将索引移动到  frozen tier 中的节点。如果冻结层中没有节点，则退回到 [cold](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/data-tiers.html # cold-tier) 层，然后是 [warm](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.17/data-tiers.html # 暖层) 层，最后是 [hot](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/data-tiers.html # 热层) 层。"],["body","\n"],["body","\n"],["body","\n"],["body","在hot阶段不允许迁移操作。初始索引分配 [自动](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/data-tiers.html # data-tier-allocation)，可以手动或通过 [索引模板](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.17/index-templates.html)。"],["body","\n"],["body","\n\n"],["headingLink","options"],["heading","Options"],["body","\n\n"],["body","\n"],["body","enabled"],["body","\n"],["body","(可选，布尔) 控制ILM是否在此阶段自动迁移索引。默认为 “true”。"],["body","\n"],["body","\n\n"],["headingLink","example"],["heading","Example"],["body","\n\n"],["body","在以下策略中，指定allocate操作以在ILM将索引迁移到warm节点之前减少副本数。"],["body","\n"],["body","不需要显式指定 Migrate 操作-除非您指定分配选项或禁用迁移，否则ILM会自动执行迁移操作。"],["body","\n\n"],["body","PUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"warm\": {\n        \"actions\": {\n          \"migrate\" : {\n          },\n          \"allocate\": {\n            \"number_of_replicas\": 1\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","disable-automatic-migration"],["heading","Disable automatic migration"],["body","\n\n"],["body","禁用以下策略中的迁移操作，并且allocate操作将索引分配给 rack_id 为 one 或 two 的节点。"],["body","\n"],["body","不需要显式禁用迁移操作-如果指定allocation选项，ILM不会注入迁移操作。"],["body","\n\n"],["body","PUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"warm\": {\n        \"actions\": {\n          \"migrate\" : {\n           \"enabled\": false\n          },\n          \"allocate\": {\n            \"include\" : {\n              \"rack_id\": \"one,two\"\n            }\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/5.IndexLifecycleActions/4.Freeze.html"],["title","Freeze.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","freeze"],["heading","Freeze"],["body","\n"],["body","Phases allowed: cold."],["body","\n"],["body","Freezes an index."],["body","\n"],["body","冻结索引会关闭索引，并在同一API调用中重新打开它。这意味着在短时间内没有分配 primaries。群集将变成红色，直到分配了主分片。这一限制将来可能会被取消。"],["body","\n"],["headingLink","options"],["heading","Options"],["body","\n"],["body","None."],["body","\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","PUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"cold\": {\n        \"actions\": {\n          \"freeze\" : { }\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/5.IndexLifecycleActions/9.Setpriority.html"],["title","Setpriority.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","set-priority"],["heading","Set priority"],["body","\n"],["body","Phases allowed: hot, warm, cold."],["body","\n\n"],["body","一旦Policy 进入Hot、wamr或Cold阶段，就设置索引的 优先级"],["body","\n"],["body","节点重新启动后，优先级较高的索引在优先级较低的索引之前恢复。"],["body","\n"],["body","通常，热阶段的索引应具有最高值，冷阶段的指标应具有最低值"],["body","\n"],["body","例如: 热阶段为100，暖阶段为50，冷阶段为0。未设置此值的索引的默认优先级为1。"],["body","\n\n"],["headingLink","options"],["heading","Options"],["body","\n\n"],["body","\n"],["body","priority"],["body","\n"],["body","(Required, integer) The priority for the index. Must be 0 or greater. Set to null to remove the priority."],["body","\n"],["body","\n\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","PUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"warm\": {\n        \"actions\": {\n          \"set_priority\" : {\n            \"priority\": 50\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/RolloverAPI.html"],["title","RolloverAPI.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","rollover-api"],["heading","Rollover API"],["body","\n"],["body","Creates a new index for a data stream or index alias."],["body","\n"],["headingLink","request"],["heading","Request"],["body","\n"],["body","POST /<rollover-target>/_rollover/\nPOST /<rollover-target>/_rollover/<target-index>\n"],["body","\n"],["headingLink","description"],["heading","Description"],["body","\n"],["body","推荐使用 ILM’s rollover action to automate rollovers. See Index lifecycle."],["body","\n"],["body","rollover API为数据流或索引别名创建新索引。API的行为取决于翻转目标。"],["body","\n"],["body","Roll over a data stream"],["body","\n"],["body","If you roll over a data stream, the API creates a new write index for the stream. The stream’s previous write index becomes a regular backing index. A rollover also increments the data stream’s generation. See Rollover."],["body","\n"],["body","Roll over an index alias with a write index"],["body","\n"],["body","Prior to Elasticsearch 7.9, you would typically use an index alias with a write index to manage time series data. Data streams replace this functionality, require less maintenance, and automatically integrate with data tiers."],["body","\n"],["body","See Convert an index alias to a data stream."],["body","\n\n"],["body","如果索引别名指向多个索引。则 其中一个必须是  write index."],["body","\n"],["body","rollover API  为 索引别名  创建新索引 。并设置  is_write_index = true"],["body","\n"],["body","同样给之前的 索引 设置  is_write_index  = false"],["body","\n\n"],["body","Roll over an index alias with one index"],["body","\n"],["body","如果滚动仅指向一个索引的索引别名，则API会为该别名创建一个新索引，并从该别名中删除原始索引。"],["body","\n"],["headingLink","increment-index-names-for-an-alias"],["heading","Increment index names for an alias"],["body","\n\n"],["body","滚动索引时可以执行索引名"],["body","\n"],["body","如果 没有指定索引名，且当前索引以   -number 结尾 例如  my-index-000001 or my-index-3, 则number递增"],["body","\n"],["body","这个 数字 是填充0的六位数"],["body","\n\n"],["body","Use date math with index alias rollovers"],["body","\n\n"],["body","可以在 time series data  中 是会用   date math"],["body","\n"],["body","<my-index-{now/d}-000001>  see Roll over an index alias with a write index."],["body","\n\n"],["headingLink","wait-for-active-shards"],["heading","Wait for active shards"],["body","\n"],["body","A rollover creates a new index and is subject to the wait_for_active_shards setting."],["body","\n"],["headingLink","path-parameters"],["heading","Path parameters"],["body","\n\n"],["body","\n"],["body","<rollover-target>"],["body","\n"],["body","(Required, string) Name of the data stream or index alias to roll over."],["body","\n"],["body","\n"],["body","\n"],["body","<target-index>"],["body","\n"],["body","(Optional, string) Name of the index to create. Supports date math."],["body","\n\n"],["body","Data streams 不支持"],["body","\n"],["body","如果名称没有 以 -number 结尾 则 必须要指定该值"],["body","\n"],["body","只能小写，不允许  \\, /, *, ? \", <, >, |,  (space character), ,, #:"],["body","\n"],["body","不能超过255个字节"],["body","\n"],["body","starting with . are deprecated, except for hidden indices and internal indices managed by plugins"],["body","\n\n"],["body","\n\n"],["headingLink","query-parameters"],["heading","Query parameters"],["body","\n"],["body","dry_run"],["body","\n"],["body","(Optional, Boolean) If true, checks whether the current index matches one or more specified conditions but does not perform a rollover. Defaults to false."],["body","\n"],["body","wait_for_active_shards"],["body","\n"],["body","(Optional, string) The number of shard copies that must be active before proceeding with the operation. Set to all or any positive integer up to the total number of shards in the index (number_of_replicas+1). Default: 1, the primary shard."],["body","\n"],["body","See Active shards."],["body","\n"],["body","master_timeout"],["body","\n"],["body","(Optional, time units) Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error. Defaults to 30s."],["body","\n"],["body","timeout"],["body","\n"],["body","(Optional, time units) Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error. Defaults to 30s."],["body","\n"],["headingLink","request-body"],["heading","Request body"],["body","\n"],["headingLink","aliases"],["heading","aliases"],["body","\n"],["body","别名对象"],["body","\n"],["headingLink","properties-of-aliases-objects"],["heading","Properties of aliases objects"],["body","\n"],["body","filter"],["body","\n"],["body","(Optional, Query DSL object) Query used to limit documents the alias can access."],["body","\n"],["body","index_routing"],["body","\n"],["body","(Optional, string) Value used to route indexing operations to a specific shard. If specified, this overwrites the routing value for indexing operations."],["body","\n"],["body","is_hidden"],["body","\n"],["body","(Optional, Boolean) If true, the alias is hidden. Defaults to false. All indices for the alias must have the same is_hidden value."],["body","\n"],["body","is_write_index"],["body","\n"],["body","(Optional, Boolean) If true, the index is the write index for the alias. Defaults to false."],["body","\n"],["body","routing"],["body","\n"],["body","(Optional, string) Value used to route indexing and search operations to a specific shard."],["body","\n"],["body","search_routing"],["body","\n"],["body","(Optional, string) Value used to route search operations to a specific shard. If specified, this overwrites the routing value for search operations."],["body","\n"],["headingLink","conditions"],["heading","conditions"],["body","\n\n"],["body","\n"],["body","只当满足 至少一个或者多个条件才 rollover"],["body","\n"],["body","\n"],["body","\n"],["body","如果未指定此参数，则Elasticsearch会无条件执行翻转。"],["body","\n"],["body","\n"],["body","\n"],["body","要翻转索引，在请求的时刻当前索引必须满足 条件"],["body","\n"],["body","\n"],["body","\n"],["body","es不会 监控 索引状态"],["body","\n"],["body","\n"],["body","\n"],["body","To automate rollover, use ILM’s rollover instead."],["body","\n"],["body","\n\n"],["headingLink","properties-of-conditions"],["heading","Properties of conditions"],["body","\n"],["body","max_age"],["body","\n"],["body","(Optional, time units)"],["body","\n\n"],["body","至 索引从创建时间以来经历的最大时间"],["body","\n"],["body","even if the index origination date is configured to a custom date, such as when using the index.lifecycle.parse_origination_date or index.lifecycle.origination_date settings."],["body","\n\n"],["body","max_docs"],["body","\n"],["body","(Optional, integer) Triggers rollover after the specified maximum number of documents is reached. Documents added since the last refresh are not included in the document count. The document count does not include documents in replica shards."],["body","\n"],["body","max_size"],["body","\n"],["body","(Optional, byte units) Triggers rollover when the index reaches a certain size. This is the total size of all primary shards in the index. Replicas are not counted toward the maximum index size."],["body","\n"],["body","To see the current index size, use the _cat indices API. The pri.store.size value shows the combined size of all primary shards."],["body","\n"],["body","max_primary_shard_size"],["body","\n"],["body","(Optional, byte units) Triggers rollover when the largest primary shard in the index reaches a certain size. This is the maximum size of the primary shards in the index. As with max_size, replicas are ignored."],["body","\n"],["body","To see the current shard size, use the _cat shards API. The store value shows the size each shard, and prirep indicates whether a shard is a primary (p) or a replica (r)."],["body","\n"],["headingLink","mappings"],["heading","mappings"],["body","\n"],["body","(Optional, mapping object) Mapping for fields in the index. If specified, this mapping can include:"],["body","\n\n"],["body","Field names"],["body","\n"],["body","Field data types"],["body","\n"],["body","Mapping parameters"],["body","\n\n"],["body","See Mapping."],["body","\n"],["body","Data streams do not support this parameter."],["body","\n"],["headingLink","settings"],["heading","settings"],["body","\n"],["body","(Optional, index setting object) Configuration options for the index. See Index Settings."],["body","\n"],["body","Data streams do not support this parameter."],["body","\n"],["headingLink","response-body"],["heading","Response body"],["body","\n"],["body","acknowledged"],["body","\n"],["body","(Boolean) If true, the request received a response from the master node within the timeout period."],["body","\n"],["body","shards_acknowledged"],["body","\n"],["body","(Boolean) If true, the request received a response from active shards within the master_timeout period."],["body","\n"],["body","old_index"],["body","\n"],["body","(string) Previous index for the data stream or index alias. For data streams and index aliases with a write index, this is the previous write index."],["body","\n"],["body","new_index"],["body","\n"],["body","(string) Index created by the rollover. For data streams and index aliases with a write index, this is the current write index."],["body","\n"],["body","rolled_over"],["body","\n"],["body","(Boolean) If true, the data stream or index alias rolled over."],["body","\n"],["body","dry_run"],["body","\n"],["body","(Boolean) If true, Elasticsearch did not perform the rollover."],["body","\n"],["body","condition"],["body","\n"],["body","(object) Result of each condition specified in the request’s conditions. If no conditions were specified, this is an empty object."],["body","\n"],["body","(Boolean) The key is each condition. The value is its result. If true, the index met the condition at rollover."],["body","\n"],["headingLink","examples"],["heading","Examples"],["body","\n"],["headingLink","roll-over-a-data-stream"],["heading","Roll over a data stream"],["body","\n"],["body","POST my-data-stream/_rollover\n\n\nPOST my-data-stream/_rollover\n{\n  \"conditions\": {\n    \"max_age\": \"7d\",\n    \"max_docs\": 1000,\n    \"max_primary_shard_size\": \"50gb\"\n  }\n}\n\n"],["body","\n"],["headingLink","按日志rollover-并设置写索引"],["heading","按日志rollover 并设置写索引"],["body","\n"],["body","PUT %3Cmy-index-%7Bnow%2Fd%7D-000001%3E\n{\n  \"aliases\": {\n    \"my-alias\": {\n      \"is_write_index\": true\n    }\n  }\n}\n"],["body","\n"],["body","如果别名的索引名称使用date math，并且您定期滚动索引，则可以使用日期数学来缩小搜索范围。例如，以下搜索目标是最近三天创建的索引。"],["body","\n"],["body","# GET /<my-index-{now/d}-*>,<my-index-{now/d-1d}-*>,<my-index-{now/d-2d}-*>/_search\nGET /%3Cmy-index-%7Bnow%2Fd%7D-*%3E%2C%3Cmy-index-%7Bnow%2Fd-1d%7D-*%3E%2C%3Cmy-index-%7Bnow%2Fd-2d%7D-*%3E/_search\n"],["body","\n"],["headingLink","roll-over-an-index-alias-with-one-index"],["heading","Roll over an index alias with one index"],["body","\n"],["body","# PUT <my-index-{now/d}-000001>\nPUT %3Cmy-index-%7Bnow%2Fd%7D-000001%3E\n{\n  \"aliases\": {\n    \"my-write-alias\": { }\n  }\n}\n"],["body","\n"],["headingLink","specify-settings-during-a-rollover"],["heading","Specify settings during a rollover"],["body","\n"],["body","POST my-alias/_rollover\n{\n  \"settings\": {\n    \"index.number_of_shards\": 2\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/10.ManageExistingIndices/README.html"],["title","ManageExistingIndices - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","manage-existing-indices"],["heading","Manage existing indices"],["body","\n"],["body","如果您一直在使用Curator 或其他机制来管理周期性索引，则在迁移到ILM时有两个选项:"],["body","\n\n"],["body","设置索引模板以使用ILM策略来管理新索引. 一旦ILM管理您当前的写入索引，您可以对旧索引应用适当的策略"],["body","\n"],["body","重新索引为ILM管理的索引。"],["body","\n"],["body","Starting in Curator version 5.7, Curator ignores ILM managed indices."],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/FrozenIndices.html"],["title","FrozenIndices.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","frozen-indices"],["heading","Frozen Indices"],["body","\n"],["body","经常搜索的索引保存在内存中，因为需要时间来重建它们并帮助进行有效的搜索。"],["body","\n"],["body","另一方面，可能有一些我们很少访问的索引。这些索引不需要占用内存，可以在需要时重新构建。这种索引被称为冻结索引。"],["body","\n"],["body","Elasticsearch在每次搜索该分片时都会构建冻结索引的每个分片的瞬态数据结构，并在搜索完成后立即丢弃这些数据结构。"],["body","\n"],["body","由于Elasticsearch不会在内存中维护这些瞬态数据结构，因此冻结的索引比正常索引消耗的堆要少得多。"],["body","\n"],["body","This allows for a much higher disk-to-heap ratio than would otherwise be possible."],["body","\n"],["headingLink","example-for-freezing-and-unfreezing"],["heading","Example for Freezing and Unfreezing"],["body","\n"],["body","POST /index_name/_freeze\nPOST /index_name/_unfreeze\n"],["body","\n"],["body","冻结索引的搜索预计会缓慢执行。"],["body","\n"],["body","冻结索引不适合高搜索负载。"],["body","\n"],["body","冻结索引的搜索可能需要几秒钟或几分钟才能完成，即使在索引未冻结的情况下以毫秒完成相同的搜索。"],["body","\n"],["body","冷冻后，不能写入"],["body","\n"],["headingLink","searching-a-frozen-index"],["heading","Searching a Frozen Index"],["body","\n"],["body","每个节点并发加载的冻结索引的数量受search_throttled threadpool中的线程数量的限制，默认情况下为1。要包含冻结索引，必须使用查询参数 − ignore_throttled = false执行搜索请求。"],["body","\n"],["body","GET /index_name/_search?q=user:tpoint&ignore_throttled=false\n"],["body","\n"],["headingLink","monitoring-frozen-indices"],["heading","Monitoring Frozen Indices"],["body","\n"],["body","GET /_cat/indices/index_name?v&h=i,sth\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/9.StartAndStopIndexLifecycleManagement/README.html"],["title","StartAndStopIndexLifecycleManagement - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","start-and-stop-index-lifecycle-management"],["heading","Start and stop index lifecycle management"],["body","\n\n"],["body","默认情况下，ILM服务处于 “运行” 状态，并管理具有生命周期策略的所有索引。"],["body","\n"],["body","您可以停止索引生命周期管理来暂停所有索引的管理操作"],["body","\n"],["body","例如，在执行计划维护或对群集进行可能影响ILM操作执行的更改时，您可能会停止索引生命周期管理。"],["body","\n"],["body","当您停止ILM时，[SLM](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/snapshots-take-snapshot.html # automate-snapshots-slm “使用SLM自动执行快照”) 操作也将暂停。在重新启动ILM之前，不会按计划拍摄快照。进行中的快照不受影响。"],["body","\n\n"],["headingLink","get-ilm-status"],["heading","Get ILM status"],["body","\n\n"],["body","要查看ILM服务的当前状态，请使用  Get Status API"],["body","\n\n"],["body","GET _ilm/status\n"],["body","\n"],["body","在正常操作下，响应显示ILM正在 “运行”:"],["body","\n"],["body","{\n  \"operation_mode\": \"RUNNING\"\n}\n"],["body","\n"],["headingLink","stop-ilm"],["heading","Stop ILM"],["body","\n\n"],["body","要停止ILM服务并暂停所有生命周期策略的执行，请使用 Stop API:"],["body","\n\n"],["body","POST _ilm/stop"],["body","\n"],["body","ILM服务将所有策略运行到可以安全停止的程度。当ILM服务关闭时，状态API显示ILM处于 “停止” 模式:"],["body","\n"],["body","{\n  \"operation_mode\": \"STOPPING\"\n}\n"],["body","\n"],["body","一旦所有策略都处于安全的停止点，ILM就会进入 “停止” 模式:"],["body","\n"],["body","{\n  \"operation_mode\": \"STOPPED\"\n}\n"],["body","\n"],["headingLink","start-ilm"],["heading","Start ILM"],["body","\n"],["body","To restart ILM and resume executing policies, use the Start API. This puts the ILM service in the RUNNING state and ILM begins executing policies from where it left off."],["body","\n"],["body","要重新启动ILM并恢复执行策略，请使用 [Start API](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-start.html “启动索引生命周期管理API”)。这将使ILM服务处于 “运行” 状态，并且ILM从停止的位置开始执行策略。"],["body","\n"],["body","POST _ilm/start"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/ILM.html"],["title","ILM.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","前序"],["heading","前序"],["body","\n"],["body","综合上述拆解分析可知："],["body","\n\n"],["body","\n"],["body","有了：冷热集群架构，集群的不同节点有了明确的角色之分，冷热数据得以物理隔离，SSD 固态盘使用效率会更高。"],["body","\n"],["body","\n"],["body","\n"],["body","有了：rollover 滚动索引，索引可以基于文档个数、时间、占用磁盘容量滚动升级，实现了索引的动态变化。"],["body","\n"],["body","\n"],["body","\n"],["body","有了：Shrink 压缩索引、Frozen 冷冻索引，索引可以物理层面压缩、冷冻，分别释放了磁盘空间和内存空间，提高了集群的可用性。"],["body","\n"],["body","\n"],["body","\n"],["body","除此之外，还有：Force merge 段合并、Delete 索引数据删除等操作，索引的“生、老、病、死”的全生命周期的"],["body","\n"],["body","\n\n"],["body","如上指令单个操作，非常麻烦和繁琐，有没有更为快捷的方法呢？"],["body","\n"],["body","第一：命令行可以 DSL 大综合实现。"],["body","\n"],["body","第二：可以借助 Kibana 图形化界面实现。"],["body","\n"],["headingLink","index-lifecycle"],["heading","Index lifecycle"],["body","\n"],["body","ILM defines five index lifecycle phases:"],["body","\n"],["headingLink","hot"],["heading","Hot"],["body","\n"],["body","频繁更新与查询"],["body","\n"],["headingLink","warm"],["heading","Warm"],["body","\n"],["body","索引不再更新，但仍在查询中。"],["body","\n"],["headingLink","cold"],["heading","Cold"],["body","\n"],["body","不在被更新。查询得不是很频繁"],["body","\n"],["headingLink","frozen"],["heading","Frozen"],["body","\n"],["body","该索引不再被更新，并且很少被查询。信息仍然需要搜索，但是如果这些查询非常慢，也可以。"],["body","\n"],["headingLink","delete"],["heading","Delete"],["body","\n"],["body","该索引不再需要，可以安全地删除。"],["body","\n"],["body","索引的生命周期策略指定了哪些阶段适用哪些操作，在每个阶段中执行了哪些操作以及何时在阶段之间过渡。"],["body","\n"],["body","您可以在创建索引时手动应用生命周期策略。"],["body","\n"],["body","对于时间序列索引，您需要将生命周期策略与用于在序列中创建新索引的索引模板相关联。"],["body","\n"],["body","当索引滚动时，手动应用的策略不会自动应用于新索引。"],["body","\n"],["headingLink","phase-transitions"],["heading","Phase transitions"],["body","\n"],["body","ILM根据索引的年龄在整个生命周期中移动索引。"],["body","\n\n"],["body","\n"],["body","为了控制这些转换的时间，您可以为每个阶段设置一个最小年龄。"],["body","\n"],["body","\n"],["body","\n"],["body","为了使索引移至下一阶段，当前阶段中的所有操作必须完成，并且索引必须早于下一阶段的最小年龄。配置的最小年龄必须在后续阶段之间增加，"],["body","\n"],["body","\n"],["body","\n"],["body","例如，最小年龄为10天的 “warm” 阶段之后，cold阶段只能 大于 10天"],["body","\n"],["body","\n"],["body","\n"],["body","最小年龄默认是 0：这就导致 ILM 完成阶段中的action 之后 就立马 转移到下一阶段"],["body","\n"],["body","\n"],["body","\n"],["body","如果索引 有未分配的 分片，集群状态 为 黄色，索引仍然能改 转移 到下一个阶段"],["body","\n"],["body","\n"],["body","\n"],["body","但是，由于Elasticsearch只能在绿色集群上执行某些清理任务，因此可能会产生意外的副作用。"],["body","\n"],["body","\n"],["body","\n"],["body","为了避免增加磁盘使用率和可靠性问题，请及时解决任何群集运行状况问题。"],["body","\n"],["body","\n\n"],["headingLink","phase-execution"],["heading","Phase execution"],["body","\n"],["body","ILM控制执行一个阶段中的动作的顺序以及执行哪些步骤来执行每个动作的必要索引操作。"],["body","\n"],["body","当索引进入某一阶段时，ILM会在索引元数据中缓存阶段定义。"],["body","\n"],["body","这样可以确保策略更新不会将索引置于永远无法退出阶段的状态"],["body","\n"],["body","如果可以安全地应用更改，则ILM会更新缓存的阶段定义。如果不能，则使用缓存的定义继续执行阶段。"],["body","\n\n"],["body","\n"],["body","ILM定期运行，检查索引是否符合策略标准，并执行所需的任何步骤。"],["body","\n"],["body","\n"],["body","\n"],["body","为了避免竞争条件，ILM可能需要运行多次才能执行完成操作所需的所有步骤。"],["body","\n"],["body","\n"],["body","\n"],["body","例如，如果ILM确定索引已满足展期标准，则它开始执行完成展期操作所需的步骤。"],["body","\n"],["body","\n"],["body","\n"],["body","如果它达到了前进到下一步不安全的地步，则执行停止。"],["body","\n"],["body","\n"],["body","\n"],["body","下次ILM运行时，ILM会从停止的地方开始执行。这意味着，即使将indices.lifecycle.poll_interval设置为10分钟，并且索引满足翻转标准，也可能需要20分钟才能完成翻转。"],["body","\n"],["body","\n\n"],["headingLink","各生命周期-actions-设定"],["heading","各生命周期 Actions 设定"],["body","\n"],["headingLink","hot-阶段"],["heading","Hot 阶段"],["body","\n\n"],["body","基于：max_age=3天、最大文档数为5、最大size为：50gb rollover 滚动索引。"],["body","\n"],["body","设置优先级为：100（值越大，优先级越高）。"],["body","\n\n"],["headingLink","warm-阶段"],["heading","Warm 阶段 \t \t \t \t"],["body","\n\n"],["body","实现段合并，max_num_segments 设置为1."],["body","\n"],["body","副本设置为 0。"],["body","\n"],["body","数据迁移到：warm 节点。"],["body","\n"],["body","优先级设置为：50。"],["body","\n\n"],["headingLink","cold-阶段"],["heading","Cold 阶段"],["body","\n\n"],["body","冷冻索引"],["body","\n"],["body","数据迁移到冷节点"],["body","\n\n"],["headingLink","delete-阶段"],["heading","Delete 阶段 \t"],["body","\n\n"],["body","删除索引"],["body","\n\n"],["body","关于触发滚动的条件："],["body","\n\n"],["body","Hot 阶段的触发条件：手动创建第一个满足模板要求的索引。"],["body","\n"],["body","其余阶段触发条件：min_age，索引自创建后的时间。"],["body","\n\n"],["body","时间类似：业务里面的 热节点保留 3 天，温节点保留 7 天，冷节点保留 30 天的概念。"],["body","\n"],["headingLink","实战"],["heading","实战"],["body","\n"],["headingLink","节点配置"],["heading","节点配置"],["body","\n\n"],["body","节点 node-022：主节点+数据节点+热节点（Hot）。"],["body","\n"],["body","节点 node-023：主节点+数据节点+温节点（Warm）。"],["body","\n"],["body","节点 node-024：主节点+数据节点+冷节点（Cold）。"],["body","\n\n"],["body","节点属性配置"],["body","\n"],["body","- node.attr.box_type: hot\n- node.attr.box_type: warm\n- node.attr.box_type: cold\n"],["body","\n"],["headingLink","集群刷新频率"],["heading","集群刷新频率"],["body","\n"],["body","PUT _cluster/settings\n{\n  \"persistent\": {\n    \"indices.lifecycle.poll_interval\": \"1s\"\n  }\n}\n"],["body","\n"],["headingLink","新建ilm-policy"],["heading","新建ILM Policy"],["body","\n"],["body","\n# step2:测试需要，值调的很小\nPUT _ilm/policy/my_custom_policy_filter\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"actions\": {\n          \"rollover\": {\n            \"max_age\": \"3d\",\n            \"max_docs\": 5,\n            \"max_size\": \"50gb\"\n          },\n          \"set_priority\": {\n            \"priority\": 100\n          }\n        }\n      },\n      \"warm\": {\n        \"min_age\": \"15s\",\n        \"actions\": {\n          \"forcemerge\": {\n            \"max_num_segments\": 1\n          },\n          \"allocate\": {\n            \"require\": {\n              \"box_type\": \"warm\"\n            },\n            \"number_of_replicas\": 0\n          },\n          \"set_priority\": {\n            \"priority\": 50\n          }\n        }\n      },\n      \"cold\": {\n        \"min_age\": \"30s\",\n        \"actions\": {\n          \"allocate\": {\n            \"require\": {\n              \"box_type\": \"cold\"\n            }\n          },\n          \"freeze\": {}\n        }\n      },\n      \"delete\": {\n        \"min_age\": \"45s\",\n        \"actions\": {\n          \"delete\": {}\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","索引模板"],["heading","索引模板"],["body","\n"],["body","\n# step3:创建模板，关联配置的ilm_policy\nPUT _index_template/timeseries_template\n{\n  \"index_patterns\": [\"timeseries-*\"],                 \n  \"template\": {\n    \"settings\": {\n      \"number_of_shards\": 1,\n      \"number_of_replicas\": 0,\n      \"index.lifecycle.name\": \"my_custom_policy_filter\",      \n      \"index.lifecycle.rollover_alias\": \"timeseries\",\n      \"index.routing.allocation.require.box_type\": \"hot\"\n    }\n  }\n}\n\n\n"],["body","\n"],["body","新建初始索引"],["body","\n"],["body","\n# step4:创建起始索引（便于滚动）\nPUT timeseries-000001\n{\n  \"aliases\": {\n    \"timeseries\": {\n      \"is_write_index\": true\n    }\n  }\n}\n"],["body","\n"],["headingLink","插入数据"],["heading","插入数据"],["body","\n"],["body","\n# step5：插入数据\nPUT timeseries/_bulk\n{\"index\":{\"_id\":1}}\n{\"title\":\"testing 01\"}\n{\"index\":{\"_id\":2}}\n{\"title\":\"testing 02\"}\n{\"index\":{\"_id\":3}}\n{\"title\":\"testing 03\"}\n{\"index\":{\"_id\":4}}\n{\"title\":\"testing 04\"}\n\n# step6：临界值（会滚动）\nPUT timeseries/_bulk\n{\"index\":{\"_id\":5}}\n{\"title\":\"testing 05\"}\n\n# 下一个索引数据写入\nPUT timeseries/_bulk\n{\"index\":{\"_id\":6}}\n{\"title\":\"testing 06\"}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/ShrinkIndexAPI.html"],["title","ShrinkIndexAPI.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","shrink-index-api"],["heading","Shrink index API"],["body","\n"],["body","缩减现有索引的主分片 到新的索引"],["body","\n"],["body","POST /my-index-000001/_shrink/shrunk-my-index-000001\n"],["body","\n"],["headingLink","request"],["heading","Request"],["body","\n"],["body","POST /<index>/_shrink/<target-index>\nPUT /<index>/_shrink/<target-index>\n"],["body","\n"],["headingLink","prerequisites"],["heading","Prerequisites"],["body","\n\n"],["body","If the Elasticsearch security features are enabled, you must have the manage index privilege for the index."],["body","\n"],["body","Before you can shrink an index:\n\n"],["body","The index must be read-only"],["body","\n"],["body","A copy of every shard in the index must reside on the same node."],["body","\n"],["body","The index must have a green health status."],["body","\n\n"],["body","\n\n"],["body","简单起见 推荐移除 索引的副本分片，之后在 重新添加副本"],["body","\n\n"],["body","You can use the following update index settings API request to remove an index’s replica shards"],["body","\n"],["body","relocates the index’s remaining shards to the same node, and make the index read-only."],["body","\n\n"],["body","PUT /my_source_index/_settings\n{\n  \"settings\": {\n    \"index.number_of_replicas\": 0,//1          \n    \"index.routing.allocation.require._name\": \"shrink_node_name\", //2\n    \"index.blocks.write\": true //3\n  }\n}\n"],["body","\n\n"],["body","Removes replica shards for the index."],["body","\n"],["body","Relocates the index’s shards to the shrink_node_name node. See Index-level shard allocation filtering."],["body","\n"],["body","Prevents write operations to this index. Metadata changes, such as deleting the index, are still allowed."],["body","\n\n"],["body","It can take a while to relocate the source index. Progress can be tracked with the _cat recovery API, or the cluster health API can be used to wait until all shards have relocated with the wait_for_no_relocating_shards parameter."],["body","\n"],["headingLink","description"],["heading","Description"],["body","\n\n"],["body","shrink API 允许你 缩减一个现有 的索引 到一个新索引。该索引只有很少的主分片"],["body","\n"],["body","目标索引中请求的主分片数量必须是源索引中分片数量的一个因子。"],["body","\n"],["body","例如 8个主分片 只能缩减为 4、2、1，15个主分片 只能缩减为 5、3、1"],["body","\n"],["body","如果是一个质数：则只能 缩减为 1个主分片"],["body","\n"],["body","在收缩之前，索引中每个分片的 (主或副本) 副本必须存在于同一节点上。"],["body","\n"],["body","当前 data stream  的 写索引 不能缩减，必须先要 rollover 才能缩减之前的索引"],["body","\n\n"],["headingLink","how-shrinking-works"],["heading","How shrinking works"],["body","\n\n"],["body","创建一个新的目标索引，其定义与源索引相同，但主分片数量较少。"],["body","\n"],["body","从源索引到目标索引的硬链接段。(如果文件系统不支持硬链接，那么所有段都将复制到新索引中，这是一个更耗时的过程。如果使用多个数据路径，如果不同数据路径上的分片不在同一磁盘上，则需要段文件的完整副本，因为硬链接无法跨磁盘工作)"],["body","\n"],["body","Recovers the target index as though it were a closed index which had just been re-opened."],["body","\n\n"],["headingLink","shrink-an-index"],["heading","Shrink an index"],["body","\n"],["body","To shrink my_source_index into a new index called my_target_index, issue the following request:"],["body","\n"],["body","POST /my_source_index/_shrink/my_target_index\n{\n  \"settings\": {\n    \"index.routing.allocation.require._name\": null, //1\n    \"index.blocks.write\": null //2\n  }\n}\n\n"],["body","\n\n"],["body","Clear the allocation requirement copied from the source index."],["body","\n"],["body","Clear the index write block copied from the source index."],["body","\n\n"],["body","The above request returns immediately once the target index has been added to the cluster state — it doesn’t wait for the shrink operation to start."],["body","\n"],["body","Indices can only be shrunk if they satisfy the following requirements:"],["body","\n\n"],["body","The target index must not exist."],["body","\n"],["body","The source index must have more primary shards than the target index."],["body","\n"],["body","The number of primary shards in the target index must be a factor of the number of primary shards in the source index. The source index must have more primary shards than the target index."],["body","\n"],["body","The index must not contain more than 2,147,483,519 documents in total across all shards that will be shrunk into a single shard on the target index as this is the maximum number of docs that can fit into a single shard."],["body","\n"],["body","The node handling the shrink process must have sufficient free disk space to accommodate a second copy of the existing index."],["body","\n\n"],["body","The _shrink API is similar to the create index API and accepts settings and aliases parameters for the target index:"],["body","\n"],["body","POST /my_source_index/_shrink/my_target_index\n{\n  \"settings\": {\n    \"index.number_of_replicas\": 1,\n    \"index.number_of_shards\": 1, \n    \"index.codec\": \"best_compression\" \n  },\n  \"aliases\": {\n    \"my_search_indices\": {}\n  }\n}\n"],["body","\n"],["body","Best compression will only take affect when new writes are made to the index, such as when force-merging the shard to a single segment."],["body","\n"],["body","Mappings may not be specified in the _shrink request."],["body","\n"],["headingLink","monitor-the-shrink-process"],["heading","Monitor the shrink process"],["body","\n"],["body","监控"],["body","\n"],["body","The shrink process can be monitored with the _cat recovery API, or the cluster health API can be used to wait until all primary shards have been allocated by setting the wait_for_status parameter to yellow."],["body","\n"],["body","shrinkAPI返回"],["body","\n\n"],["body","_shrinkAPI 一旦被添加到集群状态上，会马上返回"],["body","\n"],["body","此时分片还未分配到任务节点上 所有分片 处于 未分配状态"],["body","\n"],["body","处于某种原因，不能将 索引分配到 shrink node 。主节点会保持 未分配状态 直到 有节点可用"],["body","\n"],["body","一旦主分片 被分配 了，会成为 initializing 状态。shrink 进程就会开始。"],["body","\n"],["body","shrink 进程完成后。分片就会 active"],["body","\n"],["body","在那时，Elasticsearch将尝试分配任何副本，并可能决定将主分片重新定位到另一个节点。"],["body","\n\n"],["headingLink","wait-for-active-shards"],["heading","Wait for active shards"],["body","\n"],["body","Because the shrink operation creates a new index to shrink the shards to, the wait for active shards setting on index creation applies to the shrink index action as well."],["body","\n"],["headingLink","path-parameters"],["heading","Path parameters"],["body","\n\n"],["body","\n"],["body","<index>"],["body","\n"],["body","(Required, string) Name of the source index to shrink."],["body","\n"],["body","\n"],["body","\n"],["body","<target-index>"],["body","\n"],["body","(Required, string) Name of the target index to create.Index names must meet the following criteria:Lowercase onlyCannot include \\, /, *, ?, \", <, >, |,   (space character), ,, #Indices prior to 7.0 could contain a colon (:), but that’s been deprecated and won’t be supported in 7.0+Cannot start with -, _, +Cannot be . or ..Cannot be longer than 255 bytes (note it is bytes, so multi-byte characters will count towards the 255 limit faster)Names starting with . are deprecated, except for hidden indices and internal indices managed by plugins"],["body","\n"],["body","\n\n"],["headingLink","query-parameters"],["heading","Query parameters"],["body","\n"],["body","wait_for_active_shards"],["body","\n"],["body","(Optional, string) The number of shard copies that must be active before proceeding with the operation. Set to all or any positive integer up to the total number of shards in the index (number_of_replicas+1). Default: 1, the primary shard."],["body","\n"],["body","See Active shards."],["body","\n"],["body","master_timeout"],["body","\n"],["body","(Optional, time units) Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error. Defaults to 30s."],["body","\n"],["body","timeout"],["body","\n"],["body","(Optional, time units) Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error. Defaults to 30s."],["body","\n"],["headingLink","request-body"],["heading","Request body"],["body","\n"],["body","aliases"],["body","\n"],["body","索引对象"],["body","\n"],["body","settings"],["body","\n"],["body","(Optional, index setting object) Configuration options for the target index. See Index Settings."],["body","\n"],["body","max_primary_shard_size"],["body","\n"],["body","(Optional, byte units) The max primary shard size for the target index."],["body","\n\n"],["body","用于查找目标索引的最佳分片数。"],["body","\n"],["body","设置此参数时，每个分片在目标索引中的存储不会大于该参数"],["body","\n"],["body","目标索引的分片数仍将是源索引的分片计数的一个因素，但是，如果参数小于源索引中的单个分片大小，则目标索引的分片计数将等于源索引的分片计数。"],["body","\n"],["body","例如该 参数设置为 50GB。如果源索引有60个主分片，总共100G， 那么会收缩为2个主分片。每个50G，如果源索引 有60个主分片。总共100G。将会收缩到20个主分片。如果是60，4000G。则保持 原样不变"],["body","\n"],["body","这个参数跟  number_of_shards settings 冲突。两者只能设置其一"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/3.AutomateRolloverWithILM/README.html"],["title","AutomateRolloverWithILM - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","tutorial-automate-rollover-with-ilm"],["heading","Tutorial: Automate rollover with ILM"],["body","\n"],["body","当使用基于时间戳序列时，可以使用  data stream ，可以阶段性的 roll over 到下一个阶段"],["body","\n"],["body","可以让你启用 冷热集群架构 以满足 热数据的 性能要求，控制成本，强制执行保留策略，并且仍然可以充分利用您的数据"],["body","\n"],["body","Data streams 适用于  append-only  场景，如果频繁 更新或者 跨索引删除 ，推荐 使用  index alias and index template"],["body","\n"],["body","我们可以使用 ILM 管理 并 rollover 别名所指向的索引，Skip to Manage time series data without data streams."],["body","\n"],["body","要使用ILM自动化数据流的翻转和管理，您可以:"],["body","\n\n"],["body","Create a lifecycle policy that defines the appropriate phases and actions."],["body","\n"],["body","Create an index template to create the data stream and apply the ILM policy and the indices settings and mappings configurations for the backing indices."],["body","\n"],["body","Verify indices are moving through the lifecycle phases as expected."],["body","\n\n"],["body","For an introduction to rolling indices, see Rollover."],["body","\n"],["body","Beats或者Logstash"],["body","\n\n"],["body","\n"],["body","使用Beats or the Logstash Elasticsearch output plugin, lifecycle policies are set up automatically."],["body","\n"],["body","\n"],["body","\n"],["body","You can modify the default policies through Kibana Management or the ILM APIs."],["body","\n"],["body","\n\n"],["headingLink","create-a-lifecycle-policy"],["heading","Create a lifecycle policy"],["body","\n"],["body","\n"],["body","创建声明周期策略"],["body","\n"],["body","\n"],["body","生命周期策略指定索引生命周期中的阶段以及在每个阶段中要执行的操作。生命周期最多可以有五个阶段:  hot, warm, cold, frozen, and delete."],["body","\n"],["body","例如，您可以定义一个具有两个阶段的timeseries_policy:"],["body","\n\n"],["body","一个热阶段，它定义了一个rollover操作，用于指定索引在达到50 gb的max_primary_shard_size或30天的max_age时滚动。"],["body","\n"],["body","一个 “删除” 阶段，将min_age设置为在翻转后90天删除索引。"],["body","\n"],["body","min_age值是相对于rollover time时间，而不是索引创建时间。"],["body","\n\n"],["body","APIOrKibana"],["body","\n\n"],["body","\n"],["body","You can create the policy through Kibana or with the create or update policy API."],["body","\n"],["body","\n"],["body","\n"],["body","To create the policy from Kibana, open the menu and go to Stack Management > Index Lifecycle Policies. Click Create policy."],["body","\n"],["body","\n\n"],["headingLink","使用索引模板创建数据流来应用声明周期策略"],["heading","使用索引模板创建数据流来应用声明周期策略"],["body","\n\n"],["body","\n"],["body","要设置数据流，请首先创建索引模板以指定生命周期策略，因为模板是用于数据流的，所以它还必须包含 “data_stream” 定义."],["body","\n"],["body","\n"],["body","\n"],["body","例如，您可以创建一个 “timeseries_template”，用于将来名为 “timeseries” 的数据流。"],["body","\n"],["body","\n\n"],["body","为了使ILM能够管理数据流，模板配置一个ILM设置:"],["body","\n\n"],["body","index.lifecycle.name 指定要应用于数据流的生命周期策略的名称."],["body","\n\n"],["body","You can use the Kibana Create template wizard to add the template. From Kibana, open the menu and go to Stack Management > Index Management. In the Index Templates tab, click Create template."],["body","\n"],["body","This wizard invokes the create or update index template API to create the index template with the options you specify."],["body","\n"],["headingLink","create-the-data-stream"],["heading","Create the data stream"],["body","\n"],["body","要开始工作，请将文档索引到索引模板的index_patterns中定义的名称或通配符模式中。只要现有数据流、索引或索引别名尚未使用该名称，索引请求就会自动创建具有单个备份索引的相应数据流。Elasticsearch会自动将请求的文档索引到此支持索引中，该索引也充当流的写入索引。"],["body","\n"],["body","例如，以下请求创建timeseries数据流和调用的第一代后备索引。ds-timeseries-2099.03.08-000001。"],["body","\n"],["body","POST timeseries/_doc\n{\n  \"message\": \"logged the request\",\n  \"@timestamp\": \"1591890611\"\n}\n"],["body","\n"],["body","当满足生命周期策略中的rollover条件时，rollover操作:"],["body","\n\n"],["body","创建第二代支持索引，名为。ds-timeseries-2099.03.08-000002。因为它是timeseries数据流的后备索引，所以来自timeseries_template索引模板的配置将应用于新索引。"],["body","\n"],["body","由于它是timeseries数据流的最新一代索引，因此新创建的支持索引。ds-timeseries-2099.03.08-000002成为数据流的写索引。"],["body","\n\n"],["body","每次满足翻转条件时，都会重复此过程。您可以使用timeseries数据流名称搜索由timeseries_policy管理的所有数据流的支持索引。写操作被路由到当前写索引。读取操作将由所有支持索引处理。"],["body","\n"],["headingLink","check-lifecycle-progress"],["heading","Check lifecycle progress"],["body","\n"],["body","要获取托管索引的状态信息，请使用ILM explain API。这让你找出这样的事情:"],["body","\n\n"],["body","索引处于什么阶段以及何时进入该阶段。"],["body","\n"],["body","当前的操作以及正在执行的步骤。"],["body","\n"],["body","如果发生任何错误或进度被阻止。"],["body","\n\n"],["body","For example, the following request gets information about the timeseries data stream’s backing indices:"],["body","\n"],["body","GET .ds-timeseries-*/_ilm/explain\n"],["body","\n"],["body","The following response shows the data stream’s first generation backing index is waiting for the hot phase’s rollover action. It remains in this state and ILM continues to call check-rollover-ready until a rollover condition is met."],["body","\n"],["body","{\n  \"indices\": {\n    \".ds-timeseries-2099.03.07-000001\": {\n      \"index\": \".ds-timeseries-2099.03.07-000001\",\n      \"managed\": true,\n      \"policy\": \"timeseries_policy\",             \n      \"lifecycle_date_millis\": 1538475653281,\n      \"age\": \"30s\",                              \n      \"phase\": \"hot\",\n      \"phase_time_millis\": 1538475653317,\n      \"action\": \"rollover\",\n      \"action_time_millis\": 1538475653317,\n      \"step\": \"check-rollover-ready\",            \n      \"step_time_millis\": 1538475653317,\n      \"phase_execution\": {\n        \"policy\": \"timeseries_policy\",\n        \"phase_definition\": {                    \n          \"min_age\": \"0ms\",\n          \"actions\": {\n            \"rollover\": {\n              \"max_primary_shard_size\": \"50gb\",\n              \"max_age\": \"30d\"\n            }\n          }\n        },\n        \"version\": 1,\n        \"modified_date_in_millis\": 1539609701576\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","manage-time-series-data-without-data-streams"],["heading","Manage time series data without data streams"],["body","\n\n"],["body","\n"],["body","尽管数据流是扩展和管理时间序列数据的便捷方法，但它们被设计为 append only 。"],["body","\n"],["body","\n"],["body","\n"],["body","我们认识到可能存在一些用例，其中需要更新或删除数据，并且数据流不支持直接删除和更新请求，因此索引api需要直接用于数据流的支持索引。"],["body","\n"],["body","\n\n"],["body","在这些情况下，您可以使用索引别名来管理包含时间序列数据的索引，并定期滚动到新索引。"],["body","\n"],["body","要使用索引别名自动使用ILM对时间序列索引进行展期和管理，您可以:"],["body","\n\n"],["body","Create a lifecycle policy that defines the appropriate phases and actions. See Create a lifecycle policy above."],["body","\n"],["body","Create an index template to apply the policy to each new index."],["body","\n"],["body","Bootstrap an index as the initial write index."],["body","\n"],["body","Verify indices are moving through the lifecycle phases as expected."],["body","\n\n"],["headingLink","create-an-index-template-to-apply-the-lifecycle-policy"],["heading","Create an index template to apply the lifecycle policy"],["body","\n"],["body","要在过渡时自动将生命周期策略应用于新的写入索引，请在用于创建新索引的索引模板中指定策略。\n例如，您可以创建一个timeseries_template，该模板应用于名称与timeseries-* 索引模式匹配的新索引。\n要启用自动翻转，模板将配置两个ILM设置:"],["body","\n\n"],["body","index.lifecycle.name specifies the name of the lifecycle policy to apply to new indices that match the index pattern."],["body","\n"],["body","index.lifecycle.rollover_alias specifies the index alias to be rolled over when the rollover action is triggered for an index."],["body","\n\n"],["body","You can use the Kibana Create template wizard to add the template. To access the wizard, open the menu and go to Stack Management > Index Management. In the Index Templates tab, click Create template."],["body","\n"],["headingLink","bootstrap-the-initial-time-series-index-with-a-write-index-alias"],["heading","Bootstrap the initial time series index with a write index alias"],["body","\n"],["body","要开始工作，您需要引导一个初始索引，并将其指定为索引模板中指定的翻转别名的写索引。此索引的名称必须与模板的索引模式匹配，并以数字结尾。在翻转时，此值递增以生成新索引的名称。"],["body","\n"],["body","For example, the following request creates an index called timeseries-000001 and makes it the write index for the timeseries alias."],["body","\n"],["body","PUT timeseries-000001\n{\n  \"aliases\": {\n    \"timeseries\": {\n      \"is_write_index\": true\n    }\n  }\n}\n"],["body","\n"],["body","当满足翻转条件时，“rollover” 动作:"],["body","\n\n"],["body","Creates a new index called timeseries-000002. This matches the timeseries-* pattern, so the settings from timeseries_template are applied to the new index."],["body","\n"],["body","将新索引指定为写入索引，并使 bootstrap 索引成为只读。"],["body","\n\n"],["body","每次满足翻转条件时，此过程都会重复。您可以使用 timeseries_policy 别名搜索由 timeseries_policy 管理的所有索引。写操作被路由到当前写索引。"],["body","\n"],["headingLink","check-lifecycle-progress-1"],["heading","Check lifecycle progress"],["body","\n\n"],["body","检索托管索引的状态信息与数据流的情况非常相似。"],["body","\n"],["body","See the data stream check progress section for more information."],["body","\n"],["body","唯一的区别是indices命名空间，因此检索进度将需要以下api调用:"],["body","\n\n"],["body","GET timeseries-*/_ilm/explain\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/7.ILM/7.MigrateIndexAllocationFiltersToNodeRoles/README.html"],["title","MigrateIndexAllocationFiltersToNodeRoles - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","migrate-index-allocation-filters-to-node-roles"],["heading","Migrate index allocation filters to node roles"],["body","\n"],["body","如果你使用 自定义的节点属性 以及  attribute-based allocation filters   来通过  data tiers 迁移索引。（也称之为： hot-warm-cold architecture），我们推荐使用内置的节点角色，自动化的数据层分配  data tier allocation，使用节点角色 可以使索引自动在各层之间移动"],["body","\n"],["body","虽然我们建议依靠自动数据层分配来管理热-暖-冷体系结构中的数据，但您仍然可以使用基于属性的分配过滤器来控制分片分配以用于其他目的。"],["body","\n"],["body","Elasticsearch Service和Elastic Cloud Enterprise可以自动进行迁移。对于自管理的部署，您需要手动更新配置，ILM策略和索引以切换到节点角色。"],["body","\n"],["headingLink","migrate-to-node-roles-on-self-managed-deployments"],["heading","Migrate to node roles on self-managed deployments"],["body","\n"],["body","要切换到使用 node roles:"],["body","\n\n"],["body","Assign data nodes to the appropriate data tier."],["body","\n"],["body","Remove the attribute-based allocation settings from your index lifecycle management policy."],["body","\n"],["body","Stop setting the custom hot attribute on new indices."],["body","\n"],["body","Enforce a default tier preference on new indices."],["body","\n"],["body","Update existing indices to set a tier preference."],["body","\n\n"],["headingLink","assign-data-nodes-to-a-data-tier"],["heading","Assign data nodes to a data tier"],["body","\n\n"],["body","为每个数据节点配置适当的角色，以将其分配给一个或多个数据层\ndata_hot 、 data_content 、 data_warm 、 data_cold 或 data_frozen。节点还可以具有其他 [角色](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/modules-node.html “节点”)。默认情况下，新节点配置所有角色。"],["body","\n\n\n"],["body","When you add a data tier to an Elasticsearch Service deployment, one or more nodes are automatically configured with the corresponding role. To explicitly change the role of a node in an Elasticsearch Service deployment, use the Update deployment API. Replace the node’s node_type configuration with the appropriate node_roles. For example, the following configuration adds the node to the hot and content tiers, and enables it to act as an ingest node, remote, and transform node."],["body","\n\n"],["body","将数据层添加到Elasticsearch服务部署中时，一个或多个节点会自动配置相应的角色。要显式更改节点在Elasticsearch服务部署中的角色，请使用 [Update deployment API](https://www.elastic.co/guide/en/cloud/current/ec-api-deployment-crud.html # ec_update_a_deployment)。用适当的 node_roles 替换节点的 node_type 配置。例如，以下配置将节点添加到hot 和content tiers，并使其能够充当ingest 节点、remote程节点和转换节点。"],["body","\n"],["body","\"node_roles\": [\n  \"data_hot\",\n  \"data_content\",\n  \"ingest\",\n  \"remote_cluster_client\",\n  \"transform\"\n],\n"],["body","\n"],["body","如果直接管理自己的集群，请为 elasticsearch.yml 中的每个节点配置适当的角色。例如，以下设置将节点配置为热点层和内容层中的仅数据节点。"],["body","\n"],["body","node.roles [ data_hot, data_content ]"],["body","\n"],["headingLink","remove-custom-allocation-settings-from-existing-ilm-policies"],["heading","Remove custom allocation settings from existing ILM policies"],["body","\n\n"],["body","\n"],["body","更新每个生命周期阶段的分配操作，以删除基于属性的分配设置。这使ILM能够将 [migrate](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-migrate.html “Migrate”) 操作注入到每个阶段，以自动将索引转换到数据层中。"],["body","\n"],["body","\n"],["body","\n"],["body","如果分配操作未设置副本数，请完全删除分配操作。(空分配操作无效。)"],["body","\n"],["body","\n"],["body","\n"],["body","策略必须为架构中的每个数据层指定相应的阶段。"],["body","\n"],["body","\n"],["body","\n"],["body","必须存在每个阶段，以便ILM可以注入迁移操作以在数据层中移动索引。如果不需要执行任何其他操作，则阶段可以为空。例如，如果您为部署启用了冷暖数据层，则您的策略必须包括hot warm cold阶段"],["body","\n"],["body","\n\n"],["headingLink","stop-setting-the-custom-hot-attribute-on-new-indices"],["heading","Stop setting the custom hot attribute on new indices"],["body","\n\n"],["body","创建数据流时，它的第一个备份索引现在会自动分配给 data_hot 节点。同样，当您直接创建索引时，它会自动分配给 data_content 节点。"],["body","\n"],["body","On Elasticsearch Service deployments, remove the cloud-hot-warm-allocation-0 index template that set the hot shard allocation attribute on all indices."],["body","\n\n"],["body","DELETE _template/.cloud-hot-warm-allocation-0\n3. 如果您使用的是自定义索引模板，请对其进行更新以删除 您曾经将新索引分配给热层。 [基于属性的分配过滤器](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.17/shard-allocation-filtering.html “索引级分片分配过滤”)\n4. 为了完全避免混淆  首选项和自定义属性路由设置时引起的问题，推荐更新所有 legacy、composable、component templates 以移除 attribute-based allocation filters"],["body","\n"],["headingLink","enforce-a-default-tier-preference-on-new-indices"],["heading","Enforce a default tier preference on new indices"],["body","\n"],["body","Setting cluster.routing.allocation.enforce_default_tier_preference to true"],["body","\n"],["body","确保 新建索引 会有一个 _tier_preference,"],["body","\n"],["body","确保新创建的索引将具有覆盖 [创建索引](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/indices-create-index.html “创建索引API”) API或关联的 [索引模板](https:// Www.elastic.co/guide/en/elasticsearch/reference/7.17/index-templates.html “索引模板”)，如果其中任何一个为设置指定了 “空”。"],["body","\n"],["body","PUT _cluster/_settings\n{\n  \"persistent\": {\n    \"cluster.routing.allocation.enforce_default_tier_preference\": true\n  }\n}\n"],["body","\n"],["headingLink","set-a-tier-preference-for-existing-indices"],["heading","Set a tier preference for existing indices"],["body","\n"],["body","ILM通过将 [migrate action](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-migrate.html “Migrate”) 自动注入到每个阶段，自动将管理的索引转换到可用的数据层中。"],["body","\n"],["body","要使ILM能够在数据层中移动 istene 托管索引，请将索引设置更新为:"],["body","\n\n"],["body","Remove the custom allocation filter by setting it to null. 移除自定义分配顾虑器 设置（将其设置为NULL）"],["body","\n"],["body","Set the tier preference."],["body","\n\n"],["body","For example, if your old template set the data attribute to hot to allocate shards to the hot tier, set the data attribute to null and set the _tier_preference to data_hot."],["body","\n"],["body","PUT my-index/_settings\n{\n  \"index.routing.allocation.require.data\": null,\n  \"index.routing.allocation.include._tier_preference\": \"data_hot\"\n}\n"],["body","\n"],["body","For indices that have already transitioned out of the hot phase, the tier preference should include the appropriate fallback tiers to ensure index shards can be allocated if the preferred tier is unavailable. For example, specify the hot tier as the fallback for indices already in the warm phase."],["body","\n"],["body","对于已经过渡到热阶段的索引，tier preference 应包含适当的 fallback tiers，以确保在首选层不可用时可以分配索引分片。例如，将热层指定为已经处于wam 阶段 的索引的 fallback"],["body","\n"],["body","PUT my-index/_settings\n{\n  \"index.routing.allocation.require.data\": null,\n  \"index.routing.allocation.include._tier_preference\": \"data_warm,data_hot\"\n}\n"],["body","\n"],["body","如果索引已经处于Cold阶段，则应 包含 冷，暖和热层。"],["body","\n"],["body","For indices that have both the _tier_preference and require.data configured but the _tier_preference is outdated (ie. the node attribute configuration is \"colder\" than the configured _tier_preference), the migration needs to remove the require.data attribute and update the _tier_preference to reflect the correct tiering."],["body","\n"],["body","对于同时配置了 _ tier_preference 和 require.Data 但 _ tier_preference 已过时的索引 (即，节点属性配置比配置的 _ tier_preference “冷”)，迁移需要删除 require.Data 属性并更新 _ tier_preference 以反映正确的分层。"],["body","\n"],["body","例如。对于具有以下路由配置的索引:"],["body","\n"],["body","{\n  \"index.routing.allocation.require.data\": \"warm\",\n  \"index.routing.allocation.include._tier_preference\": \"data_hot\"\n}\n"],["body","\n"],["body","The routing configuration should be fixed like so:"],["body","\n"],["body","PUT my-index/_settings\n{\n  \"index.routing.allocation.require.data\": null,\n  \"index.routing.allocation.include._tier_preference\": \"data_warm,data_hot\"\n}\n"],["body","\n"],["body","这种情况可能发生在默认为数据层的系统中，例如，当使用节点属性的ILM策略被恢复并将托管索引从热阶段转换到Warm阶段时。在这种情况下，节点属性配置指示应在其中分配索引的正确层。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/Datamanagement/README.html"],["title","Datamanagement - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","data-management"],["heading","Data management"],["body","\n"],["body","您存储在Elasticsearch中的数据通常分为以下两类之一:"],["body","\n\n"],["body","Content: 您要搜索的项目集合，例如产品目录"],["body","\n"],["body","Time series data: 连续生成的带时间戳的数据流，例如日志条目"],["body","\n\n\n"],["body","内容可能会经常更新，但内容的值随着时间的推移保持相对恒定"],["body","\n"],["body","您希望能够快速检索项目，而不管它们的年龄有多大。"],["body","\n"],["body","时间序列数据会随着时间的推移而不断累积，因此您需要策略来平衡数据的价值与存储数据的成本"],["body","\n"],["body","随着年龄的增长，它往往变得不那么重要，并且访问频率较低，因此您可以将其移至价格更低，性能更低的硬件"],["body","\n"],["body","对于最旧的数据，重要的是您可以访问数据。如果查询需要更长的时间才能完成，这是可以接受的。"],["body","\n\n"],["body","为了帮助您管理数据，Elasticsearch使您能够:"],["body","\n\n"],["body","Define multiple tiers of data nodes with different performance characteristics：定义具有不同性能特征的多层数据节点。"],["body","\n"],["body","根据您的性能需求和保留策略，在数据层中自动转换索引，使用  index lifecycle management (ILM)."],["body","\n"],["body","利用存储在远程存储库中的可搜索快照 searchable snapshots ，为您的旧索引提供弹性，同时降低运营成本并保持搜索性能。"],["body","\n"],["body","存储在性能较差的硬件上的数据执行异步搜索 asynchronous searches"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/Datamanagement/DataTier.html"],["title","DataTier.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","data-tiers"],["heading","Data tiers"],["body","\n"],["body","数据层是具有相同 data Role 的节点的集合，这些节点通常共享相同的硬件配置文件:"],["body","\n\n"],["body","Content tier 节点  处理  诸如  产品目录之类的内容的索引和查询 负载"],["body","\n"],["body","Hot tier 节点 处理 时间序列数据 (如日志或指标) 的索引负载，并保存您最近的、最频繁访问的数据。"],["body","\n"],["body","Warm tier 节点保存访问频率较低且很少需要更新的时间序列数据。"],["body","\n"],["body","Cold tier 节点保存不经常访问且通常不更新的时间序列数据。"],["body","\n"],["body","Frozen tier 节点保存很少访问且从未更新的时间序列数据，并保留在可搜索的快照中。"],["body","\n\n\n"],["body","当您将文档直接索引到特定索引时，它们会无限期地保留在content tier nodes"],["body","\n"],["body","当您将文档索引到数据流时，它们最初驻留在热层节点上"],["body","\n"],["body","可以配置  index lifecycle management (ILM)   策略 使其自动 转化时间序列 到   the hot, warm, and cold tiers"],["body","\n"],["body","data role  配置在  elasticsearch.yml   。对于高性能节点配置  data_hot, data_content 角色"],["body","\n\n"],["body","node.roles: [\"data_hot\", \"data_content\"]\n"],["body","\n"],["headingLink","content-tier"],["heading","Content tier"],["body","\n\n"],["body","存储在  content tier 通常是  物品或文章的集合"],["body","\n"],["body","与时间序列数据不同，随着时间的推移，内容的值保持相对恒定，因此随着时间的推移，将其移动到具有不同性能特征的层是没有意义的"],["body","\n"],["body","content tier  nodes 通常针对查询性能进行了优化-它们优先考虑处理能力而不是IO吞吐量，因此它们可以处理复杂的搜索和聚合并快速返回结果"],["body","\n"],["body","虽然他们也负责索引，但 content data 的摄取速度通常不会像日志和指标等时间序列数据那样高"],["body","\n"],["body","从弹性的角度来看，此层中的索引应配置为使用一个或多个副本。"],["body","\n"],["body","内容层是必需的。系统索引和其他不属于数据流的索引会自动分配给内容层。"],["body","\n\n"],["headingLink","hot-tier"],["heading","Hot tier"],["body","\n\n"],["body","hot层是时间序列数据的Elasticsearch入口点，"],["body","\n"],["body","保存您最近、最常搜索的时间序列数据。热层中的节点需要快速的读写，这需要更多的硬件资源和更快的存储 (ssd)。为了恢复弹性，应将hot层中的索引配置为使用一个或多个副本。"],["body","\n"],["body","热层是必需的。作为数据流一部分的新索引会自动分配给热层。"],["body","\n\n"],["headingLink","warm-tier"],["heading","Warm tier"],["body","\n\n"],["body","与热层中的最近索引的数据相比，时间序列数据被查询的频率较低，可以移动到温暖层。"],["body","\n"],["body","温暖层通常保存最近几周的数据。仍然允许更新，但可能很少。"],["body","\n"],["body","暖层中的节点通常不需要像热层中的节点那样快。"],["body","\n"],["body","为了恢复弹性，应将温暖层中的索引配置为使用一个或多个副本。"],["body","\n\n"],["headingLink","cold-tier"],["heading","Cold tier"],["body","\n\n"],["body","一旦数据不再被更新，它就可以从温暖层移动到寒冷层，在那里它停留，而不经常被查询"],["body","\n"],["body","冷层仍然 响应查询请求 ，但是冷层中的数据通常不会更新。"],["body","\n"],["body","当数据过渡到冷层时，它可以被压缩和缩小"],["body","\n"],["body","为了弹性， the cold tier can use fully mounted indices of searchable snapshots, 消除了对副本的需求。"],["body","\n\n"],["headingLink","frozen-tier"],["heading","Frozen tier"],["body","\n\n"],["body","一旦数据不再被查询，或者很少被查询，它可能会从冷层移动到冻结层，在那里它会在其余生中停留。"],["body","\n"],["body","The frozen tier uses partially mounted indices to store and load data from a snapshot repository."],["body","\n"],["body","这降低了本地存储和运营成本，同时仍然让您搜索冻结的数据."],["body","\n"],["body","由于Elasticsearch有时必须从快照存储库中获取冻结的数据，因此冻结层上的搜索通常比冷层上的要慢。"],["body","\n"],["body","We recommend you use dedicated nodes in the frozen tier."],["body","\n\n"],["headingLink","data-tier-index-allocation"],["heading","Data tier index allocation"],["body","\n"],["body","\n"],["body","数据分层时的索引分配"],["body","\n"],["body","\n\n"],["body","创建索引时，默认情况下，Elasticsearch将  index.routing.allocation.include._tier_preference  设置为data_content，以自动将索引分片分配给content tier."],["body","\n"],["body","创建数据流时， index.routing.allocation.include._tier_preference  设置为 data_hot"],["body","\n"],["body","可以手动指定  shard allocation filtering settings   以 覆盖默认"],["body","\n"],["body","您还可以显式设置index.routing.allocation.include._Tier_preference以选择退出默认的基于层的分配。如果将层首选项设置为null，则Elasticsearch会在分配过程中忽略数据层角色。"],["body","\n\n"],["headingLink","automatic-data-tier-migration"],["heading","Automatic data tier migration"],["body","\n"],["body","\n"],["body","自动data tier 迁移"],["body","\n"],["body","\n\n"],["body","\n"],["body","ILM使用 “迁移” migrate  操作自动将托管索引转换到可用的数据层中。"],["body","\n"],["body","\n"],["body","\n"],["body","默认情况下，此操作会在每个阶段自动注入。"],["body","\n"],["body","\n"],["body","\n"],["body","您可以显式指定迁移操作以覆盖默认行为，或者使用分配操作 allocate action  手动指定分配规则。"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/snapshotAndRestore/DeleteSnapShot.html"],["title","DeleteSnapShot.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","delete-snapshot-api"],["heading","Delete snapshot API"],["body","\n"],["body","DELETE /_snapshot/my_repository/my_snapshot\n\n"],["body","\n"],["headingLink","request"],["heading","Request"],["body","\n"],["body","DELETE /_snapshot/<repository>/<snapshot>\n"],["body","\n"],["headingLink","description"],["heading","Description"],["body","\n\n"],["body","使用删除快照API删除快照，快照是从运行中的Elasticsearch集群获取的备份。"],["body","\n"],["body","从存储库中删除快照时，Elasticsearch会删除与该快照关联且未被任何其他快照使用的所有文件。与至少一个其他现有快照共享的所有文件都保持不变。"],["body","\n"],["body","如果在创建快照时尝试删除快照，快照过程将中止，并且所有关联的快照都将被删除。"],["body","\n"],["body","要在单个请求中删除多个快照，请使用逗号分隔快照名称或使用通配符 (*)。提示"],["body","\n"],["body","使用删除快照API取消错误启动的长时间运行的快照操作。"],["body","\n\n"],["headingLink","path-parameters"],["heading","Path parameters"],["body","\n\n"],["body","\n"],["body","<repository>"],["body","\n"],["body","(Required, string) Name of the repository to delete a snapshot from."],["body","\n"],["body","\n"],["body","\n"],["body","<snapshot>"],["body","\n"],["body","(Required, string) Comma-separated list of snapshot names to delete. Also accepts wildcards (*)."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/snapshotAndRestore/repository-s3.html"],["title","repository-s3.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","s3-repository-plugin"],["heading","S3 Repository Plugin"],["body","\n"],["body","该插件添加了 对 AWS S3 的 快照备份恢复的 支持"],["body","\n"],["headingLink","installation"],["heading","Installation"],["body","\n"],["body","sudo bin/elasticsearch-plugin install repository-s3\n"],["body","\n"],["headingLink","removal"],["heading","Removal"],["body","\n"],["body","sudo bin/elasticsearch-plugin remove repository-s3\n"],["body","\n"],["body","插件必须安装在集群中的每个节点上，并且每个节点都必须在安装后重新启动。"],["body","\n"],["body","multipass exec primary sudo /usr/share/elasticsearch/bin/elasticsearch-plugin install repository-s3\n\nmultipass exec primary sudo systemctl restart elasticsearch\n\nmultipass exec node-2 sudo /usr/share/elasticsearch/bin/elasticsearch-plugin install repository-s3\nmultipass exec node-2 sudo systemctl restart elasticsearch\n\nmultipass exec node-3 sudo /usr/share/elasticsearch/bin/elasticsearch-plugin install repository-s3\nmultipass exec node-3 sudo systemctl restart elasticsearch\n"],["body","\n"],["headingLink","配置"],["heading","配置"],["body","\n"],["body","The plugin provides a repository type named s3 which may be used when creating a repository."],["body","\n"],["body","The repository defaults to using ECS IAM Role or EC2 IAM Role credentials for authentication."],["body","\n"],["body","The only mandatory setting is the bucket name:"],["body","\n"],["body","PUT _snapshot/my_s3_repository\n{\n  \"type\": \"s3\",\n  \"settings\": {\n    \"bucket\": \"my-bucket\"\n  }\n}\n"],["body","\n"],["headingLink","客户端配置"],["heading","客户端配置"],["body","\n\n"],["body","\n"],["body","用于连接到S3的客户端具有许多可用设置。"],["body","\n"],["body","\n"],["body","\n"],["body","The settings have the form s3.client.CLIENT_NAME.SETTING_NAME."],["body","\n"],["body","\n"],["body","\n"],["body","By default, s3 repositories use a client named default, but this can be modified using the repository setting client. For example:"],["body","\n"],["body","\n\n"],["body","PUT _snapshot/my_s3_repository\n{\n  \"type\": \"s3\",\n  \"settings\": {\n    \"bucket\": \"my-bucket\",\n    \"client\": \"my-alternate-client\"\n  }\n}\n"],["body","\n\n"],["body","\n"],["body","大多数配置可以添加到 elasticsearch.yml，除了安全配置（他们被添加到Elasticsearch keystore ）"],["body","\n"],["body","\n"],["body","\n"],["body","For more information about creating and updating the Elasticsearch keystore, see Secure settings."],["body","\n"],["body","\n\n"],["body","例如，如果要使用特定凭据访问S3，则运行以下命令将这些凭据添加到密钥库:"],["body","\n"],["body","bin/elasticsearch-keystore add s3.client.default.access_key\nbin/elasticsearch-keystore add s3.client.default.secret_key\n# a session token is optional so the following command may not be needed\nbin/elasticsearch-keystore add s3.client.default.session_token\n"],["body","\n"],["body","相反，如果要使用实例角色或容器角色来访问S3，则应清空这些设置"],["body","\n"],["body","可以通过 移除以下设置，来从特定凭据   切换回  instance role or container role 的默认凭据"],["body","\n"],["body","bin/elasticsearch-keystore remove s3.client.default.access_key\nbin/elasticsearch-keystore remove s3.client.default.secret_key\n# a session token is optional so the following command may not be needed\nbin/elasticsearch-keystore remove s3.client.default.session_token\n"],["body","\n\n"],["body","All client secure settings of this plugin are reloadable."],["body","\n"],["body","当你手动 重载配置后。内部S3客户端也会重载"],["body","\n"],["body","正在进行的快照/还原任务不会被  客户端安全设置的重新加载所抢占。"],["body","\n"],["body","该任务将使用客户端完成，因为它是在操作开始时构建的。"],["body","\n\n"],["body","以下列表包含可用的客户端设置，必须存储在密钥库中的配置，且被标记为 “安全” 可以重新加载; 其他设置属于elasticsearch.yml文件。"],["body","\n\n"],["body","\n"],["body","access_key (Secure, reloadable)"],["body","\n"],["body","An S3 access key. If set, the secret_key setting must also be specified. If unset, the client will use the instance or container role instead."],["body","\n"],["body","\n"],["body","\n"],["body","secret_key (Secure, reloadable)"],["body","\n"],["body","An S3 secret key. If set, the access_key setting must also be specified."],["body","\n"],["body","\n"],["body","\n"],["body","session_token (Secure, reloadable)"],["body","\n"],["body","An S3 session token. If set, the access_key and secret_key settings must also be specified."],["body","\n"],["body","\n"],["body","\n"],["body","endpoint"],["body","\n"],["body","The S3 service endpoint to connect to. This defaults to s3.amazonaws.com but the AWS documentation lists alternative S3 endpoints. If you are using an S3-compatible service then you should set this to the service’s endpoint."],["body","\n"],["body","\n"],["body","\n"],["body","protocol"],["body","\n"],["body","The protocol to use to connect to S3. Valid values are either http or https. Defaults to https."],["body","\n"],["body","\n"],["body","\n"],["body","proxy.host"],["body","\n"],["body","The host name of a proxy to connect to S3 through."],["body","\n"],["body","\n"],["body","\n"],["body","proxy.port"],["body","\n"],["body","The port of a proxy to connect to S3 through."],["body","\n"],["body","\n"],["body","\n"],["body","proxy.username (Secure, reloadable)"],["body","\n"],["body","The username to connect to the proxy.host with."],["body","\n"],["body","\n"],["body","\n"],["body","proxy.password (Secure, reloadable)"],["body","\n"],["body","The password to connect to the proxy.host with."],["body","\n"],["body","\n"],["body","\n"],["body","read_timeout"],["body","\n"],["body","The socket timeout for connecting to S3. The value should specify the unit. For example, a value of 5s specifies a 5 second timeout. The default value is 50 seconds."],["body","\n"],["body","\n"],["body","\n"],["body","max_retries"],["body","\n"],["body","The number of retries to use when an S3 request fails. The default value is 3."],["body","\n"],["body","\n"],["body","\n"],["body","use_throttle_retries"],["body","\n"],["body","Whether retries should be throttled (i.e. should back off). Must be true or false. Defaults to true."],["body","\n"],["body","\n"],["body","\n"],["body","path_style_access"],["body","\n"],["body","Whether to force the use of the path style access pattern. If true, the path style access pattern will be used. If false, the access pattern will be automatically determined by the AWS Java SDK (See AWS documentation for details). Defaults to false."],["body","\n"],["body","\n\n"],["body","In versions 7.0, 7.1, 7.2 and 7.3 all bucket operations used the now-deprecated path style access pattern. If your deployment requires the path style access pattern then you should set this setting to true when upgrading."],["body","\n\n"],["body","\n"],["body","disable_chunked_encoding"],["body","\n"],["body","Whether chunked encoding should be disabled or not. If false, chunked encoding is enabled and will be used where appropriate. If true, chunked encoding is disabled and will not be used, which may mean that snapshot operations consume more resources and take longer to complete. It should only be set to true if you are using a storage service that does not support chunked encoding. See the AWS Java SDK documentation for details. Defaults to false."],["body","\n"],["body","\n"],["body","\n"],["body","region"],["body","\n"],["body","Allows specifying the signing region to use. Specificing this setting manually should not be necessary for most use cases. Generally, the SDK will correctly guess the signing region to use. It should be considered an expert level setting to support S3-compatible APIs that require v4 signatures and use a region other than the default us-east-1. Defaults to empty string which means that the SDK will try to automatically determine the correct signing region."],["body","\n"],["body","\n"],["body","\n"],["body","signer_override"],["body","\n"],["body","Allows specifying the name of the signature algorithm to use for signing requests by the S3 client. Specifying this setting should not be necessary for most use cases. It should be considered an expert level setting to support S3-compatible APIs that do not support the signing algorithm that the SDK automatically determines for them. See the AWS Java SDK documentation for details. Defaults to empty string which means that no signing algorithm override will be used."],["body","\n"],["body","\n\n"],["headingLink","s3-compatible-services"],["heading","S3-compatible services"],["body","\n\n"],["body","有许多存储系统提供了S3-compatible的API"],["body","\n"],["body","repository-s3 plugin 可以使 这些系统 与 AWS S3 开箱即用"],["body","\n"],["body","需要提供 s3.client.CLIENT_NAME.endpoint"],["body","\n"],["body","也可以提供  s3.client.CLIENT_NAME.protocol"],["body","\n\n"],["body","Minio兼容"],["body","\n"],["body","Minio is an example of a storage system that provides an S3-compatible API. The repository-s3 plugin allows Elasticsearch to work with Minio-backed repositories as well as repositories stored on AWS S3. Other S3-compatible storage systems may also work with Elasticsearch, but these are not covered by the Elasticsearch test suite."],["body","\n"],["body","完全的S3API兼容"],["body","\n"],["body","Note that some storage systems claim to be S3-compatible without correctly supporting the full S3 API. The repository-s3 plugin requires full compatibility with S3. In particular it must support the same set of API endpoints, return the same errors in case of failures, and offer a consistency model no weaker than S3’s when accessed concurrently by multiple nodes. Incompatible error codes and consistency models may be particularly hard to track down since errors and consistency failures are usually rare and hard to reproduce."],["body","\n"],["body","使用仓库分析来检查兼容性"],["body","\n"],["body","You can perform some basic checks of the suitability of your storage system using the repository analysis API. If this API does not complete successfully, or indicates poor performance, then your storage system is not fully compatible with AWS S3 and therefore unsuitable for use as a snapshot repository. You will need to work with the supplier of your storage system to address any incompatibilities you encounter."],["body","\n"],["headingLink","repository-settings"],["heading","Repository Settings"],["body","\n"],["body","PUT _snapshot/my_s3_repository\n{\n  \"type\": \"s3\",\n  \"settings\": {\n    \"bucket\": \"my-bucket\",\n    \"another_setting\": \"setting-value\"\n  }\n}\n"],["body","\n"],["body","bucket"],["body","\n"],["body","(Required) Name of the S3 bucket to use for snapshots."],["body","\n"],["body","The bucket name must adhere to Amazon’s S3 bucket naming rules."],["body","\n"],["body","client"],["body","\n"],["body","The name of the S3 client to use to connect to S3. Defaults to default."],["body","\n"],["body","base_path"],["body","\n"],["body","Specifies the path to the repository data within its bucket. Defaults to an empty string, meaning that the repository is at the root of the bucket. The value of this setting should not start or end with a /."],["body","\n"],["body","chunk_size"],["body","\n"],["body","Big files can be broken down into chunks during snapshotting if needed. Specify the chunk size as a value and unit, for example: 1TB, 1GB, 10MB. Defaults to the maximum size of a blob in the S3 which is 5TB."],["body","\n"],["body","compress"],["body","\n"],["body","When set to true metadata files are stored in compressed format. This setting doesn’t affect index files that are already compressed by default. Defaults to false."],["body","\n"],["body","max_restore_bytes_per_sec"],["body","\n"],["body","Throttles per node restore rate. Defaults to unlimited. Note that restores are also throttled through recovery settings."],["body","\n"],["body","max_snapshot_bytes_per_sec"],["body","\n"],["body","Throttles per node snapshot rate. Defaults to 40mb per second."],["body","\n"],["body","readonly"],["body","\n"],["body","Makes repository read-only. Defaults to false."],["body","\n"],["body","server_side_encryption"],["body","\n"],["body","When set to true files are encrypted on server side using AES256 algorithm. Defaults to false."],["body","\n"],["body","buffer_size"],["body","\n"],["body","Minimum threshold below which the chunk is uploaded using a single request. Beyond this threshold, the S3 repository will use the AWS Multipart Upload API to split the chunk into several parts, each of buffer_size length, and to upload each part in its own request. Note that setting a buffer size lower than 5mb is not allowed since it will prevent the use of the Multipart API and may result in upload errors. It is also not possible to set a buffer size greater than 5gb as it is the maximum upload size allowed by S3. Defaults to 100mb or 5% of JVM heap, whichever is smaller."],["body","\n"],["body","canned_acl"],["body","\n"],["body","The S3 repository supports all S3 canned ACLs : private, public-read, public-read-write, authenticated-read, log-delivery-write, bucket-owner-read, bucket-owner-full-control. Defaults to private. You could specify a canned ACL using the canned_acl setting. When the S3 repository creates buckets and objects, it adds the canned ACL into the buckets and objects."],["body","\n"],["body","storage_class"],["body","\n"],["body","Sets the S3 storage class for objects stored in the snapshot repository. Values may be standard, reduced_redundancy, standard_ia, onezone_ia and intelligent_tiering. Defaults to standard. Changing this setting on an existing repository only affects the storage class for newly created objects, resulting in a mixed usage of storage classes. Additionally, S3 Lifecycle Policies can be used to manage the storage class of existing objects. Due to the extra complexity with the Glacier class lifecycle, it is not currently supported by the plugin. For more information about the different classes, see AWS Storage Classes Guide"],["body","\n\n"],["body","除了上述设置之外，您还可以在存储库设置中指定所有非安全客户端设置。"],["body","\n"],["body","repository settings 优先级 大于 客户端设置"],["body","\n\n"],["body","PUT _snapshot/my_s3_repository\n{\n  \"type\": \"s3\",\n  \"settings\": {\n    \"client\": \"my-client\",\n    \"bucket\": \"my-bucket\",\n    \"endpoint\": \"my.s3.endpoint\"\n  }\n}\n"],["body","\n"],["headingLink","recommended-s3-permissions"],["heading","Recommended S3 Permissions"],["body","\n\n"],["body","\n"],["body","为了将Elasticsearch快照进程限制为所需的最低资源"],["body","\n"],["body","\n"],["body","\n"],["body","我们建议将Amazon IAM与预先存在的S3存储桶结合使用。"],["body","\n"],["body","\n"],["body","\n"],["body","Here is an example policy which will allow the snapshot access to an S3 bucket named \"snaps.example.com\". This may be configured through the AWS IAM console, by creating a Custom Policy, and using a Policy Document similar to this (changing snaps.example.com to your bucket name)."],["body","\n"],["body","\n\n"],["body","{\n  \"Statement\": [\n    {\n      \"Action\": [\n        \"s3:ListBucket\",\n        \"s3:GetBucketLocation\",\n        \"s3:ListBucketMultipartUploads\",\n        \"s3:ListBucketVersions\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": [\n        \"arn:aws:s3:::snaps.example.com\"\n      ]\n    },\n    {\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\",\n        \"s3:AbortMultipartUpload\",\n        \"s3:ListMultipartUploadParts\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": [\n        \"arn:aws:s3:::snaps.example.com/*\"\n      ]\n    }\n  ],\n  \"Version\": \"2012-10-17\"\n}\n"],["body","\n"],["body","{\n  \"Statement\": [\n    {\n      \"Action\": [\n        \"s3:ListBucket\",\n        \"s3:GetBucketLocation\",\n        \"s3:ListBucketMultipartUploads\",\n        \"s3:ListBucketVersions\"\n      ],\n      \"Condition\": {\n        \"StringLike\": {\n          \"s3:prefix\": [\n            \"foo/*\"\n          ]\n        }\n      },\n      \"Effect\": \"Allow\",\n      \"Resource\": [\n        \"arn:aws:s3:::snaps.example.com\"\n      ]\n    },\n    {\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\",\n        \"s3:AbortMultipartUpload\",\n        \"s3:ListMultipartUploadParts\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": [\n        \"arn:aws:s3:::snaps.example.com/foo/*\"\n      ]\n    }\n  ],\n  \"Version\": \"2012-10-17\"\n}\n"],["body","\n"],["headingLink","aws-vpc-bandwidth-settings"],["heading","AWS VPC Bandwidth Settings"],["body","\n"],["body","AWS instances resolve S3 endpoints to a public IP. If the Elasticsearch instances reside in a private subnet in an AWS VPC then all traffic to S3 will go through the VPC’s NAT instance. If your VPC’s NAT instance is a smaller instance size (e.g. a t2.micro) or is handling a high volume of network traffic your bandwidth to S3 may be limited by that NAT instance’s networking bandwidth limitations. Instead we recommend creating a VPC endpoint that enables connecting to S3 in instances that reside in a private subnet in an AWS VPC. This will eliminate any limitations imposed by the network bandwidth of your VPC’s NAT instance."],["body","\n"],["body","Instances residing in a public subnet in an AWS VPC will connect to S3 via the VPC’s internet gateway and not be bandwidth limited by the VPC’s NAT instance."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/snapshotAndRestore/RestoreSnapshot.html"],["title","RestoreSnapshot.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","restore-snapshot-api"],["heading","Restore snapshot API"],["body","\n"],["body","POST /_snapshot/my_repository/my_snapshot/_restore\n"],["body","\n"],["headingLink","request"],["heading","Request"],["body","\n"],["body","POST /_snapshot/<repository>/<snapshot>/_restore\n"],["body","\n"],["headingLink","description"],["heading","Description"],["body","\n\n"],["body","使用restore snapshot API恢复集群的快照，包括快照中的所有数据流和索引。"],["body","\n"],["body","如果您不想恢复整个快照，可以选择要恢复的特定数据流或索引。"],["body","\n\n"],["body","恢复的条件"],["body","\n\n"],["body","\n"],["body","您可以在"],["body","\n\n"],["body","包含所选主节点"],["body","\n"],["body","并 具有足够容量以容纳要还原的快照的数据节点的"],["body","\n\n"],["body","群集上运行还原操作。"],["body","\n"],["body","\n"],["body","\n"],["body","仅当现有索引已关闭并且与快照中的索引具有相同数量的分片时，才能恢复它们。"],["body","\n"],["body","\n"],["body","\n"],["body","如果它们已关闭：还原操作会自动打开已恢复的索引"],["body","\n"],["body","\n"],["body","\n"],["body","如果它们在群集中不存在：则会创建新的索引"],["body","\n"],["body","\n\n"],["body","数据流的恢复"],["body","\n\n"],["body","\n"],["body","如果恢复了数据流，则其后备索引也将恢复。"],["body","\n"],["body","\n"],["body","\n"],["body","或者，您可以在不恢复整个数据流的情况下恢复单个备份索引。"],["body","\n"],["body","\n"],["body","\n"],["body","如果您恢复单个备份索引，它们不会自动添加到任何现有数据流中。"],["body","\n"],["body","\n"],["body","\n"],["body","例如，如果只有。ds-logs-2099.03.08-00003备份索引从快照中恢复，则不会自动将其添加到现有日志数据流中。"],["body","\n"],["body","\n\n"],["body","重要点"],["body","\n\n"],["body","index_settings and ignore_index_settings 这两个参数只影响 数据流的后备索引"],["body","\n"],["body","新的后备索引使用  index template. 指定的设置"],["body","\n"],["body","如果在 restore过程中改变了 索引设置 推荐 最好对  index template 也做同样的配置，这确保 新的后备索引 保持同样的 设置"],["body","\n\n"],["body","​"],["body","\n"],["headingLink","path-parameters"],["heading","Path parameters"],["body","\n\n"],["body","\n"],["body","<repository>"],["body","\n"],["body","(Required, string) Name of the repository to restore a snapshot from."],["body","\n"],["body","\n"],["body","\n"],["body","<snapshot>"],["body","\n"],["body","(Required, string) Name of the snapshot to restore."],["body","\n"],["body","\n\n"],["headingLink","query-parameters"],["heading","Query parameters"],["body","\n\n"],["body","\n"],["body","master_timeout"],["body","\n"],["body","(Optional, time units) Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error. Defaults to 30s."],["body","\n"],["body","\n"],["body","\n"],["body","wait_for_completion"],["body","\n"],["body","(Optional, Boolean) If true, the request returns a response when the restore operation completes. The operation is complete when it finishes all attempts to recover primary shards for restored indices. This applies even if one or more of the recovery attempts fail.If false, the request returns a response when the restore operation initializes. Defaults to false."],["body","\n"],["body","\n\n"],["headingLink","request-body"],["heading","Request body"],["body","\n"],["headingLink","ignore_unavailable"],["heading","ignore_unavailable"],["body","\n\n"],["body","设置为FALSE。则如果索引或者数据流缺失或者关闭的情况下该请求会报错"],["body","\n\n"],["headingLink","ignore_unavailable-1"],["heading","ignore_unavailable"],["body","\n\n"],["body","(可选，字符串) 以逗号分隔的索引设置列表，不应从快照中还原。"],["body","\n\n"],["headingLink","ignore_index_settings"],["heading","ignore_index_settings"],["body","\n\n"],["body","不应该还原的配置"],["body","\n"],["body","逗号分割的key"],["body","\n\n"],["headingLink","include_aliases"],["heading","include_aliases"],["body","\n\n"],["body","是否恢复 索引别名"],["body","\n"],["body","默认TRUE"],["body","\n\n"],["headingLink","include_global_state"],["heading","include_global_state"],["body","\n\n"],["body","是否还原 全局配置。默认FALSE不还原"],["body","\n"],["body","如果为TRUE 则以下几个状态会被还原\n\n"],["body","Persistent cluster settings"],["body","\n"],["body","Index templates"],["body","\n"],["body","Legacy index templates"],["body","\n"],["body","Ingest pipelines"],["body","\n"],["body","ILM lifecycle policies"],["body","\n"],["body","For snapshots taken after 7.12.0, data stored in system indices, such as Watches and task records, replacing any existing configuration (configurable via feature_states)"],["body","\n\n"],["body","\n\n"],["body","还原细节"],["body","\n"],["body","如果include_global_state为true，则还原操作将群集中的Legacy index templates与快照中包含的模板合并，并替换名称与快照中的模板匹配的任何现有模板。它完全删除了集群中存在的所有持久性设置，非传统索引模板，摄取管道和ILM生命周期策略，并将其替换为快照中的相应项目。"],["body","\n"],["headingLink","feature_states"],["heading","feature_states"],["body","\n\n"],["body","可选的。逗号分隔的字符串"],["body","\n"],["body","指定 还原的 状态"],["body","\n"],["body","每一个  feature state  包含 系统索引状态"],["body","\n"],["body","通过  Get Snapshot API.  可以获取 feature states"],["body","\n"],["body","feature states 会直接覆盖"],["body","\n"],["body","空数组 则导致 不会恢复  feature states, 从而就忽略 include_global_state"],["body","\n"],["body","默认下，当include_global_state 设置成 TRUE表示 恢复所有的。设置为FALSE 表示都不恢复"],["body","\n\n"],["headingLink","index_settings"],["heading","index_settings"],["body","\n\n"],["body","逗号分割的 设置列表"],["body","\n"],["body","可以用来 新增或者修改 所有索引的配置"],["body","\n"],["body","对于 索引级别的配置 详见 index modules."],["body","\n\n"],["headingLink","indices"],["heading","indices"],["body","\n\n"],["body","逗号分割的字符串。指定恢复的索引"],["body","\n"],["body","Multi-index syntax is supported."],["body","\n"],["body","默认情况下，还原操作包括快照中的所有数据流和索引。如果提供此参数，则还原操作仅包括指定的数据流和索引。"],["body","\n\n"],["headingLink","partial"],["heading","partial"],["body","\n\n"],["body","设置为FALSE，如果快照中包含的一个或多个索引中 如果没有做到 所有主分片都是可用的，则整个还原操作将失败。默认为false。"],["body","\n"],["body","设置为TRUE：允许部分索引 的不可用分片出现"],["body","\n"],["body","只有成功包含在快照中的分片才会被还原。所有丢失的分片将被重新创建为空的。"],["body","\n\n"],["headingLink","rename_pattern"],["heading","rename_pattern"],["body","\n\n"],["body","\n"],["body","定义一个 索引或数据流 重命名的模式"],["body","\n"],["body","\n"],["body","\n"],["body","如果匹配这个模式 则 会被重命名"],["body","\n"],["body","\n"],["body","\n"],["body","可以使用正则替换。支持引用原始文本。根据  appendReplacement 逻辑"],["body","\n"],["body","\n"],["body","\n"],["body","如果两个不同索引被 重命名成一个名字 则 请求失败"],["body","\n"],["body","\n"],["body","\n"],["body","数据流的后备索引同样会被重命名"],["body","\n"],["body","例如 logs -> restored-logs\n.ds-logs-2099.03.09-000005 \nis renamed to \n.ds-restored-logs-2099.03.09-000005\n"],["body","\n"],["body","\n"],["body","\n"],["body","注意：要确保 数据流的 索引模板 能匹配新的 索引名字。否则就不能 rollover"],["body","\n"],["body","\n\n"],["headingLink","rename_replacement"],["heading","rename_replacement"],["body","\n\n"],["body","定义替换后的索引名"],["body","\n\n"],["headingLink","examples"],["heading","Examples"],["body","\n"],["body","POST /_snapshot/my_repository/snapshot_2/_restore?wait_for_completion=true\n{\n  \"indices\": \"index_1,index_2\",\n  \"ignore_unavailable\": true,\n  \"include_global_state\": false,\n  \"rename_pattern\": \"index_(.+)\",\n  \"rename_replacement\": \"restored_index_$1\",\n  \"include_aliases\": false\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/snapshotAndRestore/CreateSnapshot.html"],["title","CreateSnapshot.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","create-snapshot-api"],["heading","Create snapshot API"],["body","\n"],["body","PUT /_snapshot/my_repository/my_snapshot\n\n"],["body","\n"],["headingLink","request"],["heading","Request"],["body","\n"],["body","PUT /_snapshot/<repository>/<snapshot>\nPOST /_snapshot/<repository>/<snapshot>\n"],["body","\n"],["headingLink","description"],["heading","Description"],["body","\n"],["body","您可以使用create snapshot API创建快照，该快照是从正在运行的Elasticsearch集群中获取的备份。"],["body","\n"],["body","默认情况下，快照包括集群中的所有数据流和开放索引，以及集群状态。您可以通过在快照请求的request body 中 指定要备份的数据流和索引列表来更改此行为。"],["body","\n"],["body","注意必须先注册 快照仓库"],["body","\n"],["body","快照是增量的"],["body","\n"],["body","快照过程以非阻塞方式执行，因此所有索引和搜索操作都可以在 正在快照的数据流或索引同时运行。"],["body","\n"],["body","快照表示创建快照的时刻的时间点视图。快照过程开始后，没有添加到数据流或索引的记录将出现在快照中。"],["body","\n\n"],["body","\n"],["body","对于尚未启动且当前未重新定位的主分片，快照过程将立即启动。"],["body","\n"],["body","\n"],["body","\n"],["body","如果分片正在启动或重新定位，Elasticsearch会在拍摄快照之前等待这些进程完成。"],["body","\n"],["body","\n"],["body","\n"],["body","重要："],["body","\n\n"],["body","拍摄快照期间。分片移动到其他节点"],["body","\n"],["body","分配重路由、重定位 会被快照 干扰知道快照过程结束"],["body","\n"],["body","除了copy数据之外。还copy集群元配置、包括索引模板、组件模板、集群持久化配置 、"],["body","\n"],["body","瞬时配置 和 注册的 快照仓库 配置 不会快照"],["body","\n\n"],["body","\n\n"],["headingLink","query-parameters"],["heading","Query parameters"],["body","\n"],["body","master_timeout"],["body","\n"],["body","(Optional, time units) Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error. Defaults to 30s."],["body","\n"],["body","wait_for_completion"],["body","\n"],["body","(Optional, Boolean) If true, the request returns a response when the snapshot is complete. If false, the request returns a response when the snapshot initializes. Defaults to false."],["body","\n"],["headingLink","request-body"],["heading","Request body"],["body","\n"],["headingLink","ignore_unavailable"],["heading","ignore_unavailable"],["body","\n\n"],["body","是否忽略 不可用索引"],["body","\n"],["body","如果 数据流或者 索引 被关闭了。或者丢失了。则该快照过程失败"],["body","\n"],["body","默认 FALSE，不失败"],["body","\n\n"],["headingLink","indices"],["heading","indices"],["body","\n\n"],["body","逗号分割的 索引列表"],["body","\n"],["body","详见：Multi-index syntax"],["body","\n"],["body","快照默认包含集群中的所有数据流和索引。如果提供此参数，则快照仅包含指定的数据流和群集。"],["body","\n\n"],["headingLink","include_global_state"],["heading","include_global_state"],["body","\n\n"],["body","包含当前集群的全局状态"],["body","\n"],["body","默认TRUE"],["body","\n"],["body","全局状态包括\n\n"],["body","Persistent cluster settings"],["body","\n"],["body","Index templates、Legacy index templates"],["body","\n"],["body","Ingest pipelines"],["body","\n"],["body","ILM lifecycle policies"],["body","\n"],["body","Data stored in system indices, such as Watches and task records (configurable via feature_states)"],["body","\n\n"],["body","\n\n"],["headingLink","feature_states"],["heading","feature_states"],["body","\n\n"],["body","可选的 数组string"],["body","\n"],["body","快照一些列的特点。"],["body","\n"],["body","可以使用 get features API 得到 这些描述"],["body","\n"],["body","每个特征状态包括一个或多个系统索引\n\n"],["body","每个特征状态包括一个或多个系统索引，"],["body","\n"],["body","其中包含该特征的功能所需的数据。"],["body","\n"],["body","提供空数组将在快照中不包含任何特征状态，"],["body","\n"],["body","而不管include_global_state的值如何。"],["body","\n"],["body","默认的。所有可用的 特征 状态 将会被 快照"],["body","\n"],["body","如果 include_global_state 设置为TRUE。则会包含所有的。设置为FALSE 则 不包含任何"],["body","\n\n"],["body","\n\n"],["headingLink","metadata"],["heading","metadata"],["body","\n\n"],["body","附加自定义元数据。例如 谁建立的快照、为什么建立快照 。必须小于 1024个字节"],["body","\n\n"],["headingLink","partial"],["heading","partial"],["body","\n\n"],["body","如果设置成FALSE 。一旦 当中某些索引 存在 主分片不可用的情况 则 快照失败"],["body","\n"],["body","设置为TRUE表示 可以 接受部分 快照"],["body","\n\n"],["headingLink","example"],["heading","example"],["body","\n"],["body","PUT /_snapshot/my_repository/snapshot_2?wait_for_completion=true\n{\n  \"indices\": \"index_1,index_2\",\n  \"ignore_unavailable\": true,\n  \"include_global_state\": false,\n  \"metadata\": {\n    \"taken_by\": \"user123\",\n    \"taken_because\": \"backup before upgrading\"\n  }\n}\n"],["body","\n"],["body","{\n  \"snapshot\": {\n    \"snapshot\": \"snapshot_2\",\n    \"uuid\": \"vdRctLCxSketdKb54xw67g\",\n    \"version_id\": <version_id>,\n    \"version\": <version>,\n    \"indices\": [],\n    \"data_streams\": [],\n    \"feature_states\": [],\n    \"include_global_state\": false,\n    \"metadata\": {\n      \"taken_by\": \"user123\",\n      \"taken_because\": \"backup before upgrading\"\n    },\n    \"state\": \"SUCCESS\",\n    \"start_time\": \"2020-06-25T14:00:28.850Z\",\n    \"start_time_in_millis\": 1593093628850,\n    \"end_time\": \"2020-06-25T14:00:28.850Z\",\n    \"end_time_in_millis\": 1593094752018,\n    \"duration_in_millis\": 0,\n    \"failures\": [],\n    \"shards\": {\n      \"total\": 0,\n      \"failed\": 0,\n      \"successful\": 0\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/snapshotAndRestore/CloneSnapshot.html"],["title","CloneSnapshot.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","clone-snapshot-api"],["heading","Clone snapshot API"],["body","\n"],["body","Clones part or all of a snapshot into a new snapshot."],["body","\n"],["body","PUT /_snapshot/my_repository/source_snapshot/_clone/target_snapshot\n{\n  \"indices\": \"index_a,index_b\"\n}\n"],["body","\n"],["headingLink","request"],["heading","Request"],["body","\n"],["body","PUT /_snapshot/<repository>/<source_snapshot>/_clone/<target_snapshot>\n"],["body","\n"],["headingLink","description"],["heading","Description"],["body","\n"],["body","clone snapshot API允许在同一存储库中创建全部或部分现有快照的副本。"],["body","\n"],["headingLink","path-parameters"],["heading","Path parameters"],["body","\n\n"],["body","\n"],["body","<repository>"],["body","\n"],["body","(Required, string) Name of the snapshot repository that both source and target snapshot belong to."],["body","\n"],["body","\n\n"],["headingLink","query-parameters"],["heading","Query parameters"],["body","\n\n"],["body","\n"],["body","master_timeout"],["body","\n"],["body","(Optional, time units) Specifies the period of time to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error. Defaults to 30s."],["body","\n"],["body","\n"],["body","\n"],["body","timeout"],["body","\n"],["body","(Optional, time units) Specifies the period of time to wait for a response. If no response is received before the timeout expires, the request fails and returns an error. Defaults to 30s."],["body","\n"],["body","\n"],["body","\n"],["body","indices"],["body","\n"],["body","(Required, string) A comma-separated list of indices to include in the snapshot. Multi-index syntax is supported."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/snapshotAndRestore/CleanUpSnapshotRepo.html"],["title","CleanUpSnapshotRepo.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","clean-up-snapshot-repository-api"],["heading","Clean up snapshot repository API"],["body","\n"],["body","触发对快照存储库内容的审查，并删除现有快照未引用的所有陈旧数据。"],["body","\n"],["body","POST /_snapshot/my_repository/_cleanup\n"],["body","\n"],["headingLink","request"],["heading","Request"],["body","\n"],["body","POST /_snapshot/<repository>/_cleanup\n"],["body","\n"],["headingLink","description"],["heading","Description"],["body","\n"],["body","随着时间的推移，快照存储库可能会累积不再被现有快照引用的陈旧数据。"],["body","\n"],["body","尽管此未引用的数据不会对快照存储库的性能或安全性产生负面影响，但它可能导致比必要时更多的存储使用。"],["body","\n"],["body","您可以使用 “清理快照存储库” API来检测和删除此未引用的数据。"],["body","\n"],["body","小提示"],["body","\n\n"],["body","当从存储库中删除快照时，此API执行的大多数清理操作都是自动执行的。"],["body","\n"],["body","如果您定期删除快照，调用此API可能只会稍微减少您的存储空间，或者根本不会减少。"],["body","\n\n"],["headingLink","path-parameters"],["heading","Path parameters"],["body","\n"],["body","<repository>"],["body","\n"],["body","(Required, string) Name of the snapshot repository to review and clean up."],["body","\n"],["headingLink","query-parameters"],["heading","Query parameters"],["body","\n\n"],["body","\n"],["body","master_timeout"],["body","\n"],["body","(Optional, time units) Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error. Defaults to 30s."],["body","\n"],["body","\n"],["body","\n"],["body","timeout"],["body","\n"],["body","(Optional, time units) Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error. Defaults to 30s."],["body","\n"],["body","\n\n"],["headingLink","response-body"],["heading","Response body"],["body","\n"],["headingLink","results"],["heading","results"],["body","\n"],["headingLink","deleted_bytes"],["heading","deleted_bytes"],["body","\n"],["body","(整数) 通过清理操作释放的字节数。"],["body","\n"],["body","deleted_blobs"],["body","\n"],["body","(整数) 在清理操作期间从快照存储库中删除的二进制大对象 (blob) 的数量。任何非零值都意味着发现了未引用的blobs并随后进行了清理。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/snapshotAndRestore/README.html"],["title","snapshotAndRestore - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","snapshot-and-restore"],["heading","Snapshot and restore"],["body","\n\n"],["body","\n"],["body","快照是从正在运行的Elasticsearch集群获取的备份。"],["body","\n"],["body","\n"],["body","\n"],["body","您可以拍摄整个集群的快照，包括其所有数据流和索引。"],["body","\n"],["body","\n"],["body","\n"],["body","您也可以仅对集群中的特定数据流或索引进行快照。"],["body","\n"],["body","\n\n"],["headingLink","注册快照仓库"],["heading","注册快照仓库"],["body","\n\n"],["body","\n"],["body","You must register a snapshot repository before you can create snapshots."],["body","\n"],["body","\n"],["body","\n"],["body","快照可以存储在本地或远程存储库中"],["body","\n"],["body","\n"],["body","\n"],["body","Remote repositories can reside on Amazon S3, HDFS, Microsoft Azure, Google Cloud Storage, and other platforms supported by a repository plugin"],["body","\n"],["body","\n\n"],["body","Elasticsearch会增量获取快照:"],["body","\n\n"],["body","可以安全地以最小的开销非常频繁地拍摄快照"],["body","\n"],["body","这种增量仅适用于单个存储库。因为存储库之间没有共享数据"],["body","\n"],["body","快照在逻辑上也彼此独立，即使在单个存储库中也是如此: 删除快照不会影响任何其他快照的完整性。"],["body","\n"],["body","但是，您可以选择仅从快照中恢复群集状态或特定数据流或索引。"],["body","\n"],["body","You can use snapshot lifecycle management to automatically take and manage snapshots."],["body","\n\n"],["body","备份群集的唯一可靠且受支持的方法是拍摄快照。您不能通过复制Elasticsearch集群节点的数据目录来备份它。没有支持的方法可以从文件系统级备份中还原任何数据。如果您尝试从这样的备份中恢复群集，它可能会因损坏或丢失文件或其他数据不一致的报告而失败，或者它似乎已经成功地无声地丢失了一些数据。"],["body","\n"],["body","群集节点的数据目录的副本不能用作备份，因为它不是它们在单个时间点的内容的一致表示。您不能通过在复制时关闭节点来解决这个问题，也不能通过获取原子文件系统级快照来解决这个问题，因为Elasticsearch具有跨越整个集群的一致性要求。集群备份必须使用内置的快照功能。"],["body","\n"],["headingLink","version-compatibility"],["heading","Version compatibility"],["body","\n"],["body","版本兼容性是指基础Lucene索引兼容性。在版本之间迁移时，请遵循升级文档。"],["body","\n"],["body","快照包含包含索引或数据流的备份索引的磁盘上数据结构的副本。这意味着快照只能恢复到可以读取索引的Elasticsearch版本。"],["body","\n"],["body","快照包含包含索引或数据流的后备索引的磁盘上数据结构的副本。这意味着快照只能恢复到可以读取索引的Elasticsearch版本。"],["body","\n"],["body","下表显示了版本之间的快照兼容性。第一列表示可以从中恢复快照的基本版本。"],["body","\n"],["body","Cluster version"],["body","\n"],["body","Snapshot version"],["body","2.x"],["body","5.x"],["body","6.x"],["body","7.x"],["body","\n"],["body","1.x →"],["body","\n"],["body","2.x →"],["body","\n"],["body","5.x →"],["body","\n"],["body","6.x →"],["body","\n"],["body","7.x →"],["body","\n\n\n"],["body","总结："],["body","\n"],["body","快照只能向前兼容一个majar版本，没法向后兼容"],["body","\n"],["body","以下条件适用于跨版本恢复快照和索引: •"],["body","\n\n"],["body","Snapshots: 您不能将Elasticsearch版本的快照恢复到运行早期Elasticsearch版本的集群中。例如，您无法将7.6.0中拍摄的快照还原到运行7.5.0的群集。"],["body","\n"],["body","Indices: 您不能将索引还原到运行Elasticsearch版本的集群中，该版本比用于快照索引的Elasticsearch版本更新了多个 marjar verson。例如，无法将索引从5.0中获取的快照还原到运行7.0的群集。"],["body","\n\n"],["body","需要注意的是，Elasticsearch 2.0拍摄的快照可以在运行Elasticsearch 5.0的集群中恢复。"],["body","\n"],["body","每个快照都可以包含在各种版本的Elasticsearch中创建的索引。这包括为数据流创建的支持索引。还原快照时，必须可以将所有这些索引还原到目标群集中。如果快照中的任何索引都是在不兼容的版本中创建的，则将无法还原快照。"],["body","\n"],["body","注意"],["body","\n\n"],["body","在升级前备份数据时，记住 如果升级后的版本不兼容，则不能还原快照"],["body","\n"],["body","如果实在需要 还原到 不兼容的版本，可以先还原到 最近兼容版本，使用 reindex-from-remote 去重建数据流或者索引到 当前的版本"],["body","\n"],["body","获取数据 然后 重新索引 会比 快照恢复 更加慢，建议操作之前 先计算 消耗时长"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/snapshotAndRestore/RegisterASnapshot.html"],["title","RegisterASnapshot.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","register-a-snapshot-repository"],["heading","Register a snapshot repository"],["body","\n\n"],["body","必须先 注册 快照仓库"],["body","\n"],["body","Use the create or update snapshot repository API to register or update a snapshot repository."],["body","\n"],["body","We recommend creating a new snapshot repository for each major version. The valid repository settings depend on the repository type."],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/snapshotAndRestore/使用OSS备份恢复集群实战.html"],["title","使用OSS备份恢复集群实战.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","重启"],["heading","重启"],["body","\n"],["body","multipass exec primary sudo systemctl restart elasticsearch\n\nmultipass exec node-2 sudo systemctl restart elasticsearch\n \nmultipass exec node-3 sudo systemctl restart elasticsearch\n"],["body","\n"],["headingLink","配置aksk"],["heading","配置AKSK"],["body","\n"],["body","export accessKey=xxxx\nexport secretkey=xxxxxxxxxxx\nexport nodeName=node-3\necho $accessKey | multipass exec $nodeName  sudo /usr/share/elasticsearch/bin/elasticsearch-keystore \\\nadd -- -f  s3.client.default.access_key \n\necho $secretkey | multipass exec $nodeName  sudo /usr/share/elasticsearch/bin/elasticsearch-keystore \\\nadd   -- -f s3.client.default.secret_key \n"],["body","\n"],["headingLink","重载安全秘钥"],["heading","重载安全秘钥"],["body","\n"],["body","POST _nodes/reload_secure_settings\n"],["body","\n"],["headingLink","配置client"],["heading","配置Client"],["body","\n"],["body","PUT _snapshot/mys3repository\n{\n  \"type\": \"s3\",\n  \"settings\": {\n    \"bucket\": \"bucket-name\",\n    \"base_path\": \"xiaojiaquan\", \n    \"endpoint\":\"xxx-beijing.aliyuncs.com\",\n    \"protocol\":\"http\",\n    \"compress\": false,\n    \"disable_chunked_encoding\":true\n  }\n}\n"],["body","\n"],["headingLink","拍摄快照"],["heading","拍摄快照"],["body","\n"],["body","PUT /_snapshot/mys3repository/snapshot_3\n{\n  \"indices\": \"person\",\n  \"ignore_unavailable\": true,\n  \"metadata\": {\n    \"taken_by\": \"weisanju\",\n    \"taken_because\": \"firstS3Backup\"\n  }\n}\n"],["body","\n"],["headingLink","恢复快照"],["heading","恢复快照"],["body","\n"],["body","\nPOST /_snapshot/mys3repository/snapshot_2/_restore\n{\n  \"include_global_state\": false,\n  \"ignore_unavailable\": true,\n  \"indices\": \"person\",\n  \"rename_pattern\": \"(person)\",\n  \"rename_replacement\": \"$1_copy2_restore\"\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/snapshotAndRestore/CreateOrUpdateSnapshotAPI.html"],["title","CreateOrUpdateSnapshotAPI.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","create-or-update-snapshot-repository-api"],["heading","Create or update snapshot repository API"],["body","\n"],["body","Registers or updates a snapshot repository."],["body","\n"],["body","PUT /_snapshot/my_repository\n{\n  \"type\": \"fs\",\n  \"settings\": {\n    \"location\": \"my_backup_location\"\n  }\n}\n"],["body","\n"],["headingLink","request"],["heading","Request"],["body","\n"],["body","PUT /_snapshot/<repository>\nPOST /_snapshot/<repository>\n"],["body","\n"],["headingLink","description"],["heading","Description"],["body","\n"],["body","必须先注册 repository 才能执行 快照跟恢复操作 snapshot and restore"],["body","\n"],["body","快照格式每个大版本都会变，详见 See Version compatibility."],["body","\n"],["headingLink","path-parameters"],["heading","Path parameters"],["body","\n\n"],["body","\n"],["body","<repository>"],["body","\n"],["body","(Required, string) Name of the snapshot repository to register or update."],["body","\n"],["body","\n\n"],["headingLink","query-parameters"],["heading","Query parameters"],["body","\n"],["body","注意：可以使用 query parameter 或 request body parameter 指定此API的多个选项。如果两个参数都指定，则仅使用查询参数。"],["body","\n\n"],["body","\n"],["body","master_timeout"],["body","\n"],["body","(Optional, time units) Specifies the period of time to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error. Defaults to 30s."],["body","\n"],["body","\n"],["body","\n"],["body","timeout"],["body","\n"],["body","(Optional, time units) Specifies the period of time to wait for a response. If no response is received before the timeout expires, the request fails and returns an error. Defaults to 30s."],["body","\n"],["body","\n"],["body","\n"],["body","verify"],["body","\n"],["body","(Optional, Boolean) If true, the request verifies the repository is functional on all master and data nodes in the cluster. If false, this verification is skipped. Defaults to true.You can manually perform this verification using the verify snapshot repository API."],["body","\n"],["body","\n\n"],["headingLink","request-body"],["heading","Request body"],["body","\n"],["headingLink","type"],["heading","type"],["body","\n"],["body","快照仓库类型"],["body","\n\n"],["body","repository-s3 for S3 repository support"],["body","\n"],["body","repository-hdfs for HDFS repository support in Hadoop environments"],["body","\n"],["body","repository-azure for Azure storage repositories"],["body","\n"],["body","repository-gcs for Google Cloud Storage repositories"],["body","\n\n"],["headingLink","settings"],["heading","settings"],["body","\n"],["body","仓库容器配置"],["body","\n"],["headingLink","chunk_size"],["heading","chunk_size"],["body","\n\n"],["body","文件块大小：字节数"],["body","\n"],["body","如果快照比这个数值大。则快照会被拆分成几个小文件"],["body","\n"],["body","默认为空，没有限制"],["body","\n\n"],["headingLink","compress"],["heading","compress"],["body","\n\n"],["body","如果为TRUE 则 metadata files 例如 索引 mappings settings 会被压缩存储。索引数据不会压缩。"],["body","\n"],["body","默认TRUE"],["body","\n\n"],["headingLink","max_number_of_snapshots"],["heading","max_number_of_snapshots"],["body","\n\n"],["body","仓库最大 快照数"],["body","\n"],["body","默认500"],["body","\n"],["body","不建议增大这个值。因为 过大的快照仓库 会 影响 主节点的性能。带来稳定性问题。"],["body","\n"],["body","相反，删除旧的快照或者使用 多仓库"],["body","\n\n"],["headingLink","max_restore_bytes_per_sec"],["heading","max_restore_bytes_per_sec"],["body","\n\n"],["body","最大的每秒 恢复速度。字节值"],["body","\n"],["body","默认无限制。"],["body","\n"],["body","restore 也会受到 recovery settings影响"],["body","\n\n"],["headingLink","max_snapshot_bytes_per_sec"],["heading","max_snapshot_bytes_per_sec"],["body","\n\n"],["body","最大每秒 快照取样时间。单位字节"],["body","\n"],["body","默认40M/s"],["body","\n\n"],["headingLink","readonly"],["heading","readonly"],["body","\n\n"],["body","为TRUE 则 仓库是只读的"],["body","\n"],["body","只能读快照 不能写快照"],["body","\n"],["body","默认为 FALSE"],["body","\n"],["body","如果多集群中注册了同一个快照存储，则只有一个集群具有对该存储库的写访问 权限，让多个集群同时写入存储库有损坏存储库内容的风险。"],["body","\n"],["body","只有具有写访问权限的集群才能在存储库中创建快照。连接到存储库的所有其他群集都应将readonly参数设置为true。这意味着这些群集可以从存储库中检索或还原快照，但不能在其中创建快照。"],["body","\n\n"],["body","其他可接受的设置属性取决于使用type参数设置的存储库类型。"],["body","\n"],["headingLink","fs-repo-settings"],["heading","FS repo settings"],["body","\n"],["headingLink","location"],["heading","location"],["body","\n\n"],["body","必选。本地文件系统的位置"],["body","\n"],["body","必须注册在 主节点或者数据节点的 path.repo 设置的路径"],["body","\n\n"],["headingLink","source-repo-settings"],["heading","source repo settings"],["body","\n"],["headingLink","delegate_type"],["heading","delegate_type"],["body","\n\n"],["body","代码存储库类型"],["body","\n"],["body","source repositories 可以使用 代理存储库的 配置"],["body","\n"],["body","详见：source ONLY repository"],["body","\n\n"],["headingLink","url-repo-settings"],["heading","URL repo settings"],["body","\n\n"],["body","基于URL的 共享文件系统 repository 根路径"],["body","\n"],["body","支持以下格式\n\n"],["body","file"],["body","\n"],["body","ftp"],["body","\n"],["body","http"],["body","\n"],["body","https"],["body","\n"],["body","jar"],["body","\n\n"],["body","\n"],["body","使用文件协议的url必须指向群集中所有主节点和数据节点都可以访问的共享文件系统的位置。此位置必须在path.repo设置中注册。"],["body","\n"],["body","必须通过repositoriesurl.allowed_urls设置明确允许使用http、https或ftp协议的url。此设置支持在URL中代替主机、路径、查询或片段的通配符。"],["body","\n\n"],["headingLink","verify"],["heading","verify"],["body","\n\n"],["body","检验储存库是否在所有 主节点或者数据节点 起作用"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/snapshotAndRestore/SearchableSnapshotsMountAPI.html"],["title","SearchableSnapshotsMountAPI.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","mount-snapshot-api"],["heading","Mount snapshot API"],["body","\n"],["body","Mount a snapshot as a searchable snapshot index."],["body","\n"],["headingLink","request"],["heading","Request"],["body","\n"],["body","POST /_snapshot/<repository>/<snapshot>/_mount\n"],["body","\n"],["headingLink","prerequisites"],["heading","Prerequisites"],["body","\n"],["body","If the Elasticsearch security features are enabled, you must have the manage cluster privilege and the manage index privilege for any included indices to use this API. For more information, see Security privileges."],["body","\n"],["headingLink","description"],["heading","Description"],["body","\n"],["headingLink","path-parameters"],["heading","Path parameters"],["body","\n\n"],["body","\n"],["body","<repository>"],["body","\n"],["body","(Required, string) The name of the repository containing the snapshot of the index to mount."],["body","\n"],["body","\n"],["body","\n"],["body","<snapshot>"],["body","\n"],["body","(Required, string) The name of the snapshot of the index to mount."],["body","\n"],["body","\n\n"],["headingLink","query-parameters"],["heading","Query parameters"],["body","\n\n"],["body","\n"],["body","master_timeout"],["body","\n"],["body","(Optional, time units) Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error. Defaults to 30s."],["body","\n"],["body","\n"],["body","\n"],["body","wait_for_completion"],["body","\n"],["body","(Optional, Boolean) If true, the request blocks until the operation is complete. Defaults to false."],["body","\n"],["body","\n"],["body","\n"],["body","storage"],["body","\n"],["body","(Optional, string) Mount option for the searchable snapshot index. Possible values are:full_copy (Default)Fully mounted index.shared_cachePartially mounted index."],["body","\n"],["body","\n\n"],["headingLink","request-body"],["heading","Request body"],["body","\n\n"],["body","\n"],["body","index"],["body","\n"],["body","(Required, string) Name of the index contained in the snapshot whose data is to be mounted."],["body","\n"],["body","\n\n"],["body","If no renamed_index is specified this name will also be used to create the new index."],["body","\n\n"],["body","\n"],["body","renamed_index"],["body","\n"],["body","(Optional, string) Name of the index that will be created."],["body","\n"],["body","\n"],["body","\n"],["body","index_settings"],["body","\n"],["body","(Optional, object) Settings that should be added to the index when it is mounted."],["body","\n"],["body","\n"],["body","\n"],["body","ignore_index_settings"],["body","\n"],["body","(Optional, array of strings) Names of settings that should be removed from the index when it is mounted."],["body","\n"],["body","\n\n"],["headingLink","examples"],["heading","Examples"],["body","\n"],["body","Mounts the index my_docs from an existing snapshot named my_snapshot stored in the my_repository as a new index docs:"],["body","\n"],["body","POST /_snapshot/my_repository/my_snapshot/_mount?wait_for_completion=true\n{\n  \"index\": \"my_docs\", //快照中的索引\n  \"renamed_index\": \"docs\", //重命名后的索引\n  \"index_settings\": { //索引设置\n    \"index.number_of_replicas\": 0\n  },\n  \"ignore_index_settings\": [ \"index.refresh_interval\" ]  //忽略的索引设置\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/snapshotAndRestore/DeleteSnapshotRepository.html"],["title","DeleteSnapshotRepository.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","delete-snapshot-repository-api"],["heading","Delete snapshot repository API"],["body","\n"],["body","Unregisters one or more snapshot repositories."],["body","\n"],["body","当取消注册存储库时，Elasticsearch仅删除对存储库存储快照的位置的引用。快照本身保持不变。"],["body","\n"],["body","DELETE /_snapshot/my_repository\n"],["body","\n"],["headingLink","request"],["heading","Request"],["body","\n"],["body","DELETE /_snapshot/<repository>\n"],["body","\n"],["headingLink","path-parameters"],["heading","Path parameters"],["body","\n"],["body","(Required, string) Name of the snapshot repository to unregister. Wildcard (*) patterns are supported."],["body","\n"],["headingLink","query-parameters"],["heading","Query parameters"],["body","\n\n"],["body","\n"],["body","master_timeout"],["body","\n"],["body","(Optional, time units) Specifies the period of time to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error. Defaults to 30s."],["body","\n"],["body","\n"],["body","\n"],["body","timeout"],["body","\n"],["body","(Optional, time units) Specifies the period of time to wait for a response. If no response is received before the timeout expires, the request fails and returns an error. Defaults to 30s."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/snapshotAndRestore/SLM.html"],["title","SLM.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","slm-manage-the-snapshot-lifecycle"],["heading","SLM: Manage the snapshot lifecycle"],["body","\n\n"],["body","\n"],["body","您可以设置快照生命周期策略来自动控制快照的定时、频率和保留。快照策略可以应用于多个数据流和索引。"],["body","\n"],["body","\n"],["body","\n"],["body","The snapshot lifecycle management (SLM) CRUD APIs 为Kibana管理的一部分提供快照策略功能的构建块。"],["body","\n"],["body","\n"],["body","\n"],["body","Snapshot and Restore 可以轻松设置策略，注册快照存储库，查看和管理快照以及还原数据流或索引。"],["body","\n"],["body","\n"],["body","\n"],["body","您可以停止并重新启动SLM，以在执行升级或其他维护时暂时暂停自动备份。"],["body","\n"],["body","\n\n"],["headingLink","教程"],["heading","教程"],["body","\n\n"],["body","\n"],["body","本教程演示如何使用SLM策略自动备份Elasticsearch数据流和索引"],["body","\n"],["body","\n"],["body","\n"],["body","该策略对集群中的所有数据流和索引进行快照，并将它们存储在本地存储库中。它还定义了保留策略，并在不再需要快照时自动删除快照。"],["body","\n"],["body","\n\n"],["body","要使用SLM管理快照，您可以"],["body","\n\n"],["body","Register a repository."],["body","\n"],["body","Create an SLM policy."],["body","\n\n"],["body","要测试策略，您可以手动触发它以获取初始快照。"],["body","\n"],["headingLink","register-a-repository"],["heading","Register a repository"],["body","\n\n"],["body","\n"],["body","要使用SLM，您必须配置快照存储库。"],["body","\n"],["body","\n"],["body","\n"],["body","存储库可以是本地 (共享文件系统) 或远程 (云存储)。远程存储库可以驻留在S3、HDFS、Azure、谷歌云存储或存储库插件  repository plugin  支持的任何其他平台上。"],["body","\n"],["body","\n"],["body","\n"],["body","远程存储库一般用于生产部署。"],["body","\n"],["body","\n\n"],["body","PUT /_snapshot/my_repository\n{\n  \"type\": \"fs\",\n  \"settings\": {\n    \"location\": \"my_backup_location\"\n  }\n}\n"],["body","\n"],["headingLink","set-up-a-snapshot-policy"],["heading","Set up a snapshot policy"],["body","\n\n"],["body","一旦有了存储库，就可以定义SLM策略以自动拍摄快照"],["body","\n"],["body","该策略定义了何时拍摄快照，应包括哪些数据流或索引  以及如何命名快照。"],["body","\n"],["body","策略还可以指定保留策略 retention policy ，并在不再需要快照时自动删除快照。"],["body","\n"],["body","不要害怕配置频繁快照的策略。快照是增量的，可以有效地利用存储。"],["body","\n\n"],["body","注意"],["body","\n"],["body","您可以通过Kibana Management或使用创建或更新策略API定义和管理策略。"],["body","\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","例如，您可以定义一个夜间快照策略，以每天在UTC上午1:30备份所有数据流和索引。"],["body","\n"],["body","PUT /_slm/policy/nightly-snapshots\n{\n  \"schedule\": \"0 30 1 * * ?\",  // 1. Cron syntax\n  \"name\": \"<nightly-snap-{now/d}>\", //2. 快照命名\n  \"repository\": \"my_repository\",  //存储库\n  \"config\": { \n    \"indices\": [\"*\"]  //包含的索引\n  },\n  \"retention\": {  //快照维持天数\n    \"expire_after\": \"30d\",  //保持快照30天，\n    \"min_count\": 5, //无论年龄大小，至少保留5张且不超过50张快照\n    \"max_count\": 50  \n  }\n}\n"],["body","\n\n"],["body","Cron syntax"],["body","\n"],["body","date math"],["body","\n\n"],["headingLink","其他配置"],["heading","其他配置"],["body","\n"],["body","您可以指定其他快照配置选项来自定义快照的拍摄方式"],["body","\n"],["body","例如，如果缺少指定的数据流或索引之一，则可以将策略配置为使快照失败，详见： snapshot requests"],["body","\n"],["headingLink","test-the-snapshot-policy"],["heading","Test the snapshot policy"],["body","\n\n"],["body","\n"],["body","SLM拍摄的快照与其他快照一样"],["body","\n"],["body","\n"],["body","\n"],["body","您可以在Kibana Management中查看有关快照的信息，也可以使用快照api获取信息。 snapshot APIs"],["body","\n"],["body","\n"],["body","\n"],["body","此外，SLM会跟踪策略的成功和失败，因此您可以深入了解策略的工作方式"],["body","\n"],["body","\n"],["body","\n"],["body","如果策略至少执行了一次，则 get policy  将返回其他元数据，这些元数据将显示快照是否成功。"],["body","\n"],["body","\n\n"],["headingLink","手动执行策略"],["heading","手动执行策略"],["body","\n\n"],["body","可以手动立即执行快照策略"],["body","\n"],["body","这对于在进行配置更改、升级或测试新策略之前拍摄快照非常有用"],["body","\n"],["body","手动执行策略不会影响其配置的计划。"],["body","\n\n"],["body","POST /_slm/policy/nightly-snapshots/_execute\n"],["body","\n"],["body","强制运行nightly-snapshots策略后，您可以检索策略以获取成功或失败信息。"],["body","\n"],["body","GET /_slm/policy/nightly-snapshots?human\n"],["body","\n\n"],["body","仅返回最近的成功和失败，但所有策略执行都记录在 .slm-history* indices."],["body","\n"],["body","响应还显示策略何时计划下一步执行。"],["body","\n"],["body","响应显示策略是否成功启动快照，但是，这并不能保证快照成功完成，例如，如果在复制文件时丢失了与远程存储库的连接，则启动的快照可能会失败。"],["body","\n\n"],["body","{\n  \"nightly-snapshots\" : {\n    \"version\": 1,\n    \"modified_date\": \"2019-04-23T01:30:00.000Z\",\n    \"modified_date_millis\": 1556048137314,\n    \"policy\" : {\n      \"schedule\": \"0 30 1 * * ?\",\n      \"name\": \"<nightly-snap-{now/d}>\",\n      \"repository\": \"my_repository\",\n      \"config\": {\n        \"indices\": [\"*\"],\n      },\n      \"retention\": {\n        \"expire_after\": \"30d\",\n        \"min_count\": 5,\n        \"max_count\": 50\n      }\n    },\n    \"last_success\": { //关于策略最后一次成功创建快照的信息                           \n      \"snapshot_name\": \"nightly-snap-2019.04.24-tmtnyjtrsxkhbrrdcgg18a\", //成功启动的快照名称\n      \"time_string\": \"2019-04-24T16:43:49.316Z\",\n      \"time\": 1556124229316\n    } ,\n    \"last_failure\": { //有关策略上次启动快照失败的信息\n      \"snapshot_name\": \"nightly-snap-2019.04.02-lohisb5ith2n8hxacaq3mw\",\n      \"time_string\": \"2019-04-02T01:30:00.000Z\",\n      \"time\": 1556042030000,\n      \"details\": \"{\\\"type\\\":\\\"index_not_found_exception\\\",\\\"reason\\\":\\\"no such index [important]\\\",\\\"resource.type\\\":\\\"index_or_alias\\\",\\\"resource.id\\\":\\\"important\\\",\\\"index_uuid\\\":\\\"_na_\\\",\\\"index\\\":\\\"important\\\",\\\"stack_trace\\\":\\\"[important] IndexNotFoundException[no such index [important]]\\\\n\\\\tat org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.indexNotFoundException(IndexNameExpressionResolver.java:762)\\\\n\\\\tat org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.innerResolve(IndexNameExpressionResolver.java:714)\\\\n\\\\tat org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.resolve(IndexNameExpressionResolver.java:670)\\\\n\\\\tat org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:163)\\\\n\\\\tat org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndexNames(IndexNameExpressionResolver.java:142)\\\\n\\\\tat org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndexNames(IndexNameExpressionResolver.java:102)\\\\n\\\\tat org.elasticsearch.snapshots.SnapshotsService$1.execute(SnapshotsService.java:280)\\\\n\\\\tat org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:47)\\\\n\\\\tat org.elasticsearch.cluster.service.MasterService.executeTasks(MasterService.java:687)\\\\n\\\\tat org.elasticsearch.cluster.service.MasterService.calculateTaskOutputs(MasterService.java:310)\\\\n\\\\tat org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:210)\\\\n\\\\tat org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:142)\\\\n\\\\tat org.elasticsearch.cluster.service.TaskBatcher.runIfNotProcessed(TaskBatcher.java:150)\\\\n\\\\tat org.elasticsearch.cluster.service.TaskBatcher$BatchedTask.run(TaskBatcher.java:188)\\\\n\\\\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:688)\\\\n\\\\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:252)\\\\n\\\\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:215)\\\\n\\\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\\\\n\\\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\\\\n\\\\tat java.base/java.lang.Thread.run(Thread.java:834)\\\\n\\\"}\"\n    } ,\n    \"next_execution\": \"2019-04-24T01:30:00.000Z\",                        \n    \"next_execution_millis\": 1556048160000 //下次策略执行时\n  }\n}\n"],["body","\n"],["headingLink","security-and-slm"],["heading","Security and SLM"],["body","\n"],["body","启用Elasticsearch安全功能时，以下群集权限控制对SLM操作的访问:"],["body","\n\n"],["body","\n"],["body","manage_slm"],["body","\n"],["body","允许用户执行所有SLM操作，包括创建和更新策略以及启动和停止SLM。"],["body","\n"],["body","\n"],["body","\n"],["body","read_slm"],["body","\n"],["body","允许用户执行所有只读SLM操作，例如获取策略和检查SLM状态。"],["body","\n"],["body","\n"],["body","\n"],["body","cluster:admin/snapshot/\\*"],["body","\n"],["body","\n\n"],["body","​\t\t允许用户获取和删除任何索引的快照，无论他们是否有权访问该索引。"],["body","\n\n"],["body","\n"],["body","您可以通过Kibana Management创建和管理角色来分配这些权限。"],["body","\n"],["body","\n"],["body","\n"],["body","要授予创建和管理SLM策略和快照所需的权限，您可以使用manage_slm和 cluster:admin/snapshot/*   集群权限和对SLM历史索引的完全访问权限。"],["body","\n"],["body","\n\n"],["body","例如，以下请求创建了slm-admin角色:"],["body","\n"],["body","POST /_security/role/slm-admin\n{\n  \"cluster\": [\"manage_slm\", \"cluster:admin/snapshot/*\"],\n  \"indices\": [\n    {\n      \"names\": [\".slm-history-*\"],\n      \"privileges\": [\"all\"]\n    }\n  ]\n}\n"],["body","\n\n"],["body","要授予对SLM策略和快照历史记录的只读访问权限，可以设置具有read_slm 集群权限的角色，并读取对快照生命周期管理历史记录索引的访问权限。"],["body","\n"],["body","例如，以下请求创建了一个  slm-read-only"],["body","\n\n"],["body","POST /_security/role/slm-read-only\n{\n  \"cluster\": [\"read_slm\"],\n  \"indices\": [\n    {\n      \"names\": [\".slm-history-*\"],\n      \"privileges\": [\"read\"]\n    }\n  ]\n}\n"],["body","\n"],["headingLink","snapshot-retention"],["heading","Snapshot retention"],["body","\n\n"],["body","\n"],["body","您可以在SLM策略中包含保留策略，以自动删除旧快照"],["body","\n"],["body","\n"],["body","\n"],["body","Retention 作为集群级任务运行，并且不与特定策略的计划相关联"],["body","\n"],["body","\n"],["body","\n"],["body","retention criteria 作为 Retention task 的一部分进行评估，而不是在策略执行时"],["body","\n"],["body","\n"],["body","\n"],["body","为了使保留任务自动删除快照，您需要在SLM策略中包含一个 retention  Object"],["body","\n"],["body","\n"],["body","\n"],["body","要控制保留任务何时运行 配置  slm.retention_schedule 集群配置"],["body","\n"],["body","\n"],["body","\n"],["body","可以定义周期性或者 绝对时间  cron schedule."],["body","\n"],["body","\n"],["body","\n"],["body","The slm.retention_duration 设置 限制SLM删除旧快照应该花费多长时间。"],["body","\n"],["body","\n"],["body","\n"],["body","使用 update settings  动态变更 schedule and duration  设置"],["body","\n"],["body","\n"],["body","\n"],["body","可以使用  execute retention API 手动执行"],["body","\n"],["body","\n\n"],["body","The retention task 仅考虑通过 SLM策略 拍摄的快照，无论是根据策略计划还是通过 execute lifecycle"],["body","\n"],["body","手动快照将被忽略，并且不会计入 retention limits。"],["body","\n"],["body","要检索有关快照保留任务历史记录的信息， get stats API:"],["body","\n"],["body","GET /_slm/stats\n"],["body","\n"],["body","response"],["body","\n"],["body","{\n  \"retention_runs\": 13, //运行的次数\n  \"retention_failed\": 0, //失败的次数\n  \"retention_timed_out\": 0,  //超时的次数：retention次数达到slm.retention_duration时间限制，必须在删除所有符合条件的快照之前停止\n  \"retention_deletion_time\": \"1.4s\",  //定期删除快照总花费时间\n  \"retention_deletion_time_millis\": 1404, \n  \"policy_stats\": [ //被 daily-snapshots 策略 拍摄的快照信息\n    {\n      \"policy\": \"daily-snapshots\",\n      \"snapshots_taken\": 1,\n      \"snapshots_failed\": 1,\n      \"snapshots_deleted\": 0, \n      \"snapshot_deletion_failures\": 0 \n    }\n  ],\n  \"total_snapshots_taken\": 1,\n  \"total_snapshots_failed\": 1,\n  \"total_snapshots_deleted\": 0, \n  \"total_snapshot_deletion_failures\": 0 \n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/snapshotAndRestore/GetSnapshotAPI.html"],["title","GetSnapshotAPI.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","get-snapshot-api"],["heading","Get snapshot API"],["body","\n"],["body","GET /_snapshot/my_repository/my_snapshot\n"],["body","\n"],["headingLink","request"],["heading","Request"],["body","\n"],["body","GET /_snapshot/<repository>/<snapshot>\n"],["body","\n"],["headingLink","description"],["heading","Description"],["body","\n"],["body","使用get snapshot API返回有关一个或多个快照的信息，包括: •"],["body","\n\n"],["body","Start and end time values"],["body","\n"],["body","Version of Elasticsearch that created the snapshot"],["body","\n"],["body","List of included indices"],["body","\n"],["body","Current state of the snapshot"],["body","\n"],["body","List of failures that occurred during the snapshot"],["body","\n\n"],["headingLink","path-parameters"],["heading","Path parameters"],["body","\n"],["body","<repository>"],["body","\n"],["body","(Required, string) Snapshot repository name used to limit the request"],["body","\n"],["body","<snapshot>"],["body","\n"],["body","(Required, string) Comma-separated list of snapshot names to retrieve. Also accepts wildcards (*)."],["body","\n\n"],["body","To get information about all snapshots in a registered repository, use a wildcard (*) or _all."],["body","\n"],["body","To get information about any snapshots that are currently running, use _current."],["body","\n\n"],["body","Using _all in a request fails if any snapshots are unavailable. Set ignore_unavailable to true to return only available snapshots."],["body","\n"],["headingLink","query-parameters"],["heading","Query parameters"],["body","\n"],["headingLink","master_timeout"],["heading","master_timeout"],["body","\n\n"],["body","time units"],["body","\n"],["body","等待主节点连接的超时时长"],["body","\n"],["body","如果指定超时时间没有返回。则失败。"],["body","\n"],["body","默认30是"],["body","\n\n"],["headingLink","ignore_unavailable"],["heading","ignore_unavailable"],["body","\n\n"],["body","部分快照不可用。任然可以返回"],["body","\n"],["body","默认FALSE"],["body","\n\n"],["headingLink","verbose"],["heading","verbose"],["body","\n\n"],["body","返回额外信息：拍摄快照时的集群版本"],["body","\n"],["body","快照开始结束时间"],["body","\n"],["body","快照的分片数量"],["body","\n"],["body","默认TRUE。如果是FALSE 则忽略额外信息"],["body","\n\n"],["headingLink","index_details"],["heading","index_details"],["body","\n\n"],["body","返回索引的详细信息"],["body","\n"],["body","包括索引中的分片数，索引的总大小 (以字节为单位) 以及索引中每个分片的最大段数。默认为false，意味着此信息被省略。"],["body","\n\n"],["headingLink","response-body"],["heading","Response body"],["body","\n"],["headingLink","snapshot"],["heading","snapshot"],["body","\n"],["body","(string) Name of the snapshot."],["body","\n"],["headingLink","uuid"],["heading","uuid"],["body","\n"],["body","(string) Universally unique identifier (UUID) of the snapshot."],["body","\n"],["headingLink","version_id"],["heading","version_id"],["body","\n"],["body","ES 版本ID"],["body","\n"],["headingLink","version"],["heading","VERSION"],["body","\n"],["body","ES版本"],["body","\n"],["headingLink","indices"],["heading","indices"],["body","\n"],["body","快照索引"],["body","\n"],["headingLink","index_details-1"],["heading","index_details"],["body","\n"],["body","快照索引明细"],["body","\n\n"],["body","shard_count:索引分片数量"],["body","\n"],["body","size:分片总大小。当human 参数设置时才有"],["body","\n"],["body","size_in_bytes:分片总大小。当human 参数设置时才有"],["body","\n"],["body","max_segments_per_shard:\n\n"],["body","当前索引 快照的 最大 段数"],["body","\n\n"],["body","\n\n"],["headingLink","data_streams"],["heading","data_streams"],["body","\n"],["body","包含的dataStream"],["body","\n"],["headingLink","include_global_state"],["heading","include_global_state"],["body","\n"],["body","是否包含全局状态"],["body","\n"],["body","feature_states"],["body","\n"],["body","指定的全局状态"],["body","\n"],["headingLink","start_time"],["heading","start_time"],["body","\n"],["headingLink","start_time_in_millis"],["heading","start_time_in_millis"],["body","\n"],["headingLink","end_time"],["heading","end_time"],["body","\n"],["headingLink","end_time_in_millis"],["heading","end_time_in_millis"],["body","\n"],["headingLink","duration_in_millis"],["heading","duration_in_millis"],["body","\n"],["headingLink","failures"],["heading","failures"],["body","\n"],["headingLink","shards"],["heading","shards"],["body","\n"],["body","分片信息"],["body","\n"],["headingLink","total"],["heading","`total"],["body","\n"],["body","快照`中最多 分片数"],["body","\n"],["body","successful"],["body","\n"],["body","(integer) Number of shards that were successfully included in the snapshot."],["body","\n"],["body","failed"],["body","\n"],["body","(integer) Number of shards that failed to be included in the snapshot."],["body","\n"],["headingLink","state"],["heading","state"],["body","\n"],["headingLink","in_progress"],["heading","IN_PROGRESS"],["body","\n"],["body","运行中"],["body","\n"],["headingLink","success"],["heading","SUCCESS"],["body","\n"],["body","运行成功"],["body","\n"],["headingLink","failed"],["heading","FAILED"],["body","\n"],["body","失败"],["body","\n"],["headingLink","partial"],["heading","PARTIAL"],["body","\n"],["body","部分失败"],["body","\n"],["headingLink","examples"],["heading","Examples"],["body","\n"],["body","GET /_snapshot/my_repository/snapshot_2\n"],["body","\n"],["body","{\n  \"snapshots\": [\n    {\n      \"snapshot\": \"snapshot_2\",\n      \"uuid\": \"vdRctLCxSketdKb54xw67g\",\n      \"version_id\": <version_id>,\n      \"version\": <version>,\n      \"indices\": [],\n      \"data_streams\": [],\n      \"feature_states\": [],\n      \"include_global_state\": true,\n      \"state\": \"SUCCESS\",\n      \"start_time\": \"2020-07-06T21:55:18.129Z\",\n      \"start_time_in_millis\": 1593093628850,\n      \"end_time\": \"2020-07-06T21:55:18.876Z\",\n      \"end_time_in_millis\": 1593094752018,\n      \"duration_in_millis\": 0,\n      \"failures\": [],\n      \"shards\": {\n        \"total\": 0,\n        \"failed\": 0,\n        \"successful\": 0\n      }\n    }\n  ]\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/snapshotAndRestore/SearchableSnapshots.html"],["title","SearchableSnapshots.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","searchable-snapshots"],["heading","Searchable snapshots"],["body","\n\n"],["body","可搜索快照使您可以使用快照以非常经济高效的方式搜索不经常访问的数据和只读数据。"],["body","\n"],["body","冷数据层和冻结数据层使用可搜索的快照来降低存储和运营成本。"],["body","\n"],["body","可搜索的快照 消除了对副本分片的需求，从而可能将搜索数据所需的本地存储减半。可搜索的快照依赖于您已经用于备份的相同快照机制，并且对快照存储库存储成本的影响最小。"],["body","\n\n"],["headingLink","using-searchable-snapshots"],["heading","Using searchable snapshots"],["body","\n\n"],["body","搜索可搜索的快照索引与搜索任何其他索引相同。"],["body","\n"],["body","默认情况下，可搜索的快照索引没有副本"],["body","\n"],["body","底层快照提供了弹性，并且预计查询量足够低，以至于单个分片副本就足够了。"],["body","\n"],["body","但是，如果您需要支持更高的查询量，则可以通过调整index.number_of_replicas索引设置来添加副本。"],["body","\n"],["body","如果某个节点发生故障，并且需要在其他地方恢复可搜索快照的分片，则在Elasticsearch将分片分配的其他节点时，集群运行状况不为绿色，会有一个短暂的时间窗口。击中这些分片的搜索可能会失败或返回部分结果，直到将分片重新分配给健康节点为止。"],["body","\n"],["body","您通常通过ILM管理可搜索的快照。可搜索快照操作在到达冷或冻结阶段时会自动将常规索引转换为可搜索快照索引。"],["body","\n"],["body","您还可以通过使用 mount snapshot API. 手动挂载索引来使现有快照中的索引可搜索。"],["body","\n"],["body","要从包含多个索引的快照中挂载索引，我们建议创建快照的克隆 clone ，该克隆仅包含要搜索的索引，并挂载该克隆的快照"],["body","\n"],["body","如果快照有任何已挂在的索引，则不应删除该快照，因此，创建克隆使您能够独立于任何可搜索的快照来管理备份快照的生命周期。如果您使用ILM来管理您的可搜索快照，那么它将在根据需要克隆快照后自动查看。"],["body","\n"],["body","您可以使用与常规索引相同的机制来控制可搜索快照索引的分片的分配。"],["body","\n"],["body","例如可以使用  Index-level shard allocation filtering  设置 将可搜索的快照分片数限制为节点数的子集。"],["body","\n"],["body","可搜索快照索引的恢复速度受到  repository setting： max_restore_bytes_per_sec 和节点设置 indices.recovery.max_bytes_per_sec的限制，就像正常的还原操作一样"],["body","\n"],["body","默认情况下，max_restore_bytes_per_sec是无限的，但indices.recovery.max_bytes_per_sec的默认值取决于节点的配置。See Recovery settings."],["body","\n"],["body","我们建议您在  获取快照之前，将索引强制合并，使得每个分片 单个片段，该快照将作为可搜索的快照索引挂载"],["body","\n"],["body","从快照存储库中的每次读取都会花费时间并花费金钱，并且段越少，恢复快照或响应搜索所需的读取就越少。"],["body","\n"],["body","可搜索的快照是管理大量历史数据档案的理想选择。历史信息的搜索频率通常低于最近的数据，因此可能不需要副本来获得其性能优势。"],["body","\n"],["body","对于更复杂或耗时的搜索，您可以将异步搜索 Async search  与可搜索快照一起使用。"],["body","\n\n"],["body","将以下任何存储库类型与可搜索的快照一起使用:"],["body","\n\n"],["body","AWS S3"],["body","\n"],["body","Google Cloud Storage"],["body","\n"],["body","Azure Blob Storage"],["body","\n"],["body","Hadoop Distributed File Store (HDFS)"],["body","\n"],["body","Shared filesystems such as NFS"],["body","\n"],["body","Read-only HTTP and HTTPS repositories"],["body","\n\n"],["body","您还可以使用这些存储库类型的替代实现，例如 Minio，只要它们完全兼容即可。使用存储库分析API分析您的存储库是否适合与可搜索快照一起使用。"],["body","\n"],["headingLink","how-searchable-snapshots-work"],["heading","How searchable snapshots work"],["body","\n"],["body","\n"],["body","可搜索快照如何工作"],["body","\n"],["body","\n"],["body","When an index is mounted from a snapshot, Elasticsearch allocates its shards to data nodes within the cluster."],["body","\n\n"],["body","从快照挂载索引时，Elasticsearch将其分片分配给集群内的 data nodes"],["body","\n"],["body","然后，数据节点自动将相关的分片数据从存储库中检索到本地存储,基于 mount options .如果可能，搜索使用来自本地存储的数据.  如果数据在本地不可用，Elasticsearch将从快照存储库中下载所需的数据。"],["body","\n"],["body","如果持有其中一个分片的节点发生故障，Elasticsearch会自动将受影响的分片分配到另一个节点上，并且该节点会从存储库中恢复相关的分片数据。"],["body","\n"],["body","不需要副本，也不需要复杂的监视或编排来恢复丢失的分片"],["body","\n"],["body","尽管默认情况下可搜索的快照索引没有副本，您可以通过调整index.number_of_replicas将副本添加到这些索引。"],["body","\n"],["body","通过从快照存储库中复制数据来恢复可搜索快照分片的副本。就像可搜索快照 分片的主分片一样。"],["body","\n"],["body","相反，常规索引的副本 通过从主分片 复制数据来恢复。"],["body","\n\n"],["headingLink","mount-options"],["heading","Mount options"],["body","\n"],["body","要搜索快照，您必须首先将其作为索引在本地挂载。通常ILM会自动执行此操作，但是您也可以自己调用 mount snapshot API"],["body","\n"],["body","从快照挂载索引有两个选项，每个选项都具有不同的性能特征和本地存储足迹:"],["body","\n\n"],["body","\n"],["body","Fully mounted index"],["body","\n\n"],["body","将快照索引的分片的完整副本加载到群集内的节点本地存储中，ILM在 热和冷阶段 使用此选项"],["body","\n"],["body","完全挂载索引的搜索性能通常与常规索引相当，因为访问快照存储库的需求最小。"],["body","\n"],["body","在恢复过程中，搜索性能可能比常规索引慢，因为搜索可能需要一些尚未检索到本地副本中的数据，如果发生这种情况，Elasticsearch将急切地检索与正在进行的恢复  并行完成搜索所需的数据。"],["body","\n\n"],["body","\n"],["body","\n"],["body","Partially mounted index"],["body","\n\n"],["body","使用仅包含最近搜索的快照索引数据部分的本地缓存。"],["body","\n"],["body","此缓存具有固定大小，并且在冻结层中的节点之间共享。ILM在冻结阶段使用此选项，"],["body","\n"],["body","如果搜索需要不在缓存中的数据，则Elasticsearch会从快照存储库中获取丢失的数据，需要这些提取的搜索速度较慢，但是提取的数据存储在缓存中，以便将来可以更快地提供类似的搜索。"],["body","\n"],["body","Elasticsearch将从缓存中逐出不经常使用的数据以释放空间"],["body","\n"],["body","虽然比完全挂载索引或常规索引慢，部分挂载的索引仍然会快速返回搜索结果，即使对于大型数据集，因为存储库中数据的布局针对搜索进行了大量优化"],["body","\n"],["body","在返回结果之前，许多搜索将只需要检索总分片数据的一小部分。"],["body","\n"],["body","要部分挂载索引，必须有一个或多个具有共享缓存的节点。"],["body","\n"],["body","默认情况下，专用冻结数据层节点（具有data_frozen角色且没有其他数据角色的节点）具有 共享缓存，可以使用 总磁盘空间的90%，以及 总磁盘空间减100GB的净空"],["body","\n"],["body","强烈建议在生产中使用专用的冷冻层"],["body","\n"],["body","如果没有专用的冻结层，则必须配置xpack.searchable.snapshot.shared_cache.size设置为一个或多个节点上的缓存预留空间。部分挂载的索引仅分配给具有共享缓存的节点。"],["body","\n\n"],["body","\n"],["body","\n"],["body","xpack.searchable.snapshot.shared_cache.size"],["body","\n"],["body","(Static) Disk space reserved for the shared cache of partially mounted indices. Accepts a percentage of total disk space or an absolute byte value. Defaults to 90% of total disk space for dedicated frozen data tier nodes. Otherwise defaults to 0b."],["body","\n"],["body","\n"],["body","\n"],["body","xpack.searchable.snapshot.shared_cache.size.max_headroom"],["body","\n"],["body","(Static, byte value) For dedicated frozen tier nodes, the max headroom to maintain. If xpack.searchable.snapshot.shared_cache.size is not explicitly set, this setting defaults to 100GB. Otherwise it defaults to -1 (not set). You can only configure this setting if xpack.searchable.snapshot.shared_cache.size is set as a percentage."],["body","\n"],["body","\n\n"],["body","为了说明这些设置如何协同工作，让我们看两个示例，当在专用冻结节点上使用设置的默认值时:"],["body","\n\n"],["body","A 4000 GB disk will result in a shared cache sized at 3900 GB. 90% of 4000 GB is 3600 GB, leaving 400 GB headroom. The default max_headroom of 100 GB takes effect, and the result is therefore 3900 GB."],["body","\n"],["body","A 400 GB disk will result in a shared cache sized at 360 GB."],["body","\n\n"],["body","You can configure the settings in elasticsearch.yml:"],["body","\n"],["body","xpack.searchable.snapshot.shared_cache.size: 4TB\n"],["body","\n\n"],["body","目前，您可以在任何节点上配置 xpack.searchable.snapshot.shared_cache.size。"],["body","\n"],["body","如果在 没有  data_frozen  角色 的节点上设置 该 设置 它将被视为设置为0b。"],["body","\n"],["body","此外，具有共享缓存的节点只能具有单个数据路径 data path."],["body","\n\n"],["headingLink","back-up-and-restore-searchable-snapshots"],["heading","Back up and restore searchable snapshots"],["body","\n\n"],["body","您可以使用常规快照 regular snapshots  来备份包含可搜索快照索引的集群。"],["body","\n"],["body","恢复包含可搜索快照索引的快照时，这些索引将再次恢复为可搜索快照索引。"],["body","\n"],["body","在还原包含可搜索快照索引的快照之前，必须先注册  register the repository  包含原始索引快照的存储库。"],["body","\n"],["body","恢复后，可搜索的快照索引将从其原始存储库中挂载原始索引快照。"],["body","\n"],["body","如果需要，您可以将单独的存储库用于常规快照和可搜索快照。"],["body","\n"],["body","可搜索快照索引的快照仅包含少量元数据，这些元数据标识其原始索引快照，它不包含来自原始索引的任何数据"],["body","\n"],["body","备份的恢复将  无法恢复任何   原始索引快照不可用的  可搜索快照索引"],["body","\n\n"],["headingLink","reliability-of-searchable-snapshots"],["heading","Reliability of searchable snapshots"],["body","\n"],["body","\n"],["body","可搜索快照的可靠性"],["body","\n"],["body","\n\n"],["body","\n"],["body","可搜索快照索引中数据的唯一副本是存储在存储库中的基础快照。"],["body","\n"],["body","\n"],["body","\n"],["body","如果存储库失败或损坏快照的内容，则数据将丢失"],["body","\n"],["body","\n"],["body","\n"],["body","尽管Elasticsearch可能已将数据复制到本地存储中，但这些副本可能不完整，并且在存储库失败后无法用于恢复任何数据。"],["body","\n"],["body","\n"],["body","\n"],["body","您必须确保您的存储库是可靠的，并在存储库中的数据处于静止状态时防止数据损坏。"],["body","\n"],["body","\n"],["body","\n"],["body","所有主要的公共云提供商 提供的blob存储通常提供非常好的保护，防止数据丢失或损坏。如果您管理自己的存储库存储，则应对其可靠性负责。"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/4.IndexTemplates/README.html"],["title","IndexTemplates - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","index-templates"],["heading","Index templates"],["body","\n\n"],["body","\n"],["body","本主题介绍Elasticsearch 7.8中引入的可组合索引模板。"],["body","\n"],["body","\n"],["body","\n"],["body","有关索引模板以前如何工作的信息，请参阅 旧版模板文档。"],["body","\n"],["body","\n"],["body","\n"],["body","索引模板是一种告诉Elasticsearch如何在创建索引时配置索引的方法。"],["body","\n"],["body","\n"],["body","\n"],["body","对于data streams，索引模板在 创建数据流的 backing indices 时 生效的。"],["body","\n"],["body","\n"],["body","\n"],["body","在创建索引之前配置模板。创建索引时（手动或通过索引文档来创建文档）模板设置用作创建索引的基础"],["body","\n"],["body","\n"],["body","\n"],["body","模板有两种类型"],["body","\n\n"],["body","index templates"],["body","\n"],["body","component templates\n\n"],["body","组件模板是可重用的构建块，用于配置 mappings，settings 和 alias"],["body","\n"],["body","虽然您可以使用组件模板来构建索引模板，但它们不会直接应用于索引"],["body","\n"],["body","索引模板可以包含组件模板的集合，也可以直接指定  settings, mappings, and aliases."],["body","\n\n"],["body","\n\n"],["body","\n\n"],["body","以下条件适用于索引模板:"],["body","\n\n"],["body","可组合模板优先于传统模板.  如果没有可组合模板与给定索引匹配，则传统模板可能仍然匹配并且applied."],["body","\n"],["body","如果索引是使用显式设置创建的，并且还匹配索引模板, the settings from the create index request take precedence over settings specified in the index template and its component templates."],["body","\n"],["body","如果新的dataStream 或  索引匹配多个索引模板 ,使用优先级最高的索引模板."],["body","\n\n"],["body","Avoid index pattern collisions"],["body","\n"],["body","Elasticsearch具有内置索引模板，每个模板的优先级为 “100”，用于以下索引模式:"],["body","\n\n"],["body","logs-*-*"],["body","\n"],["body","metrics-*-*"],["body","\n"],["body","synthetics-*-*"],["body","\n\n"],["body","ElasticAgent 使用这些模板来创建数据流。由车队集成创建的索引模板使用类似的重叠索引模式，并且优先级高达 “200”。\n如果使用Fleet或Elastic Agent，请为索引模板分配低于 “100” 的优先级，以避免覆盖这些模板。否则，为避免意外应用模板，请执行以下一项或多项操作:"],["body","\n\n"],["body","To disable all built-in index and component templates, set stack.templates.enabled to false using the cluster update settings API."],["body","\n"],["body","Use a non-overlapping index pattern."],["body","\n"],["body","Assign templates with an overlapping pattern a priority higher than 200. For example, if you don’t use Fleet or Elastic Agent and want to create a template for the logs-* index pattern, assign your template a priority of 500. This ensures your template is applied instead of the built-in template for logs-*-*."],["body","\n\n"],["headingLink","create-index-template"],["heading","Create index template"],["body","\n\n"],["body","\n"],["body","Use the index template and put component template APIs to create and update index templates."],["body","\n"],["body","\n"],["body","\n"],["body","You can also manage index templates from Stack Management in Kibana."],["body","\n"],["body","\n\n"],["body","The following requests create two component templates."],["body","\n"],["body","PUT _component_template/component_template1\n{\n  \"template\": {\n    \"mappings\": {\n      \"properties\": {\n        \"@timestamp\": {\n          \"type\": \"date\"\n        }\n      }\n    }\n  }\n}\n\nPUT _component_template/runtime_component_template\n{\n  \"template\": {\n    \"mappings\": {\n      \"runtime\": { \n        \"day_of_week\": {\n          \"type\": \"keyword\",\n          \"script\": {\n            \"source\": \"emit(doc['@timestamp'].value.dayOfWeekEnum.getDisplayName(TextStyle.FULL, Locale.ROOT))\"\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","下面的请求创建一个由这些组件模板组成的索引模板。"],["body","\n"],["body","PUT _index_template/template_1\n{\n  \"index_patterns\": [\"te*\", \"bar*\"],\n  \"template\": {\n    \"settings\": {\n      \"number_of_shards\": 1\n    },\n    \"mappings\": {\n      \"_source\": {\n        \"enabled\": true\n      },\n      \"properties\": {\n        \"host_name\": {\n          \"type\": \"keyword\"\n        },\n        \"created_at\": {\n          \"type\": \"date\",\n          \"format\": \"EEE MMM dd HH:mm:ss Z yyyy\"\n        }\n      }\n    },\n    \"aliases\": {\n      \"mydata\": { }\n    }\n  },\n  \"priority\": 500,\n  \"composed_of\": [\"component_template1\", \"runtime_component_template\"], \n  \"version\": 3,\n  \"_meta\": {\n    \"description\": \"my custom\"\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/4.IndexTemplates/SimulateMultiComponentTemplates.html"],["title","SimulateMultiComponentTemplates.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","simulate-multi-component-templates"],["heading","Simulate multi-component templates"],["body","\n"],["body","\n"],["body","测试组件模板跟索引模板的相互叠加情况"],["body","\n"],["body","\n\n"],["body","\n"],["body","由于模板不仅可以由多个组件模板组成，还可以由索引模板本身组成，因此有两个模拟api来确定生成的索引设置将是什么。"],["body","\n"],["body","\n"],["body","\n"],["body","模拟将应用于特定索引名称的设置e:"],["body","\n"],["body","\n\n"],["body","POST /_index_template/_simulate_index/my-index-000001\n"],["body","\n"],["body","要模拟将从现有模板应用的设置:"],["body","\n"],["body","POST /_index_template/_simulate/template_1\n"],["body","\n"],["body","You can also specify a template definition in the simulate request. This enables you to verify that settings will be applied as expected before you add a new template."],["body","\n"],["body","PUT /_component_template/ct1\n{\n  \"template\": {\n    \"settings\": {\n      \"index.number_of_shards\": 2\n    }\n  }\n}\n\nPUT /_component_template/ct2\n{\n  \"template\": {\n    \"settings\": {\n      \"index.number_of_replicas\": 0\n    },\n    \"mappings\": {\n      \"properties\": {\n        \"@timestamp\": {\n          \"type\": \"date\"\n        }\n      }\n    }\n  }\n}\n\nPOST /_index_template/_simulate\n{\n  \"index_patterns\": [\"my*\"],\n  \"template\": {\n    \"settings\" : {\n        \"index.number_of_shards\" : 3\n    }\n  },\n  \"composed_of\": [\"ct1\", \"ct2\"]\n}\n"],["body","\n"],["body","The response shows the settings, mappings, and aliases that would be applied to matching indices, and any overlapping templates whose configuration would be superseded by the simulated template body or higher-priority templates."],["body","\n"],["body","The number of shards from the simulated template body"],["body","\n"],["body","{\n  \"template\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"number_of_shards\" : \"3\",   \n        \"number_of_replicas\" : \"0\"\n      }\n    },\n    \"mappings\" : {\n      \"properties\" : {\n        \"@timestamp\" : { //The `@timestamp` field inherited from the `ct2` component template\n          \"type\" : \"date\"           \n        }\n      }\n    },\n    \"aliases\" : { }\n  },\n  \"overlapping\" : [ //Any overlapping templates that would have matched, but have lower priority\n    {\n      \"name\" : \"template_1\",        \n      \"index_patterns\" : [\n        \"my*\"\n      ]\n    }\n  ]\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/4.IndexTemplates/Templates.mm.html"],["title","Templates.mm.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["body","\n"],["body","markmap:\ncolorFreezeLevel: 2"],["body","\n"],["headingLink","initialexpandlevel-2"],["heading","initialExpandLevel: 2"],["body","\n"],["headingLink","maxwidth-300"],["heading","maxWidth: 300"],["body","\n"],["headingLink","索引模板"],["heading","索引模板"],["body","\n"],["headingLink","索引模板是干什么的"],["heading","索引模板是干什么的"],["body","\n\n"],["body","定义 Settings、Mapping的索引模板，可以在索引创建时，自动应用索引模板、基于索引名称的匹配"],["body","\n\n"],["headingLink","传统模板"],["heading","传统模板"],["body","\n"],["headingLink","索引模板操作"],["heading","索引模板操作"],["body","\n\n"],["body","PUT /_template/"],["body","\n"],["body","GET /_template/"],["body","\n\n"],["headingLink","索引模板请求体"],["heading","索引模板请求体"],["body","\n\n"],["body","index_patterns\n匹配的索引名称"],["body","\n"],["body","aliases\n新建的索引别名"],["body","\n"],["body","mappings\n索引映射"],["body","\n"],["body","settings\n索引设置"],["body","\n"],["body","version\n索引版本\n\n"],["body","\n"],["body","指定版本"],["body","\n"],["body","\n"],["body","\n"],["body","PUT /_template/template_1\n{\n\"index_patterns\" : [\"my-index-*\"],\n\"order\" : 0,\n\"settings\" : {\n    \"number_of_shards\" : 1\n},\n\"version\": 123\n}\n"],["body","\n"],["body","\n"],["body","\n"],["body","根据版本查询"],["body","\n\n"],["body","GET /_template/template_1?filter_path=*.version"],["body","\n\n"],["body","\n"],["body","\n"],["body","响应"],["body","\n"],["body","\n"],["body","\n"],["body","    {\n    \"template_1\" : {\n        \"version\" : 123\n    }\n    }\n"],["body","\n"],["body","\n"],["body","\n"],["body","ES不会管理索引版本，需要外部系统自行实现"],["body","\n"],["body","\n\n"],["body","\n\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","PUT _template/template_1\n{ \n\"index_patterns\": [\"te*\", \"bar*\"],\n\"settings\": {\n    \"number_of_shards\": 1\n},\n\"mappings\": {\n    \"_source\": {\n    \"enabled\": false\n    },\n    \"properties\": {\n    \"host_name\": {\n        \"type\": \"keyword\"\n    },\n    \"created_at\": {\n        \"type\": \"date\",\n        \"format\": \"EEE MMM dd HH:mm:ss Z yyyy\"\n    }\n    }\n}\n}\n"],["body","\n"],["headingLink","可组合模板"],["heading","可组合模板"],["body","\n"],["headingLink","创建索引模板"],["heading","创建索引模板"],["body","\n\n"],["body","Request\n\n"],["body","PUT /_index_template/"],["body","\n\n"],["body","\n"],["body","Request body\n\n"],["body","\n"],["body","composed_of\ncomponent template名称数组"],["body","\n"],["body","\n"],["body","\n"],["body","data_stream"],["body","\n\n"],["body","hidden:true 表示创建的dataStream是隐藏的、默认false"],["body","\n\n"],["body","\n"],["body","\n"],["body","index_patterns\n匹配的索引"],["body","\n"],["body","\n"],["body","\n"],["body","_meta\n索引模板中的用户元数据"],["body","\n"],["body","\n"],["body","\n"],["body","priority\n应用索引模板的优先级，默认最低优先级0"],["body","\n"],["body","\n"],["body","\n"],["body","template\n指定模板的 aliases, mappings, or settings"],["body","\n"],["body","\n\n"],["body","\n\n"],["headingLink","创建组件模板"],["heading","创建组件模板"],["body","\n\n"],["body","\n"],["body","Request"],["body","\n\n"],["body","PUT /_component_template/"],["body","\n\n"],["body","\n"],["body","\n"],["body","Request body"],["body","\n\n"],["body","template\n指定模板的 aliases, mappings, or settings"],["body","\n"],["body","version\n索引版本号"],["body","\n"],["body","allow_auto_create\n_meta\n用户元数据"],["body","\n\n"],["body","\n\n"],["headingLink","example-1"],["heading","Example"],["body","\n\n"],["body","创建索引模板"],["body","\n\n"],["body","PUT _index_template/template_1\n{\n  \"index_patterns\": [\"te*\", \"bar*\"],\n  \"template\": {\n    \"settings\": {\n      \"number_of_shards\": 1\n    },\n    \"mappings\": {\n      \"_source\": {\n        \"enabled\": true\n      },\n      \"properties\": {\n        \"host_name\": {\n          \"type\": \"keyword\"\n        },\n        \"created_at\": {\n          \"type\": \"date\",\n          \"format\": \"EEE MMM dd HH:mm:ss Z yyyy\"\n        }\n      }\n    },\n    \"aliases\": {\n      \"mydata\": { }\n    }\n  },\n  \"priority\": 500,\n  \"composed_of\": [\"component_template1\", \"runtime_component_template\"], \n  \"version\": 3,\n  \"_meta\": {\n    \"description\": \"my custom\"\n  }\n}\n"],["body","\n\n"],["body","创建组件模板"],["body","\n\n"],["body","PUT _component_template/runtime_component_template\n{\n  \"template\": {\n    \"mappings\": {\n      \"runtime\": { \n        \"day_of_week\": {\n          \"type\": \"keyword\",\n          \"script\": {\n            \"source\": \"emit(doc['@timestamp'].value.dayOfWeekEnum.getDisplayName(TextStyle.FULL, Locale.ROOT))\"\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/IngestPipeline/5.IngestProcessorReference.html"],["title","IngestProcessorReference.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","ingest-processor-reference"],["heading","Ingest processor reference"],["body","\n"],["body","https://www.elastic.co/guide/en/elasticsearch/reference/7.17/processors.html"],["body","\n"],["body","Elasticsearch includes several configurable processors. To get a list of available processors, use the nodes info API."],["body","\n"],["body","GET _nodes/ingest?filter_path=nodes.*.ingest.processors\n"],["body","\n"],["headingLink","processor-plugins"],["heading","Processor plugins"],["body","\n"],["body","You can install additional processors as plugins."],["body","\n"],["body","您必须在集群中的所有节点上安装任何插件处理器。否则，Elasticsearch将无法创建包含处理器的管道。"],["body","\n"],["body","通过在elasticsearch.yml中设置plugin，将插件标记为必填项。如果未安装强制插件，则节点将无法启动。"],["body","\n"],["body","plugin.mandatory: ingest-attachment\n"],["body","\n"],["headingLink","append-processor"],["heading","Append processor"],["body","\n"],["body","Appends one or more values to an existing array if the field already exists and it is an array. Converts a scalar to an array and appends one or more values to it if the field exists and it is a scalar. Creates an array containing the provided values if the field doesn’t exist. Accepts a single value or an array of values."],["body","\n"],["body","如果字段已经存在并且是数组，则将一个或多个值附加到现有数组。将标量转换为数组，如果字段存在并且是标量，则将一个或多个值附加到数组。如果字段不存在，则创建一个包含提供的值的数组。接受单个值或值数组。"],["body","\n"],["body","Table 3. Append Options"],["body","\n"],["body","Name"],["body","Required"],["body","Default"],["body","Description"],["body","\n"],["body","field"],["body","yes"],["body","-"],["body","The field to be appended to. Supports template snippets."],["body","\n"],["body","value"],["body","yes"],["body","-"],["body","The value to be appended. Supports template snippets."],["body","\n"],["body","allow_duplicates"],["body","no"],["body","true"],["body","If false, the processor does not append values already present in the field."],["body","\n"],["body","media_type"],["body","no"],["body","application/json"],["body","The media type for encoding value. Applies only when value is a template snippet. Must be one of application/json, text/plain, or application/x-www-form-urlencoded."],["body","\n"],["body","description"],["body","no"],["body","-"],["body","Description of the processor. Useful for describing the purpose of the processor or its configuration."],["body","\n"],["body","if"],["body","no"],["body","-"],["body","Conditionally execute the processor. See Conditionally run a processor."],["body","\n"],["body","ignore_failure"],["body","no"],["body","false"],["body","Ignore failures for the processor. See Handling pipeline failures."],["body","\n"],["body","on_failure"],["body","no"],["body","-"],["body","Handle failures for the processor. See Handling pipeline failures."],["body","\n"],["body","tag"],["body","no"],["body","-"],["body","Identifier for the processor. Useful for debugging and metrics."],["body","\n\n\n"],["body","{\n  \"append\": {\n    \"field\": \"tags\",\n    \"value\": [\"production\", \"{{{app}}}\", \"{{{owner}}}\"]\n  }\n}\n"],["body","\n"],["headingLink","bytes-processor"],["heading","Bytes processor"],["body","\n"],["body","将人类可读字节值 (例如1kb) 转换为其以字节为单位的值 (例如1024)。如果该字段是字符串数组，则该数组的所有成员都将被转换。"],["body","\n"],["body","支持的人类可读单位是 “b”，“kb”，“mb”，“gb”，“tb”，“pb” 大小写不敏感。如果该字段不是受支持的格式或结果值超过2 ^ 63，则会发生错误。"],["body","\n"],["body","Name"],["body","Required"],["body","Default"],["body","Description"],["body","\n"],["body","field"],["body","yes"],["body","-"],["body","The field to convert"],["body","\n"],["body","target_field"],["body","no"],["body","field"],["body","The field to assign the converted value to, by default field is updated in-place"],["body","\n"],["body","ignore_missing"],["body","no"],["body","false"],["body","If true and field does not exist or is null, the processor quietly exits without modifying the document"],["body","\n"],["body","description"],["body","no"],["body","-"],["body","Description of the processor. Useful for describing the purpose of the processor or its configuration."],["body","\n"],["body","if"],["body","no"],["body","-"],["body","Conditionally execute the processor. See Conditionally run a processor."],["body","\n"],["body","ignore_failure"],["body","no"],["body","false"],["body","Ignore failures for the processor. See Handling pipeline failures."],["body","\n"],["body","on_failure"],["body","no"],["body","-"],["body","Handle failures for the processor. See Handling pipeline failures."],["body","\n"],["body","tag"],["body","no"],["body","-"],["body","Identifier for the processor. Useful for debugging and metrics."],["body","\n\n\n"],["headingLink","circle-processor"],["heading","Circle processor"],["body","\n"],["body","Converts circle definitions of shapes to regular polygons which approximate them."],["body","\n"],["body","详见"],["body","\n"],["headingLink","csv-processor"],["heading","CSV processor"],["body","\n"],["body","从文档中的单个文本字段中提取CSV行中的字段。CSV中的任何空字段都将被跳过。"],["body","\n"],["body","Name"],["body","Required"],["body","Default"],["body","Description"],["body","\n"],["body","field"],["body","yes"],["body","-"],["body","The field to extract data from"],["body","\n"],["body","target_fields"],["body","yes"],["body","-"],["body","The array of fields to assign extracted values to"],["body","\n"],["body","separator"],["body","no"],["body",","],["body","Separator used in CSV, has to be single character string"],["body","\n"],["body","quote"],["body","no"],["body","\""],["body","Quote used in CSV, has to be single character string"],["body","\n"],["body","ignore_missing"],["body","no"],["body","true"],["body","If true and field does not exist, the processor quietly exits without modifying the document"],["body","\n"],["body","trim"],["body","no"],["body","false"],["body","Trim whitespaces in unquoted fields"],["body","\n"],["body","empty_value"],["body","no"],["body","-"],["body","Value used to fill empty fields, empty fields will be skipped if this is not provided. Empty field is one with no value (2 consecutive separators) or empty quotes (\"\")"],["body","\n"],["body","description"],["body","no"],["body","-"],["body","Description of the processor. Useful for describing the purpose of the processor or its configuration."],["body","\n"],["body","if"],["body","no"],["body","-"],["body","Conditionally execute the processor. See Conditionally run a processor."],["body","\n"],["body","ignore_failure"],["body","no"],["body","false"],["body","Ignore failures for the processor. See Handling pipeline failures."],["body","\n"],["body","on_failure"],["body","no"],["body","-"],["body","Handle failures for the processor. See Handling pipeline failures."],["body","\n"],["body","tag"],["body","no"],["body","-"],["body","Identifier for the processor. Useful for debugging and metrics."],["body","\n\n\n"],["headingLink","grok-processor"],["heading","Grok processor"],["body","\n"],["body","从文档中的单个文本字段中提取结构化字段。您可以选择从中提取匹配字段的字段，以及您期望匹配的grok模式。"],["body","\n"],["body","grok模式就像一个正则表达式，它支持可以重复使用的别名表达式。"],["body","\n"],["body","该处理器包装有许多可重复使用的 pattern"],["body","\n"],["body","If you need help building patterns to match your logs,"],["body","\n"],["body","如果您需要帮助构建模式来匹配您的日志"],["body","\n"],["body","you will find the Grok Debugger tool quite useful! The Grok Constructor is also a useful tool."],["body","\n"],["headingLink","using-the-grok-processor-in-a-pipeline"],["heading","Using the Grok Processor in a Pipeline"],["body","\n"],["body","Table 21. Grok Options"],["body","\n"],["body","Name"],["body","Required"],["body","Default"],["body","Description"],["body","\n"],["body","field"],["body","yes"],["body","-"],["body","The field to use for grok expression parsing"],["body","\n"],["body","patterns"],["body","yes"],["body","-"],["body","An ordered list of grok expression to match and extract named captures with. Returns on the first expression in the list that matches."],["body","\n"],["body","pattern_definitions"],["body","no"],["body","-"],["body","A map of pattern-name and pattern tuples defining custom patterns to be used by the current processor. Patterns matching existing names will override the pre-existing definition."],["body","\n"],["body","ecs_compatibility"],["body","no"],["body","disabled"],["body","Must be disabled or v1. If v1, the processor uses patterns with Elastic Common Schema (ECS) field names."],["body","\n"],["body","trace_match"],["body","no"],["body","false"],["body","when true, _ingest._grok_match_index will be inserted into your matched document’s metadata with the index into the pattern found in patterns that matched."],["body","\n"],["body","ignore_missing"],["body","no"],["body","false"],["body","If true and field does not exist or is null, the processor quietly exits without modifying the document"],["body","\n"],["body","description"],["body","no"],["body","-"],["body","Description of the processor. Useful for describing the purpose of the processor or its configuration."],["body","\n"],["body","if"],["body","no"],["body","-"],["body","Conditionally execute the processor. See Conditionally run a processor."],["body","\n"],["body","ignore_failure"],["body","no"],["body","false"],["body","Ignore failures for the processor. See Handling pipeline failures."],["body","\n"],["body","on_failure"],["body","no"],["body","-"],["body","Handle failures for the processor. See Handling pipeline failures."],["body","\n"],["body","tag"],["body","no"],["body","-"],["body","Identifier for the processor. Useful for debugging and metrics."],["body","\n\n\n"],["headingLink","custom-patterns"],["heading","Custom Patterns"],["body","\n"],["body","The Grok processor comes pre-packaged with a base set of patterns. These patterns may not always have what you are looking for. Patterns have a very basic format. Each entry has a name and the pattern itself."],["body","\n"],["body","Grok处理器预先包装了一组基本模式。这些模式可能并不总是有你想要的。模式有一个非常基本的格式。每个条目都有一个名称和pattern。"],["body","\n"],["body","您可以在pattern_definitions选项下将自己的模式添加到处理器定义中。下面是指定自定义模式定义的管道示例:"],["body","\n"],["body","{\n  \"description\" : \"...\",\n  \"processors\": [\n    {\n      \"grok\": {\n        \"field\": \"message\",\n        \"patterns\": [\"my %{FAVORITE_DOG:dog} is colored %{RGB:color}\"],\n        \"pattern_definitions\" : {\n          \"FAVORITE_DOG\" : \"beagle\",\n          \"RGB\" : \"RED|GREEN|BLUE\"\n        }\n      }\n    }\n  ]\n}\n"],["body","\n"],["headingLink","providing-multiple-match-patterns"],["heading","Providing Multiple Match Patterns"],["body","\n"],["body","有时，一种模式不足以捕获子段的潜在结构。假设我们要匹配所有包含您最喜欢的猫或狗的宠物品种的消息。实现此目的的一种方法是提供可以匹配的两种不同模式，而不是一种捕获相同或行为的真正复杂的表达。"],["body","\n"],["body","Here is an example of such a configuration executed against the simulate API:"],["body","\n"],["body","POST _ingest/pipeline/_simulate\n{\n  \"pipeline\": {\n  \"description\" : \"parse multiple patterns\",\n  \"processors\": [\n    {\n      \"grok\": {\n        \"field\": \"message\",\n        \"patterns\": [\"%{FAVORITE_DOG:pet}\", \"%{FAVORITE_CAT:pet}\"],\n        \"pattern_definitions\" : {\n          \"FAVORITE_DOG\" : \"beagle\",\n          \"FAVORITE_CAT\" : \"burmese\"\n        }\n      }\n    }\n  ]\n},\n\"docs\":[\n  {\n    \"_source\": {\n      \"message\": \"I love burmese cats!\"\n    }\n  }\n  ]\n}\n"],["body","\n"],["body","两种模式都将为字段设置适当的匹配项，但是如果我们想跟踪哪个模式匹配并填充了我们的字段，该怎么办"],["body","\n"],["body","We can do this with the trace_match parameter. Here is the output of that same pipeline, but with \"trace_match\": true configured:"],["body","\n"],["body","我们可以使用trace_match参数来执行此操作。这里是同一管道的输出，但与 “trace_match”: true配置:"],["body","\n"],["body","{\n  \"docs\": [\n    {\n      \"doc\": {\n        \"_type\": \"_doc\",\n        \"_index\": \"_index\",\n        \"_id\": \"_id\",\n        \"_source\": {\n          \"message\": \"I love burmese cats!\",\n          \"pet\": \"burmese\"\n        },\n        \"_ingest\": {\n          \"_grok_match_index\": \"1\",\n          \"timestamp\": \"2016-11-08T19:43:03.850+0000\"\n        }\n      }\n    }\n  ]\n}\n"],["body","\n"],["body","In the above response, you can see that the index of the pattern that matched was \"1\"."],["body","\n"],["body","This is to say that it was the second (index starts at zero) pattern in patterns to match."],["body","\n"],["body","这个跟踪元数据可以调试匹配的模式。此信息存储在ingest metadata  中，不会被索引。"],["body","\n"],["headingLink","retrieving-patterns-from-rest-endpoint"],["heading","Retrieving patterns from REST endpoint"],["body","\n"],["body","The Grok processor comes packaged with its own REST endpoint for retrieving the patterns included with the processor."],["body","\n"],["body","Grok processor 随附了自己的REST端点，用于检索 处理器预定义的模式。"],["body","\n"],["body","GET _ingest/processor/grok\n"],["body","\n"],["body","The above request will return a response body containing a key-value representation of the built-in patterns dictionary."],["body","\n"],["body","{\n  \"patterns\" : {\n    \"BACULA_CAPACITY\" : \"%{INT}{1,3}(,%{INT}{3})*\",\n    \"PATH\" : \"(?:%{UNIXPATH}|%{WINPATH})\",\n    ...\n}\n"],["body","\n"],["body","By default, the API returns a list of legacy Grok patterns."],["body","\n"],["body","These legacy patterns predate the Elastic Common Schema (ECS) and don’t use ECS field names. To return patterns that extract ECS field names, specify v1 in the optional ecs_compatibility query parameter."],["body","\n"],["body","GET _ingest/processor/grok?ecs_compatibility=v1\n"],["body","\n"],["body","By default, the API returns patterns in the order they are read from disk."],["body","\n"],["body","默认情况下，API按照从磁盘读取的顺序返回模式。这种排序顺序保留了相关模式的分组。例如，与解析Linux syslog行相关的所有模式都保存在一起。"],["body","\n"],["body","您可以使用可选的boolean s查询参数来按键名对返回的模式进行排序。"],["body","\n"],["body","GET _ingest/processor/grok?s\n"],["body","\n"],["body","The API returns the following response."],["body","\n"],["body","{\n  \"patterns\" : {\n    \"BACULA_CAPACITY\" : \"%{INT}{1,3}(,%{INT}{3})*\",\n    \"BACULA_DEVICE\" : \"%{USER}\",\n    \"BACULA_DEVICEPATH\" : \"%{UNIXPATH}\",\n    ...\n}\n"],["body","\n"],["body","This can be useful to reference as the built-in patterns change across versions."],["body","\n"],["headingLink","grok-watchdog"],["heading","Grok watchdog"],["body","\n"],["body","执行时间太长的Grok表达式被中断，然后grok处理器出现异常失败。"],["body","\n"],["body","grok处理器有一个看门狗线程，该线程确定grok表达式的求值时间过长，并由以下设置控制:"],["body","\n"],["body","Name"],["body","Default"],["body","Description"],["body","\n"],["body","ingest.grok.watchdog.interval"],["body","1s"],["body","How often to check whether there are grok evaluations that take longer than the maximum allowed execution time."],["body","\n"],["body","ingest.grok.watchdog.max_execution_time"],["body","1s"],["body","The maximum allowed execution of a grok expression evaluation."],["body","\n\n\n"],["headingLink","grok-debugging"],["heading","Grok debugging"],["body","\n"],["body","建议使用 Grok Debugger 来调试grok模式。从那里，您可以针对示例数据测试UI中的一个或多个模式，它使用与摄取节点处理器相同的引擎。"],["body","\n"],["body","Additionally, it is recommended to enable debug logging for Grok so that any additional messages may also be seen in the Elasticsearch server log."],["body","\n"],["body","此外，建议为Grok启用调试日志记录，以便在Elasticsearch服务器日志中也可以看到任何其他消息。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/IngestPipeline/3.EnrichYourData.html"],["title","EnrichYourData.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","enrich-your-data"],["heading","Enrich your data"],["body","\n"],["body","You can use the enrich processor to add data from your existing indices to incoming documents during ingest."],["body","\n"],["body","For example, you can use the enrich processor to:"],["body","\n\n"],["body","根据已知的ip地址识别web服务或供应商"],["body","\n"],["body","根据产品id向零售订单添加产品信息"],["body","\n"],["body","根据电子邮件地址补充联系信息"],["body","\n"],["body","根据用户坐标添加邮政编码"],["body","\n\n"],["headingLink","how-the-enrich-processor-works"],["heading","How the enrich processor works"],["body","\n"],["body","大多数处理器都是自包含的，并且仅更改传入文档中的现有数据。"],["body","\n"],["body","enrich处理器将新数据添加到传入文档中，并且需要一些特殊组件:"],["body","\n"],["body","enrich policy"],["body","\n"],["body","一组配置选项，用于将正确的enrich数据添加到正确的传入文档中。"],["body","\n"],["body","An enrich policy contains:"],["body","\n\n"],["body","A list of one or more source indices which store enrich data as documents"],["body","\n"],["body","The policy type which determines how the processor matches the enrich data to incoming documents"],["body","\n"],["body","A match field from the source indices used to match incoming documents"],["body","\n"],["body","Enrich fields containing enrich data from the source indices you want to add to incoming documents"],["body","\n\n"],["body","在将其与enrich处理器一起使用之前，必须执行enrich策略。执行时，enrich策略使用策略的源索引中的enrich数据来创建称为 “enrich索引” 的简化系统索引。处理器使用此索引来匹配和丰富传入的文档。"],["body","\n"],["body","source index"],["body","\n"],["body","An index which stores enrich data you’d like to add to incoming documents. You can create and manage these indices just like a regular Elasticsearch index. You can use multiple source indices in an enrich policy. You also can use the same source index in multiple enrich policies."],["body","\n"],["body","一个索引，用于存储要添加到传入文档中的enrich data。您可以像常规Elasticsearch索引一样创建和管理这些索引。您可以在enrich策略中使用多个源索引。您还可以在多个enrich策略中使用相同的源索引。"],["body","\n"],["body","enrich index"],["body","\n"],["body","与特定的enrich策略相关的特殊系统索引。将传入文档与源索引中的文档直接匹配可能会很慢且资源密集。为了加快速度，enrich处理器使用了enrich索引。Enrich索引包含来自源索引的enrich数据，但具有一些特殊属性来帮助简化它们: 它们是系统索引，这意味着它们由Elasticsearch内部管理，仅适用于enrich处理器。它们总是以enrich- 开始。它们是只读的，这意味着你不能直接更改它们。它们是强制合并的（ force merged ），以便快速检索。"],["body","\n"],["headingLink","set-up-an-enrich-processor"],["heading","Set up an enrich processor"],["body","\n"],["body","To set up an enrich processor, follow these steps:"],["body","\n\n"],["body","Check the prerequisites."],["body","\n"],["body","Add enrich data."],["body","\n"],["body","Create an enrich policy."],["body","\n"],["body","Execute the enrich policy."],["body","\n"],["body","Add an enrich processor to an ingest pipeline."],["body","\n"],["body","Ingest and enrich documents."],["body","\n\n"],["body","Once you have an enrich processor set up, you can update your enrich data and update your enrich policies."],["body","\n"],["body","enrich处理器执行多个操作，可能会影响您的 ingest pipeline的速度。"],["body","\n"],["body","我们强烈建议在将enrich处理器部署到生产中之前对其进行测试和基准测试。"],["body","\n"],["body","我们不建议使用enrich处理器来附加实时数据。enrich处理器最适合不经常更改的参考数据。"],["body","\n"],["headingLink","prerequisites"],["heading","Prerequisites"],["body","\n"],["body","If you use Elasticsearch security features, you must have:"],["body","\n\n"],["body","read index privileges for any indices used"],["body","\n"],["body","The enrich_user built-in role"],["body","\n\n"],["headingLink","add-enrich-data"],["heading","Add enrich data"],["body","\n"],["body","首先，将文档添加到一个或多个源索引。这些文档应包含您最终要添加到传入文档中的enrich数据。"],["body","\n"],["body","You can manage source indices just like regular Elasticsearch indices using the document and index APIs."],["body","\n"],["body","You also can set up Beats, such as a Filebeat, to automatically send and index documents to your source indices. See Getting started with Beats."],["body","\n"],["headingLink","create-an-enrich-policy"],["heading","Create an enrich policy"],["body","\n"],["body","After adding enrich data to your source indices, use the create enrich policy API to create an enrich policy."],["body","\n"],["body","Once created, you can’t update or change an enrich policy. See Update an enrich policy."],["body","\n"],["headingLink","execute-the-enrich-policy"],["heading","Execute the enrich policy"],["body","\n"],["body","Once the enrich policy is created, you can execute it using the execute enrich policy API to create an enrich index."],["body","\n"],["body","The enrich index contains documents from the policy’s source indices. Enrich indices always begin with .enrich-*, are read-only, and are force merged."],["body","\n"],["body","Enrich indices should be used by the enrich processor only. Avoid using enrich indices for other purposes."],["body","\n"],["headingLink","add-an-enrich-processor-to-an-ingest-pipeline"],["heading","Add an enrich processor to an ingest pipeline"],["body","\n"],["body","一旦您有了源索引、enrich策略和相关的enrich索引，您就可以为您的策略设置包含一个enrich处理器的ingest pipeline。"],["body","\n"],["body","Define an enrich processor and add it to an ingest pipeline using the create or update pipeline API."],["body","\n"],["body","When defining the enrich processor, you must include at least the following:"],["body","\n"],["body","定义enrich处理器时，必须至少包括以下内容:"],["body","\n\n"],["body","The enrich policy to use."],["body","\n"],["body","The field used to match incoming documents to the documents in your enrich index."],["body","\n"],["body","用于将传入文档与enrich索引中的文档匹配的字段。"],["body","\n"],["body","要添加到传入文档的目标字段。此目标字段包含在您的丰富策略中指定的匹配字段和丰富字段。"],["body","\n\n"],["body","You also can use the max_matches option to set the number of enrich documents an incoming document can match. If set to the default of 1, data is added to an incoming document’s target field as a JSON object. Otherwise, the data is added as an array."],["body","\n"],["body","See Enrich for a full list of configuration options，You also can add other processors to your ingest pipeline."],["body","\n"],["headingLink","update-an-enrich-index"],["heading","Update an enrich index"],["body","\n"],["body","Once created, you cannot update or index documents to an enrich index. Instead, update your source indices and execute the enrich policy again. This creates a new enrich index from your updated source indices. The previous enrich index will deleted with a delayed maintenance job. By default this is done every 15 minutes."],["body","\n"],["body","创建后，您无法将文档更新或索引为enrich索引。相反，更新您的源索引并再次执行enrich策略。这将从更新的源索引中创建一个新的丰富索引。以前的enrich索引将随着延迟维护作业而删除。默认情况下，每15分钟完成一次。"],["body","\n"],["body","If wanted, you can reindex or update any already ingested documents using your ingest pipeline."],["body","\n"],["headingLink","update-an-enrich-policy"],["heading","Update an enrich policy"],["body","\n"],["body","Once created, you can’t update or change an enrich policy. Instead, you can:"],["body","\n\n"],["body","Create and execute a new enrich policy."],["body","\n"],["body","Replace the previous enrich policy with the new enrich policy in any in-use enrich processors."],["body","\n"],["body","Use the delete enrich policy API to delete the previous enrich policy."],["body","\n\n"],["headingLink","enrich-components"],["heading","Enrich components"],["body","\n"],["body","The enrich coordinator is a component that manages and performs the searches required to enrich documents on each ingest node. It combines searches from all enrich processors in all pipelines into bulk multi-searches."],["body","\n"],["body","The enrich policy executor is a component that manages the executions of all enrich policies. When an enrich policy is executed, this component creates a new enrich index and removes the previous enrich index. The enrich policy executions are managed from the elected master node. The execution of these policies occurs on a different node."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/IngestPipeline/2.pipeLineExample.html"],["title","pipeLineExample.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","pipelineexample"],["heading","PipeLineExample"],["body","\n"],["body","In this example tutorial, you’ll use an ingest pipeline to parse server logs in the Common Log Format before indexing."],["body","\n"],["body","The logs you want to parse look similar to this:"],["body","\n"],["body","212.87.37.154 - - [30/May/2099:16:21:15 +0000] \\\"GET /favicon.ico HTTP/1.1\\\"\n200 3638 \\\"-\\\" \\\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6)\nAppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\\\"\n"],["body","\n"],["body","这些日志包含时间戳、ip地址和用户代理。您希望在Elasticsearch中赋予这三个项目自己的字段，以实现更快的搜索和可视化。您还想知道请求是从哪里来的。"],["body","\n\n"],["body","\n"],["body","In Kibana, open the main menu and click Stack Management > Ingest Pipelines."],["body","\n"],["body","\n"],["body","\n"],["body","Click Create pipeline."],["body","\n"],["body","\n"],["body","\n"],["body","Provide a name and description for the pipeline."],["body","\n"],["body","\n"],["body","\n"],["body","Add a grok processor to parse the log message:"],["body","\n\n"],["body","\n"],["body","Click Add a processor and select the Grok processor type."],["body","\n"],["body","\n"],["body","\n"],["body","Set Field to message and Patterns to the following grok pattern:"],["body","\n"],["body","\n"],["body","\n"],["body","%{IPORHOST:source.ip} %{USER:user.id} %{USER:user.name} \\\\[%{HTTPDATE:@timestamp}\\\\] \\\"%{WORD:http.request.method} %{DATA:url.original} HTTP/%{NUMBER:http.version}\\\" %{NUMBER:http.response.status_code:int} (?:-|%{NUMBER:http.response.body.bytes:int}) %{QS:http.request.referrer} %{QS:user_agent}\n"],["body","\n"],["body","\n"],["body","\n"],["body","Click Add to save the processor."],["body","\n"],["body","\n"],["body","\n"],["body","Set the processor description to Extract fields from 'message'."],["body","\n"],["body","\n\n"],["body","\n"],["body","\n"],["body","Add processors for the timestamp, IP address, and user agent fields. Configure the processors as follows:"],["body","\n"],["body","\n\n"],["body","Processor type"],["body","Field"],["body","Additional options"],["body","Description"],["body","\n"],["body","Date"],["body","@timestamp"],["body","Formats: dd/MMM/yyyy:HH:mm:ss Z"],["body","Format '@timestamp' as 'dd/MMM/yyyy:HH:mm:ss Z'"],["body","\n"],["body","GeoIP"],["body","source.ip"],["body","Target field: source.geo"],["body","Add 'source.geo' GeoIP data for 'source.ip'"],["body","\n"],["body","User agent"],["body","user_agent"],["body","Extract fields from 'user_agent'"],["body","\n\n\n"],["body","Your form should look similar to this:"],["body","\n"],["body","The four processors will run sequentially:\nGrok > Date > GeoIP > User agent\nYou can reorder processors using the arrow icons."],["body","\n"],["body","Alternatively, you can click the Import processors link and define the processors as JSON:"],["body","\n"],["body","{\n  \"processors\": [\n    {\n      \"grok\": {\n        \"description\": \"Extract fields from 'message'\",\n        \"field\": \"message\",\n        \"patterns\": [\"%{IPORHOST:source.ip} %{USER:user.id} %{USER:user.name} \\\\[%{HTTPDATE:@timestamp}\\\\] \\\"%{WORD:http.request.method} %{DATA:url.original} HTTP/%{NUMBER:http.version}\\\" %{NUMBER:http.response.status_code:int} (?:-|%{NUMBER:http.response.body.bytes:int}) %{QS:http.request.referrer} %{QS:user_agent}\"]\n      }\n    },\n    {\n      \"date\": {\n        \"description\": \"Format '@timestamp' as 'dd/MMM/yyyy:HH:mm:ss Z'\",\n        \"field\": \"@timestamp\",\n        \"formats\": [ \"dd/MMM/yyyy:HH:mm:ss Z\" ]\n      }\n    },\n    {\n      \"geoip\": {\n        \"description\": \"Add 'source.geo' GeoIP data for 'source.ip'\",\n        \"field\": \"source.ip\",\n        \"target_field\": \"source.geo\"\n      }\n    },\n    {\n      \"user_agent\": {\n        \"description\": \"Extract fields from 'user_agent'\",\n        \"field\": \"user_agent\"\n      }\n    }\n  ]\n\n}\n"],["body","\n\n"],["body","To test the pipeline, click Add documents."],["body","\n"],["body","In the Documents tab, provide a sample document for testing:"],["body","\n\n"],["body","[\n  {\n    \"_source\": {\n      \"message\": \"212.87.37.154 - - [05/May/2099:16:21:15 +0000] \\\"GET /favicon.ico HTTP/1.1\\\" 200 3638 \\\"-\\\" \\\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\\\"\"\n    }\n  }\n]\n"],["body","\n\n"],["body","Click Run the pipeline and verify the pipeline worked as expected."],["body","\n"],["body","If everything looks correct, close the panel, and then click Create pipeline.\n\n"],["body","You’re now ready to index the logs data to a data stream."],["body","\n\n"],["body","\n"],["body","Create an index template with data stream enabled."],["body","\n\n"],["body","PUT _index_template/my-data-stream-template\n{\n  \"index_patterns\": [ \"my-data-stream*\" ],\n  \"data_stream\": { },\n  \"priority\": 500\n}\n"],["body","\n\n"],["body","Index a document with the pipeline you created."],["body","\n\n"],["body","POST my-data-stream/_doc?pipeline=my-pipeline\n{\n  \"message\": \"212.87.37.154 - - [05/May/2099:16:21:15 +0000] \\\"GET /favicon.ico HTTP/1.1\\\" 200 3638 \\\"-\\\" \\\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\\\"\"\n}\n"],["body","\n\n"],["body","To verify, search the data stream to retrieve the document. The following search uses filter_path to return only the document source."],["body","\n\n"],["body","GET my-data-stream/_search?filter_path=hits.hits._source\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/IngestPipeline/4.ExampleEnrichYourDataBasedOnGeolocation.html"],["title","ExampleEnrichYourDataBasedOnGeolocation.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","前言"],["heading","前言"],["body","\n"],["body","enRich Data 本质上是对数据进行处理后吐出新数据、并且依赖第三方数据"],["body","\n"],["headingLink","example-enrich-your-data-based-on-geolocation"],["heading","Example: Enrich your data based on geolocation"],["body","\n"],["body","geo_match enrich policies match enrich data to incoming documents based on a geographic location, using a geo_shape query."],["body","\n"],["body","The following example creates a geo_match enrich policy that adds postal codes to incoming documents based on a set of coordinates. It then adds the geo_match enrich policy to a processor in an ingest pipeline."],["body","\n"],["body","以下示例创建geo_match  enrich 策略，该策略基于一组坐标将邮政编码添加到传入文档中。然后，它将geo_match enrich策略添加到摄取管道中的处理器。"],["body","\n"],["body","Use the create index API to create a source index containing at least one geo_shape field."],["body","\n"],["body","PUT /postal_codes\n{\n  \"mappings\": {\n    \"properties\": {\n      \"location\": {\n        \"type\": \"geo_shape\"\n      },\n      \"postal_code\": {\n        \"type\": \"keyword\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","Use the index API to index enrich data to this source index."],["body","\n"],["body","PUT /postal_codes/_doc/1?refresh=wait_for\n{\n  \"location\": {\n    \"type\": \"envelope\",\n    \"coordinates\": [ [ 13.0, 53.0 ], [ 14.0, 52.0 ] ]\n  },\n  \"postal_code\": \"96598\"\n}\n"],["body","\n"],["body","Use the create enrich policy API to create an enrich policy with the geo_match policy type. This policy must include:"],["body","\n\n"],["body","One or more source indices"],["body","\n"],["body","A match_field, the geo_shape field from the source indices used to match incoming documents"],["body","\n"],["body","Enrich fields from the source indices you’d like to append to incoming documents"],["body","\n\n"],["body","PUT /_enrich/policy/postal_policy\n{\n  \"geo_match\": {\n    \"indices\": \"postal_codes\",\n    \"match_field\": \"location\",\n    \"enrich_fields\": [ \"location\", \"postal_code\" ]\n  }\n}\n"],["body","\n"],["body","Use the execute enrich policy API to create an enrich index for the policy."],["body","\n"],["body","POST /_enrich/policy/postal_policy/_execute\n"],["body","\n"],["body","Use the create or update pipeline API to create an ingest pipeline. In the pipeline, add an enrich processor that includes:"],["body","\n\n"],["body","Your enrich policy."],["body","\n"],["body","The field of incoming documents used to match the geoshape of documents from the enrich index."],["body","\n"],["body","The target_field used to store appended enrich data for incoming documents. This field contains the match_field and enrich_fields specified in your enrich policy."],["body","\n"],["body","The shape_relation, which indicates how the processor matches geoshapes in incoming documents to geoshapes in documents from the enrich index. See Spatial Relations for valid options and more information."],["body","\n\n"],["body","PUT /_ingest/pipeline/postal_lookup\n{\n  \"processors\": [\n    {\n      \"enrich\": {\n        \"description\": \"Add 'geo_data' based on 'geo_location'\",\n        \"policy_name\": \"postal_policy\",\n        \"field\": \"geo_location\",\n        \"target_field\": \"geo_data\",\n        \"shape_relation\": \"INTERSECTS\"\n      }\n    }\n  ]\n}\n"],["body","\n"],["headingLink","example-enrich-your-data-based-on-exact-values"],["heading","Example: Enrich your data based on exact values"],["body","\n"],["body","创建enrich 源索引"],["body","\n"],["body","PUT /users/_doc/1?refresh=wait_for\n{\n  \"email\": \"mardy.brown@asciidocsmith.com\",\n  \"first_name\": \"Mardy\",\n  \"last_name\": \"Brown\",\n  \"city\": \"New Orleans\",\n  \"county\": \"Orleans\",\n  \"state\": \"LA\",\n  \"zip\": 70116,\n  \"web\": \"mardy.asciidocsmith.com\"\n}\n"],["body","\n"],["body","创建enrich policy"],["body","\n"],["body","PUT /_enrich/policy/users-policy\n{\n  \"match\": {\n    \"indices\": \"users\",\n    \"match_field\": \"email\",\n    \"enrich_fields\": [\"first_name\", \"last_name\", \"city\", \"zip\", \"state\"]\n  }\n}\n"],["body","\n"],["body","根据执行策略创建enrich索引"],["body","\n"],["body","POST /_enrich/policy/users-policy/_execute\n"],["body","\n"],["body","创建PIPELINE管道"],["body","\n"],["body","PUT /_ingest/pipeline/user_lookup\n{\n  \"processors\" : [\n    {\n      \"enrich\" : {\n        \"description\": \"Add 'user' data based on 'email'\",\n        \"policy_name\": \"users-policy\",\n        \"field\" : \"email\",\n        \"target_field\": \"user\",\n        \"max_matches\": \"1\"\n      }\n    }\n  ]\n}\n"],["body","\n"],["body","使用管道上传数据"],["body","\n"],["body","PUT /my-index-000001/_doc/my_id?pipeline=user_lookup\n{\n  \"email\": \"mardy.brown@asciidocsmith.com\"\n}\n"],["body","\n"],["headingLink","example-enrich-your-data-by-matching-a-value-to-a-range"],["heading","Example: Enrich your data by matching a value to a range"],["body","\n"],["body","创建enrich 源索引"],["body","\n"],["body","PUT /networks\n{\n  \"mappings\": {\n    \"properties\": {\n      \"range\": { \"type\": \"ip_range\" },\n      \"name\": { \"type\": \"keyword\" },\n      \"department\": { \"type\": \"keyword\" }\n    }\n  }\n}\nPUT /networks/_doc/1?refresh=wait_for\n{\n  \"range\": \"10.100.0.0/16\",\n  \"name\": \"production\",\n  \"department\": \"OPS\"\n}\n"],["body","\n"],["body","创建enrich policy"],["body","\n"],["body","PUT /_enrich/policy/networks-policy\n{\n  \"range\": {\n    \"indices\": \"networks\",\n    \"match_field\": \"range\",\n    \"enrich_fields\": [\"name\", \"department\"]\n  }\n}\n"],["body","\n"],["body","根据执行策略创建enrich索引"],["body","\n"],["body","POST /_enrich/policy/networks-policy/_execute\n"],["body","\n"],["body","创建PIPELINE管道"],["body","\n"],["body","PUT /_ingest/pipeline/networks_lookup\n{\n  \"processors\" : [\n    {\n      \"enrich\" : {\n        \"description\": \"Add 'network' data based on 'ip'\",\n        \"policy_name\": \"networks-policy\",\n        \"field\" : \"ip\",\n        \"target_field\": \"network\",\n        \"max_matches\": \"10\"\n      }\n    }\n  ]\n}\n"],["body","\n"],["body","使用管道上传数据"],["body","\n"],["body","PUT /my-index-000001/_doc/my_id?pipeline=networks_lookup\n{\n  \"ip\": \"10.100.34.1\"\n}\n"],["body","\n"],["headingLink",""],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/IngestPipeline/1.IngestPipelines.html"],["title","IngestPipelines.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","ingest-pipelines"],["heading","Ingest pipelines"],["body","\n"],["body","摄取管道(Ingest pipelines)允许您在索引之前对数据执行常见的转换。"],["body","\n"],["body","例如，您可以使用管道删除字段，从文本中提取值并丰富数据。"],["body","\n"],["body","管道由一系列称为处理器( processors)的可配置任务组成。每个处理器按顺序运行，对传入文档进行特定更改。处理器运行后，Elasticsearch将转换后的文档添加到数据流或索引中。"],["body","\n"],["body","您可以使用Kibana的 “摄取管道” 功能或摄取api创建和管理摄取管道。Elasticsearch将管道存储在集群状态。"],["body","\n"],["headingLink","prerequisites"],["heading","Prerequisites"],["body","\n\n"],["body","具有ingest节点角色的节点处理管道处理。要使用  ingest pipelines，您的群集必须至少有一个具有 ingest pipelines, 角色的节点。对于繁重的摄取负载，我们建议创建专用的摄取节点。"],["body","\n"],["body","如果启用了Elasticsearch安全功能，则必须具有manage_pipeline群集权限才能管理 pipelines。要使用Kibana的摄取管道功能，您还需要群集:  cluster:monitor/nodes/info  群集权限。"],["body","\n"],["body","包括enrich处理器在内的管道需要额外的设置。 See Enrich your data."],["body","\n\n"],["headingLink","create-and-manage-pipelines"],["heading","Create and manage pipelines"],["body","\n"],["body","在Kibana中，打开主菜单，然后Stack Management > Ingest Pipelines。从列表视图中，您可以:"],["body","\n\n"],["body","View a list of your pipelines and drill down into details"],["body","\n"],["body","Edit or clone existing pipelines"],["body","\n"],["body","Delete pipelines"],["body","\n\n"],["body","To create a pipeline, click Create pipeline > New pipeline. For an example tutorial, see Example: Parse logs."],["body","\n"],["body","The New pipeline from CSV option lets you use a CSV to create an ingest pipeline that maps custom data to the Elastic Common Schema (ECS). Mapping your custom data to ECS makes the data easier to search and lets you reuse visualizations from other datasets. To get started, check Map custom data to ECS."],["body","\n"],["body","You can also use the ingest APIs to create and manage pipelines. The following create pipeline API request creates a pipeline containing two set processors followed by a lowercase processor. The processors run sequentially in the order specified."],["body","\n"],["body","PUT _ingest/pipeline/my-pipeline\n{\n  \"description\": \"My optional pipeline description\",\n  \"processors\": [\n    {\n      \"set\": {\n        \"description\": \"My optional processor description\",\n        \"field\": \"my-long-field\",\n        \"value\": 10\n      }\n    },\n    {\n      \"set\": {\n        \"description\": \"Set 'my-boolean-field' to true\",\n        \"field\": \"my-boolean-field\",\n        \"value\": true\n      }\n    },\n    {\n      \"lowercase\": {\n        \"field\": \"my-keyword-field\"\n      }\n    }\n  ]\n}\n"],["body","\n"],["headingLink","manage-pipeline-versions"],["heading","Manage pipeline versions"],["body","\n"],["body","When you create or update a pipeline, you can specify an optional version integer"],["body","\n"],["body","创建或更新管道时，可以指定可选的版本整数。"],["body","\n"],["body","。您可以将此版本号与  if_version parameter 一起使用，以有条件地更新管道。当指定if_version参数时，成功的更新会增加管道的版本"],["body","\n"],["body","PUT _ingest/pipeline/my-pipeline-id\n{\n  \"version\": 1,\n  \"processors\": [ ... ]\n}\n"],["body","\n"],["body","要使用API取消设置版本号，请在不指定版本参数的情况下替换或更新管道。"],["body","\n"],["headingLink","test-a-pipeline"],["heading","Test a pipeline"],["body","\n"],["body","在生产中使用管道之前，我们建议您使用示例文档对其进行测试。在Kibana中创建或编辑管道时，请单击 “添加文档”。在文档页签中，提供示例文档，然后单击运行管道。"],["body","\n"],["body","You can also test pipelines using the simulate pipeline API. You can specify a configured pipeline in the request path. For example, the following request tests my-pipeline."],["body","\n"],["body","POST _ingest/pipeline/my-pipeline/_simulate\n{\n  \"docs\": [\n    {\n      \"_source\": {\n        \"my-keyword-field\": \"FOO\"\n      }\n    },\n    {\n      \"_source\": {\n        \"my-keyword-field\": \"BAR\"\n      }\n    }\n  ]\n}\n"],["body","\n"],["body","Alternatively, you can specify a pipeline and its processors in the request body."],["body","\n"],["body","POST _ingest/pipeline/_simulate\n{\n  \"pipeline\": {\n    \"processors\": [\n      {\n        \"lowercase\": {\n          \"field\": \"my-keyword-field\"\n        }\n      }\n    ]\n  },\n  \"docs\": [\n    {\n      \"_source\": {\n        \"my-keyword-field\": \"FOO\"\n      }\n    },\n    {\n      \"_source\": {\n        \"my-keyword-field\": \"BAR\"\n      }\n    }\n  ]\n}\n"],["body","\n"],["body","The API returns transformed documents:"],["body","\n"],["body","{\n  \"docs\": [\n    {\n      \"doc\": {\n        \"_index\": \"_index\",\n        \"_type\": \"_doc\",\n        \"_id\": \"_id\",\n        \"_source\": {\n          \"my-keyword-field\": \"foo\"\n        },\n        \"_ingest\": {\n          \"timestamp\": \"2099-03-07T11:04:03.000Z\"\n        }\n      }\n    },\n    {\n      \"doc\": {\n        \"_index\": \"_index\",\n        \"_type\": \"_doc\",\n        \"_id\": \"_id\",\n        \"_source\": {\n          \"my-keyword-field\": \"bar\"\n        },\n        \"_ingest\": {\n          \"timestamp\": \"2099-03-07T11:04:04.000Z\"\n        }\n      }\n    }\n  ]\n}\n"],["body","\n"],["headingLink","add-a-pipeline-to-an-indexing-request"],["heading","Add a pipeline to an indexing request"],["body","\n"],["body","Use the pipeline query parameter to apply a pipeline to documents in individual or bulk indexing requests."],["body","\n"],["body","POST my-data-stream/_doc?pipeline=my-pipeline\n{\n  \"@timestamp\": \"2099-03-07T11:04:05.000Z\",\n  \"my-keyword-field\": \"foo\"\n}\n\nPUT my-data-stream/_bulk?pipeline=my-pipeline\n{ \"create\":{ } }\n{ \"@timestamp\": \"2099-03-07T11:04:06.000Z\", \"my-keyword-field\": \"foo\" }\n{ \"create\":{ } }\n{ \"@timestamp\": \"2099-03-07T11:04:07.000Z\", \"my-keyword-field\": \"bar\" }\n"],["body","\n"],["body","You can also use the pipeline parameter with the update by query or reindex APIs."],["body","\n"],["body","POST my-data-stream/_update_by_query?pipeline=my-pipeline\n\nPOST _reindex\n{\n  \"source\": {\n    \"index\": \"my-data-stream\"\n  },\n  \"dest\": {\n    \"index\": \"my-new-data-stream\",\n    \"op_type\": \"create\",\n    \"pipeline\": \"my-pipeline\"\n  }\n}\n"],["body","\n"],["headingLink","set-a-default-pipeline"],["heading","Set a default pipeline"],["body","\n"],["body","Use the index.default_pipeline index setting to set a default pipeline. Elasticsearch applies this pipeline to indexing requests if no pipeline parameter is specified."],["body","\n"],["headingLink","set-a-final-pipeline"],["heading","Set a final pipeline"],["body","\n"],["body","Use the index.final_pipeline index setting to set a final pipeline. Elasticsearch applies this pipeline after the request or default pipeline, even if neither is specified."],["body","\n"],["headingLink","pipelines-for-beats"],["heading","Pipelines for Beats"],["body","\n"],["body","To add an ingest pipeline to an Elastic Beat, specify the pipeline parameter under output.elasticsearch in <BEAT_NAME>.yml. For example, for Filebeat, you’d specify pipeline in filebeat.yml."],["body","\n"],["body","若要在Elastic Beat中添加一个Elastic pipeline，请在 <BEAT_NAME>.yml中指定output.elasticsearch下的pipeline参数。例如，对于Filebeat，您可以在filebeat.yml中指定管道。"],["body","\n"],["body","output.elasticsearch:\n  hosts: [\"localhost:9200\"]\n  pipeline: my-pipeline\n"],["body","\n"],["headingLink","access-source-fields-in-a-processor"],["heading","Access source fields in a processor"],["body","\n"],["body","Processors have read and write access to an incoming document’s source fields. To access a field key in a processor, use its field name. The following set processor accesses my-long-field."],["body","\n"],["body","处理器具有对传入文档的源字段的读写权限。要访问处理器中的 field key，请使用其字段名称。下面的set处理器访问my-long-field。"],["body","\n"],["body","PUT _ingest/pipeline/my-pipeline\n{\n  \"processors\": [\n    {\n      \"set\": {\n        \"field\": \"my-long-field\",\n        \"value\": 10\n      }\n    }\n  ]\n}\n"],["body","\n"],["body","You can also prepend the _source prefix."],["body","\n"],["body","PUT _ingest/pipeline/my-pipeline\n{\n  \"processors\": [\n    {\n      \"set\": {\n        \"field\": \"_source.my-long-field\",\n        \"value\": 10\n      }\n    }\n  ]\n}\n"],["body","\n"],["body","使用点表示法访问对象字段。"],["body","\n"],["body","注意："],["body","\n"],["body","If your document contains flattened objects, use the dot_expander processor to expand them first. Other ingest processors cannot access flattened objects."],["body","\n"],["body","PUT _ingest/pipeline/my-pipeline\n{\n  \"processors\": [\n    {\n      \"dot_expander\": {\n        \"description\": \"Expand 'my-object-field.my-property'\",\n        \"field\": \"my-object-field.my-property\"\n      }\n    },\n    {\n      \"set\": {\n        \"description\": \"Set 'my-object-field.my-property' to 10\",\n        \"field\": \"my-object-field.my-property\",\n        \"value\": 10\n      }\n    }\n  ]\n}\n"],["body","\n"],["body","Several processor parameters support Mustache template snippets."],["body","\n"],["body","支持大胡子模板引擎"],["body","\n"],["body","To access field values in a template snippet, enclose the field name in triple curly brackets:{{{field-name}}}. You can use template snippets to dynamically set field names."],["body","\n"],["headingLink","access-metadata-fields-in-a-processor"],["heading","Access metadata fields in a processor"],["body","\n"],["body","Processors can access the following metadata fields by name:"],["body","\n\n"],["body","_index"],["body","\n"],["body","_id"],["body","\n"],["body","_routing"],["body","\n"],["body","_dynamic_templates"],["body","\n\n"],["body","PUT _ingest/pipeline/my-pipeline\n{\n  \"processors\": [\n    {\n      \"set\": {\n        \"description\": \"Set '_routing' to 'geoip.country_iso_code' value\",\n        \"field\": \"_routing\",\n        \"value\": \"{{{geoip.country_iso_code}}}\"\n      }\n    }\n  ]\n}\n"],["body","\n"],["body","使用大胡子模板片段访问元数据字段值。例如 {{{_routing }}} 检索文档的路由值。"],["body","\n"],["body","PUT _ingest/pipeline/my-pipeline\n{\n  \"processors\": [\n    {\n      \"set\": {\n        \"description\": \"Use geo_point dynamic template for address field\",\n        \"field\": \"_dynamic_templates\",\n        \"value\": {\n          \"address\": \"geo_point\"\n        }\n      }\n    }\n  ]\n}\n"],["body","\n"],["body","The set processor above tells ES to use the dynamic template named geo_point for the field address if this field is not defined in the mapping of the index yet. This processor overrides the dynamic template for the field address if already defined in the bulk request, but has no effect on other dynamic templates defined in the bulk request."],["body","\n"],["body","如果索引的映射中尚未定义此字段，则上面的set处理器会告诉ES将名为geo_point的动态模板用于字段地址。如果已在批量请求中定义，则此处理器将覆盖字段地址的动态模板，但对批量请求中定义的其他动态模板没有影响。"],["body","\n"],["headingLink","handling-pipeline-failures"],["heading","Handling pipeline failures"],["body","\n"],["body","管道的处理器按顺序运行。默认情况下，当这些处理器之一出现故障或遇到错误时，管道处理将停止。\n要忽略处理器故障并运行管道的其余处理器，请将ignore_failure设置为true。"],["body","\n"],["body","PUT _ingest/pipeline/my-pipeline\n{\n  \"processors\": [\n    {\n      \"rename\": {\n        \"description\": \"Rename 'provider' to 'cloud.provider'\",\n        \"field\": \"provider\",\n        \"target_field\": \"cloud.provider\",\n        \"ignore_failure\": true\n      }\n    }\n  ]\n}\n"],["body","\n"],["body","使用on_failure参数指定处理器故障后立即运行的处理器列表。如果指定了on_failure，则即使on_failure配置为空，Elasticsearch也会随后运行管道的其余处理器。"],["body","\n"],["body","PUT _ingest/pipeline/my-pipeline\n{\n  \"processors\": [\n    {\n      \"rename\": {\n        \"description\": \"Rename 'provider' to 'cloud.provider'\",\n        \"field\": \"provider\",\n        \"target_field\": \"cloud.provider\",\n        \"on_failure\": [\n          {\n            \"set\": {\n              \"description\": \"Set 'error.message'\",\n              \"field\": \"error.message\",\n              \"value\": \"Field 'provider' does not exist. Cannot rename to 'cloud.provider'\",\n              \"override\": false,\n              \"on_failure\": [\n                {\n                  \"set\": {\n                    \"description\": \"Set 'error.message.multi'\",\n                    \"field\": \"error.message.multi\",\n                    \"value\": \"Document encountered multiple ingest errors\",\n                    \"override\": true\n                  }\n                }\n              ]\n            }\n          }\n        ]\n      }\n    }\n  ]\n}\n"],["body","\n"],["body","You can also specify on_failure for a pipeline. If a processor without an on_failure value fails, Elasticsearch uses this pipeline-level parameter as a fallback. Elasticsearch will not attempt to run the pipeline’s remaining processors."],["body","\n"],["body","PUT _ingest/pipeline/my-pipeline\n{\n  \"processors\": [ ... ],\n  \"on_failure\": [\n    {\n      \"set\": {\n        \"description\": \"Index document to 'failed-<index>'\",\n        \"field\": \"_index\",\n        \"value\": \"failed-{{{ _index }}}\"\n      }\n    }\n  ]\n}\n"],["body","\n"],["body","Additional information about the pipeline failure may be available in the document metadata fields on_failure_message, on_failure_processor_type, on_failure_processor_tag, and on_failure_pipeline. These fields are accessible only from within an on_failure block."],["body","\n"],["body","以下示例使用元数据字段在文档中包含有关管道故障的信息。"],["body","\n"],["body","PUT _ingest/pipeline/my-pipeline\n{\n  \"processors\": [ ... ],\n  \"on_failure\": [\n    {\n      \"set\": {\n        \"description\": \"Record error information\",\n        \"field\": \"error_information\",\n        \"value\": \"Processor {{ _ingest.on_failure_processor_type }} with tag {{ _ingest.on_failure_processor_tag }} in pipeline {{ _ingest.on_failure_pipeline }} failed with message {{ _ingest.on_failure_message }}\"\n      }\n    }\n  ]\n}\n"],["body","\n"],["headingLink","conditionally-run-a-processor"],["heading","Conditionally run a processor"],["body","\n"],["body","Each processor supports an optional if condition, written as a Painless script. If provided, the processor only runs when the if condition is true."],["body","\n"],["body","PUT _ingest/pipeline/my-pipeline\n{\n  \"processors\": [\n    {\n      \"drop\": {\n        \"description\": \"Drop documents with 'network.name' of 'Guest'\",\n        \"if\": \"ctx?.network?.name == 'Guest'\"\n      }\n    }\n  ]\n}\n"],["body","\n"],["body","If the script.painless.regex.enabled cluster setting is enabled, you can use regular expressions in your if condition scripts. For supported syntax, see Painless regular expressions."],["body","\n"],["body","If possible, avoid using regular expressions. Expensive regular expressions can slow indexing speeds."],["body","\n"],["body","PUT _ingest/pipeline/my-pipeline\n{\n  \"processors\": [\n    {\n      \"set\": {\n        \"description\": \"If 'url.scheme' is 'http', set 'url.insecure' to true\",\n        \"if\": \"ctx.url?.scheme =~ /^http[^s]/\",\n        \"field\": \"url.insecure\",\n        \"value\": true\n      }\n    }\n  ]\n}\n"],["body","\n"],["body","您必须将if条件指定为单行上的有效JSON。但是，您可以使用Kibana控制台的三引号语法来编写和调试更大的脚本。"],["body","\n"],["body","PUT _ingest/pipeline/my-pipeline\n{\n  \"processors\": [\n    {\n      \"drop\": {\n        \"description\": \"Drop documents that don't contain 'prod' tag\",\n        \"if\": \"\"\"\n            Collection tags = ctx.tags;\n            if(tags != null){\n              for (String tag : tags) {\n                if (tag.toLowerCase().contains('prod')) {\n                  return false;\n                }\n              }\n            }\n            return true;\n        \"\"\"\n      }\n    }\n  ]\n}\n"],["body","\n"],["body","You can also specify a stored script as the if condition."],["body","\n"],["body","PUT _scripts/my-prod-tag-script\n{\n  \"script\": {\n    \"lang\": \"painless\",\n    \"source\": \"\"\"\n      Collection tags = ctx.tags;\n      if(tags != null){\n        for (String tag : tags) {\n          if (tag.toLowerCase().contains('prod')) {\n            return false;\n          }\n        }\n      }\n      return true;\n    \"\"\"\n  }\n}\n\nPUT _ingest/pipeline/my-pipeline\n{\n  \"processors\": [\n    {\n      \"drop\": {\n        \"description\": \"Drop documents that don't contain 'prod' tag\",\n        \"if\": { \"id\": \"my-prod-tag-script\" }\n      }\n    }\n  ]\n}\n"],["body","\n"],["body","传入文档通常包含对象字段。如果处理器脚本试图访问父对象不存在的字段"],["body","\n"],["body","如果处理器脚本尝试访问父对象不存在的字段，则Elasticsearch将返回NullPointerException。"],["body","\n"],["body","要避免这些异常，请使用null safe运算符，例如 ?.，并将脚本编写为null safe。"],["body","\n"],["body"," `ctx.network?.name.equalsIgnoreCase('Guest')` is not null safe. `ctx.network?.name` can return null. \n\nRewrite the script as `'Guest'.equalsIgnoreCase(ctx.network?.name)`, which is null safe because `Guest` is always non-null.\n\n"],["body","\n"],["body","PUT _ingest/pipeline/my-pipeline\n{\n  \"processors\": [\n    {\n      \"drop\": {\n        \"description\": \"Drop documents that contain 'network.name' of 'Guest'\",\n        \"if\": \"ctx.network?.name != null && ctx.network.name.contains('Guest')\"\n      }\n    }\n  ]\n}\n"],["body","\n"],["headingLink","conditionally-apply-pipelines"],["heading","Conditionally apply pipelines"],["body","\n"],["body","将if条件与管道处理器结合使用，以根据您的条件将其他管道应用于文档。您可以将此管道用作用于配置多个数据流或索引的索引模板中的默认管道。"],["body","\n"],["body","PUT _ingest/pipeline/one-pipeline-to-rule-them-all\n{\n  \"processors\": [\n    {\n      \"pipeline\": {\n        \"description\": \"If 'service.name' is 'apache_httpd', use 'httpd_pipeline'\",\n        \"if\": \"ctx.service?.name == 'apache_httpd'\",\n        \"name\": \"httpd_pipeline\"\n      }\n    },\n    {\n      \"pipeline\": {\n        \"description\": \"If 'service.name' is 'syslog', use 'syslog_pipeline'\",\n        \"if\": \"ctx.service?.name == 'syslog'\",\n        \"name\": \"syslog_pipeline\"\n      }\n    },\n    {\n      \"fail\": {\n        \"description\": \"If 'service.name' is not 'apache_httpd' or 'syslog', return a failure message\",\n        \"if\": \"ctx.service?.name != 'apache_httpd' && ctx.service?.name != 'syslog'\",\n        \"message\": \"This pipeline requires service.name to be either `syslog` or `apache_httpd`\"\n      }\n    }\n  ]\n}\n"],["body","\n"],["headingLink","get-pipeline-usage-statistics"],["heading","Get pipeline usage statistics"],["body","\n"],["body","Use the node stats API to get global and per-pipeline ingest statistics."],["body","\n"],["body","Use these stats to determine which pipelines run most frequently or spend the most time processing."],["body","\n"],["body","GET _nodes/stats/ingest?filter_path=nodes.*.ingest\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/es堆内存如何分配.html"],["title","es堆内存如何分配.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","什么是堆内存"],["heading","什么是堆内存"],["body","\n"],["body","Java 中的堆是 JVM 所管理的最大的一块内存空间，主要用于存放各种类的实例对象。"],["body","\n"],["body","在 Java 中，堆被划分成两个不同的区域："],["body","\n\n"],["body","新生代 ( Young )、"],["body","\n"],["body","老年代 ( Old )。"],["body","\n\n"],["body","新生代 ( Young ) 又被划分为三个区域："],["body","\n\n"],["body","Eden、"],["body","\n"],["body","From Survivor、"],["body","\n"],["body","To Survivor。"],["body","\n\n"],["body","这样划分的目的是为了使 JVM 能够更好的管理堆内存中的对象，包括内存的分配以及回收。"],["body","\n"],["headingLink","堆内存的作用是什么"],["heading","堆内存的作用是什么？"],["body","\n"],["body","在虚拟机启动时创建。"],["body","\n"],["body","堆内存的唯一目的就是创建对象实例，所有的对象实例和数组都要在堆上分配。"],["body","\n"],["body","堆是由垃圾回收来负责的，因此也叫做“GC堆”，垃圾回收采用分代算法，"],["body","\n"],["body","堆由此分为新生代和老年代。"],["body","\n"],["body","堆的优势是可以动态地分配内存大小，生存期也不必事先告诉编译器，因为它是在运行时动态分配内存的，Java的垃圾收集器会自动收走这些不再使用的数据。"],["body","\n"],["body","但缺点是，由于要在运行时动态分配内存，存取速度较慢。当堆内存因为满了无法扩展时就会抛出java.lang.OutOfMemoryError:Java heap space异常。出现这种情况的解决办法具体参见java调优。"],["body","\n"],["headingLink","堆内存如何配置"],["heading","堆内存如何配置？"],["body","\n"],["body","默认情况下，Elasticsearch JVM使用堆内存最小和最大大小为2 GB（5.X版本以上）。"],["body","\n"],["body","早期版本默认1GB，官网指出：这明显不够。"],["body","\n"],["body","在转移到生产环境时，配置足够容量的堆大小以确保Elasticsearch功能和性能是必要的。"],["body","\n"],["body","Elasticsearch将通过Xms（最小堆大小）和Xmx（最大堆大小）设置来分配jvm.options中指定的整个堆。"],["body","\n"],["body","-Xms2g \n-Xmx2g\n"],["body","\n"],["body","通过环境变量设置。"],["body","\n"],["body","ES_JAVA_OPTS=\"-Xms2g -Xmx2g\" ./bin/elasticsearch \nES_JAVA_OPTS=\"-Xms4000m -Xmx4000m\" ./bin/elasticsearch\n\n"],["body","\n"],["headingLink","堆内存配置建议"],["heading","堆内存配置建议"],["body","\n\n"],["body","将最小堆大小（Xms）和最大堆大小（Xmx）设置为彼此相等。"],["body","\n"],["body","Elasticsearch可用的堆越多，可用于缓存的内存就越多。但请注意，太多的堆内存可能会使您长时间垃圾收集暂停。"],["body","\n"],["body","将Xmx设置为不超过物理内存的50％，以确保有足够的物理内存留给内核文件系统缓存。"],["body","\n\n"],["body","- 不要将Xmx设置为JVM超过32GB。"],["body","\n"],["body","宿主机内存大小的一半和31GB，取最小值。\n"],["body","\n"],["headingLink","堆内存为什么不能超过物理机内存的一半"],["heading","堆内存为什么不能超过物理机内存的一半？"],["body","\n"],["headingLink","堆对于elasticsearch绝对重要"],["heading","堆对于Elasticsearch绝对重要。"],["body","\n"],["body","它被许多内存数据结构用来提供快速操作。但还有另外一个非常重要的内存使用者：Lucene。"],["body","\n\n"],["body","\n"],["body","Lucene旨在利用底层操作系统来缓存内存中的数据结构"],["body","\n"],["body","\n"],["body","\n"],["body","Lucene段(segment)存储在单个文件中。因为段是一成不变的，所以这些文件永远不会改变"],["body","\n"],["body","\n"],["body","\n"],["body","这使得它们非常容易缓存，并且底层操作系统将愉快地将热段（hot segments）保留在内存中以便更快地访问"],["body","\n"],["body","\n"],["body","\n"],["body","这些段包括倒排索引（用于全文搜索）和dov values（用于聚合）。"],["body","\n"],["body","\n\n"],["headingLink","lucene与操作系统缓存"],["heading","Lucene与操作系统缓存"],["body","\n"],["body","Lucene的性能依赖于与操作系统的这种交互。但是如果你把所有可用的内存都给了Elasticsearch的堆，那么Lucene就不会有任何剩余的内存。这会严重影响性能。"],["body","\n"],["headingLink","建议"],["heading","建议"],["body","\n"],["body","标准建议是将可用内存的50％提供给Elasticsearch堆，而将其他50％空闲。它不会被闲置; Lucene会高兴地吞噬掉剩下的东西。"],["body","\n"],["headingLink","无聚合场景"],["heading","无聚合场景"],["body","\n"],["body","如果您不在字符串字段上做聚合操作（例如，您不需要fielddata），则可以考虑进一步降低堆。堆越小，您可以从Elasticsearch（更快的GC）和Lucene（更多内存缓存）中获得更好的性能。"],["body","\n"],["headingLink","堆内存为什么不能超过32gb"],["heading","堆内存为什么不能超过32GB？"],["body","\n"],["body","在Java中，所有对象都分配在堆上并由指针引用。普通的对象指针（OOP）指向这些对象，传统上它们是CPU本地字的大小：32位或64位，取决于处理器。"],["body","\n\n"],["body","\n"],["body","对于32位系统，这意味着最大堆大小为4 GB"],["body","\n"],["body","\n"],["body","\n"],["body","对于64位系统，堆大小可能会变得更大，但是64位指针的开销意味着仅仅因为指针较大而存在更多的浪费空间"],["body","\n"],["body","\n"],["body","\n"],["body","并且比浪费的空间更糟糕，当在主存储器和各种缓存（LLC，L1等等）之间移动值时，较大的指针消耗更多的带宽。"],["body","\n"],["body","\n\n"],["body","Java使用称为压缩oops的技巧来解决这个问题。而不是指向内存中的确切字节位置，指针引用对象偏移量。这意味着一个32位指针可以引用40亿个对象，而不是40亿个字节。最终，这意味着堆可以增长到约32 GB的物理尺寸，同时仍然使用32位指针。"],["body","\n"],["body","一旦你穿越了这个神奇的〜32 GB的边界，指针就会切换回普通的对象指针。每个指针的大小增加，使用更多的CPU内存带宽，并且实际上会丢失内存。实际上，在使用压缩oops获得32 GB以下堆的相同有效内存之前，需要大约40-50 GB的分配堆。"],["body","\n"],["body","以上小结为：即使你有足够的内存空间，尽量避免跨越32GB的堆边界。否则会导致浪费了内存，降低了CPU的性能，并使GC在大堆中挣扎。"],["body","\n"],["headingLink","最新认知"],["heading","最新认知"],["body","\n"],["body","事实上，给ES分配的内存有一个魔法上限值26GB，"],["body","\n"],["body","这样可以确保启用zero based Compressed Oops，这样性能才是最佳的。"],["body","\n"],["body","参考:https://elasticsearch.cn/question/3995\nhttps://www.elastic.co/blog/a-heap-of-trouble"],["body","\n"],["body","\n"],["body","参考链接"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/分布式检索.html"],["title","分布式检索.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","执行分布式检索"],["heading","执行分布式检索"],["body","\n"],["body","在继续之前，我们将绕道讨论一下在分布式环境中搜索是怎么执行的。"],["body","\n"],["body","一个 CRUD 操作只对单个文档进行处理，文档的唯一性由 _index, _type, 和 routing values （通常默认是该文档的 _id ）的组合来确定。 这表示我们确切的知道集群中哪个分片含有此文档。"],["body","\n"],["body","搜索需要一种更加复杂的执行模型因为我们不知道查询会命中哪些文档:这些文档有可能在集群的任何分片上"],["body","\n"],["body","但是找到所有的匹配文档仅仅完成事情的一半"],["body","\n"],["body","在 search 接口返回一个 page 结果之前,多分片中的结果必须组合成单个排序列表"],["body","\n"],["body","为此，搜索被执行成一个两阶段过程，我们称之为 query then fetch 。"],["body","\n"],["headingLink","查询阶段"],["heading","查询阶段"],["body","\n"],["body","在初始 查询阶段 时， 查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的 优先队列。"],["body","\n"],["body","一个 优先队列 仅仅是一个存有 top-n 匹配文档的有序列表。优先队列的大小取决于分页参数 from 和 size 。例如，如下搜索请求将需要足够大的优先队列来放入100条文档。"],["body","\n"],["body","查询阶段包含以下三个步骤:"],["body","\n\n"],["body","客户端发送一个 搜索请求到 任意某个节点（即协调节点），协调节点 会创建一个大小为 from + size 的空优先队列。"],["body","\n"],["body","协调节点 将查询请求转发到索引的每个主分片或副本分片中。每个分片在本地执行查询并添加结果到大小为 from + size 的本地有序优先队列中。"],["body","\n"],["body","每个分片返回各自优先队列中所有文档的 ID 和排序值给协调节点，，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。"],["body","\n\n"],["headingLink","取回阶段"],["heading","取回阶段"],["body","\n"],["body","查询阶段标识哪些文档满足搜索请求，但是我们仍然需要取回这些文档。这是取回阶段的任务"],["body","\n\n"],["body","协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求"],["body","\n"],["body","每个分片加载并 丰富 文档，如果有需要的话，接着返回文档给协调节点。"],["body","\n"],["body","一旦所有的文档都被取回了，协调节点返回结果给客户端。"],["body","\n\n"],["body","深分页（Deep Pagination）"],["body","\n"],["body","先查后取的过程支持用 from 和 size 参数分页，但是这是 有限制的 。 要记住需要传递信息给协调节点的每个分片必须先创建一个 from + size 长度的队列，协调节点需要根据 number_of_shards * (from + size) 排序文档，来找到被包含在 size 里的文档。"],["body","\n"],["body","取决于你的文档的大小，分片的数量和你使用的硬件，给 10,000 到 50,000 的结果文档深分页（ 1,000 到 5,000 页）是完全可行的。但是使用足够大的 from 值，排序过程可能会变得非常沉重，使用大量的CPU、内存和带宽。因为这个原因，我们强烈建议你不要使用深分页。"],["body","\n"],["body","实际上， “深分页” 很少符合人的行为。当2到3页过去以后，人会停止翻页，并且改变搜索标准。会不知疲倦地一页一页的获取网页直到你的服务崩溃的罪魁祸首一般是机器人或者web spider。"],["body","\n"],["body","如果你 确实 需要从你的集群取回大量的文档，你可以通过用 scroll 查询禁用排序使这个取回行为更有效率"],["body","\n"],["headingLink","搜索选项"],["heading","搜索选项"],["body","\n"],["body","有几个 查询参数可以影响搜索过程。"],["body","\n"],["headingLink","偏好"],["heading","偏好"],["body","\n"],["body","偏好这个参数 preference 允许 用来控制由哪些分片或节点来处理搜索请求。 它接受像 _primary, _primary_first, _local, _only_node:xyz, _prefer_node:xyz, 和 _shards:2,3 这样的值, 这些值在 search preference 文档页面被详细解释。"],["body","\n"],["body","但是最有用的值是某些随机字符串，它可以避免 bouncing results 问题。"],["body","\n"],["body","Bouncing Results"],["body","\n"],["body","\n"],["body","每次请求有不同的排序"],["body","\n"],["body","\n"],["body","想象一下有两个文档有同样值的时间戳字段，搜索结果用 timestamp 字段来排序。 由于搜索请求是在所有有效的分片副本间轮询的，那就有可能发生主分片处理请求时，这两个文档是一种顺序， 而副本分片处理请求时又是另一种顺序。"],["body","\n"],["body","这就是所谓的 bouncing results 问题: 每次用户刷新页面，搜索结果表现是不同的顺序。 让同一个用户始终使用同一个分片，这样可以避免这种问题， 可以设置 preference 参数为一个特定的任意值比如用户会话ID来解决。"],["body","\n"],["headingLink","超时问题"],["heading","超时问题"],["body","\n"],["body","通常分片处理完它所有的数据后再把结果返回给协同节点，协同节点把收到的所有结果合并为最终结果。"],["body","\n"],["body","这意味着花费的时间是最慢分片的处理时间加结果合并的时间。如果有一个节点有问题，就会导致所有的响应缓慢。"],["body","\n"],["body","参数 timeout 告诉 分片允许处理数据的最大时间。如果没有足够的时间处理所有数据，这个分片的结果可以是部分的，甚至是空数据。"],["body","\n"],["body","搜索的返回结果会用属性 timed_out 标明分片是否返回的是部分结果："],["body","\n"],["headingLink","路由"],["heading","路由"],["body","\n"],["body","在 路由一个文档到一个分片中 中, 我们解释过如何定制参数 routing ，它能够在索引时提供来确保相关的文档，比如属于某个用户的文档被存储在某个分片上。 在搜索的时候，不用搜索索引的所有分片，而是通过指定几个 routing 值来限定只搜索几个相关的分片："],["body","\n"],["body","GET /_search?routing=user_1,user2\n"],["body","\n"],["headingLink","搜索类型"],["heading","搜索类型"],["body","\n"],["body","缺省的搜索类型是 query_then_fetch 。 在某些情况下，你可能想明确设置 search_type 为 dfs_query_then_fetch 来改善相关性精确度："],["body","\n"],["body","GET /_search?search_type=dfs_query_then_fetch\n"],["body","\n"],["body","搜索类型 dfs_query_then_fetch 有预查询阶段，这个阶段可以从所有相关分片获取词频来计算全局词频。 我们在 被破坏的相关度！ 会再讨论它。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/索引分片分配/5.索引级别数据层分配过滤.html"],["title","索引级别数据层分配过滤.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","index-level-data-tier-allocation-filtering"],["heading","Index-level data tier allocation filtering"],["body","\n"],["body","You can use index-level allocation settings to control which data tier the index is allocated to. The data tier allocator is a shard allocation filter that uses two built-in node attributes: _tier and _tier_preference."],["body","\n"],["body","These tier attributes are set using the data node roles:"],["body","\n\n"],["body","data_content"],["body","\n"],["body","data_hot"],["body","\n"],["body","data_warm"],["body","\n"],["body","data_cold"],["body","\n"],["body","data_frozen"],["body","\n\n"],["body","The data role is not a valid data tier and cannot be used for data tier filtering. The frozen tier stores partially mounted indices exclusively."],["body","\n"],["body","index.routing.allocation.include._tier_preference"],["body","\n"],["body","Assign the index to the first tier in the list that has an available node. This prevents indices from remaining unallocated if no nodes are available in the preferred tier. For example, if you set index.routing.allocation.include._tier_preference to data_warm,data_hot, the index is allocated to the warm tier if there are nodes with the data_warm role. If there are no nodes in the warm tier, but there are nodes with the data_hot role, the index is allocated to the hot tier."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/索引分片分配/README.html"],["title","索引分片分配 - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","index-shard-allocation"],["heading","Index Shard Allocation"],["body","\n"],["body","该模块提供了  按索引设置来控制 分片分配给节点:"],["body","\n\n"],["body","Shard allocation filtering: 控制哪些分片分配给哪些节点。"],["body","\n"],["body","Delayed allocation: 当节点离开之后。分片重分配工作会延迟"],["body","\n"],["body","Total shards per node: 每个节点来自相同索引的分片数量的硬限制。"],["body","\n"],["body","Data tier allocation: Controls the allocation of indices to data tiers."],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/索引分片分配/7.cluster-allocation-explain.html"],["title","cluster-allocation-explain.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","cluster-allocation-explain-api"],["heading","Cluster allocation explain API"],["body","\n"],["body","提供分片当前分配的解释。"],["body","\n"],["body","GET _cluster/allocation/explain\n{\n  \"index\": \"my-index-000001\",\n  \"shard\": 0,\n  \"primary\": false,\n  \"current_node\": \"my-node\"\n}\n"],["body","\n"],["headingLink","description"],["heading","Description"],["body","\n"],["body","the cluster allocation explain API  的目的是为集群中的分片分配提供解释。"],["body","\n\n"],["body","\n"],["body","对于未分配的分片，explain API提供了为什么未分配分片的解释。"],["body","\n"],["body","\n"],["body","\n"],["body","对于分配的分片，explain API提供了一个解释，说明为什么分片保留在其当前节点上并且没有移动到或重新平衡到另一个节点。"],["body","\n"],["body","\n"],["body","\n"],["body","当您尝试诊断分片未分配的原因或为什么分片继续保留在其当前节点上时，此API可能非常有用。"],["body","\n"],["body","\n\n"],["headingLink","query-parameters"],["heading","Query parameters"],["body","\n\n"],["body","\n"],["body","include_disk_info"],["body","\n"],["body","(Optional, Boolean) If true, returns information about disk usage and shard sizes. Defaults to false."],["body","\n"],["body","\n"],["body","\n"],["body","include_yes_decisions"],["body","\n"],["body","(Optional, Boolean) If true, returns YES decisions in explanation. Defaults to false."],["body","\n"],["body","\n\n"],["headingLink","request-body"],["heading","Request body"],["body","\n\n"],["body","\n"],["body","current_node"],["body","\n"],["body","(Optional, string) Specifies the node ID or the name of the node to only explain a shard that is currently located on the specified node. :  申明  位于指定节点的 分片"],["body","\n"],["body","\n"],["body","\n"],["body","index"],["body","\n"],["body","(Optional, string) Specifies the name of the index that you would like an explanation for. 申明  位于指定索引的 分片"],["body","\n"],["body","\n"],["body","\n"],["body","primary"],["body","\n"],["body","(Optional, Boolean) If true, returns explanation for the primary shard for the given shard ID."],["body","\n"],["body","​\t对于给定分片ID 返回 该分片ID的 主分片"],["body","\n"],["body","\n"],["body","\n"],["body","shard"],["body","\n"],["body","(Optional, integer) Specifies the ID of the shard that you would like an explanation for."],["body","\n"],["body","\n\n"],["body","​\t\t\t指定分片ID"],["body","\n"],["headingLink","实例"],["heading","实例"],["body","\n"],["body","查看索引信息"],["body","\n"],["body","GET _cat/indices/person2?v\n"],["body","\n"],["body","[\n    {\n        \"health\": \"green\",\n        \"status\": \"open\",\n        \"index\": \"person2\",\n        \"uuid\": \"_M8Ya5LrSYKKe8Uqh4Tgvw\",\n        \"pri\": \"2\",\n        \"rep\": \"1\",\n        \"docs.count\": \"2\",\n        \"docs.deleted\": \"0\",\n        \"store.size\": \"15.6kb\",\n        \"pri.store.size\": \"7.8kb\"\n    }\n]\n"],["body","\n"],["body","查看索引分片信息"],["body","\n"],["body","GET _cat/shards/person2?v\n"],["body","\n"],["body","[\n    {\n        \"index\": \"person2\",\n        \"shard\": \"1\",\n        \"prirep\": \"r\",\n        \"state\": \"STARTED\",\n        \"docs\": \"1\",\n        \"store\": \"3.9kb\",\n        \"ip\": \"192.168.64.6\",\n        \"node\": \"node-1\"\n    },\n    {\n        \"index\": \"person2\",\n        \"shard\": \"1\",\n        \"prirep\": \"p\",\n        \"state\": \"STARTED\",\n        \"docs\": \"1\",\n        \"store\": \"3.9kb\",\n        \"ip\": \"192.168.64.12\",\n        \"node\": \"node-2\"\n    },\n    {\n        \"index\": \"person2\",\n        \"shard\": \"0\",\n        \"prirep\": \"p\",\n        \"state\": \"STARTED\",\n        \"docs\": \"1\",\n        \"store\": \"3.9kb\",\n        \"ip\": \"192.168.64.6\",\n        \"node\": \"node-1\"\n    },\n    {\n        \"index\": \"person2\",\n        \"shard\": \"0\",\n        \"prirep\": \"r\",\n        \"state\": \"STARTED\",\n        \"docs\": \"1\",\n        \"store\": \"3.9kb\",\n        \"ip\": \"192.168.64.12\",\n        \"node\": \"node-2\"\n    }\n]\n"],["body","\n"],["body","查看某一分片的分配详细信息"],["body","\n"],["body","GET _cluster/allocation/explain\n{\n  \"index\": \"my-index-000001\",\n  \"shard\": 0,\n  \"primary\": false,\n  \"current_node\": \"my-node\"\n}\n"],["body","\n"],["body","{\n  \"index\" : \"my-index-000001\",\n  \"shard\" : 0,\n  \"primary\" : true,\n  \"current_state\" : \"unassigned\",//分片分配状态    \n  \"unassigned_info\" : { //分片未分配的 原始原因\n    \"reason\" : \"INDEX_CREATED\",                   \n    \"at\" : \"2017-01-04T18:08:16.600Z\",\n    \"last_allocation_status\" : \"no\"\n  },\n  \"can_allocate\" : \"no\", //是否可分配       \n  \"allocate_explanation\" : \"cannot allocate because allocation is not permitted to any of the nodes\", //分配解释\n  \"node_allocation_decisions\" : [ //决定分片分配到节点的解释\n    {\n      \"node_id\" : \"8qt2rY-pT6KNZB3-hGfLnw\",\n      \"node_name\" : \"node-0\",\n      \"transport_address\" : \"127.0.0.1:9401\",\n      \"node_attributes\" : {},\n      \"node_decision\" : \"no\",                     \n      \"weight_ranking\" : 1,\n      \"deciders\" : [\n        {\n          \"decider\" : \"filter\",    //导致分片没有分配到节点的 员原因               \n          \"decision\" : \"NO\",\n          \"explanation\" : \"node does not match index setting [index.routing.allocation.include] filters [_name:\\\"nonexistent_node\\\"]\"  \n        }\n      ]\n    }\n  ]\n}\n"],["body","\n"],["headingLink","unassigned-primary-shard"],["heading","unassigned primary shard"],["body","\n"],["body","先前已经分配的分片，但是现在未分配的主分片的解释。"],["body","\n"],["body","{\n  \"index\" : \"my-index-000001\",\n  \"shard\" : 0,\n  \"primary\" : true,\n  \"current_state\" : \"unassigned\",\n  \"unassigned_info\" : {\n    \"reason\" : \"NODE_LEFT\",\n    \"at\" : \"2017-01-04T18:03:28.464Z\",\n    \"details\" : \"node_left[OIWe8UhhThCK0V5XfmdrmQ]\",\n    \"last_allocation_status\" : \"no_valid_shard_copy\"\n  },\n  \"can_allocate\" : \"no_valid_shard_copy\",\n  \"allocate_explanation\" : \"cannot allocate because a previous copy of the primary shard existed but can no longer be found on the nodes in the cluster\"\n}\n"],["body","\n"],["headingLink","unassigned-replica-shard"],["heading","Unassigned replica shard"],["body","\n"],["body","由于延迟分配导致的副本分片未分配 https://www.elastic.co/guide/en/elasticsearch/reference/7.13/delayed-allocation.html"],["body","\n"],["body","{\n  \"index\" : \"my-index-000001\",\n  \"shard\" : 0,\n  \"primary\" : false,\n    //未分配\n  \"current_state\" : \"unassigned\",\n  \"unassigned_info\" : {\n    \"reason\" : \"NODE_LEFT\",\n    \"at\" : \"2017-01-04T18:53:59.498Z\",\n    \"details\" : \"node_left[G92ZwuuaRY-9n8_tc-IzEg]\",\n    \"last_allocation_status\" : \"no_attempt\"\n  },\n    //延迟分配\n  \"can_allocate\" : \"allocation_delayed\",\n    //等待过期节点加入集群\n  \"allocate_explanation\" : \"cannot allocate because the cluster is still waiting 59.8s for the departed node holding a replica to rejoin, despite being allowed to allocate the shard to at least one other node\",\n    //默认1minutes\n  \"configured_delay\" : \"1m\",                      \n  \"configured_delay_in_millis\" : 60000,\n  \"remaining_delay\" : \"59.8s\",                    \n  \"remaining_delay_in_millis\" : 59824,\n  \"node_allocation_decisions\" : [\n    {\n      \"node_id\" : \"pmnHu_ooQWCPEFobZGbpWw\",\n      \"node_name\" : \"node_t2\",\n      \"transport_address\" : \"127.0.0.1:9402\",\n      \"node_decision\" : \"yes\"\n    },\n    {\n      \"node_id\" : \"3sULLVJrRneSg0EfBB-2Ew\",\n      \"node_name\" : \"node_t0\",\n      \"transport_address\" : \"127.0.0.1:9400\",\n      \"node_decision\" : \"no\",\n      \"store\" : {                                 \n        \"matching_size\" : \"4.2kb\",\n        \"matching_size_in_bytes\" : 4325\n      },\n      \"deciders\" : [\n        {\n          \"decider\" : \"same_shard\",\n          \"decision\" : \"NO\",\n          \"explanation\" : \"a copy of this shard is already allocated to this node [[my-index-000001][0], node[3sULLVJrRneSg0EfBB-2Ew], [P], s[STARTED], a[id=eV9P8BN1QPqRc3B4PLx6cg]]\"\n        }\n      ]\n    }\n  ]\n}\n"],["body","\n"],["headingLink","assigned-shard"],["heading","Assigned shard"],["body","\n"],["body","以下响应展示了：这个分片不能在待在 当前所处于的节点了，必须重新分片"],["body","\n"],["body","{\n  \"index\" : \"my-index-000001\",\n  \"shard\" : 0,\n  \"primary\" : true,\n  \"current_state\" : \"started\",\n  \"current_node\" : {\n    \"id\" : \"8lWJeJ7tSoui0bxrwuNhTA\",\n    \"name\" : \"node_t1\",\n    \"transport_address\" : \"127.0.0.1:9401\"\n  },\n    //是否能待在当前节点\n  \"can_remain_on_current_node\" : \"no\", \n    //作出决定的决定器\n  \"can_remain_decisions\" : [                      \n    {\n      \"decider\" : \"filter\",\n      \"decision\" : \"NO\",\n      \"explanation\" : \"node does not match index setting [index.routing.allocation.include] filters [_name:\\\"nonexistent_node\\\"]\"\n    }\n  ],\n    //是否能移动到其他节点\n  \"can_move_to_other_node\" : \"no\",                \n    // 不能移动的解释\n  \"move_explanation\" : \"cannot move shard to another node, even though it is not allowed to remain on its current node\",\n  \"node_allocation_decisions\" : [\n    {\n      \"node_id\" : \"_P8olZS8Twax9u6ioN-GGA\",\n      \"node_name\" : \"node_t0\",\n      \"transport_address\" : \"127.0.0.1:9400\",\n      \"node_decision\" : \"no\",\n      \"weight_ranking\" : 1,\n      \"deciders\" : [\n        {\n          \"decider\" : \"filter\",\n          \"decision\" : \"NO\",\n          \"explanation\" : \"node does not match index setting [index.routing.allocation.include] filters [_name:\\\"nonexistent_node\\\"]\"\n        }\n      ]\n    }\n  ]\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/索引分片分配/3.索引恢复优先级.html"],["title","索引恢复优先级.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","index-recovery-prioritization"],["heading","Index recovery prioritization"],["body","\n"],["body","尽可能按优先级顺序恢复未分配的分片。"],["body","\n"],["body","索引按优先级排序如下:"],["body","\n\n"],["body","the optional index.priority setting (higher before lower)"],["body","\n"],["body","the index creation date (higher before lower)"],["body","\n"],["body","the index name (higher before lower)"],["body","\n\n"],["body","This means that, by default, newer indices will be recovered before older indices."],["body","\n"],["body","Use the per-index dynamically updatable index.priority setting to customise the index prioritization order. For instance:"],["body","\n"],["body","PUT index_1\n\nPUT index_2\n\nPUT index_3\n{\n  \"settings\": {\n    \"index.priority\": 10\n  }\n}\n\nPUT index_4\n{\n  \"settings\": {\n    \"index.priority\": 5\n  }\n}\n"],["body","\n"],["body","PUT index_1\n\nPUT index_2\n\nPUT index_3\n{\n  \"settings\": {\n    \"index.priority\": 10\n  }\n}\n\nPUT index_4\n{\n  \"settings\": {\n    \"index.priority\": 5\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/索引分片分配/4.每个节点的总分片.html"],["title","每个节点的总分片.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","total-shards-per-node"],["heading","Total shards per node"],["body","\n"],["body","集群层面的索引分配 尽可能的 将 单个索引的 分片分配到 更多的节点"],["body","\n"],["body","但是，根据您拥有多少分片 和索引以及它们的大小，可能并不总是能够均匀地分布分片"],["body","\n"],["body","以下动态设置允许您从每个节点允许的单个索引中指定分片总数的硬限制:"],["body","\n"],["body","index.routing.allocation.total_shards_per_node"],["body","\n"],["body","The maximum number of shards (replicas and primaries) that will be allocated to a single node. Defaults to unbounded."],["body","\n"],["body","您还可以限制节点可以具有的分片数量，而与索引无关:"],["body","\n"],["body","cluster.routing.allocation.total_shards_per_node"],["body","\n"],["body","(Dynamic) Maximum number of primary and replica shards allocated to each node. Defaults to -1 (unlimited)."],["body","\n"],["body","Elasticsearch checks this setting during shard allocation. For example, a cluster has a cluster.routing.allocation.total_shards_per_node setting of 100 and three nodes with the following shard allocations:"],["body","\n\n"],["body","Node A: 100 shards"],["body","\n"],["body","Node B: 98 shards"],["body","\n"],["body","Node C: 1 shard"],["body","\n\n"],["body","If node C fails, Elasticsearch reallocates its shard to node B. Reallocating the shard to node A would exceed node A’s shard limit."],["body","\n"],["body","These settings impose a hard limit which can result in some shards not being allocated.\n\nUse with caution.\n\n"],["body","\n"],["headingLink",""],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/索引分片分配/2.节点离开延迟分配.html"],["title","节点离开延迟分配.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","delaying-allocation-when-a-node-leaves"],["heading","Delaying allocation when a node leaves"],["body","\n"],["body","当节点出于任何原因 (有意或其他原因) 离开群集时，主节点的反应是:"],["body","\n\n"],["body","\n"],["body","将副本分片提升到主，以替换节点上的任何主分片"],["body","\n"],["body","\n"],["body","\n"],["body","分配副本分片以替换丢失的副本 (假设有足够的节点)。"],["body","\n"],["body","\n"],["body","\n"],["body","在剩余节点上均匀地重新平衡分片。"],["body","\n"],["body","\n\n"],["body","这些操作旨在通过确保尽快完全复制每个分片来保护群集免受数据丢失。"],["body","\n"],["body","尽管我们限制了 并发 recoveries at the node level and at the cluster level,"],["body","\n"],["body","这种 “shard-shuffle” 仍然可以给集群带来很多额外的负载，如果丢失的节点可能很快返回，这可能是不必要的。想象一下这种情况:"],["body","\n\n"],["body","节点5离开集群"],["body","\n"],["body","对于节点5上的每个主节点，主节点将副本分片提升为主副本。"],["body","\n"],["body","主服务器将新副本分配给集群中的其他节点。"],["body","\n"],["body","每个新副本都会在网络上制作主分片的完整副本。"],["body","\n"],["body","更多的分片被移动到不同的节点，以重新平衡集群。"],["body","\n"],["body","节点5在几分钟后返回。"],["body","\n"],["body","主节点通过将分片分配给节点5来重新平衡集群。"],["body","\n\n"],["body","延迟等待"],["body","\n\n"],["body","\n"],["body","如果主机只是等待了几分钟，那么丢失的分片可能会以最小的网络流量重新分配给节点5。"],["body","\n"],["body","\n"],["body","\n"],["body","对于空闲分片(分片未接收索引请求)来说，这个过程会更快它们是自动的 sync-flushed](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-synced-flush-api.html)."],["body","\n"],["body","\n"],["body","\n"],["body","由于节点已离开而未分配的副本分片的 重分配可以使用 index.unassigned.node_left.delayed_timeout  动态设置 进行延迟，默认为1m。"],["body","\n"],["body","\n"],["body","\n"],["body","可以在实时索引 (或所有索引) 上更新此设置:"],["body","\n"],["body","\n\n"],["body","PUT _all/_settings\n{\n  \"settings\": {\n    \"index.unassigned.node_left.delayed_timeout\": \"5m\"\n  }\n}\n"],["body","\n"],["body","启用延迟分配后，上述方案将更改为如下所示:"],["body","\n\n"],["body","节点5离开集群"],["body","\n"],["body","对于节点5上的每个主节点，主节点将副本分片提升为主副本。"],["body","\n"],["body","主节点 记录一条消息，说明未分配的分片的分配已延迟，以及延迟了多长时间。"],["body","\n"],["body","群集保持黄色，因为存在未分配的副本分片。"],["body","\n"],["body","节点5在超时到期前几分钟后返回。"],["body","\n"],["body","丢失的副本被重新分配给节点5 (同步刷新的分片几乎立即恢复)。"],["body","\n\n"],["body","注意"],["body","\n\n"],["body","\n"],["body","此设置不会影响副本提升至主分片，也不会有影响之前未分配的 副本的分配"],["body","\n"],["body","\n"],["body","\n"],["body","特别是，延迟分配在完全集群重新启动后不会生效。"],["body","\n"],["body","\n"],["body","\n"],["body","同样，在主故障转移情况下，会忘记经过的延迟时间 (即重置为完整的初始延迟)。"],["body","\n"],["body","\n\n"],["headingLink","cancellation-of-shard-relocation"],["heading","Cancellation of shard relocation"],["body","\n\n"],["body","如果延迟分配超时，则主节点将丢失的分片分配给将开始恢复的另一个节点。"],["body","\n"],["body","如果丢失的节点重新加入集群，并且其分片仍具有与主节点相同的sync-id，则将取消分片重定位，并将同步的分片用于恢复。"],["body","\n"],["body","因此，默认超时设置为仅一分钟: 即使开始分片重定位，取消副本恢复以支持同步的分片也很便宜。"],["body","\n\n"],["headingLink","monitoring-delayed-unassigned-shards"],["heading","Monitoring delayed unassigned shards"],["body","\n"],["body","The number of shards whose allocation has been delayed by this timeout setting can be viewed with the cluster health API:"],["body","\n"],["body","GET _cluster/health \n"],["body","\n"],["body","This request will return a delayed_unassigned_shards value."],["body","\n"],["headingLink","removing-a-node-permanently"],["heading","Removing a node permanently"],["body","\n"],["body","If a node is not going to return and you would like Elasticsearch to allocate the missing shards immediately, just update the timeout to zero:"],["body","\n"],["body","如果节点不返回，并且您希望Elasticsearch立即分配丢失的分片，则只需将超时更新为零:"],["body","\n"],["body","PUT _all/_settings\n{\n  \"settings\": {\n    \"index.unassigned.node_left.delayed_timeout\": \"0\"\n  }\n}\n"],["body","\n"],["body","一旦丢失的分片开始恢复，您可以重置超时。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/索引分片分配/6.手动分配分片clusterRerouteAPI.html"],["title","手动分配分片clusterRerouteAPI.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","cluster-reroute-api"],["heading","Cluster reroute API"],["body","\n"],["body","更改集群中分片的分配。"],["body","\n"],["headingLink","request"],["heading","Request"],["body","\n"],["body","POST /_cluster/reroute\n"],["body","\n"],["headingLink","description"],["heading","Description"],["body","\n"],["body","重新路由命令允许手动更改群集中单个分片的分配。"],["body","\n\n"],["body","\n"],["body","例如，可以将分片从一个节点显式地移动到另一个节点"],["body","\n"],["body","\n"],["body","\n"],["body","可以取消分配，"],["body","\n"],["body","\n"],["body","\n"],["body","并且可以将未分配的分片显式地分配给特定节点。"],["body","\n"],["body","\n\n"],["body","重要的是，在处理任何重新路由命令之后，Elasticsearch将正常执行重新平衡 (尊重诸如cluster.routing.rebalance.enable等设置的值)，以保持平衡状态。"],["body","\n"],["body","例如，如果请求的分配包括将分片从node1移动到node2，则这可能导致分片从node2移回node1以使事情变得均匀。"],["body","\n"],["body","The cluster can be set to disable allocations using the cluster.routing.allocation.enable setting."],["body","\n"],["body","If allocations are disabled then the only allocations that will be performed are explicit ones given using the reroute command, and consequent allocations due to rebalancing."],["body","\n"],["body","dry_run"],["body","\n"],["body","It is possible to run reroute commands in \"dry run\" mode by using the ?dry_run URI query parameter, or by passing \"dry_run\": true in the request body. This will calculate the result of applying the commands to the current cluster state, and return the resulting cluster state after the commands (and re-balancing) has been applied, but will not actually perform the requested changes."],["body","\n"],["body","explain"],["body","\n"],["body","If the ?explain URI query parameter is included then a detailed explanation of why the commands could or could not be executed is included in the response."],["body","\n"],["body","集群会 尝试分配分片 ，并且重试 index.allocation.max_retries"],["body","\n"],["body","This scenario can be caused by structural problems such as having an analyzer which refers to a stopwords file which doesn’t exist on all nodes."],["body","\n"],["body","一旦问题得到纠正，可以通过 reoute?retry_failed URI查询参数，进行一轮分片分配尝试"],["body","\n"],["headingLink","query-parameters"],["heading","Query parameters"],["body","\n"],["body","dry_run"],["body","\n"],["body","(Optional, Boolean) If true, then the request simulates the operation only and returns the resulting state."],["body","\n"],["body","explain"],["body","\n"],["body","(Optional, Boolean) If true, then the response contains an explanation of why the commands can or cannot be executed."],["body","\n"],["body","metric"],["body","\n"],["body","(Optional, string) Limits the information returned to the specified metrics. Defaults to all but metadata The following options are available:"],["body","\n"],["body","**`_all`**\n\nShows all metrics.\n\n**`blocks`**\n\nShows the `blocks` part of the response.\n\n**`master_node`**\n\nShows the elected `master_node` part of the response.\n\n**`metadata`**\n\nShows the `metadata` part of the response. If you supply a comma separated list of indices, the returned output will only contain metadata for these indices.\n\n**`nodes`**\n\nShows the `nodes` part of the response.\n\n**`routing_table`**\n\nShows the `routing_table` part of the response.\n\n**`version`**\n\nShows the cluster state version.\n"],["body","\n"],["body","retry_failed"],["body","\n"],["body","(Optional, Boolean) If true, then retries allocation of shards that are blocked due to too many subsequent allocation failures."],["body","\n"],["body","master_timeout"],["body","\n"],["body","(Optional, time units) Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error. Defaults to 30s."],["body","\n"],["body","timeout"],["body","\n"],["body","(Optional, time units) Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error. Defaults to 30s."],["body","\n"],["headingLink","request-body"],["heading","Request body"],["body","\n"],["body","commands"],["body","\n"],["body","(Required, array of objects) Defines the commands to perform. Supported commands are:"],["body","\n"],["body","Properties of commands"],["body","\n"],["body","move"],["body","\n"],["body","Move a started shard from one node to another node. Accepts index and shard for index name and shard number, from_node for the node to move the shard from, and to_node for the node to move the shard to."],["body","\n"],["body","cancel"],["body","\n"],["body","Cancel allocation of a shard (or recovery). Accepts index and shard for index name and shard number, and node for the node to cancel the shard allocation on. This can be used to force resynchronization of existing replicas from the primary shard by cancelling them and allowing them to be reinitialized through the standard recovery process. By default only replica shard allocations can be cancelled. If it is necessary to cancel the allocation of a primary shard then the allow_primary flag must also be included in the request."],["body","\n"],["body","allocate_replica"],["body","\n"],["body","Allocate an unassigned replica shard to a node. Accepts index and shard for index name and shard number, and node to allocate the shard to. Takes allocation deciders into account."],["body","\n"],["body","Two more commands are available that allow the allocation of a primary shard to a node. These commands should however be used with extreme care, as primary shard allocation is usually fully automatically handled by Elasticsearch. Reasons why a primary shard cannot be automatically allocated include the following:"],["body","\n\n"],["body","A new index was created but there is no node which satisfies the allocation deciders."],["body","\n"],["body","An up-to-date shard copy of the data cannot be found on the current data nodes in the cluster. To prevent data loss, the system does not automatically promote a stale shard copy to primary."],["body","\n\n"],["body","The following two commands are dangerous and may result in data loss. They are meant to be used in cases where the original data can not be recovered and the cluster administrator accepts the loss. If you have suffered a temporary issue that can be fixed, please see the retry_failed flag described above. To emphasise: if these commands are performed and then a node joins the cluster that holds a copy of the affected shard then the copy on the newly-joined node will be deleted or overwritten."],["body","\n"],["body","allocate_stale_primary\nAllocate a primary shard to a node that holds a stale copy. Accepts the index and shard for index name and shard number, and node to allocate the shard to. Using this command may lead to data loss for the provided shard id. If a node which has the good copy of the data rejoins the cluster later on, that data will be deleted or overwritten with the data of the stale copy that was forcefully allocated with this command. To ensure that these implications are well-understood, this command requires the flag accept_data_loss to be explicitly set to true."],["body","\n"],["body","allocate_empty_primary\nAllocate an empty primary shard to a node. Accepts the index and shard for index name and shard number, and node to allocate the shard to. Using this command leads to a complete loss of all data that was indexed into this shard, if it was previously started. If a node which has a copy of the data rejoins the cluster later on, that data will be deleted. To ensure that these implications are well-understood, this command requires the flag accept_data_loss to be explicitly set to true."],["body","\n"],["body","POST /_cluster/reroute\n{\n  \"commands\": [\n    {\n      \"move\": {\n        \"index\": \"test\", \"shard\": 0,\n        \"from_node\": \"node1\", \"to_node\": \"node2\"\n      }\n    },\n    {\n      \"allocate_replica\": {\n        \"index\": \"test\", \"shard\": 1,\n        \"node\": \"node3\"\n      }\n    }\n  ]\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/索引分片分配/1.索引级别的分片分配控制.html"],["title","索引级别的分片分配控制.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","index-level-shard-allocation-filtering"],["heading","Index-level shard allocation filtering"],["body","\n\n"],["body","\n"],["body","可以使用 shard allocation filters 控制 Elasticsearch 针对 指定索引  控制其分片的分配"],["body","\n"],["body","\n"],["body","\n"],["body","These per-index filters are applied in conjunction with cluster-wide allocation filtering and allocation awareness."],["body","\n"],["body","\n"],["body","\n"],["body","Shard allocation filters 可以基于自定义 节点属性 或者 内置的 _name, _host_ip, _publish_ip, _ip, _host, _id, _tier and _tier_preference attributes"],["body","\n"],["body","\n"],["body","\n"],["body","Index lifecycle management uses filters based on custom node attributes to determine how to reallocate shards when moving between phases."],["body","\n"],["body","\n"],["body","\n"],["body","The cluster.routing.allocation settings are dynamic, enabling live indices to be moved from one set of nodes to another."],["body","\n"],["body","\n"],["body","\n"],["body","Shards are only relocated if it is possible to do so without breaking another routing constraint, such as never allocating a primary and replica shard on the same node."],["body","\n"],["body","\n"],["body","\n"],["body","例如，使用自定义节点属性 来 表明节点的性能表现。使用分片分配过滤器 给每个分片分配合适的 硬件"],["body","\n"],["body","\n\n"],["headingLink","enabling-index-level-shard-allocation-filtering"],["heading","Enabling index-level shard allocation filtering"],["body","\n"],["headingLink","指定节点属性"],["heading","指定节点属性"],["body","\n"],["body","yaml"],["body","\n"],["body","Specify the filter characteristics with a custom node attribute in each node’s elasticsearch.yml configuration file. For example, if you have small, medium, and big nodes, you could add a size attribute to filter based on node size."],["body","\n"],["body","command"],["body","\n"],["body","./bin/elasticsearch -Enode.attr.size=medium\n"],["body","\n"],["headingLink","新建索引时-添加分片分配过滤器"],["heading","新建索引时 添加分片分配过滤器"],["body","\n"],["body","index.routing.allocation 配置 include, exclude, and require"],["body","\n"],["body","PUT test/_settings\n{\n  \"index.routing.allocation.include.size\": \"big,medium\"\n}\n"],["body","\n"],["headingLink","index-allocation-filter-settings"],["heading","Index allocation filter settings"],["body","\n\n"],["body","\n"],["body","index.routing.allocation.include.{attribute}"],["body","\n"],["body","Assign the index to a node whose {attribute} has at least one of the comma-separated values."],["body","\n"],["body","\n"],["body","\n"],["body","index.routing.allocation.require.{attribute}"],["body","\n"],["body","Assign the index to a node whose {attribute} has all of the comma-separated values."],["body","\n"],["body","\n"],["body","\n"],["body","index.routing.allocation.exclude.{attribute}"],["body","\n"],["body","Assign the index to a node whose {attribute} has none of the comma-separated values."],["body","\n"],["body","\n\n"],["body","内置属性"],["body","\n"],["body","key"],["body","desc"],["body","\n"],["body","_name"],["body","Match nodes by node name"],["body","\n"],["body","_host_ip"],["body","Match nodes by host IP address (IP associated with hostname)"],["body","\n"],["body","_publish_ip"],["body","Match nodes by publish IP address"],["body","\n"],["body","_ip"],["body","Match either _host_ip or _publish_ip"],["body","\n"],["body","_host"],["body","Match nodes by hostname"],["body","\n"],["body","_id"],["body","Match nodes by node id"],["body","\n"],["body","_tier"],["body","Match nodes by the node’s data tier role. For more details see data tier allocation filtering"],["body","\n\n\n"],["body","_tier filtering is based on node roles. Only a subset of roles are data tier roles, and the generic data role will match any tier filtering."],["body","\n"],["body","PUT test/_settings\n{\n  \"index.routing.allocation.include._ip\": \"192.168.2.*\"\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/考纲解读/README.html"],["title","考纲解读 - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","官方文档"],["heading","官方文档"],["body","\n"],["body","\n"],["body","https://www.elastic.co/cn/training/elastic-certified-engineer-exam"],["body","\n"],["body","\n"],["headingLink","数据管理"],["heading","数据管理"],["body","\n\n"],["body","\n"],["body","按照要求定义索引"],["body","\n\n"],["body","定义索引"],["body","\n\n"],["body","\n"],["body","\n"],["body","使用数据可视化工具将文本文件上传到Elasticsearch"],["body","\n"],["body","\n"],["body","\n"],["body","为满足一组给定要求的给定模式定义和使用索引模板"],["body","\n\n"],["body","索引模板定义"],["body","\n\n"],["body","\n"],["body","\n"],["body","在 满足给定要求 的情况下 定义和使用的动态模板"],["body","\n\n"],["body","动态索引模板定义"],["body","\n\n"],["body","\n"],["body","\n"],["body","为时间序列索引定义索引生命周期管理策略"],["body","\n\n"],["body","ILM 索引生命周期管理"],["body","\n\n"],["body","\n"],["body","\n"],["body","定义创建新数据流的索引模板"],["body","\n\n"],["body","dataStream"],["body","\n\n"],["body","\n\n"],["headingLink","searching-data"],["heading","Searching Data"],["body","\n\n"],["body","在索引的多个字段中使用 术语或者短语匹配"],["body","\n"],["body","使用bool 联合过滤"],["body","\n"],["body","异步查询"],["body","\n"],["body","使用指标或桶查询"],["body","\n"],["body","使用桶子查询"],["body","\n"],["body","跨多个集群搜索"],["body","\n\n"],["headingLink","developing-search-applications"],["heading","Developing Search Applications"],["body","\n\n"],["body","突出显示查询响应中的搜索词"],["body","\n"],["body","按指定要求排序查询结果"],["body","\n"],["body","实现分页"],["body","\n"],["body","定义和使用 alias"],["body","\n"],["body","定义和使用 搜索模板"],["body","\n\n"],["headingLink","data-processing"],["heading","Data Processing"],["body","\n\n"],["body","按照给定要求 定义 mapping"],["body","\n"],["body","按照给定要求定义 自定义分析器"],["body","\n"],["body","定义和使用具有不同数据类型和/或分析器的多字段"],["body","\n"],["body","使用 reIndex 或者 updateByQuery 重新索引文档"],["body","\n"],["body","按照给定要求 定义和使用 ingest pipeline  包括 使用 Painless 脚本修改文档"],["body","\n"],["body","配置索引，使其正确维护对象的嵌套数组的关系"],["body","\n\n"],["headingLink","cluster-management"],["heading","Cluster Management"],["body","\n\n"],["body","诊断分片问题并修复集群的运行状况"],["body","\n"],["body","备份和还原集群和/或特定索引"],["body","\n"],["body","将快照配置为可搜索的"],["body","\n"],["body","为跨集群搜索配置集群"],["body","\n"],["body","实现跨集群复制"],["body","\n"],["body","使用Elasticsearch Security定义基于角色的访问控制"],["body","\n\n"],["headingLink","模块集合"],["heading","模块集合"],["body","\n"],["body","部署、索引、检索、聚合、分析、文档、集群、安全"],["body","\n"],["headingLink","考试路线"],["heading","考试路线"],["body","\n\n"],["body","\n梳理考点 ✅ 2023-03-02"],["body","\n"],["body","\n整里Todo"],["body","\n"],["body","\n学习并整理考点相关知识点"],["body","\n"],["body","\n学习并整理 知识星球 相关考点 解答"],["body","\n"],["body","\n刷题：主要是知识星球 真题演练"],["body","\n"],["body","\n总结经验：总结过来人经验"],["body","\n"],["body","\n报名事宜"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/考纲解读/1.ClusterManagement.html"],["title","ClusterManagement.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","cluster-management"],["heading","Cluster Management"],["body","\n"],["headingLink","普通方式与集群方式安装和基本配置"],["heading","普通方式与集群方式安装和基本配置"],["body","\n"],["headingLink","诊断分片问题并修复集群的运行状况"],["heading","诊断分片问题并修复集群的运行状况"],["body","\n"],["headingLink","文档"],["heading","文档"],["body","\n\n"],["body","https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-health.html"],["body","\n"],["body","https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-allocation-explain.html"],["body","\n"],["body","https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-reroute.html"],["body","\n\n"],["headingLink","考点梳理"],["heading","考点梳理"],["body","\n\n"],["body","cat api使用（很多，都要熟悉）"],["body","\n"],["body","诊断集群健康状态，找到黄色或红色非健康能找到原因，并变成健康绿色状态"],["body","\n"],["body","诊断集群分配未分配的原因，并恢复正常"],["body","\n"],["body","集群分配迁移等重新路由实现"],["body","\n\n"],["headingLink","备份和还原集群和或特定索引"],["heading","备份和还原集群和/或特定索引"],["body","\n"],["headingLink","文档-1"],["heading","文档"],["body","\n"],["body","https://www.elastic.co/guide/en/elasticsearch/reference/7.13/backup-cluster.html"],["body","\n"],["body","https://www.elastic.co/guide/en/elasticsearch/reference/7.13/snapshot-restore.html"],["body","\n"],["headingLink","考点梳理-1"],["heading","考点梳理"],["body","\n\n"],["body","快照备份集群并恢复"],["body","\n"],["body","快照备份指定索引并恢复"],["body","\n"],["body","一定要验证一下恢复是否正确，是否满足给定题目的条件"],["body","\n\n"],["headingLink","将快照配置为可搜索的"],["heading","将快照配置为可搜索的"],["body","\n"],["headingLink","文档-2"],["heading","文档"],["body","\n"],["body","https://www.elastic.co/guide/en/elasticsearch/reference/7.13/searchable-snapshots.html"],["body","\n"],["body","https://www.elastic.co/guide/en/elasticsearch/reference/7.13/ilm-searchable-snapshot.html"],["body","\n"],["headingLink","考点梳理-2"],["heading","考点梳理"],["body","\n\n"],["body","配置可搜索快照"],["body","\n"],["body","执行快照检索"],["body","\n\n"],["headingLink","为跨集群搜索配置集群"],["heading","为跨集群搜索配置集群"],["body","\n"],["headingLink","文档-3"],["heading","文档"],["body","\n"],["body","https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-cross-cluster-search.html"],["body","\n"],["headingLink","考点梳理-3"],["heading","考点梳理"],["body","\n\n"],["body","能实现跨集群检索配置"],["body","\n"],["body","能实现跨集群检索"],["body","\n"],["body","考试的时候，一定要验证返回结果是不同集群返回的才可以"],["body","\n\n"],["headingLink","实现跨集群复制"],["heading","实现跨集群复制"],["body","\n"],["headingLink","文档-4"],["heading","文档"],["body","\n"],["body","https://www.elastic.co/guide/en/elasticsearch/reference/current/xpack-ccr.html"],["body","\n"],["body","https://www.elastic.co/guide/en/elasticsearch/reference/7.13/ccr-apis.html"],["body","\n"],["headingLink","考点梳理-4"],["heading","考点梳理"],["body","\n\n"],["body","跨集群复制"],["body","\n\n"],["headingLink","使用elasticsearch-security定义基于角色的访问控制"],["heading","使用Elasticsearch Security定义基于角色的访问控制"],["body","\n"],["headingLink","文档-5"],["heading","文档"],["body","\n"],["body","https://www.elastic.co/guide/en/elasticsearch/reference/7.13/security-api-put-role.html"],["body","\n"],["body","https://www.elastic.co/guide/en/elasticsearch/reference/7.13/security-api-put-user.html"],["body","\n"],["headingLink","考点梳理-5"],["heading","考点梳理"],["body","\n\n"],["body","x-pack 一般会结合role 角色一起考"],["body","\n"],["body","新建角色"],["body","\n"],["body","新建用户&密码，修改密码"],["body","\n"],["body","官方我咨询过：命令行或者kibana操作都可以，但要确保结果对。建议kibana，毕竟比较简洁。"],["body","\n"],["body","kibana权限设置，一定要加上能访问kibana，否则新建了用户会无法登录（可能会扣分）"],["body","\n"],["body","举例：设置x-pack属性后（默认未开启），设置用户名、密码（可以kibana设置）、设置访问权限等。"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/考纲解读/2.DataManagement.html"],["title","DataManagement.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","data-management-数据管理"],["heading","Data Management 数据管理"],["body","\n"],["headingLink","11-define-an-index-that-satisfies-a-given-set-of-requirements"],["heading","1.1 Define an index that satisfies a given set of requirements"],["body","\n"],["body","https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-create-index.html"],["body","\n"],["body","考点梳理："],["body","\n\n"],["body","创建满足给定条件的索引"],["body","\n"],["body","主分片数、副本分片数修改主分片、副本分片数\nsetting设置（参数建议都过一下，如：刷新频率等）"],["body","\n\n"],["headingLink","12-新增考点-use-the-data-visualizer-to-upload-a-text-file-into-elasticsearch"],["heading","1.2 【新增考点】 Use the Data Visualizer to upload a text file into Elasticsearch"],["body","\n"],["body","考点梳理："],["body","\n\n"],["body","偏 Kibana 实操的考点\nhttps://www.elastic.co/guide/en/kibana/7.13/connect-to-elasticsearch.html"],["body","\n\n"],["headingLink","13-define-and-use-an-index-template-for-a-given-pattern-that-satisfies-a-given-set-of-requirements"],["heading","1.3 Define and use an index template for a given pattern that satisfies a given set of requirements"],["body","\n"],["body","https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-templates.html\nhttps://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-templates-v1.html"],["body","\n"],["body","考点梳理："],["body","\n\n"],["body","\n"],["body","创建满足给定条件的索引模板"],["body","\n"],["body","\n"],["body","\n"],["body","组合考点"],["body","\n"],["body","\n"],["body","\n\n"],["body","创建模板同时：指定mapping，指定setting，指定ingest，指定analyzer，指定别名，指定order优先级"],["body","\n\n"],["body","\n\n"],["headingLink","14-define-and-use-a-dynamic-template-that-satisfies-a-given-set-of-requirements"],["heading","1.4 Define and use a dynamic template that satisfies a given set of requirements"],["body","\n"],["body","https://www.elastic.co/guide/en/elasticsearch/reference/7.13/dynamic-templates.html"],["body","\n"],["body","考点梳理："],["body","\n\n"],["body","创建满足给定模板条件的索引，如：text_*开头指定为text类型"],["body","\n"],["body","创建满足给定模板条件的模板，可以结合2.4 一起考！"],["body","\n\n"],["headingLink","15-新增考点define-an-index-lifecycle-management-policy-for-a-time-series-index"],["heading","1.5 【新增考点】Define an Index Lifecycle Management policy for a time-series index"],["body","\n"],["body","https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-lifecycle-management.html"],["body","\n"],["body","考点："],["body","\n\n"],["body","为给定时序数据添加索引生命周期管理"],["body","\n"],["body","两种实现方式：Kibana + DSL命令行"],["body","\n\n"],["headingLink","16-新增考点define-an-index-template-that-creates-a-new-data-stream"],["heading","1.6 【新增考点】Define an index template that creates a new data stream"],["body","\n"],["body","https://www.elastic.co/guide/en/elasticsearch/reference/7.13/data-streams.html\n考点："],["body","\n\n"],["body","为 data stream数据流类型添加索引模板处理。"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/乐观并发控制.html"],["title","乐观并发控制.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","乐观并发控制"],["heading","乐观并发控制"],["body","\n"],["headingLink","场景描述"],["heading","场景描述"],["body","\n"],["body","Elasticsearch 是分布式的。当文档创建、更新或删除时， 新版本的文档必须复制到集群中的其他节点"],["body","\n"],["body","Elasticsearch 也是异步和并发的，这意味着这些复制请求被并行发送，并且到达目的地时也许 顺序是乱的"],["body","\n"],["body","Elasticsearch 需要一种方法确保文档的旧版本不会覆盖新的版本。"],["body","\n"],["headingLink","解决方式版本号"],["heading","解决方式：版本号"],["body","\n"],["body","每个文档都有一个 _version （版本）号，当文档被修改时版本号递增。 Elasticsearch 使用这个 _version 号来确保变更以正确顺序得到执行。如果旧版本的文档在新版本之后到达，它可以被简单的忽略。"],["body","\n"],["headingLink","通过外部系统使用版本控制"],["heading","通过外部系统使用版本控制"],["body","\n"],["body","主副数据库"],["body","\n"],["body","一个常见的设置是使用其它数据库作为主要的数据存储，使用 Elasticsearch 做数据检索"],["body","\n"],["body","数据同步"],["body","\n"],["body","这意味着主数据库的所有更改发生时都需要被复制到 Elasticsearch ，如果多个进程负责这一数据同步，你可能遇到类似于之前描述的并发问题。"],["body","\n"],["body","指定外部版本号"],["body","\n"],["body","如果你的主数据库已经有了版本号 — 或一个能作为版本号的字段值比如 timestamp — 那么你就可以在 Elasticsearch 中通过增加 version_type=external 到查询字符串的方式重用这些相同的版本号， 版本号必须是大于零的整数， 且小于 9.2E+18 — 一个 Java 中 long 类型的正值。"],["body","\n"],["body","外部版本号的处理"],["body","\n"],["body","外部版本号的处理方式和我们之前讨论的内部版本号的处理方式有些不同， Elasticsearch 不是检查当前 _version 和请求中指定的版本号是否相同， 而是检查当前 _version 是否 小于 指定的版本号。 如果请求成功，外部的版本号作为文档的新 _version 进行存储。"],["body","\n"],["body","外部版本号不仅在索引和删除请求是可以指定，而且在 创建 新文档时也可以指定。"],["body","\n"],["body","例如，要创建一个新的具有外部版本号 5 的博客文章，我们可以按以下方法进行："],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/分词.html"],["title","分词.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","倒排索引"],["heading","倒排索引"],["body","\n"],["body","Elasticsearch 使用一种称为 倒排索引 的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。"],["body","\n"],["body","例如，假设我们有两个文档，每个文档的 content 域包含如下内容："],["body","\n\n"],["body","The quick brown fox jumped over the lazy dog"],["body","\n"],["body","Quick brown foxes leap over lazy dogs in summer"],["body","\n\n"],["body","为了创建倒排索引，我们首先将每个文档的 content 域拆分成单独的 词（我们称它为 词条 或 tokens ），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。结果如下所示："],["body","\n"],["body","Term      Doc_1  Doc_2\n-------------------------\nQuick   |       |  X\nThe     |   X   |\nbrown   |   X   |  X\ndog     |   X   |\ndogs    |       |  X\nfox     |   X   |\nfoxes   |       |  X\nin      |       |  X\njumped  |   X   |\nlazy    |   X   |  X\nleap    |       |  X\nover    |   X   |  X\nquick   |   X   |\nsummer  |       |  X\nthe     |   X   |\n------------------------\n"],["body","\n"],["body","现在，如果我们想搜索 quick brown ，我们只需要查找包含每个词条的文档："],["body","\n"],["body","Term      Doc_1  Doc_2\n-------------------------\nbrown   |   X   |  X\nquick   |   X   |\n------------------------\nTotal   |   2   |  1\n"],["body","\n"],["body","两个文档都匹配，但是第一个文档比第二个匹配度更高。如果我们使用仅计算匹配词条数量的简单 相似性算法 ，那么，我们可以说，对于我们查询的相关性来讲，第一个文档比第二个文档更佳。"],["body","\n"],["body","但是，我们目前的倒排索引有一些问题："],["body","\n\n"],["body","Quick 和 quick 以独立的词条出现，然而用户可能认为它们是相同的词。"],["body","\n"],["body","fox 和 foxes 非常相似, 就像 dog 和 dogs ；他们有相同的词根。"],["body","\n"],["body","jumped 和 leap, 尽管没有相同的词根，但他们的意思很相近。他们是同义词。"],["body","\n\n"],["body","使用前面的索引搜索 +Quick +fox 不会得到任何匹配文档。（记住，+ 前缀表明这个词必须存在。）只有同时出现 Quick 和 fox 的文档才满足这个查询条件，但是第一个文档包含 quick fox ，第二个文档包含 Quick foxes 。"],["body","\n"],["body","我们的用户可以合理的期望两个文档与查询匹配。我们可以做的更好。"],["body","\n"],["body","如果我们将词条规范为标准模式，那么我们可以找到与用户搜索的词条不完全一致，但具有足够相关性的文档。例如："],["body","\n\n"],["body","Quick 可以小写化为 quick 。"],["body","\n"],["body","foxes 可以 词干提取 --变为词根的格式-- 为 fox 。类似的， dogs 可以为提取为 dog 。"],["body","\n"],["body","jumped 和 leap 是同义词，可以索引为相同的单词 jump 。"],["body","\n\n"],["body","现在索引看上去像这样："],["body","\n"],["body","Term      Doc_1  Doc_2\n-------------------------\nbrown   |   X   |  X\ndog     |   X   |  X\nfox     |   X   |  X\nin      |       |  X\njump    |   X   |  X\nlazy    |   X   |  X\nover    |   X   |  X\nquick   |   X   |  X\nsummer  |       |  X\nthe     |   X   |  X\n------------------------\n"],["body","\n"],["body","这还远远不够。我们搜索 +Quick +fox 仍然 会失败，因为在我们的索引中，已经没有 Quick 了。但是，如果我们对搜索的字符串使用与 content 域相同的标准化规则，会变成查询 +quick +fox ，这样两个文档都会匹配！"],["body","\n"],["body","这非常重要。你只能搜索在索引中出现的词条，所以索引文本和查询字符串必须标准化为相同的格式。"],["body","\n"],["body","分词和标准化的过程称为 分析 ， 我们会在下个章节讨论。"],["body","\n"],["headingLink","分析"],["heading","分析"],["body","\n"],["body","分析 包含下面的过程："],["body","\n\n"],["body","首先，将一块文本分成适合于倒排索引的独立的 词条 ，"],["body","\n"],["body","之后，将这些词条统一化为标准格式以提高它们的“可搜索性”，或者 recall"],["body","\n\n"],["body","分析器执行上面的工作。 分析器 实际上是将三个功能封装到了一个包里："],["body","\n"],["body","字符过滤器"],["body","\n"],["body","首先，字符串按顺序通过每个 字符过滤器 。他们的任务是在分词前整理字符串。一个字符过滤器可以用来去掉HTML，或者将 & 转化成 and。"],["body","\n"],["body","分词器"],["body","\n"],["body","其次，字符串被 分词器 分为单个的词条。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条。"],["body","\n"],["body","Token 过滤器"],["body","\n"],["body","最后，词条按顺序通过每个 token 过滤器 。这个过程可能会改变词条（例如，小写化 Quick ），删除词条（例如， 像 a， and， the 等无用词），或者增加词条（例如，像 jump 和 leap 这种同义词）。"],["body","\n"],["headingLink","内置分析器"],["heading","内置分析器"],["body","\n"],["body","但是， Elasticsearch还附带了可以直接使用的预包装的分析器。接下来我们会列出最重要的分析器。为了证明它们的差异，我们看看每个分析器会从下面的字符串得到哪些词条："],["body","\n"],["body","\"Set the shape to semi-transparent by calling set_trans(5)\"\n"],["body","\n\n"],["body","\n"],["body","标准分析器"],["body","\n"],["body","标准分析器是Elasticsearch默认使用的分析器。它是分析各种语言文本最常用的选择。它根据 Unicode 联盟 定义的 单词边界 划分文本。删除绝大部分标点。最后，将词条小写。它会产生set, the, shape, to, semi, transparent, by, calling, set_trans, 5"],["body","\n"],["body","\n"],["body","\n"],["body","简单分析器"],["body","\n"],["body","简单分析器在任何不是字母的地方分隔文本，将词条小写。它会产生set, the, shape, to, semi, transparent, by, calling, set, trans"],["body","\n"],["body","\n"],["body","\n"],["body","空格分析器"],["body","\n"],["body","空格分析器在空格的地方划分文本。它会产生Set, the, shape, to, semi-transparent, by, calling, set_trans(5)"],["body","\n"],["body","\n"],["body","\n"],["body","语言分析器"],["body","\n"],["body","特定语言分析器可用于 很多语言。它们可以考虑指定语言的特点。例如， 英语 分析器附带了一组英语无用词（常用单词，例如 and 或者 the ，它们对相关性没有多少影响），它们会被删除。 由于理解英语语法的规则，这个分词器可以提取英语单词的 词干 。英语 分词器会产生下面的词条：set, shape, semi, transpar, call, set_tran, 5注意看 transparent、 calling 和 set_trans 已经变为词根格式。"],["body","\n"],["body","\n\n"],["body","什么时候使用分析器"],["body","\n\n"],["body","当我们 索引 一个文档，它的全文域被分析成词条以用来创建倒排索引"],["body","\n"],["body","但是，当我们在全文域 搜索 的时候，我们需要将查询字符串通过 相同的分析过程 ，以保证我们搜索的词条格式与索引中的词条格式一致。"],["body","\n\n"],["body","全文查询，理解每个域是如何定义的，因此它们可以做正确的事："],["body","\n\n"],["body","当你查询一个 全文 域时， 会对查询字符串应用相同的分析器，以产生正确的搜索词条列表。"],["body","\n"],["body","当你查询一个 精确值 域时，不会分析查询字符串，而是搜索你指定的精确值。"],["body","\n\n"],["body","测试分析器"],["body","\n"],["body","GET /_analyze\n{\n  \"analyzer\": \"standard\",\n  \"text\": \"Text to analyze\"\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/elasticsearch-docker.html"],["title","elasticsearch-docker.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","docker-compose-单机配置"],["heading","Docker-Compose 单机配置"],["body","\n"],["body","version: \"3.2\"\nservices:\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}\n    environment:\n      ELASTIC_PASSWORD: ${ELASTIC_PASSWORD}\n      ES_SETTING_DISCOVERY_TYPE: single-node\n      ES_SETTING_XPACK_SECURITY_ENABLED: false\n    ports:\n      - 9200:9200\n    volumes:\n      - /home/weisanju/softwares/elasticsearch/data:/usr/share/elasticsearch/data\n\n  kibana:\n    depends_on:\n      - elasticsearch\n    image: docker.elastic.co/kibana/kibana:${STACK_VERSION}\n    volumes:\n      #      - certs:/usr/share/kibana/config/certs\n      - /home/weisanju/softwares/elasticsearch/kibanadata:/usr/share/kibana/data\n    ports:\n      - 5601:5601\n    environment:\n      - SERVERNAME=kibana\n      - ELASTICSEARCH_HOSTS=https://elasticsearch:9200\n      - ELASTICSEARCH_USERNAME=kibana_system\n      - ELASTICSEARCH_PASSWORD=${ELASTIC_PASSWORD}\n#      - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=config/certs/ca/ca.crt\n    healthcheck:\n      test:\n        [\n          \"CMD-SHELL\",\n          \"curl -s -I http://localhost:5601 | grep -q 'HTTP/1.1 302 Found'\",\n        ]\n      interval: 10s\n      timeout: 10s\n      retries: 120\n\n"],["body","\n"],["headingLink","官方示例配置"],["heading","官方示例配置"],["body","\n"],["body","https://github.com/elastic/elasticsearch/blob/8.13/docs/reference/setup/install/docker/docker-compose.yml#L10\n\n\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/集群.html"],["title","集群 - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/3.Built-inAnalyzerReference/2.FingerprintAnalyzer.html"],["title","FingerprintAnalyzer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","fingerprint-analyzer"],["heading","Fingerprint analyzer"],["body","\n"],["body","fingerprint analyzer  实现了一种指纹算法  fingerprinting algorithm 它是 OpenRefine项目使用该算法来辅助聚类。"],["body","\n"],["body","Input text is lowercased, normalized to remove extended characters, sorted, deduplicated and concatenated into a single token,If a stopword list is configured, stop words will also be removed."],["body","\n"],["headingLink","example-output"],["heading","Example output"],["body","\n"],["body","POST _analyze\n{\n  \"analyzer\": \"fingerprint\",\n  \"text\": \"Yes yes, Gödel said this sentence is consistent and.\"\n}\n"],["body","\n"],["body","The above sentence would produce the following single term:"],["body","\n"],["body","[ and consistent godel is said sentence this yes ]\n"],["body","\n"],["headingLink","configuration"],["heading","Configuration"],["body","\n"],["body","The fingerprint analyzer accepts the following parameters:"],["body","\n"],["body","\n"],["body","separator"],["body","The character to use to concatenate the terms. Defaults to a space."],["body","\n"],["body","max_output_size"],["body","The maximum token size to emit. Defaults to 255. Tokens larger than this size will be discarded."],["body","\n"],["body","stopwords"],["body","A pre-defined stop words list like _english_ or an array containing a list of stop words. Defaults to _none_."],["body","\n"],["body","stopwords_path"],["body","The path to a file containing stop words."],["body","\n\n\n"],["body","See the Stop Token Filter for more information about stop word configuration."],["body","\n"],["headingLink","example-configuration"],["heading","Example configuration"],["body","\n"],["body","In this example, we configure the fingerprint analyzer to use the pre-defined list of English stop words:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_fingerprint_analyzer\": {\n          \"type\": \"fingerprint\",\n          \"stopwords\": \"_english_\"\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_fingerprint_analyzer\",\n  \"text\": \"Yes yes, Gödel said this sentence is consistent and.\"\n}\n"],["body","\n"],["body","The above example produces the following term:"],["body","\n"],["body","[ consistent godel said sentence yes ]\n"],["body","\n"],["headingLink","definition"],["heading","Definition"],["body","\n"],["body","The fingerprint tokenizer consists of:"],["body","\n\n"],["body","\n"],["body","Tokenizer"],["body","\n"],["body","Standard Tokenizer"],["body","\n"],["body","\n"],["body","\n"],["body","Token Filters (in order)"],["body","\n\n"],["body","Lower Case Token Filter"],["body","\n"],["body","ASCII folding"],["body","\n"],["body","Stop Token Filter (disabled by default)"],["body","\n"],["body","Fingerprint"],["body","\n\n"],["body","\n\n"],["body","If you need to customize the fingerprint analyzer beyond the configuration parameters then you need to recreate it as a custom analyzer and modify it, usually by adding token filters. This would recreate the built-in fingerprint analyzer and you can use it as a starting point for further customization:"],["body","\n"],["body","PUT /fingerprint_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"rebuilt_fingerprint\": {\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"lowercase\",\n            \"asciifolding\",\n            \"fingerprint\"\n          ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/3.Built-inAnalyzerReference/README.html"],["title","Built-inAnalyzerReference - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","built-in-analyzer-reference"],["heading","Built-in analyzer reference"],["body","\n"],["body","Elasticsearch附带各种内置分析仪，开箱即用"],["body","\n\n"],["body","\n"],["body","Standard Analyzer"],["body","\n"],["body","标准分析器 通过单词边界 将文本换分成 词项，这个是在  Unicode Text Segmentation algorithm 定义的。它会移除 连字符、小写化词项、支持移除停用词"],["body","\n"],["body","\n"],["body","\n"],["body","Simple Analyzer"],["body","\n"],["body","通过非字母符号分割，并且使所有词项都小写。"],["body","\n"],["body","\n"],["body","\n"],["body","Whitespace Analyzer"],["body","\n"],["body","空格分割不会小写词项"],["body","\n"],["body","\n"],["body","\n"],["body","Stop Analyzer"],["body","\n"],["body","在 Simple Analyzer 分析器的基础上增加了 停用词过滤"],["body","\n"],["body","\n"],["body","\n"],["body","Keyword Analyzer"],["body","\n"],["body","单词项的无任何操作的分析器"],["body","\n"],["body","\n"],["body","\n"],["body","Pattern Analyzer"],["body","\n"],["body","使用正则表达式分割词项的分析器，同时支持小写与停用词"],["body","\n"],["body","\n"],["body","\n"],["body","Language Analyzers"],["body","\n"],["body","Elasticsearch提供了许多特定于语言的分析器，例如英语或法语。"],["body","\n"],["body","\n"],["body","\n"],["body","Fingerprint Analyzer"],["body","\n"],["body","指纹分析器 是一种专业的分析器，它创建了可用于重复检测的指纹。"],["body","\n"],["body","\n\n"],["headingLink","custom-analyzers"],["heading","Custom analyzers"],["body","\n"],["body","如果您没有找到适合您需要的分析器"],["body","\n"],["body","使用  character filters, tokenizer, and token filters. 创建自定义的分析器"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/3.Built-inAnalyzerReference/7.StopAnalyzer.html"],["title","StopAnalyzer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","stop-analyzer"],["heading","Stop analyzer"],["body","\n"],["body","The stop analyzer is the same as the simple analyzer but adds support for removing stop words. It defaults to using the _english_ stop words."],["body","\n"],["headingLink","example-output"],["heading","Example output"],["body","\n"],["body","POST _analyze\n{\n  \"analyzer\": \"stop\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n"],["body","\n"],["body","[ quick, brown, foxes, jumped, over, lazy, dog, s, bone ]\n"],["body","\n"],["headingLink","configuration"],["heading","Configuration"],["body","\n"],["body","The stop analyzer accepts the following parameters:"],["body","\n"],["body","\n"],["body","stopwords"],["body","A pre-defined stop words list like _english_ or an array containing a list of stop words. Defaults to _english_."],["body","\n"],["body","stopwords_path"],["body","The path to a file containing stop words. This path is relative to the Elasticsearch config directory."],["body","\n\n\n"],["body","See the Stop Token Filter for more information about stop word configuration."],["body","\n"],["headingLink","example-configuration"],["heading","Example configuration"],["body","\n"],["body","In this example, we configure the stop analyzer to use a specified list of words as stop words:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_stop_analyzer\": {\n          \"type\": \"stop\",\n          \"stopwords\": [\"the\", \"over\"]\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_stop_analyzer\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n"],["body","\n"],["body","The above example produces the following terms:"],["body","\n"],["body","[ quick, brown, foxes, jumped, lazy, dog, s, bone ]\n"],["body","\n"],["headingLink","definition"],["heading","Definition"],["body","\n"],["body","It consists of:"],["body","\n\n"],["body","\n"],["body","Tokenizer"],["body","\n"],["body","Lower Case Tokenizer"],["body","\n"],["body","\n"],["body","\n"],["body","Token filters"],["body","\n"],["body","Stop Token Filter"],["body","\n"],["body","\n\n"],["body","If you need to customize the stop analyzer beyond the configuration parameters then you need to recreate it as a custom analyzer and modify it, usually by adding token filters. This would recreate the built-in stop analyzer and you can use it as a starting point for further customization:"],["body","\n"],["body","PUT /stop_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"filter\": {\n        \"english_stop\": {\n          \"type\":       \"stop\",\n          \"stopwords\":  \"_english_\" \n        }\n      },\n      \"analyzer\": {\n        \"rebuilt_stop\": {\n          \"tokenizer\": \"lowercase\",\n          \"filter\": [\n            \"english_stop\"          \n          ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/3.Built-inAnalyzerReference/1.StandardAnalyzer.html"],["title","StandardAnalyzer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","standard-analyzer"],["heading","Standard analyzer"],["body","\n\n"],["body","标准分析器是默认分析器，如果没有指定，则使用"],["body","\n"],["body","It provides grammar based tokenization (based on the Unicode Text Segmentation algorithm, as specified in Unicode Standard Annex #29) and works well for most languages."],["body","\n\n"],["headingLink","example-output"],["heading","Example output"],["body","\n"],["body","POST _analyze\n{\n  \"analyzer\": \"standard\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n"],["body","\n"],["body","The above sentence would produce the following terms:"],["body","\n"],["body","[ the, 2, quick, brown, foxes, jumped, over, the, lazy, dog's, bone ]\n"],["body","\n"],["headingLink","configuration"],["heading","Configuration"],["body","\n"],["body","The standard analyzer accepts the following parameters:"],["body","\n"],["body","\n"],["body","max_token_length"],["body","The maximum token length. If a token is seen that exceeds this length then it is split at max_token_length intervals. Defaults to 255."],["body","\n"],["body","stopwords"],["body","A pre-defined stop words list like _english_ or an array containing a list of stop words. Defaults to _none_."],["body","\n"],["body","stopwords_path"],["body","The path to a file containing stop words."],["body","\n\n\n"],["body","See the Stop Token Filter for more information about stop word configuration."],["body","\n"],["headingLink","example-configuration"],["heading","Example configuration"],["body","\n"],["body","In this example, we configure the standard analyzer to have a max_token_length of 5 (for demonstration purposes), and to use the pre-defined list of English stop words:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_english_analyzer\": {\n          \"type\": \"standard\",\n          \"max_token_length\": 5,\n          \"stopwords\": \"_english_\"\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_english_analyzer\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n"],["body","\n"],["body","The above example produces the following terms:"],["body","\n"],["body","[ 2, quick, brown, foxes, jumpe, d, over, lazy, dog's, bone ]\n"],["body","\n"],["headingLink","definition"],["heading","Definition"],["body","\n"],["body","The standard analyzer consists of:"],["body","\n\n"],["body","\n"],["body","Tokenizer"],["body","\n"],["body","Standard Tokenizer"],["body","\n"],["body","\n"],["body","\n"],["body","Token Filters"],["body","\n"],["body","Lower Case Token Filter"],["body","\n"],["body","Stop Token Filter (disabled by default)"],["body","\n"],["body","\n\n"],["body","If you need to customize the standard analyzer beyond the configuration parameters then you need to recreate it as a custom analyzer and modify it, usually by adding token filters. This would recreate the built-in standard analyzer and you can use it as a starting point:"],["body","\n"],["body","PUT /standard_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"rebuilt_standard\": {\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"lowercase\"       \n          ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/3.Built-inAnalyzerReference/8.WhitespaceAnalyzer.html"],["title","WhitespaceAnalyzer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","whitespace-analyzer"],["heading","Whitespace analyzer"],["body","\n"],["body","The whitespace analyzer breaks text into terms whenever it encounters a whitespace character."],["body","\n"],["headingLink","example-output"],["heading","Example output"],["body","\n"],["body","POST _analyze\n{\n  \"analyzer\": \"whitespace\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n"],["body","\n"],["body","The above sentence would produce the following terms:"],["body","\n"],["body","[ The, 2, QUICK, Brown-Foxes, jumped, over, the, lazy, dog's, bone. ]\n"],["body","\n"],["headingLink","configuration"],["heading","Configuration"],["body","\n"],["body","The whitespace analyzer is not configurable."],["body","\n"],["headingLink","definition"],["heading","Definition"],["body","\n"],["body","It consists of:"],["body","\n\n"],["body","\n"],["body","Tokenizer"],["body","\n"],["body","Whitespace Tokenizer"],["body","\n"],["body","\n\n"],["body","If you need to customize the whitespace analyzer then you need to recreate it as a custom analyzer and modify it, usually by adding token filters. This would recreate the built-in whitespace analyzer and you can use it as a starting point for further customization:"],["body","\n"],["body","PUT /whitespace_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"rebuilt_whitespace\": {\n          \"tokenizer\": \"whitespace\",\n          \"filter\": [         \n          ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/3.Built-inAnalyzerReference/10.OpenRefine指纹算法.html"],["title","OpenRefine指纹算法.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["body","OpenRefine fingerprint算法是一种用于文本聚类的算法，它可以帮助你找出相似的文本值。它的基本步骤是："],["body","\n\n"],["body","将文本转换为小写"],["body","\n"],["body","移除特殊字符"],["body","\n"],["body","对单词进行排序和去重"],["body","\n"],["body","移除停用词（如果有配置）"],["body","\n"],["body","将单词连接成一个标记\n这种算法是OpenRefine项目使用的一种指纹算法1也被Elasticsearch作为一种分析器2。"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/3.Built-inAnalyzerReference/9.Unicode文本分割算法.html"],["title","Unicode文本分割算法.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","原理"],["heading","原理"],["body","\n"],["body","Unicode文本分割算法是一种用于将Unicode编码的文本字符串分割为用户感知的字符、单词、句子和行的算法\b\b，这些文本元素在不同的语言和文化中可能有不同的定义和规则，因此Unicode提供了一些通用的指导原则和数据表来帮助实现文本分割"],["body","\n"],["headingLink","例子1"],["heading","例子1"],["body","\n"],["body","假设我们有一个字符串  €1 234,56，它表示欧元的金额，使用空格作为千位分隔符。如果我们想要将这个字符串分割为单词，我们不能简单地按照空格来划分，因为那样会将一个数字分成三个部分。我们需要使用Unicode文本分割算法中的MidNum规则，它指定了哪些空格字符可以用来连接数字而不是分隔单词。这样，我们就可以正确地将字符串分割为两个单词：“€1 234,56\"和”.\"。"],["body","\n"],["headingLink","例子2"],["heading","例子2"],["body","\n"],["body","假设我们有一个字符串\"Hello.😊\"，它表示一个带有表情符号的问候。如果我们想要将这个字符串分割为字符，我们不能简单地按照码点来划分，因为那样会将表情符号分成两个部分。我们需要使用Unicode文本分割算法中的Emoji规则，它指定了哪些码点可以用来组成一个表情符号或一个表情符号序列。这样，我们就可以正确地将字符串分割为三个字符：“H”、\"ello.“和\"😊”。"],["body","\n"],["headingLink","规则和数据表"],["heading","规则和数据表"],["body","\n"],["body","这些规则和数据表可以在Unicode官方网站的Unicode文本分割报告中找到。这个报告包含了不同类型的边界的定义、算法、示例和测试数据，以及一些附录和参考资料。其中，"],["body","\n\n"],["body","表3a列出了用于判断字符边界的属性值，"],["body","\n"],["body","表4a列出了用于判断单词边界的属性值，"],["body","\n"],["body","表5a列出了用于判断句子边界的属性值。\n这些属性值是根据Unicode字符数据库中的信息来确定的。"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/3.Built-inAnalyzerReference/5.PatternAnalyzer.html"],["title","PatternAnalyzer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","pattern-analyzer"],["heading","Pattern analyzer"],["body","\n\n"],["body","\n"],["body","The pattern analyzer 使用正则表达式将文本拆分为terms。"],["body","\n"],["body","\n"],["body","\n"],["body","正则表达式应该匹配token分隔符 而不是令牌本身。正则表达式默认为 \\ W (或所有非单词字符)。"],["body","\n"],["body","\n\n"],["headingLink","beware-of-pathological-regular-expressions"],["heading","Beware of Pathological Regular Expressions"],["body","\n\n"],["body","\n"],["body","The pattern analyzer uses Java Regular Expressions."],["body","\n"],["body","\n"],["body","\n"],["body","A badly written regular expression could run very slowly or even throw a StackOverflowError and cause the node it is running on to exit suddenly."],["body","\n"],["body","\n"],["body","\n"],["body","Read more about pathological regular expressions and how to avoid them."],["body","\n"],["body","\n\n"],["headingLink","example-output"],["heading","Example output"],["body","\n"],["body","POST _analyze\n{\n  \"analyzer\": \"pattern\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n"],["body","\n"],["body","The above sentence would produce the following terms:"],["body","\n"],["body","[ the, 2, quick, brown, foxes, jumped, over, the, lazy, dog, s, bone ]\n"],["body","\n"],["headingLink","configuration"],["heading","Configuration"],["body","\n"],["body","The pattern analyzer accepts the following parameters:"],["body","\n"],["body","\n"],["body","pattern"],["body","A Java regular expression, defaults to \\W+."],["body","\n"],["body","flags"],["body","Java regular expression flags. Flags should be pipe-separated, eg `\"CASE_INSENSITIVE"],["body","\n"],["body","lowercase"],["body","Should terms be lowercased or not. Defaults to true."],["body","\n"],["body","stopwords"],["body","A pre-defined stop words list like _english_ or an array containing a list of stop words. Defaults to _none_."],["body","\n"],["body","stopwords_path"],["body","The path to a file containing stop words."],["body","\n\n\n"],["headingLink","example-configuration"],["heading","Example configuration"],["body","\n"],["body","In this example, we configure the pattern analyzer to split email addresses on non-word characters or on underscores (\\W|_), and to lower-case the result:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_email_analyzer\": {\n          \"type\":      \"pattern\",\n          \"pattern\":   \"\\\\W|_\", //将模式指定为JSON字符串时，需要转义模式中的反斜杠。\n          \"lowercase\": true\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_email_analyzer\",\n  \"text\": \"John_Smith@foo-bar.com\"\n}\n"],["body","\n"],["body","The above example produces the following terms:"],["body","\n"],["body","[ john, smith, foo, bar, com ]\n"],["body","\n"],["headingLink","camelcase-tokenizer"],["heading","CamelCase tokenizer"],["body","\n"],["body","以下更复杂的示例将CamelCase文本拆分为令牌:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"camel\": {\n          \"type\": \"pattern\",\n          \"pattern\": \"([^\\\\p{L}\\\\d]+)|(?<=\\\\D)(?=\\\\d)|(?<=\\\\d)(?=\\\\D)|(?<=[\\\\p{L}&&[^\\\\p{Lu}]])(?=\\\\p{Lu})|(?<=\\\\p{Lu})(?=\\\\p{Lu}[\\\\p{L}&&[^\\\\p{Lu}]])\"\n        }\n      }\n    }\n  }\n}\n\nGET my-index-000001/_analyze\n{\n  \"analyzer\": \"camel\",\n  \"text\": \"MooseX::FTPClass2_beta\"\n}\n"],["body","\n"],["body","The above example produces the following terms:"],["body","\n"],["body","[ moose, x, ftp, class, 2, beta ]\n"],["body","\n"],["body","The regex above is easier to understand as:"],["body","\n"],["body","  ([^\\p{L}\\d]+)                 # swallow non letters and numbers,\n| (?<=\\D)(?=\\d)                 # or non-number followed by number,\n| (?<=\\d)(?=\\D)                 # or number followed by non-number,\n| (?<=[ \\p{L} && [^\\p{Lu}]])    # or lower case\n  (?=\\p{Lu})                    #   followed by upper case,\n| (?<=\\p{Lu})                   # or upper case\n  (?=\\p{Lu}                     #   followed by upper case\n    [\\p{L}&&[^\\p{Lu}]]          #   then lower case\n  )\n"],["body","\n"],["headingLink","definition"],["heading","Definition"],["body","\n"],["body","The pattern anlayzer consists of:"],["body","\n\n"],["body","\n"],["body","Tokenizer"],["body","\n"],["body","Pattern Tokenizer"],["body","\n"],["body","\n"],["body","\n"],["body","Token Filters"],["body","\n"],["body","Lower Case Token Filter"],["body","\n"],["body","Stop Token Filter (disabled by default)"],["body","\n"],["body","\n\n"],["body","If you need to customize the pattern analyzer beyond the configuration parameters then you need to recreate it as a custom analyzer and modify it, usually by adding token filters. This would recreate the built-in pattern analyzer and you can use it as a starting point for further customization:"],["body","\n"],["body","PUT /pattern_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"tokenizer\": {\n        \"split_on_non_word\": {\n          \"type\":       \"pattern\",\n          \"pattern\":    \"\\\\W+\"  //The default pattern is `\\W+` which splits on non-word characters and this is where you’d change it.\n        }\n      },\n      \"analyzer\": {\n        \"rebuilt_pattern\": {\n          \"tokenizer\": \"split_on_non_word\",\n          \"filter\": [\n            \"lowercase\"       \n          ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/3.Built-inAnalyzerReference/3.KeywordAnalyzer.html"],["title","KeywordAnalyzer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","keyword-analyzer"],["heading","Keyword analyzer"],["body","\n"],["body","The keyword analyzer is a “noop” analyzer which returns the entire input string as a single token."],["body","\n"],["headingLink","example-output"],["heading","Example output"],["body","\n"],["body","POST _analyze\n{\n  \"analyzer\": \"keyword\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n"],["body","\n"],["body","The above sentence would produce the following single term:"],["body","\n"],["body","[ The 2 QUICK Brown-Foxes jumped over the lazy dog's bone. ]\n"],["body","\n"],["headingLink","configuration"],["heading","Configuration"],["body","\n"],["body","The keyword analyzer is not configurable."],["body","\n"],["headingLink","definition"],["heading","Definition"],["body","\n"],["body","The keyword analyzer consists of:"],["body","\n\n"],["body","\n"],["body","Tokenizer"],["body","\n"],["body","Keyword Tokenizer"],["body","\n"],["body","\n\n"],["body","If you need to customize the keyword analyzer then you need to recreate it as a custom analyzer and modify it, usually by adding token filters. Usually, you should prefer the Keyword type when you want strings that are not split into tokens, but just in case you need it, this would recreate the built-in keyword analyzer and you can use it as a starting point for further customization:"],["body","\n"],["body","PUT /keyword_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"rebuilt_keyword\": {\n          \"tokenizer\": \"keyword\",\n          \"filter\": [         \n          ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/3.Built-inAnalyzerReference/6.SimpleAnalyzer.html"],["title","SimpleAnalyzer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","simple-analyzer"],["heading","Simple analyzer"],["body","\n"],["body","The simple analyzer breaks text into tokens at any non-letter character, such as numbers, spaces, hyphens and apostrophes, discards non-letter characters, and changes uppercase to lowercase."],["body","\n"],["body","简单分析器在任何非字母字符 (例如数字，空格，连字符和撇号) 处将文本分解为令牌，丢弃非字母字符，并将大写改为小写。"],["body","\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","POST _analyze\n{\n  \"analyzer\": \"simple\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n"],["body","\n"],["body","The simple analyzer parses the sentence and produces the following tokens:"],["body","\n"],["body","[ the, quick, brown, foxes, jumped, over, the, lazy, dog, s, bone ]\n"],["body","\n"],["headingLink","definition"],["heading","Definition"],["body","\n"],["body","The simple analyzer is defined by one tokenizer:"],["body","\n\n"],["body","\n"],["body","Tokenizer"],["body","\n"],["body","Lowercase Tokenizer"],["body","\n"],["body","\n\n"],["headingLink","customize"],["heading","Customize"],["body","\n"],["body","To customize the simple analyzer, duplicate it to create the basis for a custom analyzer. This custom analyzer can be modified as required, usually by adding token filters."],["body","\n"],["body","PUT /my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_custom_simple_analyzer\": {\n          \"tokenizer\": \"lowercase\",\n          \"filter\": [                          \n          ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/3.Built-inAnalyzerReference/4.LanguageAnalyzers.html"],["title","LanguageAnalyzers.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","language-analyzers"],["heading","Language analyzers"],["body","\n"],["body","一套旨在分析特定语言文本的分析器。支持以下类型: arabic, armenian, basque, bengali, brazilian, bulgarian, catalan, cjk, czech, danish, dutch, english, estonian, finnish, french, galician, german, greek, hindi, hungarian, indonesian, irish, italian, latvian, lithuanian, norwegian, persian, portuguese, romanian, russian, sorani, spanish, swedish, turkish, thai."],["body","\n"],["headingLink","configuring-language-analyzers"],["heading","Configuring language analyzers"],["body","\n"],["headingLink","stopwords"],["heading","Stopwords"],["body","\n"],["body","所有分析器都支持在内部配置 设置自定义停用词，或者通过设置stopwords_path使用外部停用词文件。Check Stop Analyzer for more details."],["body","\n"],["headingLink","excluding-words-from-stemming"],["heading","Excluding words from stemming"],["body","\n\n"],["body","\n"],["body","stem_exclusion参数允许您指定不应被词干化的小写单词数组"],["body","\n"],["body","\n"],["body","\n"],["body","在内部，此功能是通过添加 keyword_marker token filter with the keywords set to the value of the stem_exclusion parameter."],["body","\n"],["body","\n"],["body","\n"],["body","以下分析器支持设置自定义 stem_exclusion 列表"],["body","\n"],["body","\n\n"],["body","arabic, armenian, basque, bengali, bulgarian, catalan, czech, dutch, english, finnish, french, galician, german, hindi, hungarian, indonesian, irish, italian, latvian, lithuanian, norwegian, portuguese, romanian, russian, sorani, spanish, swedish, turkish."],["body","\n"],["headingLink","reimplementing-language-analyzers"],["heading","Reimplementing language analyzers"],["body","\n"],["body","The built-in language analyzers can be reimplemented as custom analyzers (as described below) in order to customize their behaviour."],["body","\n"],["body","If you do not intend to exclude words from being stemmed (the equivalent of the stem_exclusion parameter above), then you should remove the keyword_marker token filter from the custom analyzer configuration."],["body","\n"],["headingLink","cjk-analyzer"],["heading","cjk analyzer"],["body","\n"],["body","您可能会发现在  ICU analysis plugin中的   icu_analyzer 中 对于CJK文本比cjk分析器更好"],["body","\n"],["body","The cjk analyzer could be reimplemented as a custom analyzer as follows:"],["body","\n"],["body","PUT /cjk_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"filter\": {\n        \"english_stop\": {\n          \"type\":       \"stop\",\n          \"stopwords\":  [ \n            \"a\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\",\n            \"if\", \"in\", \"into\", \"is\", \"it\", \"no\", \"not\", \"of\", \"on\",\n            \"or\", \"s\", \"such\", \"t\", \"that\", \"the\", \"their\", \"then\",\n            \"there\", \"these\", \"they\", \"this\", \"to\", \"was\", \"will\",\n            \"with\", \"www\"\n          ]\n        }\n      },\n      \"analyzer\": {\n        \"rebuilt_cjk\": {\n          \"tokenizer\":  \"standard\",\n          \"filter\": [\n            \"cjk_width\",\n            \"lowercase\",\n            \"cjk_bigram\",\n            \"english_stop\"\n          ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","可以使用stopwords或stopwords _ path参数来覆盖默认的stopwords。默认的停止词与 _english _ set几乎相同，但不完全相同。"],["body","\n"],["headingLink","english-analyzer"],["heading","english analyzer"],["body","\n"],["body","The english analyzer could be reimplemented as a custom analyzer as follows:"],["body","\n"],["body","PUT /english_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"filter\": {\n        \"english_stop\": {\n          \"type\":       \"stop\",\n          \"stopwords\":  \"_english_\" \n        },\n        \"english_keywords\": {\n          \"type\":       \"keyword_marker\",\n          \"keywords\":   [\"example\"] \n        },\n        \"english_stemmer\": {\n          \"type\":       \"stemmer\",\n          \"language\":   \"english\"\n        },\n        \"english_possessive_stemmer\": {\n          \"type\":       \"stemmer\",\n          \"language\":   \"possessive_english\"\n        }\n      },\n      \"analyzer\": {\n        \"rebuilt_english\": {\n          \"tokenizer\":  \"standard\",\n          \"filter\": [\n            \"english_possessive_stemmer\",\n            \"lowercase\",\n            \"english_stop\",\n            \"english_keywords\",\n            \"english_stemmer\"\n          ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/README.html"],["title","TextAnalysis - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","text-analysis"],["heading","Text analysis"],["body","\n"],["body","文本分析 是将非结构化文本 (例如电子邮件正文或产品说明) 转换为针对搜索进行优化的结构化格式的过程。"],["body","\n"],["headingLink","when-to-configure-text-analysis"],["heading","When to configure text analysis"],["body","\n"],["body","Elasticsearch在索引或搜索文本字段  text  时执行文本分析。"],["body","\n"],["headingLink","in-this-section"],["heading","In this section"],["body","\n\n"],["body","Overview"],["body","\n"],["body","Concepts"],["body","\n"],["body","Configure text analysis"],["body","\n"],["body","Built-in analyzer reference"],["body","\n"],["body","Tokenizer reference"],["body","\n"],["body","Token filter reference"],["body","\n"],["body","Character filters reference"],["body","\n"],["body","Normalizers"],["body","\n\n"],["headingLink","text-analysis-overview"],["heading","Text analysis overview"],["body","\n\n"],["body","Text analysis  使得 Elasticsearch 执行 全文索引，以返回所有相关结果。而不是精确匹配"],["body","\n"],["body","例如 你搜索  Quick fox jumps 。你可能想要  文档包含 A quick brown fox jumps over the lazy dog 但是仍然 需要包含  fast fox or foxes leap 的 文档"],["body","\n\n"],["headingLink","tokenization"],["heading","Tokenization"],["body","\n\n"],["body","通过 tokenization 去做全文搜索。将文章 分解成小块。这种小块又称作  token"],["body","\n"],["body","如果 把 the quick brown fox jumps  当做 单字符串 则 搜索  quick fox 不会匹配"],["body","\n"],["body","如果 把其中的每个单词都 分词。则 搜索  quick fox, fox brown会匹配"],["body","\n\n"],["headingLink","normalization"],["heading","Normalization"],["body","\n"],["body","Tokenization 可以使得 我们对 每个词项都匹配，但是每个词项还是按字面上匹配 这意味着"],["body","\n\n"],["body","大小写：搜索Quick不会匹配quick，即使您可能希望这两者相互匹配"],["body","\n"],["body","词根：Although fox and foxes share the same root word, a search for foxes would not match fox or vice versa."],["body","\n"],["body","词义：A search for jumps would not match leaps. While they don’t share a root word, they are synonyms and have a similar meaning."],["body","\n\n"],["body","为了解决这些问题，文本分析可以将这些token normalize 为标准格式"],["body","\n"],["body","这允许您匹配与搜索词不完全相同但足够相似 所以仍然要相关的token。例如:"],["body","\n"],["body","This allows you to match tokens that are not exactly the same as the search terms, but similar enough to still be relevant. For example:"],["body","\n\n"],["body","大小写：Quick can be lowercased: quick."],["body","\n"],["body","词根：foxes can be stemmed, or reduced to its root word: fox."],["body","\n"],["body","同义词：jump and leap are synonyms and can be indexed as a single word: jump."],["body","\n\n"],["body","确保搜索词与预期的这些单词匹配,您可以将相同的 tokenization和 normalize 规则应用于查询字符串"],["body","\n"],["body","例如，Foxes leap 被正规化为 fox jump"],["body","\n"],["headingLink","customize-text-analysis"],["heading","Customize text analysis"],["body","\n\n"],["body","\n"],["body","文本分析 由 分析器analyzer 执行 管理整个过程的一套规则。"],["body","\n"],["body","\n"],["body","\n"],["body","Elasticsearch 有一个默认的 标准分析器 standard analyzer,"],["body","\n"],["body","\n"],["body","\n"],["body","如果你想定制你的搜索体验，可以选择不同的 内置分析器 built-in analyzer   或者 自定义分析器 configure a custom one."],["body","\n"],["body","\n"],["body","\n"],["body","自定义分析器可让您控制分析过程的每个步骤，包括:"],["body","\n\n"],["body","\n"],["body","before_token:Changes to the text before tokenization"],["body","\n"],["body","\n"],["body","\n"],["body","token：How text is converted to tokens"],["body","\n"],["body","\n"],["body","\n"],["body","Normalization：changes made to tokens before indexing or search"],["body","\n"],["body","\n\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/5.TokenFilterReference/6.ConditionalTokenFilter.html"],["title","ConditionalTokenFilter.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","conditional-token-filter"],["heading","Conditional token filter"],["body","\n"],["body","Applies a set of token filters to tokens that match conditions in a provided predicate script."],["body","\n"],["body","This filter uses Lucene’s ConditionalTokenFilter."],["body","\n"],["body","Conditional token filter 是一个用于将一组词元过滤器应用于符合提供的谓词脚本中的条件的词元的过滤器，它使用 Lucene 的 ConditionalTokenFilter¹。"],["body","\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","The following analyze API request uses the condition filter to match tokens with fewer than 5 characters in THE QUICK BROWN FOX. It then applies the lowercase filter to those matching tokens, converting them to lowercase."],["body","\n"],["body","GET /_analyze\n{\n  \"tokenizer\": \"standard\",\n  \"filter\": [\n    {\n      \"type\": \"condition\",\n      \"filter\": [ \"lowercase\" ],\n      \"script\": {\n        \"source\": \"token.getTerm().length() < 5\"\n      }\n    }\n  ],\n  \"text\": \"THE QUICK BROWN FOX\"\n}\n"],["body","\n"],["body","The filter produces the following tokens:"],["body","\n"],["body","[ the, QUICK, BROWN, fox ]\n"],["body","\n"],["headingLink","configurable-parameters"],["heading","Configurable parameters"],["body","\n\n"],["body","\n"],["body","filter"],["body","\n"],["body","(Required, array of token filters) Array of token filters. If a token matches the predicate script in the script parameter, these filters are applied to the token in the order provided.These filters can include custom token filters defined in the index mapping."],["body","\n"],["body","\n"],["body","\n"],["body","script"],["body","\n"],["body","(Required, script object) Predicate script used to apply token filters. If a token matches this script, the filters in the filter parameter are applied to the token.For valid parameters, see How to write scripts. Only inline scripts are supported. Painless scripts are executed in the analysis predicate context and require a token property."],["body","\n"],["body","\n\n"],["headingLink","customize-and-add-to-an-analyzer"],["heading","Customize and add to an analyzer"],["body","\n"],["body","To customize the condition filter, duplicate it to create the basis for a new custom token filter. You can modify the filter using its configurable parameters."],["body","\n"],["body","For example, the following create index API request uses a custom condition filter to configure a new custom analyzer. The custom condition filter matches the first token in a stream. It then reverses that matching token using the reverse filter."],["body","\n"],["body","PUT /palindrome_list\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"whitespace_reverse_first_token\": {\n          \"tokenizer\": \"whitespace\",\n          \"filter\": [ \"reverse_first_token\" ]\n        }\n      },\n      \"filter\": {\n        \"reverse_first_token\": {\n          \"type\": \"condition\",\n          \"filter\": [ \"reverse\" ],\n          \"script\": {\n            \"source\": \"token.getPosition() === 0\"\n          }\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/5.TokenFilterReference/3.CJKWidthTokenFilter.html"],["title","CJKWidthTokenFilter.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","cjk-width-token-filter"],["heading","CJK width token filter"],["body","\n"],["body","CJK width token filter 是一个用于处理中日韩字符宽度差异的过滤器"],["body","\n"],["body","Normalizes width differences in CJK (Chinese, Japanese, and Korean) characters as follows:"],["body","\n\n"],["body","将全角ASCII字符变体折叠为等效的基本拉丁字符"],["body","\n"],["body","将半角片假名字符变体( half-width Katakana character variants)折叠成等效的 Kana characters"],["body","\n\n"],["body","This filter is included in Elasticsearch’s built-in CJK language analyzer. It uses Lucene’s CJKWidthFilter."],["body","\n"],["body","This token filter can be viewed as a subset of NFKC/NFKD Unicode normalization. See the analysis-icu plugin for full normalization support."],["body","\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","GET /_analyze\n{\n  \"tokenizer\" : \"standard\",\n  \"filter\" : [\"cjk_width\"],\n  \"text\" : \"ｼｰｻｲﾄﾞﾗｲﾅｰ\"\n}\n"],["body","\n"],["body","The filter produces the following token:"],["body","\n"],["body","シーサイドライナー\n"],["body","\n"],["headingLink","add-to-an-analyzer"],["heading","Add to an analyzer"],["body","\n"],["body","The following create index API request uses the CJK width token filter to configure a new custom analyzer."],["body","\n"],["body","PUT /cjk_width_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"standard_cjk_width\": {\n          \"tokenizer\": \"standard\",\n          \"filter\": [ \"cjk_width\" ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/5.TokenFilterReference/20.TokenFilter.html"],["title","TokenFilter.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","hunspell-token-filter"],["heading","Hunspell token filter"],["body","\n\n"],["body","基于 Hunspell的词典的token替换"],["body","\n\n"],["headingLink","hyphenation-decompounder-token-filter"],["heading","Hyphenation decompounder token filter"],["body","\n"],["body","基于XML的子串查找"],["body","\n"],["headingLink","keep-types-token-filter"],["heading","Keep types token filter"],["body","\n"],["body","保留特定类型的token"],["body","\n"],["headingLink","keep-words-token-filter"],["heading","Keep words token filter"],["body","\n"],["body","只保留指定的单词"],["body","\n"],["headingLink","keywordmarker"],["heading","keywordMarker"],["body","\n"],["body","标记为 keyword 。使得不进行 词根化"],["body","\n"],["headingLink","keywordrepeat"],["heading","keywordRepeat"],["body","\n"],["headingLink","kstream"],["heading","KStream"],["body","\n"],["body","基于 kstream 的词干化过滤器"],["body","\n"],["headingLink","length-token-filter"],["heading","Length token filter"],["body","\n"],["body","移除不在指定长度的token"],["body","\n"],["headingLink","limit-token-count-token-filter"],["heading","Limit token count token filter"],["body","\n"],["body","显示输出的token总数"],["body","\n"],["headingLink","lowercase"],["heading","lowercase"],["body","\n"],["body","将token小写"],["body","\n"],["headingLink","minhash"],["heading","MinHash"],["body","\n"],["body","Uses the MinHash technique to produce a signature for a token stream."],["body","\n"],["headingLink","n-gram-token-filter"],["heading","N-gram token filter"],["body","\n"],["body","产生N-gram"],["body","\n"],["headingLink","pattern-capture-token-filter"],["heading","Pattern capture token filter"],["body","\n"],["body","基于正则的捕获"],["body","\n"],["headingLink","pattern-replace-token-filter"],["heading","Pattern replace token filter"],["body","\n"],["body","基于正则的token替换"],["body","\n"],["headingLink","porter-stem-token-filter"],["heading","Porter stem token filter"],["body","\n"],["body","Provides algorithmic stemming for the English language, based on the Porter stemming algorithm."],["body","\n"],["headingLink","predicate-script-token-filter"],["heading","Predicate script token filter"],["body","\n"],["body","移除不满足指定 script的token"],["body","\n"],["headingLink","remove-duplicates-token-filter"],["heading","Remove duplicates token filter"],["body","\n"],["body","移除重复token"],["body","\n"],["headingLink","reverse-token-filter"],["heading","Reverse token filter"],["body","\n"],["body","反转token本身"],["body","\n"],["headingLink","shingle-token-filter"],["heading","Shingle token filter"],["body","\n"],["body","基于单词的 n-gram"],["body","\n"],["headingLink","snowball-token-filter"],["heading","Snowball token filter"],["body","\n"],["body","词干过滤器"],["body","\n"],["headingLink","stemmer-token-filter"],["heading","Stemmer token filter"],["body","\n"],["body","基于各种语音的词干过滤器"],["body","\n"],["headingLink","stemmer-override-token-filter"],["heading","Stemmer override token filter"],["body","\n"],["body","覆盖词干算法"],["body","\n"],["headingLink","stop-token-filter"],["heading","Stop token filter"],["body","\n"],["body","停用词 过滤"],["body","\n"],["headingLink","synonym-token-filter"],["heading","Synonym token filter"],["body","\n"],["body","同义词过滤器"],["body","\n"],["headingLink","synonym-graph-token-filter"],["heading","Synonym graph token filter"],["body","\n"],["body","multi-word synonyms correctly during the analysis process."],["body","\n"],["headingLink","trim-token-filter"],["heading","Trim token filter"],["body","\n"],["body","移除前导后置空格"],["body","\n"],["headingLink","truncate-token-filter"],["heading","Truncate token filter"],["body","\n"],["body","截断超长的token"],["body","\n"],["headingLink","unique-token-filter"],["heading","Unique token filter"],["body","\n"],["body","移除重复的token"],["body","\n"],["headingLink","uppercase-token-filter"],["heading","Uppercase token filter"],["body","\n"],["body","token大写"],["body","\n"],["headingLink","word-delimiter-token-filter"],["heading","Word delimiter token filter"],["body","\n"],["body","基于非字符切割词项"],["body","\n"],["headingLink","word-delimiter-graph-token-filter"],["heading","Word delimiter graph token filter"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/5.TokenFilterReference/9.DictionaryDecompounderFokenFilter.html"],["title","DictionaryDecompounderFokenFilter.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","dictionary-decompounder-token-filter"],["heading","Dictionary decompounder token filter"],["body","\n\n"],["body","推荐更快的 hyphenation_decompounder token filter in place of this filter."],["body","\n"],["body","However, you can use the dictionary_decompounder filter to check the quality of a word list before implementing it in the hyphenation_decompounder filter."],["body","\n"],["body","使用指定的单词列表和蛮力方法在复合词中查找子单词。如果找到，这些子词将包含在令牌输出中。"],["body","\n"],["body","This filter uses Lucene’s DictionaryCompoundWordTokenFilter, which was built for Germanic languages."],["body","\n\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","The following analyze API request uses the dictionary_decompounder filter to find subwords in Donaudampfschiff."],["body","\n"],["body","The filter then checks these subwords against the specified list of words: Donau, dampf, meer, and schiff."],["body","\n"],["body","GET _analyze\n{\n  \"tokenizer\": \"standard\",\n  \"filter\": [\n    {\n      \"type\": \"dictionary_decompounder\",\n      \"word_list\": [\"Donau\", \"dampf\", \"meer\", \"schiff\"]\n    }\n  ],\n  \"text\": \"Donaudampfschiff\"\n}\n"],["body","\n"],["body","The filter produces the following tokens:"],["body","\n"],["body","[ Donaudampfschiff, Donau, dampf, schiff ]\n"],["body","\n"],["headingLink","configurable-parameters"],["heading","Configurable parameters"],["body","\n\n"],["body","\n"],["body","word_list"],["body","\n"],["body","(Required*, array of strings) A list of subwords to look for in the token stream. If found, the subword is included in the token output.Either this parameter or word_list_path must be specified."],["body","\n"],["body","\n"],["body","\n"],["body","word_list_path"],["body","\n"],["body","(Required*, string) Path to a file that contains a list of subwords to find in the token stream. If found, the subword is included in the token output.This path must be absolute or relative to the config location, and the file must be UTF-8 encoded. Each token in the file must be separated by a line break.Either this parameter or word_list must be specified."],["body","\n"],["body","\n"],["body","\n"],["body","max_subword_size"],["body","\n"],["body","(Optional, integer) Maximum subword character length. Longer subword tokens are excluded from the output. Defaults to 15."],["body","\n"],["body","\n"],["body","\n"],["body","min_subword_size"],["body","\n"],["body","(Optional, integer) Minimum subword character length. Shorter subword tokens are excluded from the output. Defaults to 2."],["body","\n"],["body","\n"],["body","\n"],["body","min_word_size"],["body","\n"],["body","(Optional, integer) Minimum word character length. Shorter word tokens are excluded from the output. Defaults to 5."],["body","\n"],["body","\n"],["body","\n"],["body","only_longest_match"],["body","\n"],["body","(Optional, Boolean) If true, only include the longest matching subword. Defaults to false."],["body","\n"],["body","\n\n"],["headingLink","customize-and-add-to-an-analyzer"],["heading","Customize and add to an analyzer"],["body","\n"],["body","To customize the dictionary_decompounder filter, duplicate it to create the basis for a new custom token filter. You can modify the filter using its configurable parameters."],["body","\n"],["body","For example, the following create index API request uses a custom dictionary_decompounder filter to configure a new custom analyzer."],["body","\n"],["body","The custom dictionary_decompounder filter find subwords in the analysis/example_word_list.txt file. Subwords longer than 22 characters are excluded from the token output."],["body","\n"],["body","PUT dictionary_decompound_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"standard_dictionary_decompound\": {\n          \"tokenizer\": \"standard\",\n          \"filter\": [ \"22_char_dictionary_decompound\" ]\n        }\n      },\n      \"filter\": {\n        \"22_char_dictionary_decompound\": {\n          \"type\": \"dictionary_decompounder\",\n          \"word_list_path\": \"analysis/example_word_list.txt\",\n          \"max_subword_size\": 22\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/5.TokenFilterReference/2.CJKBigramTokenFilter.html"],["title","CJKBigramTokenFilter.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","cjk-bigram-token-filter"],["heading","CJK bigram token filter"],["body","\n"],["body","CJK bigram token filter是一种用于处理中文、日文和韩文（CJK）的分词过滤器，\n它可以将由标准分词器或ICU分词器生成的CJK单词组合成二元组。例如，如果一个CJK单词是“你好”，那么它会被分割成“你”和“好”，然后再组合成“你好”作为一个二元组。默认情况下，当一个CJK字符没有相邻的字符来形成二元组时，它会以单字形式输出¹²。如果你想要同时输出单字和二元组，你可以设置output_unigrams参数为true¹²。"],["body","\n\n"],["body","\n"],["body","Forms bigrams out of CJK (Chinese, Japanese, and Korean) tokens."],["body","\n"],["body","\n"],["body","\n"],["body","This filter is included in Elasticsearch’s built-in CJK language analyzer. It uses Lucene’s CJKBigramFilter."],["body","\n"],["body","\n\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","The following analyze API request demonstrates how the CJK bigram token filter works."],["body","\n"],["body","GET /_analyze\n{\n  \"tokenizer\" : \"standard\",\n  \"filter\" : [\"cjk_bigram\"],\n  \"text\" : \"東京都は、日本の首都であり\"\n}\n"],["body","\n"],["body","The filter produces the following tokens:"],["body","\n"],["body","[ 東京, 京都, 都は, 日本, 本の, の首, 首都, 都で, であ, あり ]\n"],["body","\n"],["headingLink","add-to-an-analyzer"],["heading","Add to an analyzer"],["body","\n"],["body","The following create index API request uses the CJK bigram token filter to configure a new custom analyzer."],["body","\n"],["body","PUT /cjk_bigram_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"standard_cjk_bigram\": {\n          \"tokenizer\": \"standard\",\n          \"filter\": [ \"cjk_bigram\" ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","configurable-parameters"],["heading","Configurable parameters"],["body","\n\n"],["body","\n"],["body","ignored_scripts"],["body","\n"],["body","(Optional, array of character scripts) Array of character scripts for which to disable bigrams. Possible values"],["body","\n\n"],["body","han"],["body","\n"],["body","hangul"],["body","\n"],["body","hiragana"],["body","\n"],["body","katakana"],["body","\n\n"],["body","所有非CJK的字符都不会被修改"],["body","\n"],["body","\n"],["body","\n"],["body","output_unigrams"],["body","\n\n"],["body","(Optional, Boolean) If true, emit tokens in both bigram and unigram form."],["body","\n"],["body","If false, a CJK character is output in unigram form when it has no adjacent characters. Defaults to false."],["body","\n\n"],["body","\n\n"],["headingLink","customize"],["heading","Customize"],["body","\n"],["body","要自定义CJK bigram  token filte。您可以使用其可配置参数修改过滤器。"],["body","\n"],["body","PUT /cjk_bigram_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"han_bigrams\": {\n          \"tokenizer\": \"standard\",\n          \"filter\": [ \"han_bigrams_filter\" ]\n        }\n      },\n      \"filter\": {\n        \"han_bigrams_filter\": {\n          \"type\": \"cjk_bigram\",\n          \"ignored_scripts\": [\n            \"hangul\",\n            \"hiragana\",\n            \"katakana\"\n          ],\n          \"output_unigrams\": true\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/5.TokenFilterReference/README.html"],["title","TokenFilterReference - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","token-filter-reference"],["heading","Token filter reference"],["body","\n"],["body","Token filters accept a stream of tokens from a tokenizer and can modify tokens (eg lowercasing), delete tokens (eg remove stopwords) or add tokens (eg synonyms)."],["body","\n"],["body","Elasticsearch has a number of built-in token filters you can use to build custom analyzers."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/5.TokenFilterReference/10.EdgeN-gramTokenFilter.html"],["title","EdgeN-gramTokenFilter.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","edge-n-gram-token-filter"],["heading","Edge n-gram token filter"],["body","\n\n"],["body","\n"],["body","Forms an n-gram of a specified length from the beginning of a token."],["body","\n"],["body","\n"],["body","\n"],["body","For example, you can use the edge_ngram token filter to change quick to qu."],["body","\n"],["body","\n"],["body","\n"],["body","When not customized, the filter creates 1-character edge n-grams by default."],["body","\n"],["body","\n"],["body","\n"],["body","This filter uses Lucene’s EdgeNGramTokenFilter."],["body","\n"],["body","\n"],["body","\n"],["body","The edge_ngram filter is similar to the ngram token filter. However, the edge_ngram only outputs n-grams that start at the beginning of a token. These edge n-grams are useful for search-as-you-type queries."],["body","\n"],["body","\n\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","The following analyze API request uses the edge_ngram filter to convert the quick brown fox jumps to 1-character and 2-character edge n-grams:"],["body","\n"],["body","GET _analyze\n{\n  \"tokenizer\": \"standard\",\n  \"filter\": [\n    { \"type\": \"edge_ngram\",\n      \"min_gram\": 1,\n      \"max_gram\": 2\n    }\n  ],\n  \"text\": \"the quick brown fox jumps\"\n}\n"],["body","\n"],["body","Copy as curlView in Console"],["body","\n"],["body","The filter produces the following tokens:"],["body","\n"],["body","[ t, th, q, qu, b, br, f, fo, j, ju ]\n"],["body","\n"],["headingLink","add-to-an-analyzer"],["heading","Add to an analyzer"],["body","\n"],["body","The following create index API request uses the edge_ngram filter to configure a new custom analyzer."],["body","\n"],["body","PUT edge_ngram_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"standard_edge_ngram\": {\n          \"tokenizer\": \"standard\",\n          \"filter\": [ \"edge_ngram\" ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","Copy as curlView in Console"],["body","\n"],["headingLink","configurable-parameters"],["heading","Configurable parameters"],["body","\n\n"],["body","\n"],["body","max_gram"],["body","\n"],["body","(Optional, integer) Maximum character length of a gram. For custom token filters, defaults to 2. For the built-in edge_ngram filter, defaults to 1.See Limitations of the max_gram parameter."],["body","\n"],["body","\n"],["body","\n"],["body","min_gram"],["body","\n"],["body","(Optional, integer) Minimum character length of a gram. Defaults to 1."],["body","\n"],["body","\n"],["body","\n"],["body","preserve_original"],["body","\n"],["body","(Optional, Boolean) Emits original token when set to true. Defaults to false."],["body","\n"],["body","\n"],["body","\n"],["body","side"],["body","\n"],["body","(Optional, string) Deprecated. Indicates whether to truncate tokens from the front or back. Defaults to front.Instead of using the back value, you can use the reverse token filter before and after the edge_ngram filter to achieve the same results."],["body","\n"],["body","\n\n"],["headingLink","customize"],["heading","Customize"],["body","\n"],["body","To customize the edge_ngram filter, duplicate it to create the basis for a new custom token filter. You can modify the filter using its configurable parameters."],["body","\n"],["body","For example, the following request creates a custom edge_ngram filter that forms n-grams between 3-5 characters."],["body","\n"],["body","PUT edge_ngram_custom_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"default\": {\n          \"tokenizer\": \"whitespace\",\n          \"filter\": [ \"3_5_edgegrams\" ]\n        }\n      },\n      \"filter\": {\n        \"3_5_edgegrams\": {\n          \"type\": \"edge_ngram\",\n          \"min_gram\": 3,\n          \"max_gram\": 5\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","limitations-of-the-max_gram-parameter"],["heading","Limitations of the max_gram parameter"],["body","\n"],["body","The edge_ngram filter’s max_gram value limits the character length of tokens. When the edge_ngram filter is used with an index analyzer, this means search terms longer than the max_gram length may not match any indexed terms."],["body","\n"],["body","For example, if the max_gram is 3, searches for apple won’t match the indexed term app."],["body","\n"],["body","To account for this, you can use the truncate filter with a search analyzer to shorten search terms to the max_gram character length. However, this could return irrelevant results."],["body","\n"],["body","For example, if the max_gram is 3 and search terms are truncated to three characters, the search term apple is shortened to app. This means searches for apple return any indexed terms matching app, such as apply, snapped, and apple."],["body","\n"],["body","We recommend testing both approaches to see which best fits your use case and desired search experience."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/5.TokenFilterReference/8.DelimitedPayloadTokenFilter.html"],["title","DelimitedPayloadTokenFilter.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","delimited-payload-token-filter"],["heading","Delimited payload token filter"],["body","\n\n"],["body","\n"],["body","The older name delimited_payload_filter is deprecated and should not be used with new indices. Use delimited_payload instead."],["body","\n"],["body","\n"],["body","\n"],["body","根据指定的分隔符将token 流分为token 和有效负载。"],["body","\n"],["body","\n"],["body","\n"],["body","For example, you can use the delimited_payload filter with a | delimiter to split the|1 quick|2 fox|3 into the tokens the, quick, and fox with respective payloads of 1, 2, and 3.This filter uses Lucene’s DelimitedPayloadTokenFilter."],["body","\n"],["body","\n\n"],["headingLink","payloads"],["heading","Payloads"],["body","\n\n"],["body","\n"],["body","有效载荷是与token 位置相关联并存储为base64-encoded字节的用户定义的二进制数据。"],["body","\n"],["body","\n"],["body","\n"],["body","Elasticsearch does not store token payloads by default. To store payloads, you must:"],["body","\n\n"],["body","\n"],["body","Set the term_vector mapping parameter to with_positions_payloads or with_positions_offsets_payloads for any field storing payloads."],["body","\n"],["body","\n"],["body","\n"],["body","Use an index analyzer that includes the delimited_payload filter"],["body","\n"],["body","\n"],["body","\n"],["body","You can view stored payloads using the term vectors API."],["body","\n"],["body","\n\n"],["body","\n\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","The following analyze API request uses the delimited_payload filter with the default | delimiter to split the|0 brown|10 fox|5 is|0 quick|10 into tokens and payloads."],["body","\n"],["body","GET _analyze\n{\n  \"tokenizer\": \"whitespace\",\n  \"filter\": [\"delimited_payload\"],\n  \"text\": \"the|0 brown|10 fox|5 is|0 quick|10\"\n}\n"],["body","\n"],["body","The filter produces the following tokens:"],["body","\n"],["body","[ the, brown, fox, is, quick ]\n"],["body","\n"],["body","Note that the analyze API does not return stored payloads. For an example that includes returned payloads, see Return stored payloads."],["body","\n"],["headingLink","add-to-an-analyzer"],["heading","Add to an analyzer"],["body","\n"],["body","The following create index API request uses the delimited-payload filter to configure a new custom analyzer."],["body","\n"],["body","PUT delimited_payload\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"whitespace_delimited_payload\": {\n          \"tokenizer\": \"whitespace\",\n          \"filter\": [ \"delimited_payload\" ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","Copy as curlView in Console"],["body","\n"],["headingLink","configurable-parameters"],["heading","Configurable parameters"],["body","\n\n"],["body","\n"],["body","delimiter"],["body","\n"],["body","(Optional, string) Character used to separate tokens from payloads. Defaults to |."],["body","\n"],["body","\n"],["body","\n"],["body","encoding"],["body","\n"],["body","(Optional, string) Data type for the stored payload. Valid values are:float(Default) Float**identityCharactersint**Integer"],["body","\n"],["body","\n\n"],["headingLink","customize-and-add-to-an-analyzer"],["heading","Customize and add to an analyzer"],["body","\n"],["body","To customize the delimited_payload filter, duplicate it to create the basis for a new custom token filter. You can modify the filter using its configurable parameters."],["body","\n"],["body","For example, the following create index API request uses a custom delimited_payload filter to configure a new custom analyzer. The custom delimited_payload filter uses the + delimiter to separate tokens from payloads. Payloads are encoded as integers."],["body","\n"],["body","PUT delimited_payload_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"whitespace_plus_delimited\": {\n          \"tokenizer\": \"whitespace\",\n          \"filter\": [ \"plus_delimited\" ]\n        }\n      },\n      \"filter\": {\n        \"plus_delimited\": {\n          \"type\": \"delimited_payload\",\n          \"delimiter\": \"+\",\n          \"encoding\": \"int\"\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","return-stored-payloads"],["heading","Return stored payloads"],["body","\n"],["body","Use the create index API to create an index that:"],["body","\n\n"],["body","Includes a field that stores term vectors with payloads."],["body","\n"],["body","Uses a custom index analyzer with the delimited_payload filter."],["body","\n\n"],["body","PUT text_payloads\n{\n  \"mappings\": {\n    \"properties\": {\n      \"text\": {\n        \"type\": \"text\",\n        \"term_vector\": \"with_positions_payloads\",\n        \"analyzer\": \"payload_delimiter\"\n      }\n    }\n  },\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"payload_delimiter\": {\n          \"tokenizer\": \"whitespace\",\n          \"filter\": [ \"delimited_payload\" ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","Add a document containing payloads to the index."],["body","\n"],["body","POST text_payloads/_doc/1\n{\n  \"text\": \"the|0 brown|3 fox|4 is|0 quick|10\"\n}\n"],["body","\n"],["body","Use the term vectors API to return the document’s tokens and base64-encoded payloads."],["body","\n"],["body","GET text_payloads/_termvectors/1\n{\n  \"fields\": [ \"text\" ],\n  \"payloads\": true\n}\n"],["body","\n"],["body","The API returns the following response:"],["body","\n"],["body","{\n  \"_index\": \"text_payloads\",\n  \"_type\": \"_doc\",\n  \"_id\": \"1\",\n  \"_version\": 1,\n  \"found\": true,\n  \"took\": 8,\n  \"term_vectors\": {\n    \"text\": {\n      \"field_statistics\": {\n        \"sum_doc_freq\": 5,\n        \"doc_count\": 1,\n        \"sum_ttf\": 5\n      },\n      \"terms\": {\n        \"brown\": {\n          \"term_freq\": 1,\n          \"tokens\": [\n            {\n              \"position\": 1,\n              \"payload\": \"QEAAAA==\"\n            }\n          ]\n        },\n        \"fox\": {\n          \"term_freq\": 1,\n          \"tokens\": [\n            {\n              \"position\": 2,\n              \"payload\": \"QIAAAA==\"\n            }\n          ]\n        },\n        \"is\": {\n          \"term_freq\": 1,\n          \"tokens\": [\n            {\n              \"position\": 3,\n              \"payload\": \"AAAAAA==\"\n            }\n          ]\n        },\n        \"quick\": {\n          \"term_freq\": 1,\n          \"tokens\": [\n            {\n              \"position\": 4,\n              \"payload\": \"QSAAAA==\"\n            }\n          ]\n        },\n        \"the\": {\n          \"term_freq\": 1,\n          \"tokens\": [\n            {\n              \"position\": 0,\n              \"payload\": \"AAAAAA==\"\n            }\n          ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/5.TokenFilterReference/13.FlattenGraphTokenFilter.html"],["title","FlattenGraphTokenFilter.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","flatten-graph-token-filter"],["heading","Flatten graph token filter"],["body","\n"],["body","Flatten graph token filter 是一种用于将由 graph token filter（如 synonym_graph 或 word_delimiter_graph）生成的 token graph 平铺的分词过滤器。平铺含有多位置 token 的 token graph 可以使其适合索引。否则，索引不支持含有多位置 token 的 token graph。平铺图形是一个有损的过程"],["body","\n"],["body","Flattens a token graph produced by a graph token filter, such as synonym_graph or word_delimiter_graph."],["body","\n"],["body","Flattening a token graph containing multi-position tokens makes the graph suitable for indexing. Otherwise, indexing does not support token graphs containing multi-position tokens."],["body","\n"],["body","Flattening graphs is a lossy process."],["body","\n"],["body","If possible, avoid using the flatten_graph filter. Instead, use graph token filters in search analyzers only. This eliminates the need for the flatten_graph filter."],["body","\n"],["body","The flatten_graph filter uses Lucene’s FlattenGraphFilter."],["body","\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","To see how the flatten_graph filter works, you first need to produce a token graph containing multi-position tokens."],["body","\n"],["body","The following analyze API request uses the synonym_graph filter to add dns as a multi-position synonym for domain name system in the text domain name system is fragile:"],["body","\n"],["body","GET /_analyze\n{\n  \"tokenizer\": \"standard\",\n  \"filter\": [\n    {\n      \"type\": \"synonym_graph\",\n      \"synonyms\": [ \"dns, domain name system\" ]\n    }\n  ],\n  \"text\": \"domain name system is fragile\"\n}\n"],["body","\n"],["body","The filter produces the following token graph with dns as a multi-position token."],["body","\n"],["body","\n"],["body","Indexing does not support token graphs containing multi-position tokens. To make this token graph suitable for indexing, it needs to be flattened."],["body","\n"],["body","To flatten the token graph, add the flatten_graph filter after the synonym_graph filter in the previous analyze API request."],["body","\n"],["body","GET /_analyze\n{\n  \"tokenizer\": \"standard\",\n  \"filter\": [\n    {\n      \"type\": \"synonym_graph\",\n      \"synonyms\": [ \"dns, domain name system\" ]\n    },\n    \"flatten_graph\"\n  ],\n  \"text\": \"domain name system is fragile\"\n}\n"],["body","\n"],["body","The filter produces the following flattened token graph, which is suitable for indexing."],["body","\n"],["body","\n"],["headingLink","add-to-an-analyzer"],["heading","Add to an analyzer"],["body","\n"],["body","The following create index API request uses the flatten_graph token filter to configure a new custom analyzer."],["body","\n"],["body","In this analyzer, a custom word_delimiter_graph filter produces token graphs containing catenated, multi-position tokens. The flatten_graph filter flattens these token graphs, making them suitable for indexing."],["body","\n"],["body","PUT /my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_custom_index_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"my_custom_word_delimiter_graph_filter\",\n            \"flatten_graph\"\n          ]\n        }\n      },\n      \"filter\": {\n        \"my_custom_word_delimiter_graph_filter\": {\n          \"type\": \"word_delimiter_graph\",\n          \"catenate_all\": true\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/5.TokenFilterReference/11.ElisionFokenFilter.html"],["title","ElisionFokenFilter.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","elision-token-filter"],["heading","Elision token filter"],["body","\n\n"],["body","Removes specified elisions from the beginning of tokens. For example, you can use this filter to change l'avion to avion."],["body","\n"],["body","When not customized, the filter removes the following French elisions by default:"],["body","\n"],["body","Elision token filter 是一种用于移除词首的省略符号的分词过滤器。例如，您可以使用这个过滤器将法语中的 l’avion 变成 avion 123。这个过滤器可以配置一个包含省略符号的 token map13。默认情况下，这个过滤器会移除以下几种法语的省略符号3:"],["body","\n\n"],["body","l'`, `m'`, `t'`, `qu'`, `n'`, `s'`, `j'`, `d'`, `c'`, `jusqu'`, `quoiqu'`, `lorsqu'`, `puisqu'\n"],["body","\n"],["body","Customized versions of this filter are included in several of Elasticsearch’s built-in language analyzers:"],["body","\n\n"],["body","Catalan analyzer"],["body","\n"],["body","French analyzer"],["body","\n"],["body","Irish analyzer"],["body","\n"],["body","Italian analyzer"],["body","\n\n"],["body","This filter uses Lucene’s ElisionFilter."],["body","\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","The following analyze API request uses the elision filter to remove j' from j’examine près du wharf:"],["body","\n"],["body","GET _analyze\n{\n  \"tokenizer\" : \"standard\",\n  \"filter\" : [\"elision\"],\n  \"text\" : \"j’examine près du wharf\"\n}\n"],["body","\n"],["body","Copy as curlView in Console"],["body","\n"],["body","The filter produces the following tokens:"],["body","\n"],["body","[ examine, près, du, wharf ]\n"],["body","\n"],["headingLink","add-to-an-analyzer"],["heading","Add to an analyzer"],["body","\n"],["body","The following create index API request uses the elision filter to configure a new custom analyzer."],["body","\n"],["body","PUT /elision_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"whitespace_elision\": {\n          \"tokenizer\": \"whitespace\",\n          \"filter\": [ \"elision\" ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","Copy as curlView in Console"],["body","\n"],["headingLink","configurable-parameters"],["heading","Configurable parameters"],["body","\n\n"],["body","\n"],["body","articles"],["body","\n"],["body","(Required*, array of string) List of elisions to remove.To be removed, the elision must be at the beginning of a token and be immediately followed by an apostrophe. Both the elision and apostrophe are removed.For custom elision filters, either this parameter or articles_path must be specified."],["body","\n"],["body","\n"],["body","\n"],["body","articles_path"],["body","\n"],["body","(Required*, string) Path to a file that contains a list of elisions to remove.This path must be absolute or relative to the config location, and the file must be UTF-8 encoded. Each elision in the file must be separated by a line break.To be removed, the elision must be at the beginning of a token and be immediately followed by an apostrophe. Both the elision and apostrophe are removed.For custom elision filters, either this parameter or articles must be specified."],["body","\n"],["body","\n"],["body","\n"],["body","articles_case"],["body","\n"],["body","(Optional, Boolean) If true, elision matching is case insensitive. If false, elision matching is case sensitive. Defaults to false."],["body","\n"],["body","\n\n"],["headingLink","customize"],["heading","Customize"],["body","\n"],["body","To customize the elision filter, duplicate it to create the basis for a new custom token filter. You can modify the filter using its configurable parameters."],["body","\n"],["body","For example, the following request creates a custom case-insensitive elision filter that removes the l', m', t', qu', n', s', and j' elisions:"],["body","\n"],["body","PUT /elision_case_insensitive_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"default\": {\n          \"tokenizer\": \"whitespace\",\n          \"filter\": [ \"elision_case_insensitive\" ]\n        }\n      },\n      \"filter\": {\n        \"elision_case_insensitive\": {\n          \"type\": \"elision\",\n          \"articles\": [ \"l\", \"m\", \"t\", \"qu\", \"n\", \"s\", \"j\" ],\n          \"articles_case\": true\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/5.TokenFilterReference/4.ClassicTokenFilter.html"],["title","ClassicTokenFilter.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","classic-token-filter"],["heading","Classic token filter"],["body","\n"],["body","Performs optional post-processing of terms generated by the classic tokenizer."],["body","\n"],["body","This filter removes the english possessive ('s) from the end of words and removes dots from acronyms. It uses Lucene’s ClassicFilter."],["body","\n"],["body","Classic token filter 是一个用于对 classic tokenizer 生成的词元进行可选的后处理的过滤器，它可以去除单词末尾的英语所有格 ('s) 和缩略词中的点号¹²。它使用 Lucene 的 ClassicFilter¹。"],["body","\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","The following analyze API request demonstrates how the classic token filter works."],["body","\n"],["body","GET /_analyze\n{\n  \"tokenizer\" : \"classic\",\n  \"filter\" : [\"classic\"],\n  \"text\" : \"The 2 Q.U.I.C.K. Brown-Foxes jumped over the lazy dog's bone.\"\n}\n"],["body","\n"],["body","The filter produces the following tokens:"],["body","\n"],["body","[ The, 2, QUICK, Brown, Foxes, jumped, over, the, lazy, dog, bone ]\n"],["body","\n"],["headingLink","add-to-an-analyzer"],["heading","Add to an analyzer"],["body","\n"],["body","The following create index API request uses the classic token filter to configure a new custom analyzer."],["body","\n"],["body","PUT /classic_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"classic_analyzer\": {\n          \"tokenizer\": \"classic\",\n          \"filter\": [ \"classic\" ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/5.TokenFilterReference/7.DecimalDigitTokenFilter.html"],["title","DecimalDigitTokenFilter.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","decimal-digit-token-filter"],["heading","Decimal digit token filter"],["body","\n\n"],["body","Converts all digits in the Unicode Decimal_Number General Category to 0-9."],["body","\n"],["body","For example, the filter changes the Bengali numeral ৩ to 3."],["body","\n"],["body","This filter uses Lucene’s DecimalDigitFilter."],["body","\n\n"],["body","Decimal digit token filter 是一个用于将 Unicode Decimal_Number (Unicode 中的一个字符类别，它包含了十进制数字)一般类别中的所有数字转换为 0-9 的过滤器，例如，过滤器将孟加拉数字 ৩ 更改为 3。此过滤器使用 Lucene 的 DecimalDigitFilter"],["body","\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","The following analyze API request uses the decimal_digit filter to convert Devanagari numerals to 0-9:"],["body","\n"],["body","GET /_analyze\n{\n  \"tokenizer\" : \"whitespace\",\n  \"filter\" : [\"decimal_digit\"],\n  \"text\" : \"१-one two-२ ३\"\n}\n"],["body","\n"],["body","The filter produces the following tokens:"],["body","\n"],["body","[ 1-one, two-2, 3]\n"],["body","\n"],["headingLink","add-to-an-analyzer"],["heading","Add to an analyzer"],["body","\n"],["body","The following create index API request uses the decimal_digit filter to configure a new custom analyzer."],["body","\n"],["body","PUT /decimal_digit_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"whitespace_decimal_digit\": {\n          \"tokenizer\": \"whitespace\",\n          \"filter\": [ \"decimal_digit\" ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/5.TokenFilterReference/1.ASCIIFoldingTokenFilter.html"],["title","ASCIIFoldingTokenFilter.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","ascii-folding-token-filter"],["heading","ASCII folding token filter"],["body","\n\n"],["body","\n"],["body","将不在 Basic Latin Unicode block（ASCII的前127个字目）的字母、数字、符号 转换成 等效的 ASCII 例如  à to a."],["body","\n"],["body","\n"],["body","\n"],["body","This filter uses Lucene’s ASCIIFoldingFilter."],["body","\n"],["body","\n\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","下面使用 asciifolding 过滤器 过滤 其中的变音字符 açaí à la carte:"],["body","\n"],["body","GET /_analyze\n{\n  \"tokenizer\" : \"standard\",\n  \"filter\" : [\"asciifolding\"],\n  \"text\" : \"açaí à la carte\"\n}\n"],["body","\n"],["body","The filter produces the following tokens:"],["body","\n"],["body","[ acai, a, la, carte ]\n"],["body","\n"],["headingLink","add-to-an-analyzer"],["heading","Add to an analyzer"],["body","\n"],["body","The following create index API request uses the asciifolding filter to configure a new custom analyzer."],["body","\n"],["body","PUT /asciifold_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"standard_asciifolding\": {\n          \"tokenizer\": \"standard\",\n          \"filter\": [ \"asciifolding\" ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","configurable-parameters"],["heading","Configurable parameters"],["body","\n\n"],["body","\n"],["body","preserve_original"],["body","\n"],["body","(Optional, Boolean) If true, emit both original tokens and folded tokens. Defaults to false."],["body","\n"],["body","\n\n"],["headingLink","customize"],["heading","Customize"],["body","\n"],["body","To customize the asciifolding filter, duplicate it to create the basis for a new custom token filter. You can modify the filter using its configurable parameters."],["body","\n"],["body","For example, the following request creates a custom asciifolding filter with preserve_original set to true:"],["body","\n"],["body","PUT /asciifold_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"standard_asciifolding\": {\n          \"tokenizer\": \"standard\",\n          \"filter\": [ \"my_ascii_folding\" ]\n        }\n      },\n      \"filter\": {\n        \"my_ascii_folding\": {\n          \"type\": \"asciifolding\",\n          \"preserve_original\": true\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/5.TokenFilterReference/5.CommonGramsTokenFilter.html"],["title","CommonGramsTokenFilter.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","common-grams-token-filter"],["heading","Common grams token filter"],["body","\n"],["body","Generates bigrams for a specified set of common words."],["body","\n"],["body","For example, you can specify is and the as common words. This filter then converts the tokens [the, quick, fox, is, brown] to [the, the_quick, quick, fox, fox_is, is, is_brown, brown]."],["body","\n"],["body","You can use the common_grams filter in place of the stop token filter when you don’t want to completely ignore common words."],["body","\n"],["body","This filter uses Lucene’s CommonGramsFilter."],["body","\n"],["body","Common grams token filter 是一个用于为指定的一组常用词生成双字词的过滤器，例如，你可以指定 is 和 the 为常用词。这个过滤器会将词元 [the, quick, fox, is, brown] 转换为 [the, the_quick, quick, fox, fox_is, is, is_brown, brown]。你可以在不想完全忽略常用词的情况下，使用 common_grams 过滤器代替 stop token filter³。"],["body","\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","The following analyze API request creates bigrams for is and the:"],["body","\n"],["body","GET /_analyze\n{\n  \"tokenizer\" : \"whitespace\",\n  \"filter\" : [\n    {\n      \"type\": \"common_grams\",\n      \"common_words\": [\"is\", \"the\"]\n    }\n  ],\n  \"text\" : \"the quick fox is brown\"\n}\n"],["body","\n"],["body","The filter produces the following tokens:"],["body","\n"],["body","[ the, the_quick, quick, fox, fox_is, is, is_brown, brown ]\n"],["body","\n"],["headingLink","add-to-an-analyzer"],["heading","Add to an analyzer"],["body","\n"],["body","The following create index API request uses the common_grams filter to configure a new custom analyzer:"],["body","\n"],["body","PUT /common_grams_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"index_grams\": {\n          \"tokenizer\": \"whitespace\",\n          \"filter\": [ \"common_grams\" ]\n        }\n      },\n      \"filter\": {\n        \"common_grams\": {\n          \"type\": \"common_grams\",\n          \"common_words\": [ \"a\", \"is\", \"the\" ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","Copy as curlView in Console"],["body","\n"],["headingLink","configurable-parameters"],["heading","Configurable parameters"],["body","\n\n"],["body","\n"],["body","common_words"],["body","\n"],["body","(Required*, array of strings) A list of tokens. The filter generates bigrams for these tokens.Either this or the common_words_path parameter is required."],["body","\n"],["body","\n"],["body","\n"],["body","common_words_path"],["body","\n"],["body","(Required*, string) Path to a file containing a list of tokens. The filter generates bigrams for these tokens.This path must be absolute or relative to the config location. The file must be UTF-8 encoded. Each token in the file must be separated by a line break.Either this or the common_words parameter is required."],["body","\n"],["body","\n"],["body","\n"],["body","ignore_case"],["body","\n"],["body","(Optional, Boolean) If true, matches for common words matching are case-insensitive. Defaults to false."],["body","\n"],["body","\n"],["body","\n"],["body","query_mode"],["body","\n"],["body","(Optional, Boolean) If true, the filter excludes the following tokens from the output:Unigrams for common wordsUnigrams for terms followed by common wordsDefaults to false. We recommend enabling this parameter for search analyzers.For example, you can enable this parameter and specify is and the as common words. This filter converts the tokens [the, quick, fox, is, brown] to [the_quick, quick, fox_is, is_brown,]."],["body","\n"],["body","\n\n"],["headingLink","customize"],["heading","Customize"],["body","\n"],["body","To customize the common_grams filter, duplicate it to create the basis for a new custom token filter. You can modify the filter using its configurable parameters."],["body","\n"],["body","For example, the following request creates a custom common_grams filter with ignore_case and query_mode set to true:"],["body","\n"],["body","PUT /common_grams_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"index_grams\": {\n          \"tokenizer\": \"whitespace\",\n          \"filter\": [ \"common_grams_query\" ]\n        }\n      },\n      \"filter\": {\n        \"common_grams_query\": {\n          \"type\": \"common_grams\",\n          \"common_words\": [ \"a\", \"is\", \"the\" ],\n          \"ignore_case\": true,\n          \"query_mode\": true\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/5.TokenFilterReference/12.FingerprintFokenFilter.html"],["title","FingerprintFokenFilter.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","fingerprint-token-filter"],["heading","Fingerprint token filter"],["body","\n"],["body","从token流中排序并删除重复的令牌，然后将该流 输出为单个token。"],["body","\n"],["body","For example, this filter changes the [ the, fox, was, very, very, quick ] token stream as follows:"],["body","\n\n"],["body","Sorts the tokens alphabetically to [ fox, quick, the, very, very, was ]"],["body","\n"],["body","Removes a duplicate instance of the very token."],["body","\n"],["body","Concatenates the token stream to a output single token: [fox quick the very was ]"],["body","\n\n"],["body","Output tokens produced by this filter are useful for fingerprinting and clustering a body of text as described in the OpenRefine project."],["body","\n"],["body","This filter uses Lucene’s FingerprintFilter."],["body","\n"],["body","Fingerprint token filter 是一种用于对 token 流进行排序、去重和连接的分词过滤器。例如，这个过滤器可以将 [ the, fox, was, very, very, quick ] 的 token 流变成 [ fox quick the very was ]¹²。这个过滤器可以配置一些参数，如连接符号、最大输出长度和停用词表¹。Elasticsearch 中提供了一个内置的 fingerprint 分析器，使用了 fingerprint token filter 和 standard tokenizer¹。"],["body","\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","The following analyze API request uses the fingerprint filter to create a single output token for the text zebra jumps over resting resting dog:"],["body","\n"],["body","GET _analyze\n{\n  \"tokenizer\" : \"whitespace\",\n  \"filter\" : [\"fingerprint\"],\n  \"text\" : \"zebra jumps over resting resting dog\"\n}\n"],["body","\n"],["body","The filter produces the following token:"],["body","\n"],["body","[ dog jumps over resting zebra ]\n"],["body","\n"],["headingLink","add-to-an-analyzer"],["heading","Add to an analyzer"],["body","\n"],["body","The following create index API request uses the fingerprint filter to configure a new custom analyzer."],["body","\n"],["body","PUT fingerprint_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"whitespace_fingerprint\": {\n          \"tokenizer\": \"whitespace\",\n          \"filter\": [ \"fingerprint\" ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","Copy as curlView in Console"],["body","\n"],["headingLink","configurable-parameters"],["heading","Configurable parameters"],["body","\n\n"],["body","\n"],["body","max_output_size"],["body","\n"],["body","(Optional, integer) Maximum character length, including whitespace, of the output token. Defaults to 255. Concatenated tokens longer than this will result in no token output."],["body","\n"],["body","\n"],["body","\n"],["body","separator"],["body","\n"],["body","(Optional, string) Character to use to concatenate the token stream input. Defaults to a space."],["body","\n"],["body","\n\n"],["headingLink","customize"],["heading","Customize"],["body","\n"],["body","To customize the fingerprint filter, duplicate it to create the basis for a new custom token filter. You can modify the filter using its configurable parameters."],["body","\n"],["body","For example, the following request creates a custom fingerprint filter with that use + to concatenate token streams. The filter also limits output tokens to 100 characters or fewer."],["body","\n"],["body","PUT custom_fingerprint_example\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"whitespace_\": {\n          \"tokenizer\": \"whitespace\",\n          \"filter\": [ \"fingerprint_plus_concat\" ]\n        }\n      },\n      \"filter\": {\n        \"fingerprint_plus_concat\": {\n          \"type\": \"fingerprint\",\n          \"max_output_size\": 100,\n          \"separator\": \"+\"\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/6.CharacterFiltersReference/1.HTMLStripCharacterFilter.html"],["title","HTMLStripCharacterFilter.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","html-strip-character-filter"],["heading","HTML strip character filter"],["body","\n\n"],["body","\n"],["body","去除HTML标签、解码URLENCODE"],["body","\n"],["body","\n"],["body","\n"],["body","The html_strip filter uses Lucene’s HTMLStripCharFilter."],["body","\n"],["body","\n\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","The following analyze API request uses the html_strip filter to change the text <p>I'm so <b>happy</b>!</p> to \\nI'm so happy!\\n."],["body","\n"],["body","GET /_analyze\n{\n  \"tokenizer\": \"keyword\",\n  \"char_filter\": [\n    \"html_strip\"\n  ],\n  \"text\": \"<p>I&apos;m so <b>happy</b>!</p>\"\n}\n"],["body","\n"],["body","Copy as curlView in Console"],["body","\n"],["body","The filter produces the following text:"],["body","\n"],["body","[ \\nI'm so happy!\\n ]\n"],["body","\n"],["headingLink","add-to-an-analyzer"],["heading","Add to an analyzer"],["body","\n"],["body","The following create index API request uses the html_strip filter to configure a new custom analyzer."],["body","\n"],["body","PUT /my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"keyword\",\n          \"char_filter\": [\n            \"html_strip\"\n          ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","Copy as curlView in Console"],["body","\n"],["headingLink","configurable-parameters"],["heading","Configurable parameters"],["body","\n\n"],["body","\n"],["body","escaped_tags"],["body","\n"],["body","(Optional, array of strings) Array of HTML elements without enclosing angle brackets (< >). The filter skips these HTML elements when stripping HTML from the text. For example, a value of [ \"p\" ] skips the <p> HTML element."],["body","\n"],["body","\n\n"],["headingLink","customize"],["heading","Customize"],["body","\n"],["body","To customize the html_strip filter, duplicate it to create the basis for a new custom character filter. You can modify the filter using its configurable parameters."],["body","\n"],["body","The following create index API request configures a new custom analyzer using a custom html_strip filter, my_custom_html_strip_char_filter."],["body","\n"],["body","The my_custom_html_strip_char_filter filter skips the removal of the <b> HTML element."],["body","\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"keyword\",\n          \"char_filter\": [\n            \"my_custom_html_strip_char_filter\"\n          ]\n        }\n      },\n      \"char_filter\": {\n        \"my_custom_html_strip_char_filter\": {\n          \"type\": \"html_strip\",\n          \"escaped_tags\": [\n            \"b\"\n          ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/6.CharacterFiltersReference/3.PatternReplaceCharacterFilter.html"],["title","PatternReplaceCharacterFilter.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","pattern-replace-character-filter"],["heading","Pattern replace character filter"],["body","\n"],["body","The pattern_replace character filter uses a regular expression to match characters which should be replaced with the specified replacement string. The replacement string can refer to capture groups in the regular expression."],["body","\n"],["headingLink","beware-of-pathological-regular-expressions"],["heading","Beware of Pathological Regular Expressions"],["body","\n"],["body","The pattern replace character filter uses Java Regular Expressions."],["body","\n"],["body","A badly written regular expression could run very slowly or even throw a StackOverflowError and cause the node it is running on to exit suddenly."],["body","\n"],["body","Read more about pathological regular expressions and how to avoid them."],["body","\n"],["headingLink","configuration"],["heading","Configuration"],["body","\n"],["body","The pattern_replace character filter accepts the following parameters:"],["body","\n"],["body","pattern"],["body","A Java regular expression. Required."],["body","\n"],["body","replacement"],["body","The replacement string, which can reference capture groups using the $1..$9 syntax, as explained here."],["body","\n"],["body","flags"],["body","Java regular expression flags. Flags should be pipe-separated, eg `\"CASE_INSENSITIVE"],["body","\n\n\n"],["headingLink","example-configuration"],["heading","Example configuration"],["body","\n"],["body","In this example, we configure the pattern_replace character filter to replace any embedded dashes in numbers with underscores, i.e 123-456-789 → 123_456_789:"],["body","\n"],["body","PUT my-index-00001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"standard\",\n          \"char_filter\": [\n            \"my_char_filter\"\n          ]\n        }\n      },\n      \"char_filter\": {\n        \"my_char_filter\": {\n          \"type\": \"pattern_replace\",\n          \"pattern\": \"(\\\\d+)-(?=\\\\d)\",\n          \"replacement\": \"$1_\"\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-00001/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"My credit card is 123-456-789\"\n}\n"],["body","\n"],["body","The above example produces the following terms:"],["body","\n"],["body","[ My, credit, card, is, 123_456_789 ]\n"],["body","\n"],["body","Using a replacement string that changes the length of the original text will work for search purposes, but will result in incorrect highlighting, as can be seen in the following example."],["body","\n"],["body","This example inserts a space whenever it encounters a lower-case letter followed by an upper-case letter (i.e. fooBarBaz → foo Bar Baz), allowing camelCase words to be queried individually:"],["body","\n"],["body","PUT my-index-00001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"standard\",\n          \"char_filter\": [\n            \"my_char_filter\"\n          ],\n          \"filter\": [\n            \"lowercase\"\n          ]\n        }\n      },\n      \"char_filter\": {\n        \"my_char_filter\": {\n          \"type\": \"pattern_replace\",\n          \"pattern\": \"(?<=\\\\p{Lower})(?=\\\\p{Upper})\",\n          \"replacement\": \" \"\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"text\": {\n        \"type\": \"text\",\n        \"analyzer\": \"my_analyzer\"\n      }\n    }\n  }\n}\n\nPOST my-index-00001/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"The fooBarBaz method\"\n}\n"],["body","\n"],["body","The above returns the following terms:"],["body","\n"],["body","[ the, foo, bar, baz, method ]\n"],["body","\n"],["body","Querying for bar will find the document correctly, but highlighting on the result will produce incorrect highlights, because our character filter changed the length of the original text:"],["body","\n"],["body","PUT my-index-00001/_doc/1?refresh\n{\n  \"text\": \"The fooBarBaz method\"\n}\n\nGET my-index-00001/_search\n{\n  \"query\": {\n    \"match\": {\n      \"text\": \"bar\"\n    }\n  },\n  \"highlight\": {\n    \"fields\": {\n      \"text\": {}\n    }\n  }\n}\n"],["body","\n"],["body","The output from the above is:"],["body","\n"],["body","{\n  \"timed_out\": false,\n  \"took\": $body.took,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\" : 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\" : {\n        \"value\": 1,\n        \"relation\": \"eq\"\n    },\n    \"max_score\": 0.2876821,\n    \"hits\": [\n      {\n        \"_index\": \"my-index-00001\",\n        \"_type\": \"_doc\",\n        \"_id\": \"1\",\n        \"_score\": 0.2876821,\n        \"_source\": {\n          \"text\": \"The fooBarBaz method\"\n        },\n        \"highlight\": {\n          \"text\": [\n            \"The foo<em>Ba</em>rBaz method\" \n          ]\n        }\n      }\n    ]\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/6.CharacterFiltersReference/2.MappingCharacterFilter.html"],["title","MappingCharacterFilter.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","mapping-character-filter"],["heading","Mapping character filter"],["body","\n\n"],["body","\n"],["body","The mapping character filter accepts a map of keys and values. Whenever it encounters a string of characters that is the same as a key, it replaces them with the value associated with that key."],["body","\n"],["body","\n"],["body","\n"],["body","Matching is greedy; the longest pattern matching at a given point wins. Replacements are allowed to be the empty string."],["body","\n"],["body","\n"],["body","\n"],["body","The mapping filter uses Lucene’s MappingCharFilter."],["body","\n"],["body","\n\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","The following analyze API request uses the mapping filter to convert Hindu-Arabic numerals (٠‎١٢٣٤٥٦٧٨‎٩‎) into their Arabic-Latin equivalents (0123456789), changing the text My license plate is ٢٥٠١٥ to My license plate is 25015."],["body","\n"],["body","GET /_analyze\n{\n  \"tokenizer\": \"keyword\",\n  \"char_filter\": [\n    {\n      \"type\": \"mapping\",\n      \"mappings\": [\n        \"٠ => 0\",\n        \"١ => 1\",\n        \"٢ => 2\",\n        \"٣ => 3\",\n        \"٤ => 4\",\n        \"٥ => 5\",\n        \"٦ => 6\",\n        \"٧ => 7\",\n        \"٨ => 8\",\n        \"٩ => 9\"\n      ]\n    }\n  ],\n  \"text\": \"My license plate is ٢٥٠١٥\"\n}\n"],["body","\n"],["body","Copy as curlView in Console"],["body","\n"],["body","The filter produces the following text:"],["body","\n"],["body","[ My license plate is 25015 ]\n"],["body","\n"],["headingLink","configurable-parameters"],["heading","Configurable parameters"],["body","\n\n"],["body","\n"],["body","mappings"],["body","\n"],["body","(Required*, array of strings) Array of mappings, with each element having the form key => value.Either this or the mappings_path parameter must be specified."],["body","\n"],["body","\n"],["body","\n"],["body","mappings_path"],["body","\n"],["body","(Required*, string) Path to a file containing key => value mappings.This path must be absolute or relative to the config location, and the file must be UTF-8 encoded. Each mapping in the file must be separated by a line break.Either this or the mappings parameter must be specified."],["body","\n"],["body","\n\n"],["headingLink","customize-and-add-to-an-analyzer"],["heading","Customize and add to an analyzer"],["body","\n"],["body","To customize the mappings filter, duplicate it to create the basis for a new custom character filter. You can modify the filter using its configurable parameters."],["body","\n"],["body","The following create index API request configures a new custom analyzer using a custom mappings filter, my_mappings_char_filter."],["body","\n"],["body","The my_mappings_char_filter filter replaces the :) and :( emoticons with a text equivalent."],["body","\n"],["body","PUT /my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"standard\",\n          \"char_filter\": [\n            \"my_mappings_char_filter\"\n          ]\n        }\n      },\n      \"char_filter\": {\n        \"my_mappings_char_filter\": {\n          \"type\": \"mapping\",\n          \"mappings\": [\n            \":) => _happy_\",\n            \":( => _sad_\"\n          ]\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","The following analyze API request uses the custom my_mappings_char_filter to replace :( with _sad_ in the text I'm delighted about it :(."],["body","\n"],["body","GET /my-index-000001/_analyze\n{\n  \"tokenizer\": \"keyword\",\n  \"char_filter\": [ \"my_mappings_char_filter\" ],\n  \"text\": \"I'm delighted about it :(\"\n}\n"],["body","\n"],["body","The filter produces the following text:"],["body","\n"],["body","[ I'm delighted about it _sad_ ]\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/6.CharacterFiltersReference/README.html"],["title","CharacterFiltersReference - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","character-filters-reference"],["heading","Character filters reference"],["body","\n"],["body","Character filters are used to preprocess the stream of characters before it is passed to the tokenizer."],["body","\n"],["body","A character filter receives the original text as a stream of characters and can transform the stream by adding, removing, or changing characters. For instance, a character filter could be used to convert Hindu-Arabic numerals (٠‎١٢٣٤٥٦٧٨‎٩‎) into their Arabic-Latin equivalents (0123456789), or to strip HTML elements like <b> from the stream."],["body","\n"],["body","Elasticsearch has a number of built in character filters which can be used to build custom analyzers."],["body","\n\n"],["body","\n"],["body","HTML Strip Character Filter"],["body","\n"],["body","The html_strip character filter strips out HTML elements like <b> and decodes HTML entities like &."],["body","\n"],["body","\n"],["body","\n"],["body","Mapping Character Filter"],["body","\n"],["body","The mapping character filter replaces any occurrences of the specified strings with the specified replacements."],["body","\n"],["body","\n"],["body","\n"],["body","Pattern Replace Character Filter"],["body","\n"],["body","The pattern_replace character filter replaces any characters matching a regular expression with the specified replacement."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/1.Concepts/README.html"],["title","Concepts - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/1.Concepts/2.IndexAndSearchAnalysis.html"],["title","IndexAndSearchAnalysis.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","index-and-search-analysis"],["heading","Index and search analysis"],["body","\n"],["body","文本分析发生在两个时间:"],["body","\n\n"],["body","\n"],["body","Index time"],["body","\n"],["body","When a document is indexed, any text field values are analyzed."],["body","\n"],["body","\n"],["body","\n"],["body","Search time"],["body","\n"],["body","When running a full-text search on a text field, the query string (the text the user is searching for) is analyzed.Search time is also called query time."],["body","\n"],["body","\n\n"],["body","每次使用的分析器或分析规则集分别称为"],["body","\n\n"],["body","\n"],["body","索引分析器"],["body","\n"],["body","\n"],["body","\n"],["body","搜索分析器。"],["body","\n"],["body","\n\n"],["headingLink","how-the-index-and-search-analyzer-work-together"],["heading","How the index and search analyzer work together"],["body","\n"],["body","在大多数情况下，在索引和搜索时应该使用相同的分析器。这样可以确保将字段的值和查询字符串更改为相同形式的token 。"],["body","\n"],["body","反过来，这确保了令牌在搜索过程中与预期的匹配。"],["body","\n"],["headingLink","when-to-use-a-different-search-analyzer"],["heading","When to use a different search analyzer"],["body","\n"],["body","虽然不太常见，但有时在索引和搜索时使用不同的分析仪是有意义的。"],["body","\n"],["body","要启用此功能，Elasticsearch允许您指定单独的搜索分析器。 specify a separate search analyzer."],["body","\n"],["body","当对 字段值 或者 查询字符串 使用  同一种形式的token  会产生 意料之外的不相关的匹配时，才应指定单独的搜索分析器"],["body","\n"],["headingLink","example"],["heading","Example"],["body","\n\n"],["body","Elasticsearch 经常用来创建仅匹配以提供的前缀开头的单词的搜索引擎"],["body","\n"],["body","例如 tr 应该返回  tram、trope"],["body","\n"],["body","将文档添加到搜索引擎的索引中; 此文档在文本字段中包含一个这样的单词:\n\n"],["body","\"Apple\""],["body","\n\n"],["body","\n\n"],["body","字段的索引分析器将值转换为令牌并对其进行归一化。在这种情况下，每个标记表示单词的潜在前缀:"],["body","\n"],["body","[ a, ap, app, appl, apple]\n"],["body","\n\n"],["body","These tokens are then indexed."],["body","\n"],["body","Later, a user searches the same text field for:"],["body","\n\n"],["body","\"appli\"\n"],["body","\n"],["body","用户希望输入 appli. 不要匹配 apple"],["body","\n"],["body","如果，使用索引分析器 去分析 查询字符串将会产生以下 tokens"],["body","\n"],["body","[ a, ap, app, appl, appli ]\n"],["body","\n"],["body","当Elasticsearch将这些查询字符串令牌与为apple索引的token 进行比较时，它会找到多个匹配项。"],["body","\n"],["body","Token"],["body","appli"],["body","apple"],["body","\n"],["body","a"],["body","X"],["body","X"],["body","\n"],["body","ap"],["body","X"],["body","X"],["body","\n"],["body","app"],["body","X"],["body","X"],["body","\n"],["body","appl"],["body","X"],["body","X"],["body","\n"],["body","appli"],["body","X"],["body","\n\n\n\n"],["body","这意味着搜索将错误地匹配apple。不仅如此，它将匹配以a开头的任何单词。"],["body","\n"],["body","可以通过 指定 不同的 查询分析器来解决这个问题。例如可以指定 将查询字符串 分词成单个 token 的分词器 [ appli ]"],["body","\n"],["body","此查询字符串令牌将仅匹配以appli开头的单词的令牌，从而更好地与用户的搜索期望保持一致。"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/1.Concepts/3.Stemming.html"],["title","Stemming.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","stemming"],["heading","Stemming"],["body","\n\n"],["body","Stemming  是将 一个单词 转换成 它的词干形式的 处理器 。它确保了 单词的各种变体都能在查询中匹配"],["body","\n"],["body","例如 walk walked 它们的 词干是 walk"],["body","\n"],["body","词干依赖于语言，但通常包括 从单词中删除前缀和后缀。"],["body","\n"],["body","如果一个单词的所有变体都简化为相同的词根形式，则它们将正确匹配。"],["body","\n\n"],["headingLink","stemmer-token-filters"],["heading","Stemmer token filters"],["body","\n"],["body","In Elasticsearch, stemming is handled by stemmer token filters. These token filters can be categorized based on how they stem words:"],["body","\n\n"],["body","Algorithmic stemmers, which stem words based on a set of rules"],["body","\n"],["body","Dictionary stemmers, which stem words by looking them up in a dictionary"],["body","\n\n"],["body","由于词干会更改token，因此我们建议在索引和搜索分析期间使用相同的词干过滤器"],["body","\n"],["headingLink","algorithmic-stemmers"],["heading","Algorithmic stemmers"],["body","\n\n"],["body","算法词干器对每个单词应用一系列规则，将其简化为其根形式"],["body","\n"],["body","例如，英语的算法词干可以从复数单词的末尾删除-s和-es后缀。"],["body","\n\n"],["body","算法干算器有几个优点:"],["body","\n\n"],["body","它们几乎不需要设置，通常开箱即用。"],["body","\n"],["body","他们几乎没有内存消耗"],["body","\n"],["body","比字典词根要快 dictionary stemmers."],["body","\n\n"],["body","然而，大多数算法词干者只会更改单词的现有文本，。这意味着它们可能无法很好地处理不包含其根形式的不规则单词，例如:"],["body","\n\n"],["body","be, are, and am"],["body","\n"],["body","mouse and mice"],["body","\n"],["body","foot and feet"],["body","\n\n"],["body","以下 token filters 使用了算法词根"],["body","\n\n"],["body","stemmer, 它为几种语言提供了算法词干，有些语言具有其他变体。"],["body","\n"],["body","kstem, 英语词干器，将算法词干与内置词典相结合。"],["body","\n"],["body","porter_stem, 我们推荐的英语算法词干器。"],["body","\n"],["body","snowball, which uses Snowball-based stemming rules for several languages."],["body","\n\n"],["headingLink","dictionary-stemmers"],["heading","Dictionary stemmers"],["body","\n\n"],["body","\n"],["body","Dictionary stemmers look up words in a provided dictionary, replacing unstemmed word variants with stemmed words from the dictionary."],["body","\n"],["body","\n"],["body","\n"],["body","从理论上讲，字典词干非常适合:"],["body","\n\n"],["body","\n"],["body","不规则的词干"],["body","\n"],["body","\n"],["body","\n"],["body","拼写相似但概念上不相关的单词之间进行辨别，例如:"],["body","\n\n"],["body","organ and organization"],["body","\n"],["body","broker and broken"],["body","\n\n"],["body","\n\n"],["body","\n\n"],["body","在实践中，算法词干者通常优于字典词干者。这是因为字典词干具有以下缺点:"],["body","\n\n"],["body","Dictionary quality\n字典词干器的好坏与其字典相关。为了很好地工作，这些词典必须包括大量的单词，定期更新，并随着语言趋势而变化。通常，当一本字典被提供时，它是不完整的，它的一些条目已经过时了。"],["body","\n"],["body","Size and performance\n\n"],["body","字典词干必须将字典中的所有单词，前缀和后缀加载到内存中，这可能使用大量的RAM。"],["body","\n"],["body","删除前缀和后缀后，低质量的字典也可能效率较低，这可能会大大减慢词干过程。"],["body","\n\n"],["body","\n\n\n"],["body","\n"],["body","You can use the hunspell token filter to perform dictionary stemming."],["body","\n"],["body","\n"],["body","\n"],["body","在使用 hunspell token filter之前， 如果可用，我们建议为您的语言尝试算法词干器"],["body","\n"],["body","\n\n"],["headingLink","control-stemming"],["heading","Control stemming"],["body","\n\n"],["body","\n"],["body","有时词干会产生拼写相似但在概念上不相关的共享词根。"],["body","\n"],["body","\n"],["body","\n"],["body","例如  skies and skiing 的 词干是  ski"],["body","\n"],["body","\n"],["body","\n"],["body","为了防止这种情况并更好地控制词干，您可以使用以下令牌过滤器:"],["body","\n\n"],["body","\n"],["body","stemmer_override, 它允许您定义用于阻止特定令牌的规则。"],["body","\n"],["body","\n"],["body","\n"],["body","keyword_marker, 使得指定的词干作为关键字，Keyword tokens 不会被后续的词干过滤器 词干化"],["body","\n"],["body","\n"],["body","\n"],["body","conditional, 可用于将tokens 标记为 keywords , similar to the keyword_marker filter."],["body","\n"],["body","\n\n"],["body","\n"],["body","\n"],["body","For built-in language analyzers, you also can use the stem_exclusion parameter to specify a list of words that won’t be stemmed."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/1.Concepts/4.TokenGraphs.html"],["title","TokenGraphs.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","token-graphs"],["heading","Token graphs"],["body","\n"],["body","当 tokenizer  将文本转换为 token 流时，它还记录以下内容:"],["body","\n\n"],["body","position：流中 Token 的位置，每个position代表一个Token"],["body","\n"],["body","positionLength：the number of positions that a token spans （token跨越的位置数，一般来说，一个Token只跨越一个位置）"],["body","\n\n\n"],["body","使用以上数据 创建一个有向无环图、可以称作 token grapgh"],["body","\n"],["body","在 词项图中  每个 position 代表一个节点，每个 Token 表示 一条边或者弧，指向下一个 position"],["body","\n\n"],["body","\n"],["headingLink","synonyms"],["heading","Synonyms"],["body","\n\n"],["body","一些 词项过滤器 添加新的 tokens 。例如同义词，这些同义词通常与现有 token 跨越相同的 postion"],["body","\n"],["body","例如 quick 与 fast 他们的 position 都是 0"],["body","\n\n"],["body","\n"],["headingLink","multi-position-tokens"],["heading","Multi-position tokens"],["body","\n\n"],["body","\n"],["body","一些 token filters 可以跨 多个 positions 添加 token"],["body","\n"],["body","\n"],["body","\n"],["body","这些可以包括  multi-word synonyms 的token 、例如 atm 作为 automatic teller machine 的多Token 同义词"],["body","\n"],["body","\n"],["body","\n"],["body","但是，只有某些令牌过滤器 (称为 graph token filters ) 才能准确记录多位置 token 的位置长度。这些过滤器包括:"],["body","\n\n"],["body","\n"],["body","synonym_graph"],["body","\n"],["body","\n"],["body","\n"],["body","word_delimiter_graph"],["body","\n"],["body","\n\n"],["body","\n\n"],["body","一些tokenizers，例如 nori_tokenizer,，也将复合tokens 精确地分解为多位置标记。"],["body","\n"],["body","例如 domain name system  和同义词 dns 都有一个 position 0 。但是 dns 的 positionLength 为3 其他token的 默认 positionLength 默认为1"],["body","\n"],["body","\n"],["headingLink","using-token-graphs-for-search"],["heading","Using token graphs for search"],["body","\n\n"],["body","索引过程会忽略 positionLength 属性。不支持包含 多位置 token 的  token graph"],["body","\n"],["body","然而 查询语句例如 match match_phrase。可以使用这些图从单个查询字符串生成多个子查询。"],["body","\n\n"],["headingLink","example"],["heading","Example"],["body","\n"],["body","A user runs a search for the following phrase using the match_phrase query:"],["body","\n\n"],["body","用户使用match_phrase查询运行以下短语的搜索:"],["body","\n\n"],["body","domain name system is fragile\n"],["body","\n"],["body","在检索阶段，dns, a synonym for domain name system, is added to the query string’s token stream. The dns token has a positionLength of 3."],["body","\n"],["body","\n"],["body","match_phrase查询使用此图生成以下短语的子查询:"],["body","\n"],["body","dns is fragile\ndomain name system is fragile\n"],["body","\n"],["body","这意味着查询匹配包含  dns is fragile or domain name system is fragile."],["body","\n"],["headingLink","invalid-token-graphs"],["heading","Invalid token graphs"],["body","\n"],["body","以下 token filters 可以添加跨越多个位置但仅记录默认位置长度为1的令牌:"],["body","\n\n"],["body","synonym"],["body","\n"],["body","word_delimiter"],["body","\n\n"],["body","这意味着这些过滤器将为包含此类 token 的流生成无效的 token graph"],["body","\n\n"],["body","以下图中，dns 是  domain name system.  的一个 多  position  的 同义词"],["body","\n"],["body","但是，dns 的 默认positionLength值为1，导致图形无效。"],["body","\n\n"],["body","\n"],["body","避免使用无效的 token graph 进行搜索。无效的graph 会导致意外的搜索结果。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/1.Concepts/1.AnatomyOfAnAnalyzer.html"],["title","AnatomyOfAnAnalyzer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","anatomy-of-an-analyzer"],["heading","Anatomy of an analyzer"],["body","\n\n"],["body","\n"],["body","分析器 (无论是内置的还是自定义的) 只是一个包，其中包含三个较低级别的构建块: character filters，tokenizers 和token filters。"],["body","\n"],["body","\n"],["body","\n"],["body","内置的分析器 analyzers  将这些构建块预打包到适用于不同语言和文本类型的分析器中。"],["body","\n"],["body","\n"],["body","\n"],["body","Elasticsearch还公开了各个构建块，以便可以组合它们以定义新的自定义分 custom  析器。"],["body","\n"],["body","\n\n"],["headingLink","character-filters"],["heading","Character filters"],["body","\n\n"],["body","\n"],["body","字符过滤器以字符流的形式接收原始文本，并且可以通过添加，删除或更改字符来转换流。"],["body","\n"],["body","\n"],["body","\n"],["body","例如，字符过滤器可用于将印度教-阿拉伯数字 (0123456789) 转换为阿拉伯语-拉丁语等效项 ()，或从流中剥离HTML元素，例如 <b>。"],["body","\n"],["body","\n"],["body","\n"],["body","分析器可能具有零个或多个字符过滤器，这些字符过滤器按顺序应用。"],["body","\n"],["body","\n\n"],["headingLink","tokenizer"],["heading","Tokenizer"],["body","\n\n"],["body","tokenizer 接收字符流，将其分解为单个标记 (通常是单个单词)，并输出tokens 流。"],["body","\n"],["body","例如， whitespace tokenizer  通过空格分割token，它将 \"Quick brown fox!\" into the terms [Quick, brown, fox!]"],["body","\n"],["body","tokenizer 还负责记录每个词项的顺序或位置以及该术语表示的原始单词的 start and end character offsets"],["body","\n"],["body","一个分析器 必须有一个  tokenizer."],["body","\n\n"],["headingLink","token-filters"],["heading","Token filters"],["body","\n\n"],["body","token filter  接收 token 流，并可以添加、删除或更改 token。"],["body","\n"],["body","例如， lowercase token filter  将所有 token 小写。stop token 过滤所有 常见的 单词 例如：the  synonym  将同义词引入令牌流"],["body","\n"],["body","不允许令牌过滤器更改每个令牌的位置或字符偏移量。"],["body","\n"],["body","一个分析器 可以有0个或者多个  token filters 。它们是按顺序执行"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/4.TokenizerReference/10.PathHierarchyTokenizer.html"],["title","PathHierarchyTokenizer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","path-hierarchy-tokenizer"],["heading","Path hierarchy tokenizer"],["body","\n"],["body","The path_hierarchy tokenizer takes a hierarchical value like a filesystem path, splits on the path separator, and emits a term for each component in the tree."],["body","\n"],["headingLink","example-output"],["heading","Example output"],["body","\n"],["body","POST _analyze\n{\n  \"tokenizer\": \"path_hierarchy\",\n  \"text\": \"/one/two/three\"\n}\n"],["body","\n"],["body","The above text would produce the following terms:"],["body","\n"],["body","[ /one, /one/two, /one/two/three ]\n"],["body","\n"],["headingLink","configuration"],["heading","Configuration"],["body","\n"],["body","The path_hierarchy tokenizer accepts the following parameters:"],["body","\n"],["body","\n"],["body","delimiter"],["body","The character to use as the path separator. Defaults to /."],["body","\n"],["body","replacement"],["body","An optional replacement character to use for the delimiter. Defaults to the delimiter."],["body","\n"],["body","buffer_size"],["body","The number of characters read into the term buffer in a single pass. Defaults to 1024. The term buffer will grow by this size until all the text has been consumed. It is advisable not to change this setting."],["body","\n"],["body","reverse"],["body","If set to true, emits the tokens in reverse order. Defaults to false."],["body","\n"],["body","skip"],["body","The number of initial tokens to skip. Defaults to 0."],["body","\n\n\n"],["headingLink","example-configuration"],["heading","Example configuration"],["body","\n"],["body","In this example, we configure the path_hierarchy tokenizer to split on - characters, and to replace them with /. The first two tokens are skipped:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"my_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"my_tokenizer\": {\n          \"type\": \"path_hierarchy\",\n          \"delimiter\": \"-\",\n          \"replacement\": \"/\",\n          \"skip\": 2\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"one-two-three-four-five\"\n}\n"],["body","\n"],["body","The above example produces the following terms:"],["body","\n"],["body","[ /three, /three/four, /three/four/five ]\n"],["body","\n"],["body","If we were to set reverse to true, it would produce the following:"],["body","\n"],["body","[ one/two/three/, two/three/, three/ ]\n"],["body","\n"],["headingLink","detailed-examples"],["heading","Detailed examples"],["body","\n\n"],["body","\n"],["body","path_hierarchy 分词器 的常见用例是通过文件路径过滤结果"],["body","\n"],["body","\n"],["body","\n"],["body","如果将文件路径与数据一起建立索引，则使用path_hierarchy分词器分析路径可以按文件路径字符串的不同部分过滤结果。"],["body","\n"],["body","\n"],["body","\n"],["body","此示例将索引配置为具有两个自定义分析器"],["body","\n"],["body","\n"],["body","\n"],["body","这些分析器应用于  file_path 字段 （这个字段用于存储文件名）中的 multifields"],["body","\n"],["body","\n"],["body","\n"],["body","两个分析器中的一个使用反向token。然后对一些示例文档进行索引，以表示两个不同用户的照片文件夹中的照片的某些文件路径。"],["body","\n"],["body","\n\n"],["body","PUT file-path-test\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"custom_path_tree\": {\n          \"tokenizer\": \"custom_hierarchy\"\n        },\n        \"custom_path_tree_reversed\": {\n          \"tokenizer\": \"custom_hierarchy_reversed\"\n        }\n      },\n      \"tokenizer\": {\n        \"custom_hierarchy\": {\n          \"type\": \"path_hierarchy\",\n          \"delimiter\": \"/\"\n        },\n        \"custom_hierarchy_reversed\": {\n          \"type\": \"path_hierarchy\",\n          \"delimiter\": \"/\",\n          \"reverse\": \"true\"\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"file_path\": {\n        \"type\": \"text\",\n        \"fields\": {\n          \"tree\": {\n            \"type\": \"text\",\n            \"analyzer\": \"custom_path_tree\"\n          },\n          \"tree_reversed\": {\n            \"type\": \"text\",\n            \"analyzer\": \"custom_path_tree_reversed\"\n          }\n        }\n      }\n    }\n  }\n}\n\nPOST file-path-test/_doc/1\n{\n  \"file_path\": \"/User/alice/photos/2017/05/16/my_photo1.jpg\"\n}\n\nPOST file-path-test/_doc/2\n{\n  \"file_path\": \"/User/alice/photos/2017/05/16/my_photo2.jpg\"\n}\n\nPOST file-path-test/_doc/3\n{\n  \"file_path\": \"/User/alice/photos/2017/05/16/my_photo3.jpg\"\n}\n\nPOST file-path-test/_doc/4\n{\n  \"file_path\": \"/User/alice/photos/2017/05/15/my_photo1.jpg\"\n}\n\nPOST file-path-test/_doc/5\n{\n  \"file_path\": \"/User/bob/photos/2017/05/16/my_photo1.jpg\"\n}\n"],["body","\n"],["body","原始字段相关性"],["body","\n"],["body","这个搜索会匹配所有文档，但是 bob的文档相关性是最高的"],["body","\n"],["body","GET file-path-test/_search\n{\n  \"query\": {\n    \"match\": {\n      \"file_path\": \"/User/bob/photos/2017/05\"\n    }\n  }\n}\n"],["body","\n"],["body","KeyWord字段准确性搜索"],["body","\n"],["body","It’s simple to match or filter documents with file paths that exist within a particular directory using the file_path.tree field."],["body","\n"],["body","GET file-path-test/_search\n{\n  \"query\": {\n    \"term\": {\n      \"file_path.tree\": \"/User/alice/photos/2017/05/16\"\n    }\n  }\n}\n"],["body","\n"],["body","反向token字段深目录检索"],["body","\n"],["body","With the reverse parameter for this tokenizer, it’s also possible to match from the other end of the file path, such as individual file names or a deep level subdirectory. The following example shows a search for all files named my_photo1.jpg within any directory via the file_path.tree_reversed field configured to use the reverse parameter in the mapping."],["body","\n"],["body","GET file-path-test/_search\n{\n  \"query\": {\n    \"term\": {\n      \"file_path.tree_reversed\": {\n        \"value\": \"my_photo1.jpg\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","Viewing the tokens generated with both forward and reverse is instructive in showing the tokens created for the same file path value."],["body","\n"],["body","POST file-path-test/_analyze\n{\n  \"analyzer\": \"custom_path_tree\",\n  \"text\": \"/User/alice/photos/2017/05/16/my_photo1.jpg\"\n}\n\nPOST file-path-test/_analyze\n{\n  \"analyzer\": \"custom_path_tree_reversed\",\n  \"text\": \"/User/alice/photos/2017/05/16/my_photo1.jpg\"\n}\n"],["body","\n"],["body","当与其他类型的搜索相结合时，能够用文件路径进行过滤也很有用，比如这个例子寻找任何16的文件路径，这些路径也必须在爱丽丝的照片目录中。"],["body","\n"],["body","GET file-path-test/_search\n{\n  \"query\": {\n    \"bool\" : {\n      \"must\" : {\n        \"match\" : { \"file_path\" : \"16\" }\n      },\n      \"filter\": {\n        \"term\" : { \"file_path.tree\" : \"/User/alice\" }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/4.TokenizerReference/12.WhitespaceTokenizer.html"],["title","WhitespaceTokenizer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","whitespace-tokenizer"],["heading","Whitespace tokenizer"],["body","\n"],["body","The whitespace tokenizer breaks text into terms whenever it encounters a whitespace character."],["body","\n"],["headingLink","example-output"],["heading","Example output"],["body","\n"],["body","POST _analyze\n{\n  \"tokenizer\": \"whitespace\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n"],["body","\n"],["body","The above sentence would produce the following terms:"],["body","\n"],["body","[ The, 2, QUICK, Brown-Foxes, jumped, over, the, lazy, dog's, bone. ]\n"],["body","\n"],["headingLink","configuration"],["heading","Configuration"],["body","\n"],["body","The whitespace tokenizer accepts the following parameters:"],["body","\n"],["body","\n"],["body","max_token_length"],["body","The maximum token length. If a token is seen that exceeds this length then it is split at max_token_length intervals. Defaults to 255."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/4.TokenizerReference/9.LowercaseTokenizer.html"],["title","LowercaseTokenizer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","lowercase-tokenizer"],["heading","Lowercase tokenizer"],["body","\n\n"],["body","The lowercase tokenizer, like the letter tokenizer breaks text into terms whenever it encounters a character which is not a letter, but it also lowercases all terms."],["body","\n"],["body","It is functionally equivalent to the letter tokenizer combined with the lowercase token filter, but is more efficient as it performs both steps in a single pass."],["body","\n\n"],["headingLink","example-output"],["heading","Example output"],["body","\n"],["body","POST _analyze\n{\n  \"tokenizer\": \"lowercase\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n"],["body","\n"],["body","The above sentence would produce the following terms:"],["body","\n"],["body","[ the, quick, brown, foxes, jumped, over, the, lazy, dog, s, bone ]\n"],["body","\n"],["headingLink","configuration"],["heading","Configuration"],["body","\n"],["body","The lowercase tokenizer is not configurable."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/4.TokenizerReference/README.html"],["title","TokenizerReference - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","tokenizer-reference"],["heading","Tokenizer reference"],["body","\n\n"],["body","\n"],["body","A tokenizer  接收字符流、将其分解为 tokens"],["body","\n"],["body","\n"],["body","\n"],["body","The tokenizer 还负责记录以下信息"],["body","\n\n"],["body","\n"],["body","每个term 的顺序或位置 (used for phrase and word proximity queries)"],["body","\n"],["body","\n"],["body","\n"],["body","Start and end character offsets of the original word which the term represents (used for highlighting search snippets)."],["body","\n"],["body","\n"],["body","\n"],["body","Token type, a classification of each term produced, such as <ALPHANUM>, <HANGUL>, or <NUM>. Simpler analyzers only produce the word token type."],["body","\n"],["body","\n"],["body","\n"],["body","Token type型，产生的每个词项的分类，如 <ALPHANUM> 、 <HANGUL> 或 <NUM>。更简单的分析器只产生 word token type."],["body","\n"],["body","\n\n"],["body","\n"],["body","\n"],["body","Elasticsearch具有许多内置的tokenizer，可用于构建自定义分析器。"],["body","\n"],["body","\n\n"],["headingLink","word-oriented-tokenizers"],["heading","Word Oriented Tokenizers"],["body","\n"],["body","以下 tokenizer  通常用于将全文分解成 为独立的单词:"],["body","\n\n"],["body","\n"],["body","Standard Tokenizer"],["body","\n"],["body","标准分词器 根据Unicode Text Segmentation algorithm 的定义，根据单词边界划分文本。它去掉了大多数标点符号。它是大多数语言的最佳选择。"],["body","\n"],["body","\n"],["body","\n"],["body","Letter Tokenizer"],["body","\n"],["body","非字母分词器"],["body","\n"],["body","\n"],["body","\n"],["body","Lowercase Tokenizer"],["body","\n\n"],["body","除了使用非字母分割以外"],["body","\n"],["body","还小写所有的 词项"],["body","\n\n"],["body","\n"],["body","\n"],["body","Whitespace Tokenizer"],["body","\n\n"],["body","空格符分割词项"],["body","\n\n"],["body","\n"],["body","\n"],["body","UAX URL Email Tokenizer"],["body","\n"],["body","The uax_url_email tokenizer is like the standard tokenizer except that it recognises URLs and email addresses as single tokens."],["body","\n\n"],["body","和标准分词器一样"],["body","\n"],["body","只是它将url和电子邮件地址识别为单个标记。"],["body","\n\n"],["body","\n"],["body","\n"],["body","Classic Tokenizer"],["body","\n\n"],["body","基于英文语法的分词器"],["body","\n\n"],["body","\n"],["body","\n"],["body","Thai Tokenizer"],["body","\n"],["body","The thai tokenizer segments Thai text into words."],["body","\n"],["body","\n\n"],["headingLink","partial-word-tokenizers"],["heading","Partial Word Tokenizers"],["body","\n"],["body","These tokenizers break up text or words into small fragments, for partial word matching:"],["body","\n"],["body","这些分词器将文本或单词分解为小片段，以进行部分单词匹配:"],["body","\n\n"],["body","N-Gram Tokenizer\n\n"],["body","ngram分词器 可以指定 分隔符数组。返回 n个单词 ，连续字母的滑动窗口，. quick → [qu, ui, ic, ck]."],["body","\n\n"],["body","\n"],["body","Edge N-Gram Tokenizer\n\n"],["body","The edge_ngram tokenizer can break up text into words when it encounters any of a list of specified characters (e.g. whitespace or punctuation),"],["body","\n"],["body","then it returns n-grams of each word which are anchored to the start of the word, e.g. quick → [q, qu, qui, quic, quick]."],["body","\n\n"],["body","\n\n"],["headingLink","structured-text-tokenizers"],["heading","Structured Text Tokenizers"],["body","\n"],["body","The following tokenizers are usually used with structured text like identifiers, email addresses, zip codes, and paths, rather than with full text:"],["body","\n"],["body","以下分词器通常与结构化文本 (例如标识符，电子邮件地址，邮政编码和路径) 一起使用，而不是与全文一起使用:"],["body","\n\n"],["body","\n"],["body","Keyword Tokenizer"],["body","\n"],["body","The keyword tokenizer is a “noop” tokenizer that accepts whatever text it is given and outputs the exact same text as a single term. It can be combined with token filters like lowercase to normalise the analysed terms."],["body","\n"],["body","\n"],["body","\n"],["body","Pattern Tokenizer"],["body","\n"],["body","The pattern tokenizer uses a regular expression to either split text into terms whenever it matches a word separator, or to capture matching text as terms."],["body","\n"],["body","\n"],["body","\n"],["body","Simple Pattern Tokenizer"],["body","\n"],["body","The simple_pattern tokenizer uses a regular expression to capture matching text as terms."],["body","\n"],["body","It uses a restricted subset of regular expression features and is generally faster than the pattern tokenizer."],["body","\n"],["body","\n"],["body","\n"],["body","Char Group Tokenizer"],["body","\n"],["body","The char_group tokenizer is configurable through sets of characters to split on, which is usually less expensive than running regular expressions."],["body","\n"],["body","\n"],["body","\n"],["body","Simple Pattern Split Tokenizer"],["body","\n"],["body","The simple_pattern_split tokenizer uses the same restricted regular expression subset as the simple_pattern tokenizer, but splits the input at matches rather than returning the matches as terms."],["body","\n"],["body","\n"],["body","\n"],["body","Path Tokenizer"],["body","\n"],["body","The path_hierarchy tokenizer takes a hierarchical value like a filesystem path, splits on the path separator, and emits a term for each component in the tree, e.g. /foo/bar/baz → [/foo, /foo/bar, /foo/bar/baz ]."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/4.TokenizerReference/13.PatternTokenizer.html"],["title","PatternTokenizer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","pattern-tokenizer"],["heading","Pattern tokenizer"],["body","\n\n"],["body","使用正则切分词项或者匹配词项"],["body","\n"],["body","默认 \\W+, 切分词汇"],["body","\n\n"],["headingLink","beware-of-pathological-regular-expressions"],["heading","Beware of Pathological Regular Expressions"],["body","\n\n"],["body","\n"],["body","The pattern tokenizer uses Java Regular Expressions."],["body","\n"],["body","\n"],["body","\n"],["body","A badly written regular expression could run very slowly or even throw a StackOverflowError and cause the node it is running on to exit suddenly."],["body","\n"],["body","\n"],["body","\n"],["body","Read more about pathological regular expressions and how to avoid them."],["body","\n"],["body","\n\n"],["headingLink","example-output"],["heading","Example output"],["body","\n"],["body","POST _analyze\n{\n  \"tokenizer\": \"pattern\",\n  \"text\": \"The foo_bar_size's default is 5.\"\n}\n"],["body","\n"],["body","The above sentence would produce the following terms:"],["body","\n"],["body","[ The, foo_bar_size, s, default, is, 5 ]\n"],["body","\n"],["headingLink","configuration"],["heading","Configuration"],["body","\n"],["body","The pattern tokenizer accepts the following parameters:"],["body","\n"],["body","\n"],["body","pattern"],["body","A Java regular expression, defaults to \\W+."],["body","\n"],["body","flags"],["body","Java regular expression flags. Flags should be pipe-separated, eg `\"CASE_INSENSITIVE"],["body","\n"],["body","group"],["body","Which capture group to extract as tokens. Defaults to -1 (split)."],["body","\n\n\n"],["headingLink","example-configuration"],["heading","Example configuration"],["body","\n"],["body","In this example, we configure the pattern tokenizer to break text into tokens when it encounters commas:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"my_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"my_tokenizer\": {\n          \"type\": \"pattern\",\n          \"pattern\": \",\"\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"comma,separated,values\"\n}\n"],["body","\n"],["body","The above example produces the following terms:"],["body","\n"],["body","[ comma, separated, values ]\n"],["body","\n"],["body","In the next example, we configure the pattern tokenizer to capture values enclosed in double quotes (ignoring embedded escaped quotes \\\"). The regex itself looks like this:"],["body","\n"],["body","\"((?:\\\\\"|[^\"]|\\\\\")*)\"\n"],["body","\n"],["body","And reads as follows:"],["body","\n\n"],["body","A literal \""],["body","\n"],["body","Start capturing:\n\n"],["body","A literal \\\" OR any character except \""],["body","\n"],["body","Repeat until no more characters match"],["body","\n\n"],["body","\n"],["body","A literal closing \""],["body","\n\n"],["body","When the pattern is specified in JSON, the \" and \\ characters need to be escaped, so the pattern ends up looking like:"],["body","\n"],["body","\\\"((?:\\\\\\\\\\\"|[^\\\"]|\\\\\\\\\\\")+)\\\"\nPUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"my_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"my_tokenizer\": {\n          \"type\": \"pattern\",\n          \"pattern\": \"\\\"((?:\\\\\\\\\\\"|[^\\\"]|\\\\\\\\\\\")+)\\\"\",\n          \"group\": 1\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"\\\"value\\\", \\\"value with embedded \\\\\\\" quote\\\"\"\n}\n"],["body","\n"],["body","The above example produces the following two terms:"],["body","\n"],["body","[ value, value with embedded \\\" quote ]\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/4.TokenizerReference/4.SimplePatternSplitTokenizer.html"],["title","SimplePatternSplitTokenizer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","simple-pattern-split-tokenizer"],["heading","Simple pattern split tokenizer"],["body","\n\n"],["body","\n"],["body","The simple_pattern_split tokenizer 使用正则表达式切割 词项"],["body","\n"],["body","\n"],["body","\n"],["body","regular expression 特性 支持受限的集合，但是更快"],["body","\n"],["body","\n"],["body","\n"],["body","This tokenizer uses Lucene regular expressions. For an explanation of the supported features and syntax, see Regular Expression Syntax."],["body","\n"],["body","\n"],["body","\n"],["body","默认模式是空字符串，它会产生一个包含完整输入的项。此分词器 应始终使用非默认模式进行配置。"],["body","\n"],["body","\n\n"],["headingLink","configuration"],["heading","Configuration"],["body","\n"],["body","The simple_pattern_split tokenizer accepts the following parameters:"],["body","\n"],["body","\n"],["body","pattern"],["body","A Lucene regular expression, defaults to the empty string."],["body","\n\n\n"],["headingLink","example-configuration"],["heading","Example configuration"],["body","\n"],["body","This example configures the simple_pattern_split tokenizer to split the input text on underscores."],["body","\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"my_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"my_tokenizer\": {\n          \"type\": \"simple_pattern_split\",\n          \"pattern\": \"_\"\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"an_underscored_phrase\"\n}\n"],["body","\n"],["body","The above example produces these terms:"],["body","\n"],["body","[ an, underscored, phrase ]\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/4.TokenizerReference/3.SimplePattern.html"],["title","SimplePattern.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","simple-pattern-tokenizer"],["heading","Simple pattern tokenizer"],["body","\n\n"],["body","\n"],["body","simple_pattern tokenizer 使用正则表达式来捕获匹配的文本作为词项"],["body","\n"],["body","\n"],["body","\n"],["body","它支持的正则表达式特征集比 pattern tokenizer 更有限，但是tokenization 通常更快"],["body","\n"],["body","\n"],["body","\n"],["body","This tokenizer uses Lucene regular expressions. For an explanation of the supported features and syntax, see Regular Expression Syntax."],["body","\n"],["body","\n"],["body","\n"],["body","默认模式是空字符串，它不产生任何词项"],["body","\n"],["body","\n\n"],["headingLink","configuration"],["heading","Configuration"],["body","\n"],["body","The simple_pattern tokenizer accepts the following parameters:"],["body","\n"],["body","\n"],["body","pattern"],["body","Lucene regular expression, defaults to the empty string."],["body","\n\n\n"],["headingLink","example-configuration"],["heading","Example configuration"],["body","\n"],["body","This example configures the simple_pattern tokenizer to produce terms that are three-digit numbers"],["body","\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"my_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"my_tokenizer\": {\n          \"type\": \"simple_pattern\",\n          \"pattern\": \"[0123456789]{3}\"\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"fd-786-335-514-x\"\n}\n"],["body","\n"],["body","The above example produces these terms:"],["body","\n"],["body","[ 786, 335, 514 ]\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/4.TokenizerReference/1.Ng-gram.html"],["title","Ng-gram.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","n-gram-tokenizer"],["heading","N-gram tokenizer"],["body","\n\n"],["body","\n"],["body","ngram分词器 首先在遇到指定字符列表中的一个时将文本分解为单词，"],["body","\n"],["body","\n"],["body","\n"],["body","然后它针对每个Token 发出指定长度的  N-grams"],["body","\n"],["body","\n"],["body","\n"],["body","N-gram就像在单词上移动的滑动窗口-指定长度的连续字符序列。它们对于查询不使用空格或具有长复合词的语言 (例如德语) 很有用。"],["body","\n"],["body","\n\n"],["headingLink","example-output"],["heading","Example output"],["body","\n"],["body","With the default settings, the ngram tokenizer treats the initial text as a single token and produces N-grams with minimum length 1 and maximum length 2:"],["body","\n"],["body","POST _analyze\n{\n  \"tokenizer\": \"ngram\",\n  \"text\": \"Quick Fox\"\n}\n"],["body","\n"],["body","The above sentence would produce the following terms:"],["body","\n"],["body","[ Q, Qu, u, ui, i, ic, c, ck, k, \"k \", \" \", \" F\", F, Fo, o, ox, x ]\n"],["body","\n"],["headingLink","configuration"],["heading","Configuration"],["body","\n"],["body","The ngram tokenizer accepts the following parameters:"],["body","\n"],["body","\n"],["body","min_gram"],["body","Minimum length of characters in a gram. Defaults to 1."],["body","\n"],["body","max_gram"],["body","Maximum length of characters in a gram. Defaults to 2."],["body","\n"],["body","token_chars"],["body","Character classes that should be included in a token. Elasticsearch will split on characters that don’t belong to the classes specified. Defaults to [] (keep all characters).Character classes may be any of the following:letter —  for example a, b, ï or 京``digit —  for example 3 or 7``whitespace —  for example \" \" or \"\\n\"``punctuation — for example ! or \"``symbol —  for example $ or √``custom —  custom characters which need to be set using the custom_token_chars setting."],["body","\n"],["body","custom_token_chars"],["body","Custom characters that should be treated as part of a token. For example, setting this to +-_ will make the tokenizer treat the plus, minus and underscore sign as part of a token."],["body","\n\n\n"],["body","It usually makes sense to set min_gram and max_gram to the same value. The smaller the length, the more documents will match but the lower the quality of the matches. The longer the length, the more specific the matches. A tri-gram (length 3) is a good place to start."],["body","\n"],["body","The index level setting index.max_ngram_diff controls the maximum allowed difference between max_gram and min_gram."],["body","\n"],["headingLink","example-configuration"],["heading","Example configuration"],["body","\n"],["body","In this example, we configure the ngram tokenizer to treat letters and digits as tokens, and to produce tri-grams (grams of length 3):"],["body","\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"my_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"my_tokenizer\": {\n          \"type\": \"ngram\",\n          \"min_gram\": 3,\n          \"max_gram\": 3,\n          \"token_chars\": [\n            \"letter\",\n            \"digit\"\n          ]\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"2 Quick Foxes.\"\n}\n"],["body","\n"],["body","[ Qui, uic, ick, Fox, oxe, xes ]\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/4.TokenizerReference/14.EdgeN-gramTokenizer.html"],["title","EdgeN-gramTokenizer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","edge-n-gram-tokenizer"],["heading","Edge n-gram tokenizer"],["body","\n\n"],["body","\n"],["body","首先根据 指定分隔符 分割词项"],["body","\n"],["body","\n"],["body","\n"],["body","然后发出 N-grams  ，这些发出的单词的开头 的都是原始单词的开头"],["body","\n"],["body","\n"],["body","\n"],["body","Edge N-Grams are useful for search-as-you-type queries."],["body","\n"],["body","\n"],["body","\n"],["body","When you need search-as-you-type for text which has a widely known order, such as movie or song titles, the completion suggester is a much more efficient choice than edge N-grams. （suggesterAPI）"],["body","\n"],["body","\n"],["body","\n"],["body","Edge N-grams have the advantage when trying to autocomplete words that can appear in any order."],["body","\n"],["body","\n\n"],["headingLink","example-output"],["heading","Example output"],["body","\n"],["body","默认不切割、并且产生最小1最大2的tokens"],["body","\n"],["body","POST _analyze\n{\n  \"tokenizer\": \"edge_ngram\",\n  \"text\": \"Quick Fox\"\n}\n"],["body","\n"],["body","The above sentence would produce the following terms:"],["body","\n"],["body","[ Q, Qu ]\n"],["body","\n"],["body","These default gram lengths are almost entirely useless. You need to configure the edge_ngram before using it."],["body","\n"],["headingLink","configuration"],["heading","Configuration"],["body","\n"],["body","The edge_ngram tokenizer accepts the following parameters:"],["body","\n\n"],["body","\n"],["body","min_gram"],["body","\n"],["body","Minimum length of characters in a gram. Defaults to 1."],["body","\n"],["body","\n"],["body","\n"],["body","max_gram"],["body","\n"],["body","Maximum length of characters in a gram. Defaults to 2.See Limitations of the max_gram parameter."],["body","\n"],["body","\n"],["body","\n"],["body","token_chars"],["body","\n\n"],["body","应该包含在token中的字符类."],["body","\n"],["body","Elasticsearch将对不属于指定类的字符进行拆分，Defaults to [] (keep all characters)."],["body","\n"],["body","Character classes may be any of the following:\n\n"],["body","letter —  for example a, b, ï or 京"],["body","\n"],["body","digit —  for example 3 or 7"],["body","\n"],["body","whitespace —  for example \" \" or `\"\\n\"``"],["body","\n"],["body","punctuation — for example ! or \""],["body","\n"],["body","symbol —  for example $ or √"],["body","\n"],["body","custom —  custom characters which need to be set using the custom_token_chars setting."],["body","\n\n"],["body","\n\n"],["body","\n"],["body","\n"],["body","custom_token_chars"],["body","\n"],["body","Custom characters that should be treated as part of a token. For example, setting this to +-_ will make the tokenizer treat the plus, minus and underscore sign as part of a token."],["body","\n"],["body","\n\n"],["headingLink","limitations-of-the-max_gram-parameter"],["heading","Limitations of the max_gram parameter"],["body","\n"],["body","The edge_ngram tokenizer’s max_gram value limits the character length of tokens."],["body","\n\n"],["body","edge_ngram分词器的max_gram值限制了token的字符长度。"],["body","\n"],["body","当 edge_ngram tokenizer 应用于索引时，大于 max_gram的 terms检索 不会命中任何文档"],["body","\n"],["body","例如，如果max_gram为3，则搜索apple将不匹配索引术语 app,为了解决这个问题，可以在搜索分析器中  使用 truncate token filter。来缩短检索的terms 为 max_gram的长度，当然这回导致 产生不相关的结果"],["body","\n\n"],["headingLink","example-configuration"],["heading","Example configuration"],["body","\n"],["body","In this example, we configure the edge_ngram tokenizer to treat letters and digits as tokens, and to produce grams with minimum length 2 and maximum length 10:"],["body","\n"],["body","PUT my-index-00001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"my_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"my_tokenizer\": {\n          \"type\": \"edge_ngram\",\n          \"min_gram\": 2,\n          \"max_gram\": 10,\n          \"token_chars\": [\n            \"letter\",\n            \"digit\"\n          ]\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-00001/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"2 Quick Foxes.\"\n}\n"],["body","\n"],["body","The above example produces the following terms:"],["body","\n"],["body","[ Qu, Qui, Quic, Quick, Fo, Fox, Foxe, Foxes ]\n"],["body","\n"],["body","Usually we recommend using the same analyzer at index time and at search time. In the case of the edge_ngram tokenizer, the advice is different."],["body","\n\n"],["body","通常 在索引时和 检索时 保持一样的 分析器"],["body","\n"],["body","但是 edge_ngran 分词器 情况不一样，"],["body","\n\n"],["body","Below is an example of how to set up a field for search-as-you-type."],["body","\n"],["body","Note that the max_gram value for the index analyzer is 10, which limits indexed terms to 10 characters. Search terms are not truncated, meaning that search terms longer than 10 characters may not match any indexed terms."],["body","\n"],["body","PUT my-index-00001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"autocomplete\": {\n          \"tokenizer\": \"autocomplete\",\n          \"filter\": [\n            \"lowercase\"\n          ]\n        },\n        \"autocomplete_search\": {\n          \"tokenizer\": \"lowercase\"\n        }\n      },\n      \"tokenizer\": {\n        \"autocomplete\": {\n          \"type\": \"edge_ngram\",\n          \"min_gram\": 2,\n          \"max_gram\": 10,\n          \"token_chars\": [\n            \"letter\"\n          ]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\",\n        \"analyzer\": \"autocomplete\",\n        \"search_analyzer\": \"autocomplete_search\"\n      }\n    }\n  }\n}\n\n# The `autocomplete` analyzer indexes the terms `[qu, qui, quic, quick, fo, fox, foxe, foxes]`. 索引\nPUT my-index-00001/_doc/1\n{\n  \"title\": \"Quick Foxes\" \n}\n\nPOST my-index-00001/_refresh\n\n# 查询The `autocomplete_search` analyzer searches for the terms `[quick, fo]`, both of which appear in the index.\nGET my-index-00001/_search\n{\n  \"query\": {\n    \"match\": {\n      \"title\": {\n        \"query\": \"Quick Fo\", \n        \"operator\": \"and\"\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/4.TokenizerReference/5.StandardTokenizer.html"],["title","StandardTokenizer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","standard-tokenizer"],["heading","Standard tokenizer"],["body","\n"],["body","The standard tokenizer provides grammar based tokenization (based on the Unicode Text Segmentation algorithm, as specified in Unicode Standard Annex #29) and works well for most languages."],["body","\n"],["headingLink","example-output"],["heading","Example output"],["body","\n"],["body","POST _analyze\n{\n  \"tokenizer\": \"standard\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n"],["body","\n"],["body","The above sentence would produce the following terms:"],["body","\n"],["body","[ The, 2, QUICK, Brown, Foxes, jumped, over, the, lazy, dog's, bone ]\n"],["body","\n"],["headingLink","configuration"],["heading","Configuration"],["body","\n"],["body","The standard tokenizer accepts the following parameters:"],["body","\n"],["body","\n"],["body","max_token_length"],["body","The maximum token length. If a token is seen that exceeds this length then it is split at max_token_length intervals. Defaults to 255."],["body","\n\n\n"],["headingLink","example-configuration"],["heading","Example configuration"],["body","\n"],["body","In this example, we configure the standard tokenizer to have a max_token_length of 5 (for demonstration purposes):"],["body","\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"my_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"my_tokenizer\": {\n          \"type\": \"standard\",\n          \"max_token_length\": 5\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n"],["body","\n"],["body","The above example produces the following terms:"],["body","\n"],["body","[ The, 2, QUICK, Brown, Foxes, jumpe, d, over, the, lazy, dog's, bone ]\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/4.TokenizerReference/7.ClassicTokenizer.html"],["title","ClassicTokenizer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","classic-tokenizer"],["heading","Classic tokenizer"],["body","\n\n"],["body","\n"],["body","The classic tokenizer 是 基于英语的分析器"],["body","\n"],["body","\n"],["body","\n"],["body","此分词器具有启发式方法，用于对首字母缩写词，公司名称，电子邮件地址和internet主机名进行特殊处理"],["body","\n"],["body","\n"],["body","\n"],["body","然而，这些规则并不总是有效的，此分词器不适用于英语以外的大多数语言:"],["body","\n\n"],["body","\n"],["body","它会在大多数标点符号上拆分单词，从而删除标点符号。但是，点后面未接空格会被认为是一个token。"],["body","\n"],["body","\n"],["body","\n"],["body","\n"],["body","除非token中有数字，否则它会在连字符处拆分单词，在这种情况下，整个token将被解释为产品编号，不会拆分。"],["body","\n"],["body","\n"],["body","\n"],["body","它将电子邮件地址和internet主机名识别为一个token。"],["body","\n"],["body","\n\n"],["body","\n\n"],["headingLink","example-output"],["heading","Example output"],["body","\n"],["body","POST _analyze\n{\n  \"tokenizer\": \"classic\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n"],["body","\n"],["body","The above sentence would produce the following terms:"],["body","\n"],["body","[ The, 2, QUICK, Brown, Foxes, jumped, over, the, lazy, dog's, bone ]\n"],["body","\n"],["headingLink","configuration"],["heading","Configuration"],["body","\n"],["body","The classic tokenizer accepts the following parameters:"],["body","\n"],["body","\n"],["body","max_token_length"],["body","The maximum token length. If a token is seen that exceeds this length then it is split at max_token_length intervals. Defaults to 255."],["body","\n\n\n"],["headingLink","example-configuration"],["heading","Example configuration"],["body","\n"],["body","In this example, we configure the classic tokenizer to have a max_token_length of 5 (for demonstration purposes):"],["body","\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"my_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"my_tokenizer\": {\n          \"type\": \"classic\",\n          \"max_token_length\": 5\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n}\n"],["body","\n"],["body","The above example produces the following terms:"],["body","\n"],["body","[ The, 2, QUICK, Brown, Foxes, jumpe, d, over, the, lazy, dog's, bone ]\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/4.TokenizerReference/2.Keyword.html"],["title","Keyword.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","keyword-tokenizer"],["heading","Keyword tokenizer"],["body","\n"],["body","The keyword tokenizer is a “noop” tokenizer that accepts whatever text it is given and outputs the exact same text as a single term. It can be combined with token filters to normalise output, e.g. lower-casing email addresses."],["body","\n"],["headingLink","example-output"],["heading","Example output"],["body","\n"],["body","POST _analyze\n{\n  \"tokenizer\": \"keyword\",\n  \"text\": \"New York\"\n}\n"],["body","\n"],["body","The above sentence would produce the following term:"],["body","\n"],["body","[ New York ]\n"],["body","\n"],["headingLink","combine-with-token-filters"],["heading","Combine with token filters"],["body","\n"],["body","You can combine the keyword tokenizer with token filters to normalise structured data, such as product IDs or email addresses."],["body","\n"],["body","For example, the following analyze API request uses the keyword tokenizer and lowercase filter to convert an email address to lowercase."],["body","\n"],["body","POST _analyze\n{\n  \"tokenizer\": \"keyword\",\n  \"filter\": [ \"lowercase\" ],\n  \"text\": \"john.SMITH@example.COM\"\n}\n"],["body","\n"],["body","Copy as curlView in Console"],["body","\n"],["body","The request produces the following token:"],["body","\n"],["body","[ john.smith@example.com ]\n"],["body","\n"],["headingLink","configuration"],["heading","Configuration"],["body","\n"],["body","The keyword tokenizer accepts the following parameters:"],["body","\n"],["body","\n"],["body","buffer_size"],["body","The number of characters read into the term buffer in a single pass. Defaults to 256. The term buffer will grow by this size until all the text has been consumed. It is advisable not to change this setting."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/4.TokenizerReference/11.UAXURLEmailTokenizer.html"],["title","UAXURLEmailTokenizer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","uax-url-email-tokenizer"],["heading","UAX URL email tokenizer"],["body","\n"],["body","The uax_url_email tokenizer is like the standard tokenizer except that it recognises URLs and email addresses as single tokens."],["body","\n"],["headingLink","example-output"],["heading","Example output"],["body","\n"],["body","POST _analyze\n{\n  \"tokenizer\": \"uax_url_email\",\n  \"text\": \"Email me at john.smith@global-international.com\"\n}\n"],["body","\n"],["body","The above sentence would produce the following terms:"],["body","\n"],["body","[ Email, me, at, john.smith@global-international.com ]\n"],["body","\n"],["body","while the standard tokenizer would produce:"],["body","\n"],["body","[ Email, me, at, john.smith, global, international.com ]\n"],["body","\n"],["headingLink","configuration"],["heading","Configuration"],["body","\n"],["body","The uax_url_email tokenizer accepts the following parameters:"],["body","\n"],["body","\n"],["body","max_token_length"],["body","The maximum token length. If a token is seen that exceeds this length then it is split at max_token_length intervals. Defaults to 255."],["body","\n\n\n"],["headingLink","example-configuration"],["heading","Example configuration"],["body","\n"],["body","In this example, we configure the uax_url_email tokenizer to have a max_token_length of 5 (for demonstration purposes):"],["body","\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_analyzer\": {\n          \"tokenizer\": \"my_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"my_tokenizer\": {\n          \"type\": \"uax_url_email\",\n          \"max_token_length\": 5\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_analyzer\",\n  \"text\": \"john.smith@global-international.com\"\n}\n"],["body","\n"],["body","The above example produces the following terms:"],["body","\n"],["body","[ john, smith, globa, l, inter, natio, nal.c, om ]\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/4.TokenizerReference/6.CharacterGroupTokenizer.html"],["title","CharacterGroupTokenizer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","character-group-tokenizer"],["heading","Character group tokenizer"],["body","\n\n"],["body","char_group tokenizer 分词器 使用给定的 字符集合 来分割 词项"],["body","\n"],["body","通常用于简单的自定义分词器、而且  pattern tokenizer  的开销无法接受"],["body","\n\n"],["headingLink","configuration"],["heading","Configuration"],["body","\n"],["body","The char_group tokenizer accepts one parameter:"],["body","\n"],["body","\n"],["body","tokenize_on_chars"],["body","A list containing a list of characters to tokenize the string on. Whenever a character from this list is encountered, a new token is started. This accepts either single characters like e.g. -, or character groups: whitespace, letter, digit, punctuation, symbol."],["body","\n"],["body","max_token_length"],["body","The maximum token length. If a token is seen that exceeds this length then it is split at max_token_length intervals. Defaults to 255."],["body","\n\n\n"],["headingLink","example-output"],["heading","Example output"],["body","\n"],["body","POST _analyze\n{\n  \"tokenizer\": {\n    \"type\": \"char_group\",\n    \"tokenize_on_chars\": [\n      \"whitespace\",\n      \"-\",\n      \"\\n\"\n    ]\n  },\n  \"text\": \"The QUICK brown-fox\"\n}\n"],["body","\n"],["body","returns"],["body","\n"],["body","{\n  \"tokens\": [\n    {\n      \"token\": \"The\",\n      \"start_offset\": 0,\n      \n      \"end_offset\": 3,\n      \"type\": \"word\",\n      \"position\": 0\n    },\n    {\n      \"token\": \"QUICK\",\n      \"start_offset\": 4,\n      \"end_offset\": 9,\n      \"type\": \"word\",\n      \"position\": 1\n    },\n    {\n      \"token\": \"brown\",\n      \"start_offset\": 10,\n      \"end_offset\": 15,\n      \"type\": \"word\",\n      \"position\": 2\n    },\n    {\n      \"token\": \"fox\",\n      \"start_offset\": 16,\n      \"end_offset\": 19,\n      \"type\": \"word\",\n      \"position\": 3\n    }\n  ]\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/4.TokenizerReference/8.LetterTokenizer.html"],["title","LetterTokenizer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","letter-tokenizer"],["heading","Letter tokenizer"],["body","\n\n"],["body","\n"],["body","The letter tokenizer breaks text into terms whenever it encounters a character which is not a letter."],["body","\n"],["body","\n"],["body","\n"],["body","It does a reasonable job for most European languages, but does a terrible job for some Asian languages, where words are not separated by spaces."],["body","\n"],["body","\n\n"],["headingLink","example-output"],["heading","Example output"],["body","\n"],["body","\n\n"],["body","\n"],["body","The above sentence would produce the following terms:"],["body","\n"],["body","[ The, QUICK, Brown, Foxes, jumped, over, the, lazy, dog, s, bone ]\n"],["body","\n"],["headingLink","configuration"],["heading","Configuration"],["body","\n"],["body","The letter tokenizer is not configurable."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/7.Normalizers/README.html"],["title","Normalizers - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","normalizers"],["heading","Normalizers"],["body","\n\n"],["body","归一化器与分析器相似，不同之处在于它们只能发出一个令牌"],["body","\n"],["body","As a consequence, they do not have a tokenizer and only accept a subset of the available char filters and token filters."],["body","\n"],["body","Only the filters that work on a per-character basis are allowed."],["body","\n"],["body","For instance a lowercasing filter would be allowed, but not a stemming filter, which needs to look at the keyword as a whole. The current list of filters that can be used in a normalizer is following: arabic_normalization, asciifolding, bengali_normalization, cjk_width, decimal_digit, elision, german_normalization, hindi_normalization, indic_normalization, lowercase, persian_normalization, scandinavian_folding, serbian_normalization, sorani_normalization, uppercase."],["body","\n\n"],["body","Elasticsearch ships with a lowercase built-in normalizer. For other forms of normalization a custom configuration is required."],["body","\n"],["headingLink","custom-normalizers"],["heading","Custom normalizers"],["body","\n"],["body","Custom normalizers take a list of character filters and a list of token filters."],["body","\n"],["body","PUT index\n{\n  \"settings\": {\n    \"analysis\": {\n      \"char_filter\": {\n        \"quote\": {\n          \"type\": \"mapping\",\n          \"mappings\": [\n            \"« => \\\"\",\n            \"» => \\\"\"\n          ]\n        }\n      },\n      \"normalizer\": {\n        \"my_normalizer\": {\n          \"type\": \"custom\",\n          \"char_filter\": [\"quote\"],\n          \"filter\": [\"lowercase\", \"asciifolding\"]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"foo\": {\n        \"type\": \"keyword\",\n        \"normalizer\": \"my_normalizer\"\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/2.ConfigureTextAnalysis/1.TestAnAnalyzer.html"],["title","TestAnAnalyzer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","test-an-analyzer"],["heading","Test an analyzer"],["body","\n"],["body","analyze API  用于查看被 分析器产生的词项"],["body","\n"],["body","POST _analyze\n{\n  \"analyzer\": \"whitespace\",\n  \"text\":     \"The quick brown fox.\"\n}\n"],["body","\n"],["body","Positions and character offsets"],["body","\n"],["body","从analyze API的输出可以看出，分析器不仅将单词转换为terms"],["body","\n"],["body","还记录每个术语的顺序或相对位置 (used for phrase queries or word proximity queries)，以及原文中每个术语的开始和结束字符偏移量 (用于突出显示搜索片段)。"],["body","\n"],["body","或者，在特定索引上运行analyze API时，可以参考自定义分析器:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"std_folded\": {  // Define a `custom` analyzer called `std_folded`.\n          \"type\": \"custom\",\n          \"tokenizer\": \"standard\",\n          \"filter\": [\n            \"lowercase\",\n            \"asciifolding\"\n          ]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"my_text\": {\n        \"type\": \"text\",\n        \"analyzer\": \"std_folded\"  The field `my_text` uses the `std_folded` analyzer.\n      }\n    }\n  }\n}\n\nGET my-index-000001/_analyze  To refer to this analyzer, the `analyze` API must specify the index name.\n{\n  \"analyzer\": \"std_folded\", // Refer to the analyzer by name.\n  \"text\":     \"Is this déjà vu?\"\n}\n\nGET my-index-000001/_analyze \n{\n  \"field\": \"my_text\",  //Refer to the analyzer used by field `my_text`.\n  \"text\":  \"Is this déjà vu?\"\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/2.ConfigureTextAnalysis/2.ConfiguringBuilt-inAnalyzers.html"],["title","ConfiguringBuilt-inAnalyzers.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","configuring-built-in-analyzers"],["heading","Configuring built-in analyzers"],["body","\n"],["body","配置内置分析器的停用词"],["body","\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"std_english\": {  // define the `std_english` analyzer\n          \"type\":      \"standard\", //be based on the `standard` analyzer,\n          \"stopwords\": \"_english_\" //,但配置为删除预先定义的英语停止词列表。\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"my_text\": {// `my_text` field uses the `standard` analyzer directly, \n        \"type\":     \"text\",\n        \"analyzer\": \"standard\", \n        \"fields\": {\n          \"english\": { // `my_text.english` field uses the `std_english` analyzer\n            \"type\":     \"text\",\n            \"analyzer\": \"std_english\" \n          }\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"field\": \"my_text\", \n  \"text\": \"The old brown cow\"\n}\n\nPOST my-index-000001/_analyze\n{\n  \"field\": \"my_text.english\", \n  \"text\": \"The old brown cow\"\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/2.ConfigureTextAnalysis/README.html"],["title","ConfigureTextAnalysis - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","configure-text-analysis"],["heading","Configure text analysis"],["body","\n\n"],["body","\n"],["body","ES默认使用 标准分析器 standard analyzer  ，开箱即用"],["body","\n"],["body","\n"],["body","\n"],["body","还提供了内置的 built-in analyzers  ，这些分析器有的可以支持选项来调整行为，例如标准分析器可以配置停用词"],["body","\n"],["body","\n"],["body","\n"],["body","还可以  通过组合不同的分析器组件analyzer components,  自定义分析器，"],["body","\n\n"],["body","\n"],["body","Test an analyzer"],["body","\n"],["body","\n"],["body","\n"],["body","Configuring built-in analyzers"],["body","\n"],["body","\n"],["body","\n"],["body","Create a custom analyzer"],["body","\n"],["body","\n"],["body","\n"],["body","Specify an analyzer"],["body","\n"],["body","\n\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/2.ConfigureTextAnalysis/3.CreateACustomAnalyzer.html"],["title","CreateACustomAnalyzer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","create-a-custom-analyzer"],["heading","Create a custom analyzer"],["body","\n"],["body","组合以下分析器组件"],["body","\n\n"],["body","zero or more character filters"],["body","\n"],["body","a tokenizer"],["body","\n"],["body","zero or more token filters."],["body","\n\n"],["headingLink","configuration"],["heading","Configuration"],["body","\n"],["body","自定义分析器接受以下参数:"],["body","\n"],["body","type"],["body","Analyzer type. Accepts built-in analyzer types. For custom analyzers, use custom or omit this parameter."],["body","\n"],["body","tokenizer"],["body","分词器：A built-in or customised tokenizer. (Required)"],["body","\n"],["body","char_filter"],["body","字符过滤器：An optional array of built-in or customised character filters."],["body","\n"],["body","filter"],["body","词过滤器：An optional array of built-in or customised token filters."],["body","\n"],["body","position_increment_gap"],["body","索引文本值数组时，Elasticsearch 在数组元素间的相邻词项之间插入 虚拟的词项间距.默认 100. 详见： position_increment_gap for more."],["body","\n\n\n"],["headingLink","example-configuration"],["heading","Example configuration"],["body","\n"],["body","Here is an example that combines the following:"],["body","\n\n"],["body","\n"],["body","Character Filter"],["body","\n"],["body","HTML Strip Character Filter"],["body","\n"],["body","\n"],["body","\n"],["body","Tokenizer"],["body","\n"],["body","Standard Tokenizer"],["body","\n"],["body","\n"],["body","\n"],["body","Token Filters"],["body","\n"],["body","Lowercase Token FilterASCII-Folding Token Filter"],["body","\n"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_custom_analyzer\": {\n          \"type\": \"custom\", //自定义的分析器使用 type=custom\n          \"tokenizer\": \"standard\",\n          \"char_filter\": [\n            \"html_strip\"\n          ],\n          \"filter\": [\n            \"lowercase\",\n            \"asciifolding\"\n          ]\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_custom_analyzer\",\n  \"text\": \"Is this <b>déjà vu</b>?\"\n}\n"],["body","\n"],["body","可以配置更加复杂的分析器"],["body","\n\n"],["body","\n"],["body","Character Filter"],["body","\n"],["body","Mapping Character Filter, configured to replace :) with _happy_ and :( with _sad_"],["body","\n"],["body","\n"],["body","\n"],["body","Tokenizer"],["body","\n"],["body","Pattern Tokenizer, configured to split on punctuation characters"],["body","\n"],["body","\n"],["body","\n"],["body","Token Filters"],["body","\n"],["body","Lowercase Token FilterStop Token Filter, configured to use the pre-defined list of English stop words"],["body","\n"],["body","\n\n"],["body","Here is an example:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_custom_analyzer\": {  //使用自定义的分析器，省略了type参数\n          \"char_filter\": [ //字符过滤器\n            \"emoticons\"\n          ],\n          \"tokenizer\": \"punctuation\", //自定义的tokenizer\n          \"filter\": [\n            \"lowercase\",\n            \"english_stop\"\n          ]\n        }\n      },\n      \"tokenizer\": {\n        \"punctuation\": { \n          \"type\": \"pattern\",\n          \"pattern\": \"[ .,!?]\"\n        }\n      },\n      \"char_filter\": {\n        \"emoticons\": { \n          \"type\": \"mapping\",\n          \"mappings\": [\n            \":) => _happy_\",\n            \":( => _sad_\"\n          ]\n        }\n      },\n      \"filter\": {\n        \"english_stop\": { \n          \"type\": \"stop\",\n          \"stopwords\": \"_english_\"\n        }\n      }\n    }\n  }\n}\n\nPOST my-index-000001/_analyze\n{\n  \"analyzer\": \"my_custom_analyzer\",\n  \"text\": \"I'm a :) person, and you?\"\n}\n"],["body","\n"],["body","The above example produces the following terms:"],["body","\n"],["body","[ i'm, _happy_, person, you ]\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/2.ConfigureTextAnalysis/0.AnalyzeAPI.html"],["title","AnalyzeAPI.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","analyze-api"],["heading","Analyze API"],["body","\n"],["body","使用标准分析器执行分析"],["body","\n"],["body","GET /_analyze\n{\n  \"analyzer\" : \"standard\",\n  \"text\" : \"Quick Brown Foxes!\"\n}\n"],["body","\n"],["headingLink","request"],["heading","Request"],["body","\n"],["body","GET /_analyze\nPOST /_analyze\nGET /<index>/_analyze\nPOST /<index>/_analyze\n"],["body","\n"],["headingLink","path-parameters"],["heading","Path parameters"],["body","\n\n"],["body","\n"],["body","<index>"],["body","\n\n"],["body","\n"],["body","(Optional, string) 用于派生分析器的索引"],["body","\n"],["body","\n"],["body","\n"],["body","the analyzer or <field> 参数覆盖这个值"],["body","\n"],["body","\n"],["body","\n"],["body","如果没有手动指定分析器 则analyze API使用 索引的默认分析器"],["body","\n"],["body","\n"],["body","\n"],["body","如果索引没有默认的分析器，则使用 standard analyzer."],["body","\n"],["body","\n\n"],["body","\n\n"],["headingLink","query-parameters"],["heading","Query parameters"],["body","\n\n"],["body","analyzer\n\n"],["body","(Optional, string) 分析器的名称"],["body","\n"],["body","可以使内置的分析器 built-in analyzer  或者是被配置在索引的分配器"],["body","\n\n"],["body","\n"],["body","attributes\n\n"],["body","(Optional, array of strings) Array of token attributes used to filter the output of the explain parameter."],["body","\n\n"],["body","\n"],["body","char_filter\n\n"],["body","(Optional, array of strings) Array of character filters used to preprocess characters before the tokenizer."],["body","\n"],["body","See Character filters reference for a list of character filters."],["body","\n\n"],["body","\n"],["body","explain\n\n"],["body","(Optional, Boolean) If true, the response includes token attributes and additional details. Defaults to false."],["body","\n"],["body","[preview]The format of the additional detail information is labelled as experimental in Lucene and it may change in the future."],["body","\n\n"],["body","\n"],["body","field\n\n"],["body","(Optional, string) 用于生成分析器的字段."],["body","\n"],["body","要使用此参数，您必须指定索引"],["body","\n\n"],["body","\n"],["body","filter\n\n"],["body","(Optional, Array of strings) Array of token filters used to apply after the tokenizer."],["body","\n"],["body","See Token filter reference for a list of token filters."],["body","\n\n"],["body","\n"],["body","normalizer\n\n"],["body","(Optional, string) Normalizer to use to convert text into a single token."],["body","\n"],["body","See Normalizers for a list of normalizers."],["body","\n\n"],["body","\n"],["body","text\n\n"],["body","(Required, string or array of strings) Text to analyze."],["body","\n"],["body","If an array of strings is provided, it is analyzed as a multi-value field."],["body","\n\n"],["body","\n"],["body","tokenizer\n\n"],["body","(Optional, string) Tokenizer to use to convert text into tokens."],["body","\n"],["body","See Tokenizer reference for a list of tokenizers."],["body","\n\n"],["body","\n\n"],["headingLink","examples"],["heading","Examples"],["body","\n"],["headingLink","no-index-specified"],["heading","No index specified"],["body","\n"],["body","You can apply any of the built-in analyzers to the text string without specifying an index."],["body","\n"],["body","GET /_analyze\n{\n  \"analyzer\" : \"standard\",\n  \"text\" : \"this is a test\"\n}\n"],["body","\n"],["headingLink","array-of-text-strings"],["heading","Array of text strings"],["body","\n"],["body","If the text parameter is provided as array of strings, it is analyzed as a multi-value field."],["body","\n"],["body","GET /_analyze\n{\n  \"analyzer\" : \"standard\",\n  \"text\" : [\"this is a test\", \"the second text\"]\n}\n"],["body","\n"],["headingLink","custom-analyzer"],["heading","Custom analyzer"],["body","\n"],["body","You can use the analyze API to test a custom transient analyzer built from tokenizers, token filters, and char filters. Token filters use the filter parameter:"],["body","\n"],["body","GET /_analyze\n{\n  \"tokenizer\" : \"keyword\",\n  \"filter\" : [\"lowercase\"],\n  \"text\" : \"this is a test\"\n}\n"],["body","\n"],["body","GET /_analyze\n{\n  \"tokenizer\" : \"keyword\",\n  \"filter\" : [\"lowercase\"],\n  \"char_filter\" : [\"html_strip\"],\n  \"text\" : \"this is a <b>test</b>\"\n}\n"],["body","\n"],["body","Custom tokenizers, token filters, and character filters can be specified in the request body as follows:"],["body","\n"],["body","GET /_analyze\n{\n  \"tokenizer\" : \"whitespace\",\n  \"filter\" : [\"lowercase\", {\"type\": \"stop\", \"stopwords\": [\"a\", \"is\", \"this\"]}],\n  \"text\" : \"this is a test\"\n}\n"],["body","\n"],["headingLink","specific-index"],["heading","Specific index"],["body","\n"],["body","You can also run the analyze API against a specific index:"],["body","\n"],["body","GET /analyze_sample/_analyze\n{\n  \"text\" : \"this is a test\"\n}\n"],["body","\n"],["body","The above will run an analysis on the \"this is a test\" text, using the default index analyzer associated with the analyze_sample index. An analyzer can also be provided to use a different analyzer:"],["body","\n"],["body","GET /analyze_sample/_analyze\n{\n  \"analyzer\" : \"whitespace\",\n  \"text\" : \"this is a test\"\n}\n"],["body","\n"],["headingLink","derive-analyzer-from-a-field-mapping"],["heading","Derive analyzer from a field mapping"],["body","\n"],["body","The analyzer can be derived based on a field mapping, for example:"],["body","\n"],["body","GET /analyze_sample/_analyze\n{\n  \"field\" : \"obj1.field1\",\n  \"text\" : \"this is a test\"\n}\n"],["body","\n"],["body","Will cause the analysis to happen based on the analyzer configured in the mapping for obj1.field1 (and if not, the default index analyzer)."],["body","\n"],["headingLink","normalizer"],["heading","Normalizer"],["body","\n"],["body","A normalizer can be provided for keyword field with normalizer associated with the analyze_sample index."],["body","\n"],["body","GET /analyze_sample/_analyze\n{\n  \"normalizer\" : \"my_normalizer\",\n  \"text\" : \"BaR\"\n}\n"],["body","\n"],["body","Or by building a custom transient normalizer out of token filters and char filters."],["body","\n"],["body","GET /_analyze\n{\n  \"filter\" : [\"lowercase\"],\n  \"text\" : \"BaR\"\n}\n"],["body","\n"],["headingLink","explain-analyze"],["heading","Explain analyze"],["body","\n"],["body","If you want to get more advanced details, set explain to true (defaults to false). It will output all token attributes for each token. You can filter token attributes you want to output by setting attributes option."],["body","\n"],["body","The format of the additional detail information is labelled as experimental in Lucene and it may change in the future."],["body","\n"],["body","GET /_analyze\n{\n  \"tokenizer\" : \"standard\",\n  \"filter\" : [\"snowball\"],\n  \"text\" : \"detailed output\",\n  \"explain\" : true,\n  \"attributes\" : [\"keyword\"]  //Set \"keyword\" to output \"keyword\" attribute only\n}\n"],["body","\n"],["body","The request returns the following result:"],["body","\n"],["body","{\n  \"detail\" : {\n    \"custom_analyzer\" : true,\n    \"charfilters\" : [ ],\n    \"tokenizer\" : {\n      \"name\" : \"standard\",\n      \"tokens\" : [ {\n        \"token\" : \"detailed\",\n        \"start_offset\" : 0,\n        \"end_offset\" : 8,\n        \"type\" : \"<ALPHANUM>\",\n        \"position\" : 0\n      }, {\n        \"token\" : \"output\",\n        \"start_offset\" : 9,\n        \"end_offset\" : 15,\n        \"type\" : \"<ALPHANUM>\",\n        \"position\" : 1\n      } ]\n    },\n    \"tokenfilters\" : [ {\n      \"name\" : \"snowball\",\n      \"tokens\" : [ {\n        \"token\" : \"detail\",\n        \"start_offset\" : 0,\n        \"end_offset\" : 8,\n        \"type\" : \"<ALPHANUM>\",\n        \"position\" : 0,\n        \"keyword\" : false  Output only \"keyword\" attribute, since specify \"attributes\" in the request.\n      }, {\n        \"token\" : \"output\",\n        \"start_offset\" : 9,\n        \"end_offset\" : 15,\n        \"type\" : \"<ALPHANUM>\",\n        \"position\" : 1,\n        \"keyword\" : false \n      } ]\n    } ]\n  }\n}\n"],["body","\n"],["headingLink","setting-a-token-limit"],["heading","Setting a token limit"],["body","\n"],["body","生成过多的token可能会导致节点内存不足"],["body","\n"],["body","以下设置允许限制可以产生的令牌数量:"],["body","\n\n"],["body","\n"],["body","index.analyze.max_token_count"],["body","\n"],["body","The maximum number of tokens that can be produced using _analyze API. The default value is 10000. If more than this limit of tokens gets generated, an error will be thrown. The _analyze endpoint without a specified index will always use 10000 value as a limit. This setting allows you to control the limit for a specific index:"],["body","\n"],["body","\n\n"],["body","PUT /analyze_sample\n{\n  \"settings\" : {\n    \"index.analyze.max_token_count\" : 20000\n  }\n}\n"],["body","\n"],["body","GET /analyze_sample/_analyze\n{\n  \"text\" : \"this is a test\"\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/2.TextAnalysis/2.ConfigureTextAnalysis/4.SpecifyAnAnalyzer.html"],["title","SpecifyAnAnalyzer.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","specify-an-analyzer"],["heading","Specify an analyzer"],["body","\n"],["body","Elasticsearch提供了多种方式来指定内置或自定义分析器:"],["body","\n\n"],["body","By text field, index, or query"],["body","\n"],["body","For index or search time"],["body","\n\n"],["headingLink","keep-it-simple"],["heading","Keep it simple"],["body","\n"],["body","在不同级别和不同时间指定分析仪的灵活性是很大的…… 但只有在需要的时候。"],["body","\n"],["body","多数情况下，为每个文本字段指定有一个分析器，就够了"],["body","\n"],["body","使用get mapping API 来查看索引字段使用的是哪个分析器"],["body","\n"],["headingLink","how-elasticsearch-determines-the-index-analyzer"],["heading","How Elasticsearch determines the index analyzer"],["body","\n"],["body","Elasticsearch通过按顺序检查以下参数来确定要使用的索引分析器:"],["body","\n\n"],["body","\n"],["body","The analyzer mapping parameter for the field. See Specify the analyzer for a field."],["body","\n"],["body","\n"],["body","\n"],["body","The analysis.analyzer.default index setting. See Specify the default analyzer for an index."],["body","\n"],["body","\n"],["body","\n"],["body","If none of these parameters are specified, the standard analyzer is used."],["body","\n"],["body","\n\n"],["headingLink","specify-the-analyzer-for-a-field"],["heading","Specify the analyzer for a field"],["body","\n"],["body","映射索引时，可以使用分析器映射参数为每个文本字段指定分析器。"],["body","\n"],["body","The following create index API request sets the whitespace analyzer as the analyzer for the title field."],["body","\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\",\n        \"analyzer\": \"whitespace\"\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","specify-the-default-analyzer-for-an-index"],["heading","Specify the default analyzer for an index"],["body","\n\n"],["body","\n"],["body","指定索引级别的默认分析器  analysis.analyzer.default"],["body","\n"],["body","\n"],["body","\n"],["body","The following create index API request sets the simple analyzer as the fallback analyzer for my-index-000001."],["body","\n"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"default\": {\n          \"type\": \"simple\"\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","how-elasticsearch-determines-the-search-analyzer"],["heading","How Elasticsearch determines the search analyzer"],["body","\n\n"],["body","\n"],["body","在大多数情况下，指定不同的搜索分析器是不必要的，这样做可能会对相关性产生负面影响，并导致意外的搜索结果。"],["body","\n"],["body","\n"],["body","\n"],["body","在搜索时，Elasticsearch通过按顺序检查以下参数来确定要使用的分析器:"],["body","\n"],["body","\n"],["body","\n"],["body","The analyzer parameter in the search query. See Specify the search analyzer for a query."],["body","\n"],["body","\n"],["body","\n"],["body","The search_analyzer mapping parameter for the field. See Specify the search analyzer for a field."],["body","\n"],["body","\n"],["body","\n"],["body","The analysis.analyzer.default_search index setting. See Specify the default search analyzer for an index."],["body","\n"],["body","\n"],["body","\n"],["body","The analyzer mapping parameter for the field. See Specify the analyzer for a field."],["body","\n"],["body","\n"],["body","\n"],["body","If none of these parameters are specified, the standard analyzer is used."],["body","\n"],["body","\n\n"],["headingLink","specify-the-search-analyzer-for-a-query"],["heading","Specify the search analyzer for a query"],["body","\n\n"],["body","\n"],["body","When writing a full-text query, you can use the analyzer parameter to specify a search analyzer. If provided, this overrides any other search analyzers."],["body","\n"],["body","\n"],["body","\n"],["body","The following search API request sets the stop analyzer as the search analyzer for a match query."],["body","\n"],["body","\n\n"],["body","GET my-index-000001/_search\n{\n  \"query\": {\n    \"match\": {\n      \"message\": {\n        \"query\": \"Quick foxes\",\n        \"analyzer\": \"stop\"\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","specify-the-search-analyzer-for-a-field"],["heading","Specify the search analyzer for a field"],["body","\n\n"],["body","\n"],["body","When mapping an index, you can use the search_analyzer mapping parameter to specify a search analyzer for each text field."],["body","\n"],["body","\n"],["body","\n"],["body","If a search analyzer is provided, the index analyzer must also be specified using the analyzer parameter."],["body","\n"],["body","\n"],["body","\n"],["body","The following create index API request sets the simple analyzer as the search analyzer for the title field."],["body","\n"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"mappings\": {\n    \"properties\": {\n      \"title\": {\n        \"type\": \"text\",\n        \"analyzer\": \"whitespace\",\n        \"search_analyzer\": \"simple\"\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","specify-the-default-search-analyzer-for-an-index"],["heading","Specify the default search analyzer for an index"],["body","\n\n"],["body","\n"],["body","When creating an index, you can set a default search analyzer using the analysis.analyzer.default_search setting."],["body","\n"],["body","\n"],["body","\n"],["body","If a search analyzer is provided, a default index analyzer must also be specified using the analysis.analyzer.default setting."],["body","\n"],["body","\n"],["body","\n"],["body","The following create index API request sets the whitespace analyzer as the default search analyzer for the my-index-000001 index."],["body","\n"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"default\": {\n          \"type\": \"simple\"\n        },\n        \"default_search\": {\n          \"type\": \"whitespace\"\n        }\n      }\n    }\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/索引管理.html"],["title","索引管理.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","创建索引"],["heading","创建索引"],["body","\n"],["body","PUT /my_index\n{\n    \"settings\": { ... any settings ... },\n    \"mappings\": {\n        \"type_one\": { ... any mappings ... },\n        \"type_two\": { ... any mappings ... },\n        ...\n    }\n}\n"],["body","\n"],["body","如果你想禁止自动创建索引，你 可以通过在 config/elasticsearch.yml 的每个节点下添加下面的配置："],["body","\n"],["body","action.auto_create_index: false\n"],["body","\n"],["headingLink","删除一个索引"],["heading","删除一个索引"],["body","\n"],["body","用以下的请求来 删除索引:"],["body","\n"],["body","DELETE /my_index\n"],["body","\n"],["body","你也可以这样删除多个索引："],["body","\n"],["body","DELETE /index_one,index_two\nDELETE /index_*\n"],["body","\n"],["body","你甚至可以这样删除 全部 索引："],["body","\n"],["body","DELETE /_all\nDELETE /*\n"],["body","\n"],["body","如果你想要避免意外的大量删除, 你可以在你的 elasticsearch.yml 做如下配置："],["body","\n"],["body","action.destructive_requires_name: true\n"],["body","\n"],["body","这个设置使删除只限于特定名称指向的数据, 而不允许通过指定 _all 或通配符来删除指定索引库。"],["body","\n"],["headingLink","索引设置"],["heading","索引设置"],["body","\n"],["body","你可以通过修改配置来自定义索引行为，详细配置参照"],["body","\n"],["body","下面是两个 最重要的设置："],["body","\n\n"],["body","\n"],["body","number_of_shards"],["body","\n"],["body","每个索引的主分片数，默认值是 5 。这个配置在索引创建后不能修改。"],["body","\n"],["body","\n"],["body","\n"],["body","number_of_replicas"],["body","\n"],["body","每个主分片的副本数，默认值是 1 。对于活动的索引库，这个配置可以随时修改。"],["body","\n"],["body","\n\n"],["body","例如，我们可以创建只有 一个主分片，没有副本的小索引："],["body","\n"],["body","PUT /my_temp_index\n{\n    \"settings\": {\n        \"number_of_shards\" :   1,\n        \"number_of_replicas\" : 0\n    }\n}\n"],["body","\n"],["body","然后，我们可以用 update-index-settings API 动态修改副本数："],["body","\n"],["body","PUT /my_temp_index/_settings\n{\n    \"number_of_replicas\": 1\n}\n"],["body","\n"],["headingLink","配置分析器"],["heading","配置分析器"],["body","\n"],["body","用来配置已存在的分析器或针对你的索引创建新的自定义分析器。"],["body","\n"],["body","standard 分析器是用于全文字段的默认分析器，对于大部分西方语系来说是一个不错的选择。 它包括了以下几点："],["body","\n\n"],["body","standard 分词器，通过单词边界分割输入的文本。"],["body","\n"],["body","standard 语汇单元过滤器，目的是整理分词器触发的语汇单元（但是目前什么都没做）。"],["body","\n"],["body","lowercase 语汇单元过滤器，转换所有的语汇单元为小写。"],["body","\n"],["body","stop 语汇单元过滤器，删除停用词—对搜索相关性影响不大的常用词，如 a ， the ， and ， is 。"],["body","\n\n"],["body","默认情况下，停用词过滤器是被禁用的。如需启用它，你可以通过创建一个基于 standard 分析器的自定义分析器并设置 stopwords 参数。 可以给分析器提供一个停用词列表，或者告知使用一个基于特定语言的预定义停用词列表。"],["body","\n"],["body","在下面的例子中，我们创建了一个新的分析器，叫做 es_std ， 并使用预定义的西班牙语停用词列表："],["body","\n"],["body","PUT /spanish_docs\n{\n    \"settings\": {\n        \"analysis\": {\n            \"analyzer\": {\n                \"es_std\": {\n                    \"type\":      \"standard\",\n                    \"stopwords\": \"_spanish_\"\n                }\n            }\n        }\n    }\n}\n"],["body","\n"],["headingLink","自定义分析器"],["heading","自定义分析器"],["body","\n"],["body","一个 分析器 就是在一个包里面组合了三种函数的一个包装器， 三种函数按照顺序被执行"],["body","\n"],["headingLink","字符过滤器"],["heading","字符过滤器"],["body","\n"],["body","字符过滤器 用来 整理 一个尚未被分词的字符串。例如，如果我们的文本是HTML格式的，它会包含像 <p> 或者 <div> 这样的HTML标签，这些标签是我们不想索引的。我们可以使用 html清除 字符过滤器 来移除掉所有的HTML标签，并且像把 Á 转换为相对应的Unicode字符 Á 这样，转换HTML实体。"],["body","\n"],["body","一个分析器可能有0个或者多个字符过滤器。"],["body","\n"],["headingLink","分词器"],["heading","分词器"],["body","\n"],["body","一个分析器 必须 有一个唯一的分词器。 分词器把字符串分解成单个词条或者词汇单元。 标准 分析器里使用的 标准 分词器 把一个字符串根据单词边界分解成单个词条，并且移除掉大部分的标点符号，然而还有其他不同行为的分词器存在。"],["body","\n"],["body","例如， 关键词 分词器 完整地输出 接收到的同样的字符串，并不做任何分词。 空格 分词器 只根据空格分割文本 。 正则 分词器 根据匹配正则表达式来分割文本 。"],["body","\n"],["headingLink","词单元过滤器"],["heading","词单元过滤器"],["body","\n"],["body","经过分词，作为结果的 词单元流 会按照指定的顺序通过指定的词单元过滤器 。"],["body","\n"],["body","词单元过滤器可以修改、添加或者移除词单元。我们已经提到过 lowercase 和 stop 词过滤器 ，但是在 Elasticsearch 里面还有很多可供选择的词单元过滤器。 词干过滤器 把单词 遏制 为 词干。 ascii_folding 过滤器移除变音符，把一个像 \"très\" 这样的词转换为 \"tres\" 。 ngram 和 edge_ngram 词单元过滤器 可以产生 适合用于部分匹配或者自动补全的词单元。"],["body","\n"],["headingLink","创建一个自定义分析器"],["heading","创建一个自定义分析器"],["body","\n"],["body","基本语法"],["body","\n"],["body","PUT /my_index\n{\n    \"settings\": {\n        \"analysis\": { \n            \"char_filter\": { ... custom character filters ... },\n            \"tokenizer\":   { ...    custom tokenizers     ... },\n            \"filter\":      { ...   custom token filters   ... },\n            \"analyzer\":    { ...    custom analyzers      ... }\n        }\n    }\n}\n"],["body","\n"],["body","自定义映射过滤器"],["body","\n"],["body","\"char_filter\": {\n    \"&_to_and\": {\n        \"type\":       \"mapping\",\n        \"mappings\": [ \"&=> and \"]\n    }\n}\n"],["body","\n"],["body","自定义token filter"],["body","\n"],["body","\"filter\": {\n    \"my_stopwords\": {\n        \"type\":        \"stop\",\n        \"stopwords\": [ \"the\", \"a\" ]\n    }\n}\n"],["body","\n"],["body","创建分析器"],["body","\n"],["body","\"analyzer\": {\n    \"my_analyzer\": {\n        \"type\":           \"custom\",\n        \"char_filter\":  [ \"html_strip\", \"&_to_and\" ],\n        \"tokenizer\":      \"standard\",\n        \"filter\":       [ \"lowercase\", \"my_stopwords\" ]\n    }\n}\n"],["body","\n"],["headingLink","类型和映射"],["heading","类型和映射"],["body","\n"],["headingLink","lucene-如何处理文档"],["heading","Lucene 如何处理文档"],["body","\n\n"],["body","在 Lucene 中，一个文档由一组简单的键值对组成。 每个字段都可以有多个值，但至少要有一个值"],["body","\n"],["body","一个字符串可以通过分析过程转化为多个值"],["body","\n"],["body","Lucene 不关心这些值是字符串、数字或日期—所有的值都被当做 不透明字节 。"],["body","\n"],["body","当我们在 Lucene 中索引一个文档时，每个字段的值都被添加到相关字段的倒排索引中"],["body","\n\n"],["body","你也可以将未处理的原始数据 存储 起来，以便这些原始数据在之后也可以被检索到。"],["body","\n"],["headingLink","类型是如何实现的"],["heading","类型是如何实现的"],["body","\n"],["body","Elasticsearch 类型是以 Lucene 处理文档的这个方式为基础来实现的"],["body","\n"],["body","一个索引可以有多个类型，这些类型的文档可以存储在相同的索引中。"],["body","\n"],["body","Lucene 没有文档类型的概念，每个文档的类型名被存储在一个叫 _type 的元数据字段上。 当我们要检索某个类型的文档时, Elasticsearch 通过在 _type 字段上使用过滤器限制只返回这个类型的文档。"],["body","\n"],["body","Lucene 也没有映射的概念。 映射是 Elasticsearch 将复杂 JSON 文档 映射 成 Lucene 需要的扁平化数据的方式。"],["body","\n"],["headingLink","根对象"],["heading","根对象"],["body","\n"],["body","映射的最高一层被称为 根对象 ，它可能包含下面几项："],["body","\n\n"],["body","一个 properties 节点，列出了文档中可能包含的每个字段的映射"],["body","\n"],["body","各种元数据字段，它们都以一个下划线开头，例如 _type 、 _id 和 _source"],["body","\n"],["body","设置项，控制如何动态处理新的字段，例如 analyzer 、 dynamic_date_formats 和 dynamic_templates"],["body","\n"],["body","其他设置，可以同时应用在根对象和其他 object 类型的字段上，例如 enabled 、 dynamic 和 include_in_all"],["body","\n\n"],["headingLink","属性"],["heading","属性"],["body","\n\n"],["body","\n"],["body","type"],["body","\n"],["body","字段的数据类型，例如 string 或 date"],["body","\n"],["body","\n"],["body","\n"],["body","index"],["body","\n"],["body","字段是否应当被当成全文来搜索（ analyzed ），或被当成一个准确的值（ not_analyzed ），还是完全不可被搜索（ no ）"],["body","\n"],["body","\n"],["body","\n"],["body","analyzer"],["body","\n"],["body","确定在索引和搜索时全文字段使用的 analyzer"],["body","\n"],["body","\n\n"],["body","我们将在本书的后续部分讨论其他字段类型，例如 ip 、 geo_point 和 geo_shape 。"],["body","\n"],["headingLink","元数据-_source-字段"],["heading","元数据: _source 字段"],["body","\n"],["body","默认地，Elasticsearch 在 _source 字段存储代表文档体的JSON字符串，和所有被存储的字段一样， _source 字段在被写入磁盘之前先会被压缩。"],["body","\n"],["body","这个字段的存储几乎总是我们想要的，因为它意味着下面的这些："],["body","\n\n"],["body","没必要从另一个存储库中拉取源数据"],["body","\n"],["body","update 请求需要该字段"],["body","\n"],["body","可以部分取出某些字段"],["body","\n"],["body","可以方便重建索引"],["body","\n\n"],["body","禁用元数据存储"],["body","\n"],["body","PUT /my_index\n{\n    \"mappings\": {\n        \"my_type\": {\n            \"_source\": {\n                \"enabled\":  false\n            }\n        }\n    }\n}\n"],["body","\n"],["headingLink","元数据-_all-字段"],["heading","元数据: _all 字段"],["body","\n"],["body","_all 字段在新应用的探索阶段，当你还不清楚文档的最终结构时是比较有用的。你可以使用这个字段来做任何查询，并且有很大可能找到需要的文档："],["body","\n"],["body","禁用"],["body","\n"],["body","PUT /my_index/_mapping/my_type\n{\n    \"my_type\": {\n        \"_all\": { \"enabled\": false }\n    }\n}\n"],["body","\n"],["body","过 include_in_all 设置来逐个控制字段是否要包含在 _all 字段中，默认值是 true"],["body","\n"],["body","在一个对象(或根对象)上设置 include_in_all 可以修改这个对象中的所有字段的默认行为。"],["body","\n"],["body","记住，_all 字段仅仅是一个 经过分词的 string 字段。它使用默认分词器来分析它的值，不管这个值原本所在字段指定的分词器。就像所有 string 字段，你可以配置 _all 字段使用的分词器："],["body","\n"],["headingLink","元数据文档标识"],["heading","元数据：文档标识"],["body","\n"],["body","文档标识与四个元数据字段相关："],["body","\n\n"],["body","\n"],["body","_id"],["body","\n"],["body","文档的 ID 字符串"],["body","\n"],["body","\n"],["body","\n"],["body","_type"],["body","\n"],["body","文档的类型名"],["body","\n"],["body","\n"],["body","\n"],["body","_index"],["body","\n"],["body","文档所在的索引"],["body","\n"],["body","\n"],["body","\n"],["body","_uid"],["body","\n"],["body","_type 和 _id 连接在一起构造成 type#id"],["body","\n"],["body","\n\n"],["body","默认情况下， _uid 字段是被存储（可取回）和索引（可搜索）的。 _type 字段被索引但是没有存储， _id 和 _index 字段则既没有被索引也没有被存储，这意味着它们并不是真实存在的。"],["body","\n"],["headingLink","动态映射"],["heading","动态映射"],["body","\n"],["body","当 Elasticsearch 遇到文档中以前 未遇到的字段，它用 dynamic mapping 来确定字段的数据类型并自动把新的字段添加到类型映射。"],["body","\n"],["body","有时这是想要的行为有时又不希望这样。通常没有人知道以后会有什么新字段加到文档，但是又希望这些字段被自动的索引。也许你只想忽略它们。如果Elasticsearch是作为重要的数据存储，可能就会期望遇到新字段就会抛出异常，这样能及时发现问题。"],["body","\n"],["body","幸运的是可以用 dynamic 配置来控制这种行为 ，可接受的选项如下："],["body","\n\n"],["body","\n"],["body","true"],["body","\n"],["body","动态添加新的字段—缺省"],["body","\n"],["body","\n"],["body","\n"],["body","false"],["body","\n"],["body","忽略新的字段"],["body","\n"],["body","\n"],["body","\n"],["body","strict"],["body","\n"],["body","如果遇到新字段抛出异常"],["body","\n"],["body","\n\n"],["body","PUT /my_index\n{\n    \"mappings\": {\n        \"my_type\": {\n            \"dynamic\":      \"strict\", \n            \"properties\": {\n                \"title\":  { \"type\": \"string\"},\n                \"stash\":  {\n                    \"type\":     \"object\",\n                    \"dynamic\":  true \n                }\n            }\n        }\n    }\n}\n"],["body","\n\n"],["body","\n"],["body","my_type不允许新增字段"],["body","\n"],["body","\n"],["body","\n"],["body","stash 可以新增字段"],["body","\n"],["body","\n\n"],["headingLink","自定义动态映射"],["heading","自定义动态映射"],["body","\n"],["body","有时候，动态映射 规则 可能不太智能，我们可以通过设置去自定义这些规则"],["body","\n"],["headingLink","日期检测"],["heading","日期检测"],["body","\n"],["body","日期检测可以通过在根对象上设置 date_detection 为 false 来关闭："],["body","\n"],["body","PUT /my_index\n{\n    \"mappings\": {\n        \"my_type\": {\n            \"date_detection\": false\n        }\n    }\n}\n"],["body","\n"],["body","使用这个映射，字符串将始终作为 string 类型。如果你需要一个 date 字段，你必须手动添加。"],["body","\n"],["body","Elasticsearch 判断字符串为日期的规则可以通过 dynamic_date_formats setting 来设置。"],["body","\n"],["headingLink","动态模板"],["heading","动态模板"],["body","\n"],["body","使用 dynamic_templates ，你可以完全控制新检测生成字段的映射。你甚至可以通过字段名称或数据类型来应用不同的映射。"],["body","\n"],["body","每个模板都有一个名称，你可以用来描述这个模板的用途， 一个 mapping 来指定映射应该怎样使用，以及至少一个参数 (如 match) 来定义这个模板适用于哪个字段。"],["body","\n"],["body","模板按照顺序来检测；第一个匹配的模板会被启用。例如，我们给 string 类型字段定义两个模板："],["body","\n"],["body","PUT /my_index\n{\n    \"mappings\": {\n        \"my_type\": {\n            \"dynamic_templates\": [\n                { \"es\": {\n                      \"match\":              \"*_es\", \n                      \"match_mapping_type\": \"string\",\n                      \"mapping\": {\n                          \"type\":           \"string\",\n                          \"analyzer\":       \"spanish\"\n                      }\n                }},\n                { \"en\": {\n                      \"match\":              \"*\", \n                      \"match_mapping_type\": \"string\",\n                      \"mapping\": {\n                          \"type\":           \"string\",\n                          \"analyzer\":       \"english\"\n                      }\n                }}\n            ]\n}}}\n"],["body","\n"],["headingLink","缺省映射"],["heading","缺省映射"],["body","\n"],["body","通常，一个索引中的所有类型共享相同的字段和设置。 _default_ 映射更加方便地指定通用设置，而不是每次创建新类型时都要重复设置。 _default_ 映射是新类型的模板。在设置 _default_ 映射之后创建的所有类型都将应用这些缺省的设置，除非类型在自己的映射中明确覆盖这些设置。"],["body","\n"],["body","PUT /my_index\n{\n    \"mappings\": {\n        \"_default_\": {\n            \"_all\": { \"enabled\":  false }\n        },\n        \"blog\": {\n            \"_all\": { \"enabled\":  true  }\n        }\n    }\n}\n"],["body","\n"],["headingLink","重新索引数据"],["heading","重新索引数据"],["body","\n"],["body","字段 _source 的一个优点是在Elasticsearch中已经有整个文档。你不必从源数据中重建索引，而且那样通常比较慢。"],["body","\n"],["body","为了有效的重新索引所有在旧的索引中的文档，用 scroll 从旧的索引检索批量文档 ， 然后用 bulk API 把文档推送到新的索引中。"],["body","\n"],["body","从Elasticsearch v2.3.0开始， Reindex API 被引入。它能够对文档重建索引而不需要任何插件或外部工具。"],["body","\n"],["body","GET /old_index/_search?scroll=1m\n{\n    \"query\": {\n        \"range\": {\n            \"date\": {\n                \"gte\":  \"2014-01-01\",\n                \"lt\":   \"2014-02-01\"\n            }\n        }\n    },\n    \"sort\": [\"_doc\"],\n    \"size\":  1000\n}\n"],["body","\n"],["headingLink","索引别名和零停机"],["heading","索引别名和零停机"],["body","\n"],["body","重建索引的问题是必须更新应用中的索引名称。 索引别名就是用来解决这个问题的！"],["body","\n"],["body","索引 别名 就像一个快捷方式或软连接，可以指向一个或多个索引，也可以给任何一个需要索引名的API来使用。别名 带给我们极大的灵活性，允许我们做下面这些："],["body","\n\n"],["body","无缝切换索引"],["body","\n"],["body","索引分组"],["body","\n"],["body","当做视图使用"],["body","\n\n"],["body","有两种方式管理别名： _alias 用于单个操作， _aliases 用于执行多个原子级操作。"],["body","\n"],["body","# 创建索引\nPUT /my_index_v1 \n# 创建别名\nPUT /my_index_v1/_alias/my_index \n# 这个别名指向哪一个索引：\nGET /*/_alias/my_index\n# 哪些别名指向这个索引：\nGET /my_index_v1/_alias/*\n\n# 重新索引\n# 然后我们将数据从 my_index_v1 索引到 my_index_v2 \nPUT /my_index_v2\n{\n    \"mappings\": {\n        \"my_type\": {\n            \"properties\": {\n                \"tags\": {\n                    \"type\":   \"string\",\n                    \"index\":  \"not_analyzed\"\n                }\n            }\n        }\n    }\n}\n"],["body","\n"],["body","一个别名可以指向多个索引，所以我们在添加别名到新索引的同时必须从旧的索引中删除它。这个操作需要原子化，这意味着我们需要使用 _aliases 操作："],["body","\n"],["body","POST /_aliases\n{\n    \"actions\": [\n        { \"remove\": { \"index\": \"my_index_v1\", \"alias\": \"my_index\" }},\n        { \"add\":    { \"index\": \"my_index_v2\", \"alias\": \"my_index\" }}\n    ]\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/集群/Health.html"],["title","Health.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","cluster-health-api"],["heading","Cluster health API"],["body","\n"],["body","返回集群健康信息"],["body","\n"],["headingLink","request"],["heading","Request"],["body","\n"],["body","GET /_cluster/health/<target>\n"],["body","\n"],["body","集群运行状况API返回集群运行状况的简单状态。您也可以使用API仅获取指定数据流和索引的健康状态。"],["body","\n"],["body","对于数据流，API检索流的支持索引的健康状态。"],["body","\n"],["body","集群健康状态为: 绿色、黄色或红色。"],["body","\n\n"],["body","在分片级别上，"],["body","\n\n\n"],["body","红色表示集群中未分配特定shard，"],["body","\n"],["body","黄色表示已分配主shard但副本未分配，"],["body","\n"],["body","绿色表示已分配所有shard。"],["body","\n\n\n"],["body","\n"],["body","索引级别状态由最差分片状态控制。"],["body","\n"],["body","\n"],["body","\n"],["body","集群状态由最差索引状态控制。"],["body","\n"],["body","\n\n"],["body","API的主要好处之一是可以等到集群达到一定的高水标健康水平。"],["body","\n"],["body","例如，以下内容将等待50秒，以使群集达到黄色级别 (如果在50秒过去之前达到绿色或黄色状态，它将在该点上返回):"],["body","\n"],["body","GET /_cluster/health?wait_for_status=yellow&timeout=50s\n"],["body","\n"],["headingLink","path-parameters"],["heading","Path parameters"],["body","\n"],["headingLink","target"],["heading","<target>"],["body","\n\n"],["body","可选的逗号分割的字符串"],["body","\n"],["body","可以表示 索引、别名 dataStreams"],["body","\n"],["body","可以支持 *"],["body","\n\n"],["headingLink","query-parameters"],["heading","Query parameters"],["body","\n"],["headingLink","level"],["heading","level"],["body","\n\n"],["body","集群的级别：\n\n"],["body","cluster 默认"],["body","\n"],["body","indices"],["body","\n"],["body","shards"],["body","\n\n"],["body","\n\n"],["headingLink","local"],["heading","local"],["body","\n\n"],["body","只从管理节点拿状态"],["body","\n"],["body","默认false"],["body","\n"],["body","false表示从 主节点拿状态"],["body","\n\n"],["headingLink","master_timeout"],["heading","master_timeout"],["body","\n\n"],["body","可选的 时间单位"],["body","\n"],["body","连接主节点的超时时间"],["body","\n"],["body","默认30s"],["body","\n\n"],["headingLink","timeout"],["heading","timeout"],["body","\n\n"],["body","可选的 时间单位"],["body","\n"],["body","等待响应的 时间"],["body","\n"],["body","默认30s"],["body","\n\n"],["headingLink","wait_for_active_shards"],["heading","wait_for_active_shards"],["body","\n\n"],["body","等待多少个活跃分片"],["body","\n"],["body","all表示 等全部活跃分片"],["body","\n"],["body","默认0 表示 不等待"],["body","\n\n"],["headingLink","wait_for_events"],["heading","wait_for_events"],["body","\n"],["body","(Optional, string) Can be one of immediate, urgent, high, normal, low, languid. Wait until all currently queued events with the given priority are processed."],["body","\n"],["headingLink","wait_for_no_initializing_shards"],["heading","wait_for_no_initializing_shards"],["body","\n"],["body","(可选，布尔值) 一个布尔值，该值控制是否等待 (直到提供超时) 以使群集没有分片初始化。默认为false，这意味着它不会等待初始化分片。"],["body","\n"],["headingLink","wait_for_no_relocating_shards"],["heading","wait_for_no_relocating_shards"],["body","\n"],["body","(可选，布尔值) 一个布尔值，该值控制是否等待 (直到提供超时) 以使群集没有分片重定位。默认为false，这意味着它不会等待重新定位分片。"],["body","\n"],["headingLink","wait_for_nodes"],["heading","wait_for_nodes"],["body","\n"],["body","(可选，字符串) 请求等待，直到指定数量的N个节点可用。它也接受> = N，<= N，> N和 <N。或者，可以使用ge(N)，le(N)，gt(N) 和lt(N) 表示法。"],["body","\n"],["headingLink","wait_for_status"],["heading","wait_for_status"],["body","\n"],["body","(可选，字符串) 绿色，黄色或红色之一。将等待 (直到提供的超时)，直到群集的状态更改为提供的状态或更好的状态，即绿色> 黄色> 红色。默认情况下，不会等待任何状态。"],["body","\n"],["headingLink","response-body"],["heading","Response body"],["body","\n"],["headingLink","cluster_name"],["heading","cluster_name"],["body","\n"],["body","(string) The name of the cluster."],["body","\n"],["headingLink","status"],["heading","status"],["body","\n"],["body","(string) Health status of the cluster, based on the state of its primary and replica shards. Statuses are:"],["body","\n\n"],["body","\n"],["body","green"],["body","\n"],["body","All shards are assigned."],["body","\n"],["body","\n"],["body","\n"],["body","yellow"],["body","\n"],["body","All primary shards are assigned, but one or more replica shards are unassigned. If a node in the cluster fails, some data could be unavailable until that node is repaired."],["body","\n"],["body","\n"],["body","\n"],["body","red"],["body","\n"],["body","One or more primary shards are unassigned, so some data is unavailable. This can occur briefly during cluster startup as primary shards are assigned."],["body","\n"],["body","\n\n"],["body","timed_out"],["body","\n"],["body","(Boolean) If false the response returned within the period of time that is specified by the timeout parameter (30s by default)."],["body","\n"],["headingLink","number_of_nodes"],["heading","number_of_nodes"],["body","\n"],["body","(integer) The number of nodes within the cluster."],["body","\n"],["headingLink","number_of_data_nodes"],["heading","number_of_data_nodes"],["body","\n"],["body","(integer) The number of nodes that are dedicated data nodes."],["body","\n"],["headingLink","active_primary_shards"],["heading","active_primary_shards"],["body","\n"],["body","(integer) The number of active primary shards."],["body","\n"],["headingLink","active_shards"],["heading","active_shards"],["body","\n"],["body","(integer) The total number of active primary and replica shards."],["body","\n"],["headingLink","relocating_shards"],["heading","relocating_shards"],["body","\n"],["body","(integer) The number of shards that are under relocation."],["body","\n"],["headingLink","initializing_shards"],["heading","initializing_shards"],["body","\n"],["body","(integer) The number of shards that are under initialization."],["body","\n"],["headingLink","unassigned_shards"],["heading","unassigned_shards"],["body","\n"],["body","(integer) The number of shards that are not allocated."],["body","\n"],["headingLink","delayed_unassigned_shards"],["heading","delayed_unassigned_shards"],["body","\n"],["body","(integer) The number of shards whose allocation has been delayed by the timeout settings."],["body","\n"],["headingLink","number_of_pending_tasks"],["heading","number_of_pending_tasks"],["body","\n"],["body","(integer) The number of cluster-level changes that have not yet been executed."],["body","\n"],["headingLink","number_of_in_flight_fetch"],["heading","number_of_in_flight_fetch"],["body","\n"],["body","(integer) The number of unfinished fetches."],["body","\n"],["headingLink","task_max_waiting_in_queue_millis"],["heading","task_max_waiting_in_queue_millis"],["body","\n"],["body","(整数) 任务最长等待时间"],["body","\n"],["headingLink","active_shards_percent_as_number"],["heading","active_shards_percent_as_number"],["body","\n"],["body","(float) The ratio of active shards in the cluster expressed as a percentage."],["body","\n"],["headingLink","example"],["heading","example"],["body","\n"],["body","{\n    \"cluster_name\": \"my-application\",\n    \"status\": \"green\",\n    \"timed_out\": false,\n    \"number_of_nodes\": 1,\n    \"number_of_data_nodes\": 1,\n    \"active_primary_shards\": 3,\n    \"active_shards\": 3,\n    \"relocating_shards\": 0,\n    \"initializing_shards\": 0,\n    \"unassigned_shards\": 0,\n    \"delayed_unassigned_shards\": 0,\n    \"number_of_pending_tasks\": 0,\n    \"number_of_in_flight_fetch\": 0,\n    \"task_max_waiting_in_queue_millis\": 0,\n    \"active_shards_percent_as_number\": 100\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/1.IndexModules/4.Mapper.html"],["title","Mapper.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","mapper"],["heading","Mapper"],["body","\n\n"],["body","mapper module 在创建索引时或使用 update mapping API 充当添加到索引的类型映射定义的注册表"],["body","\n"],["body","它还处理对没有预先定义的显式映射的类型的动态映射支持"],["body","\n"],["body","check out the mapping section."],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/1.IndexModules/9.Translog.html"],["title","Translog.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","translog"],["heading","Translog"],["body","\n\n"],["body","对Lucene的更改仅在Lucene提交期间保留到磁盘，这是相对昂贵的操作，因此无法在每次索引或删除操作后执行"],["body","\n"],["body","在进程退出或硬件故障的情况下，Lucene将在一次提交之后和另一次提交之前发生的更改从索引中删除。"],["body","\n"],["body","Lucene提交过于昂贵，无法对每个单独的更改执行，因此每个分片副本还将操作写入其称为translog的事务日志中。"],["body","\n"],["body","在内部Lucene索引处理后但在确认提交之前，所有索引和删除操作都将写入translog"],["body","\n"],["body","如果发生崩溃，则在分片恢复时，将从translog中恢复已确认但尚未包含在上次Lucene提交中的最近操作。"],["body","\n"],["body","Elasticsearch刷新 flush 是执行Lucene提交并开始新的translog生成的过程。"],["body","\n"],["body","刷新在后台自动执行，以确保translog不会增长太大，如果太大这将使重放其操作在恢复过程中花费大量时间"],["body","\n"],["body","手动执行刷新的能力也通过API公开，尽管这很少需要。"],["body","\n\n"],["headingLink","translog-settings"],["heading","Translog settings"],["body","\n\n"],["body","translog中的数据只有在fsynced和committed translog时才会持久化到磁盘"],["body","\n"],["body","如果发生硬件故障或操作系统崩溃或JVM崩溃或分片故障，则自上一次 的translog 提交以来写入的任何数据都将丢失。"],["body","\n"],["body","默认情况下， index.translog.durability is set to request ，这意味着Elasticsearch仅在成功fsyned并在主副本和每个已分配副本上提交translog后，才会向客户端报告索引、删除、更新或批量请求的成功。"],["body","\n"],["body","如果设置成 index.translog.durability is set to async ，然后Elasticsearch fsyncs并提交translog，在每个index.translog.sync_interval 期间这意味着在崩溃之前执行的任何操作都可能在节点恢复时丢失。"],["body","\n\n"],["body","以下可动态更新的每个索引设置控制translog的行为:"],["body","\n\n"],["body","\n"],["body","index.translog.sync_interval"],["body","\n"],["body","How often the translog is fsynced to disk and committed, regardless of write operations. Defaults to 5s. Values less than 100ms are not allowed."],["body","\n"],["body","无论写操作如何，translog多久被同步到磁盘并提交一次。默认为5s。小于100ms的值是不允许的。"],["body","\n"],["body","\n"],["body","\n"],["body","index.translog.durability"],["body","\n\n"],["body","是否在每个索引、删除、更新或批量请求后fsync和提交translog。"],["body","\n"],["body","此设置接受以下参数"],["body","\n"],["body","request(default) Fsync在每次请求后提交。如果发生硬件故障，所有确认的写入都将已提交到磁盘"],["body","\n"],["body","async   fsync and commit in the background every sync_interval. 如果发生故障，自上次自动提交以来的所有已确认写入都将被丢弃。"],["body","\n\n"],["body","\n"],["body","\n"],["body","index.translog.flush_threshold_size"],["body","\n\n"],["body","translog存储尚未安全地保存在Lucene中的所有操作 (即，不是Lucene提交点的一部分)"],["body","\n"],["body","尽管这些操作可用于读取，但如果分片已停止并必须恢复，则需要对其进行重放。"],["body","\n"],["body","此设置控制这些操作的最大总大小，以防止恢复时间过长。"],["body","\n"],["body","一旦达到最大大小，将发生刷新，生成新的Lucene提交点。默认为512mb。"],["body","\n\n"],["body","\n\n"],["headingLink","translog-retention"],["heading","Translog retention"],["body","\n"],["headingLink","deprecated-in-740"],["heading","Deprecated in 7.4.0."],["body","\n"],["body","Translog retention settings are deprecated in favor of soft deletes. These settings are effectively ignored since 7.4 and will be removed in a future version."],["body","\n"],["body","If an index is not using soft deletes to retain historical operations then Elasticsearch recovers each replica shard by replaying operations from the primary’s translog. This means it is important for the primary to preserve extra operations in its translog in case it needs to rebuild a replica. Moreover it is important for each replica to preserve extra operations in its translog in case it is promoted to primary and then needs to rebuild its own replicas in turn. The following settings control how much translog is retained for peer recoveries."],["body","\n\n"],["body","\n"],["body","index.translog.retention.size"],["body","\n"],["body","This controls the total size of translog files to keep for each shard. Keeping more translog files increases the chance of performing an operation based sync when recovering a replica. If the translog files are not sufficient, replica recovery will fall back to a file based sync. Defaults to 512mb. This setting is ignored, and should not be set, if soft deletes are enabled. Soft deletes are enabled by default in indices created in Elasticsearch versions 7.0.0 and later."],["body","\n"],["body","\n"],["body","\n"],["body","index.translog.retention.age"],["body","\n"],["body","This controls the maximum duration for which translog files are kept by each shard. Keeping more translog files increases the chance of performing an operation based sync when recovering replicas. If the translog files are not sufficient, replica recovery will fall back to a file based sync. Defaults to 12h. This setting is ignored, and should not be set, if soft deletes are enabled. Soft deletes are enabled by default in indices created in Elasticsearch versions 7.0.0 and later."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/1.IndexModules/5.Merge.html"],["title","Merge.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","merge"],["heading","Merge"],["body","\n\n"],["body","Elasticsearch中的一个分片是Lucene索引，Lucene 索引 被分解成若干段"],["body","\n"],["body","段是索引中存储索引数据的内部存储元素，并且是不可变的"],["body","\n"],["body","较小的段会定期合并到较大的段中，以保持索引大小不变并删除删除内容。"],["body","\n"],["body","合并过程使用自动节流（auto-throttling）来平衡合并和其他活动 (如搜索) 之间硬件资源的使用。"],["body","\n\n"],["headingLink","merge-scheduling"],["heading","Merge scheduling"],["body","\n\n"],["body","合并调度程序 (ConcurrentMergeScheduler) 在需要时控制合并操作的执行。"],["body","\n"],["body","Merges在单独的线程中运行，当达到最大线程数时，进一步的merges将等待，直到合并线程变得可用。"],["body","\n\n"],["body","合并调度程序支持以下动态设置:"],["body","\n\n"],["body","index.merge.scheduler.max_thread_count\n\n"],["body","单个分片上可能一次合并的最大线程数"],["body","\n"],["body","Defaults to Math.max(1, Math.min(4, <<node.processors, node.processors>> / 2)) 这对于一个好的固态磁盘 (SSD) 来说效果很好。"],["body","\n"],["body","如果您的索引位于旋转盘片驱动器上，请将其减小为1。"],["body","\n\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/1.IndexModules/README.html"],["title","IndexModules - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","index-modules"],["heading","Index modules"],["body","\n"],["body","索引模块是每个索引创建的模块，可控制与索引相关的所有方面。"],["body","\n"],["headingLink","先验知识"],["heading","先验知识"],["body","\n\n"],["body","closed index"],["body","\n"],["body","索引拆分 split"],["body","\n"],["body","routing field"],["body","\n"],["body","cached filters"],["body","\n"],["body","allocation filtering"],["body","\n"],["body","total shards per node"],["body","\n\n"],["headingLink","index-settings"],["heading","Index Settings"],["body","\n"],["body","索引级别的设置可能是:"],["body","\n\n"],["body","\n"],["body","static"],["body","\n"],["body","它们只能在索引创建时或在 closed index 索引上设置。"],["body","\n"],["body","\n"],["body","\n"],["body","dynamic"],["body","\n"],["body","可以通过 update-index-settings API 改变的设置"],["body","\n"],["body","\n\n"],["body","更改已关闭索引上的静态或动态索引设置可能会导致不正确的设置，而如果不删除和重新创建索引，则无法纠正这些设置。"],["body","\n"],["headingLink","static-index-settings"],["heading","Static index settings"],["body","\n"],["body","以下是索引通用的静态设置的列表:"],["body","\n\n"],["body","\n"],["body","index.number_of_shards"],["body","\n\n"],["body","索引应具有的主分片数量"],["body","\n"],["body","默认1。只能在索引创建时设置。可以作用于closed index"],["body","\n"],["body","每个索引的主分片数最多 1024"],["body","\n"],["body","此限制是一个安全限制，以防止意外创建索引，这些索引会由于资源分配而使群集不稳定。"],["body","\n"],["body","这个限制可以通过  export ES_JAVA_OPTS=\"-Des.index.max_number_of_shards=128\" 系统变量修改（每个节点都要修改）"],["body","\n\n"],["body","\n"],["body","\n"],["body","index.number_of_routing_shards"],["body","\n\n"],["body","\n"],["body","用于拆分 split  索引的路由分片数"],["body","\n"],["body","\n"],["body","\n"],["body","五分片的索引，设置  number_of_routing_shards 为 30 （5x2x3）"],["body","\n"],["body","\n"],["body","\n"],["body","换句话说，"],["body","\n"],["body","`5` → `10` → `30`\n\n`5` → `15` → `30` \n\n`5` → `30`\n"],["body","\n"],["body","\n"],["body","\n"],["body","此设置的默认值取决于索引中主分片的数量。"],["body","\n"],["body","\n"],["body","\n"],["body","默认值旨在允许您按2的因子拆分，最多1024个分片"],["body","\n"],["body","\n"],["body","\n"],["body","在Elasticsearch 7.0.0及更高版本中，此设置会影响文档在各个分片之间的分布方式"],["body","\n"],["body","\n"],["body","\n"],["body","使用自定义路由重新索引较旧的索引时，您必须显式设置index.number_of_routing_shards以保持相同的文档分布 See the related breaking change."],["body","\n"],["body","\n\n"],["body","\n"],["body","\n"],["body","index.shard.check_on_startup"],["body","\n"],["body","Whether or not shards should be checked for corruption before opening."],["body","\n\n"],["body","打开分片前 是否检查 分片是否损坏。如果一损坏则不会打开"],["body","\n"],["body","默认，false。不检查"],["body","\n"],["body","checksum：检查物理 校验和"],["body","\n"],["body","true： 既检查物理也检查逻辑损坏。这个很耗CPU跟内存、仅限专家。在大型索引上检查分片可能需要大量时间。"],["body","\n\n"],["body","\n"],["body","\n"],["body","index.codec"],["body","\n\n"],["body","默认值使用LZ4压缩来压缩存储的数据，"],["body","\n"],["body","但这可以设置为best_compression，它使用 DEFLATE 来获得更高的压缩比，但以较慢的存储字段性能为代价。"],["body","\n"],["body","如果要更新compression type，则在合并段后将应用新的压缩类型"],["body","\n"],["body","Segment merging can be forced using force merge."],["body","\n\n"],["body","\n"],["body","\n"],["body","index.routing_partition_size"],["body","\n"],["body","The number of shards a custom routing value can go to."],["body","\n\n"],["body","自定义路由值。可以 routing  的分片数。"],["body","\n"],["body","默认为1，只能在索引创建时设置。"],["body","\n"],["body","必须小于  index.number_of_shards 。除非  index.number_of_shards设置为1"],["body","\n"],["body","详见： Routing to an index partition"],["body","\n\n"],["body","\n"],["body","\n"],["body","index.soft_deletes.enabled"],["body","\n\n"],["body","[7.6.0] Deprecated in 7.6.0."],["body","\n"],["body","不建议使用禁用软删除的创建索引，并且将在以后的Elasticsearch版本中删除。"],["body","\n"],["body","指示是否在索引上启用了软删除"],["body","\n"],["body","软删除只能在索引创建时配置，并且只能在Elasticsearch 6.5.0上或之后创建的索引上配置。默认为true。"],["body","\n\n"],["body","\n"],["body","\n"],["body","index.soft_deletes.retention_lease.period"],["body","\n\n"],["body","保留分片历史记录的最长时间"],["body","\n"],["body","碎片历史记录保留租约确保在合并Lucene索引期间保留软删除。"],["body","\n"],["body","如果软删除在可以复制到flollower 之前被合并了"],["body","\n"],["body","以下过程将由于 leader 的历史记录不完整而失败."],["body","\n"],["body","Defaults to 12h"],["body","\n\n"],["body","\n"],["body","\n"],["body","index.load_fixed_bitset_filters_eagerly"],["body","\n\n"],["body","load_fixed_bitset_filters_eagerly"],["body","\n"],["body","是否对 嵌套查询 预加载  cached filters"],["body","\n"],["body","默认FALSE"],["body","\n\n"],["body","\n"],["body","\n"],["body","index.hidden"],["body","\n\n"],["body","指示默认情况下是否应该隐藏索引"],["body","\n"],["body","使用通配符表达式时，默认情况下不返回隐藏索引。"],["body","\n"],["body","通过使用expand_wildcards参数来控制此行为。"],["body","\n"],["body","默认FALSE"],["body","\n\n"],["body","\n\n"],["headingLink","dynamic-index-settings"],["heading","Dynamic index settings"],["body","\n"],["body","以下是与任何特定索引模块不关联的所有动态索引设置"],["body","\n\n"],["body","\n"],["body","index.number_of_replicas"],["body","\n\n"],["body","每个主分片的副本数。默认为1。"],["body","\n\n"],["body","\n"],["body","\n"],["body","index.auto_expand_replicas"],["body","\n\n"],["body","根据集群中的数据节点数自动扩展副本数"],["body","\n"],["body","设置为限定下限和上限的破折号 (例如. 0-5) or use all for the upper bound (e.g. 0-all)."],["body","\n"],["body","默认FALSE禁用。"],["body","\n"],["body","分片自动扩展数量 只 会考虑  allocation filtering  会忽略  total shards per node, 如果适用的规则阻止分配所有副本，则这可能导致群集运行状况变为黄色。"],["body","\n"],["body","If the upper bound is all then shard allocation awareness and cluster.routing.allocation.same_shard.host are ignored for this index."],["body","\n\n"],["body","\n"],["body","\n"],["body","index.search.idle.after"],["body","\n\n"],["body","在被认为搜索空闲之前，分片无法接收搜索或获取请求多长时间。(默认为30秒)"],["body","\n\n"],["body","\n"],["body","\n"],["body","index.refresh_interval"],["body","\n\n"],["body","执行刷新操作的频率，这使索引的最新更改对搜索可见"],["body","\n"],["body","默认1s。-1 则是禁止刷新"],["body","\n"],["body","如果未明确设置此设置，则至少在index.search.idle秒前。没有看到搜索流量的分片，在收到搜索请求之前，它们将不会收到后台刷新。"],["body","\n"],["body","命中空闲且刷新被阻塞的分片的搜索请求。将会在下次后台刷新（1s）"],["body","\n"],["body","此行为旨在在不执行搜索的默认情况下自动优化批量索引。"],["body","\n"],["body","为了选择退出此行为，应将此值显示的设置为 1"],["body","\n\n"],["body","\n"],["body","\n"],["body","index.max_result_window"],["body","\n\n"],["body","from+size 搜索的最大大小"],["body","\n"],["body","默认1w"],["body","\n"],["body","搜索请求 消耗大量堆、跟时间"],["body","\n"],["body","详见： Scroll or Search After"],["body","\n\n"],["body","\n"],["body","\n"],["body","index.max_inner_result_window"],["body","\n\n"],["body","The maximum value of from + size for inner hits definition and top hits aggregations to this index."],["body","\n"],["body","分片内部返回的最大值"],["body","\n"],["body","默认100。"],["body","\n\n"],["body","\n"],["body","\n"],["body","index.max_rescore_window"],["body","\n\n"],["body","The maximum value of window_size for rescore requests in searches of this index."],["body","\n"],["body",". Defaults to index.max_result_window which defaults to 10000."],["body","\n"],["body","Search requests take heap memory and time proportional to max(window_size, from + size) and this limits that memory."],["body","\n\n"],["body","\n"],["body","\n"],["body","index.max_docvalue_fields_search"],["body","\n\n"],["body","The maximum number of docvalue_fields that are allowed in a query."],["body","\n"],["body","Defaults to 100. Doc-value fields are costly since they might incur a per-field per-document seek."],["body","\n\n"],["body","\n"],["body","\n"],["body","index.max_script_fields"],["body","\n"],["body","The maximum number of script_fields that are allowed in a query. Defaults to 32."],["body","\n"],["body","\n"],["body","\n"],["body","index.max_ngram_diff"],["body","\n"],["body","对于NGramTokenizer和NGramTokenFilter，min_gram和max_gram之间的最大允许差。默认为1。"],["body","\n"],["body","\n"],["body","\n"],["body","index.max_shingle_diff"],["body","\n"],["body","The maximum allowed difference between max_shingle_size and min_shingle_size for the shingle token filter. Defaults to 3."],["body","\n"],["body","\n"],["body","\n"],["body","index.max_refresh_listeners"],["body","\n"],["body","Maximum number of refresh listeners available on each shard of the index. These listeners are used to implement refresh=wait_for."],["body","\n"],["body","\n"],["body","\n"],["body","index.analyze.max_token_count"],["body","\n"],["body","The maximum number of tokens that can be produced using _analyze API. Defaults to 10000."],["body","\n"],["body","\n"],["body","\n"],["body","index.highlight.max_analyzed_offset"],["body","\n"],["body","The maximum number of characters that will be analyzed for a highlight request. This setting is only applicable when highlighting is requested on a text that was indexed without offsets or term vectors. Defaults to 1000000."],["body","\n"],["body","\n"],["body","\n"],["body","index.max_terms_count"],["body","\n"],["body","The maximum number of terms that can be used in Terms Query. Defaults to 65536."],["body","\n"],["body","\n"],["body","\n"],["body","index.max_regex_length"],["body","\n"],["body","The maximum length of regex that can be used in Regexp Query. Defaults to 1000."],["body","\n"],["body","\n"],["body","\n"],["body","index.query.default_field"],["body","\n"],["body","(string or array of strings) Wildcard (*) patterns matching one or more fields. The following query types search these matching fields by default:More like thisMulti-matchQuery stringSimple query stringDefaults to *, which matches all fields eligible for term-level queries, excluding metadata fields."],["body","\n"],["body","\n"],["body","\n"],["body","index.routing.allocation.enable"],["body","\n\n"],["body","控制此索引的分片分配。"],["body","\n"],["body","It can be set to:all (default) - 允许所有分片的分片分配。"],["body","\n"],["body","primaries -仅允许主分片分配."],["body","\n"],["body","new_primaries - 只允许新创建的主分片分配."],["body","\n"],["body","none - 不允许分片分配。"],["body","\n\n"],["body","\n"],["body","\n"],["body","index.routing.rebalance.enable"],["body","\n\n"],["body","启用此索引的分片再平衡"],["body","\n"],["body","It can be set to:all (default) - Allows shard rebalancing for all shards."],["body","\n"],["body","primaries - Allows shard rebalancing only for primary shards."],["body","\n"],["body","replicas - Allows shard rebalancing only for replica shards."],["body","\n"],["body","none - No shard rebalancing is allowed."],["body","\n\n"],["body","\n"],["body","\n"],["body","index.gc_deletes"],["body","\n"],["body","The length of time that a deleted document’s version number remains available for further versioned operations. Defaults to 60s."],["body","\n"],["body","\n"],["body","\n"],["body","index.default_pipeline"],["body","\n\n"],["body","The default ingest node pipeline for this index."],["body","\n"],["body","Index requests will fail if the default pipeline is set and the pipeline does not exist."],["body","\n"],["body","The default may be overridden using the pipeline parameter."],["body","\n"],["body","The special pipeline name _none indicates no ingest pipeline should be run."],["body","\n\n"],["body","\n"],["body","\n"],["body","index.final_pipeline"],["body","\n\n"],["body","The final ingest node pipeline for this index."],["body","\n"],["body","Indexing requests will fail if the final pipeline is set and the pipeline does not exist."],["body","\n"],["body","The final pipeline always runs after the request pipeline (if specified) and the default pipeline (if it exists)."],["body","\n"],["body","The special pipeline name _none indicates no ingest pipeline will run."],["body","\n"],["body","You can’t use a final pipelines to change the _index field. If the pipeline attempts to change the _index field, the indexing request will fail."],["body","\n\n"],["body","\n\n"],["headingLink","settings-in-other-index-modules"],["heading","Settings in other index modules"],["body","\n"],["body","Other index settings are available in index modules:"],["body","\n\n"],["body","\n"],["body","Analysis"],["body","\n"],["body","Settings to define analyzers, tokenizers, token filters and character filters."],["body","\n"],["body","\n"],["body","\n"],["body","Index shard allocation"],["body","\n"],["body","Control over where, when, and how shards are allocated to nodes."],["body","\n"],["body","\n"],["body","\n"],["body","Mapping"],["body","\n"],["body","Enable or disable dynamic mapping for an index."],["body","\n"],["body","\n"],["body","\n"],["body","Merging"],["body","\n"],["body","Control over how shards are merged by the background merge process."],["body","\n"],["body","\n"],["body","\n"],["body","Similarities"],["body","\n"],["body","Configure custom similarity settings to customize how search results are scored."],["body","\n"],["body","\n"],["body","\n"],["body","Slowlog"],["body","\n"],["body","Control over how slow queries and fetch requests are logged."],["body","\n"],["body","\n"],["body","\n"],["body","Store"],["body","\n"],["body","Configure the type of filesystem used to access shard data."],["body","\n"],["body","\n"],["body","\n"],["body","Translog"],["body","\n"],["body","Control over the transaction log and background flush operations."],["body","\n"],["body","\n"],["body","\n"],["body","History retention"],["body","\n"],["body","Control over the retention of a history of operations in the index."],["body","\n"],["body","\n"],["body","\n"],["body","Indexing pressure"],["body","\n"],["body","Configure indexing back pressure limits."],["body","\n"],["body","\n\n"],["headingLink","x-pack-index-settings"],["heading","X-Pack index settings"],["body","\n\n"],["body","Index lifecycle management\nSpecify the lifecycle policy and rollover alias for an index."],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/1.IndexModules/8.Store.html"],["title","Store.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","store"],["heading","Store"],["body","\n"],["body","存储模块允许您控制索引数据在磁盘上的存储和访问方式。"],["body","\n"],["body","这是一个低级的设置。某些存储实现的并发性较差，或者禁用了对堆内存使用的优化。我们建议坚持使用默认值。"],["body","\n"],["headingLink","file-system-storage-types"],["heading","File system storage types"],["body","\n"],["body","有不同的文件系统实现或存储类型。默认情况下，Elasticsearch会根据操作环境选择最佳实现。"],["body","\n"],["body","还可以通过在config/elasticsearch.yml文件中配置存储类型，为所有索引显式设置存储类型:"],["body","\n"],["body","index.store.type: hybridfs\n"],["body","\n"],["body","它是一个静态设置，可以在索引创建时按索引设置:"],["body","\n"],["body","PUT /my-index-000001\n{\n  \"settings\": {\n    \"index.store.type\": \"hybridfs\"\n  }\n}\n"],["body","\n"],["body","这是一个仅限专家的设置，将来可能会删除。"],["body","\n"],["body","以下部分列出了支持的所有不同存储类型。"],["body","\n\n"],["body","\n"],["body","fs"],["body","\n"],["body","Default file system implementation. This will pick the best implementation depending on the operating environment, which is currently hybridfs on all supported systems but is subject to change."],["body","\n"],["body","\n"],["body","\n"],["body","simplefs"],["body","\n"],["body","The Simple FS type is a straightforward implementation of file system storage (maps to Lucene SimpleFsDirectory) using a random access file. This implementation has poor concurrent performance (multiple threads will bottleneck) and disables some optimizations for heap memory usage."],["body","\n"],["body","\n"],["body","\n"],["body","niofs"],["body","\n"],["body","The NIO FS type stores the shard index on the file system (maps to Lucene NIOFSDirectory) using NIO. It allows multiple threads to read from the same file concurrently. It is not recommended on Windows because of a bug in the SUN Java implementation and disables some optimizations for heap memory usage."],["body","\n"],["body","\n"],["body","\n"],["body","mmapfs"],["body","\n"],["body","The MMap FS type stores the shard index on the file system (maps to Lucene MMapDirectory) by mapping a file into memory (mmap). Memory mapping uses up a portion of the virtual memory address space in your process equal to the size of the file being mapped. Before using this class, be sure you have allowed plenty of virtual address space."],["body","\n"],["body","\n"],["body","\n"],["body","hybridfs"],["body","\n"],["body","The hybridfs type is a hybrid of niofs and mmapfs, which chooses the best file system type for each type of file based on the read access pattern. Currently only the Lucene term dictionary, norms and doc values files are memory mapped. All other files are opened using Lucene NIOFSDirectory. Similarly to mmapfs be sure you have allowed plenty of virtual address space."],["body","\n"],["body","\n\n\n"],["body","You can restrict the use of the mmapfs and the related hybridfs store type via the setting node.store.allow_mmap."],["body","\n"],["body","This is a boolean setting indicating whether or not memory-mapping is allowed."],["body","\n"],["body","The default is to allow it. This setting is useful, for example, if you are in an environment where you can not control the ability to create a lot of memory maps so you need disable the ability to use memory-mapping."],["body","\n\n"],["headingLink","preloading-data-into-the-file-system-cache"],["heading","Preloading data into the file system cache"],["body","\n"],["body","\n"],["body","This is an expert setting, the details of which may change in the future."],["body","\n"],["body","\n\n"],["body","默认情况下，Elasticsearch完全依赖操作系统文件系统缓存来缓存I/O操作"],["body","\n"],["body","It is possible to set index.store.preload in order to tell the operating system to load the content of hot index files into memory upon opening."],["body","\n"],["body","可以设置index.store.preload，以便告诉操作系统在打开时将热索引文件的内容加载到内存中"],["body","\n"],["body","此设置接受以逗号分隔的文件扩展名列表: 所有扩展名在列表中的文件将在打开时预加载"],["body","\n"],["body","这对于提高索引的搜索性能非常有用，尤其是在重新启动主机操作系统时，因为这会导致文件系统缓存被丢弃"],["body","\n"],["body","但是请注意，这可能会减慢索引的打开速度，因为它们仅在数据已加载到物理内存后才可用。"],["body","\n\n"],["body","此设置仅是尽力而为，根据store 类型和主机操作系统的不同，可能根本不起作用。"],["body","\n"],["body","The index.store.preload is a static setting that can either be set in the config/elasticsearch.yml:"],["body","\n"],["body","index.store.preload: [\"nvd\", \"dvd\"]\n"],["body","\n"],["body","or in the index settings at index creation time:"],["body","\n"],["body","PUT /my-index-000001\n{\n  \"settings\": {\n    \"index.store.preload\": [\"nvd\", \"dvd\"]\n  }\n}\n"],["body","\n\n"],["body","默认值是空数组，这意味着什么都不会被急切地加载到文件系统缓存中。"],["body","\n"],["body","对于主动搜索的索引，您可能希望将其设置为 [“nvd”，“dvd”]，这将导致 norms 和doc values 急切地加载到物理内存中"],["body","\n"],["body","These are the two first extensions to look at since Elasticsearch performs random access on them."],["body","\n"],["body","这是两个 首要的扩展名。因为 Elasticsearch 执行 在它们身上在执行 random access"],["body","\n"],["body","可以使用通配符来指示应预加载所有文件 index.store.preload: [\"*\"]."],["body","\n"],["body","但是请注意，将所有文件加载到内存中通常没有用，尤其是存储字段和term vectors 的文件。"],["body","\n"],["body","所以更好的选择可能是将其设置为 [\"nvd\", \"dvd\", \"tim\", \"doc\", \"dim\"], which will preload norms, doc values, terms dictionaries, postings lists and points,这些是索引中搜索和聚合的最重要部分。"],["body","\n"],["body","请注意，此设置对于大于主机主内存大小的索引可能是危险的，因为它会导致文件系统缓存在大合并后重新打开时被丢弃，这将使索引和搜索速度变慢。"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/1.IndexModules/7.Slowlog.html"],["title","Slowlog.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","slow-log"],["heading","Slow Log"],["body","\n"],["headingLink","search-slow-log"],["heading","Search Slow Log"],["body","\n"],["body","分片级慢搜索日志允许将慢速搜索 (查询和获取阶段) 记录到专用日志文件中。"],["body","\n"],["body","可以为执行的查询阶段和fetch阶段设置阈值，这里是一个示例:"],["body","\n"],["body","index.search.slowlog.threshold.query.warn: 10s\nindex.search.slowlog.threshold.query.info: 5s\nindex.search.slowlog.threshold.query.debug: 2s\nindex.search.slowlog.threshold.query.trace: 500ms\n\nindex.search.slowlog.threshold.fetch.warn: 1s\nindex.search.slowlog.threshold.fetch.info: 800ms\nindex.search.slowlog.threshold.fetch.debug: 500ms\nindex.search.slowlog.threshold.fetch.trace: 200ms\n\nindex.search.slowlog.level: info\n"],["body","\n"],["body","All of the above settings are dynamic and can be set for each index using the update indices settings API. For example:"],["body","\n"],["body","PUT /my-index-000001/_settings\n{\n  \"index.search.slowlog.threshold.query.warn\": \"10s\",\n  \"index.search.slowlog.threshold.query.info\": \"5s\",\n  \"index.search.slowlog.threshold.query.debug\": \"2s\",\n  \"index.search.slowlog.threshold.query.trace\": \"500ms\",\n  \"index.search.slowlog.threshold.fetch.warn\": \"1s\",\n  \"index.search.slowlog.threshold.fetch.info\": \"800ms\",\n  \"index.search.slowlog.threshold.fetch.debug\": \"500ms\",\n  \"index.search.slowlog.threshold.fetch.trace\": \"200ms\",\n  \"index.search.slowlog.level\": \"info\"\n}\n"],["body","\n"],["body","默认情况下，没有启用 (设置为-1)。级别(warn, info, debug, trace) 允许控制将日志记录在哪个日志记录级别下。"],["body","\n"],["body","并非所有都需要配置 (例如，只能设置warn阈值)。几个级别的好处是能够针对违反的特定阈值快速 “grep”。"],["body","\n"],["body","The logging is done on the shard level scope, meaning the execution of a search request within a specific shard."],["body","\n"],["body","日志记录是在分片级别范围上完成的，这意味着在特定分片中执行搜索请求。"],["body","\n"],["body","它不包含整个搜索请求，可以将其广播到多个分片以执行"],["body","\n"],["body","与请求级别相比，分片级别日志记录的一些好处是在特定计算机上实际执行的关联。"],["body","\n"],["body","默认情况下，日志记录文件使用以下配置 (在log4j2.properties中找到):"],["body","\n"],["body","appender.index_search_slowlog_rolling.type = RollingFile\nappender.index_search_slowlog_rolling.name = index_search_slowlog_rolling\nappender.index_search_slowlog_rolling.fileName = ${sys:es.logs.base_path}${sys:file.separator}${sys:es.logs.cluster_name}_index_search_slowlog.log\nappender.index_search_slowlog_rolling.layout.type = PatternLayout\nappender.index_search_slowlog_rolling.layout.pattern = [%d{ISO8601}][%-5p][%-25c] [%node_name]%marker %.-10000m%n\nappender.index_search_slowlog_rolling.filePattern = ${sys:es.logs.base_path}${sys:file.separator}${sys:es.logs.cluster_name}_index_search_slowlog-%i.log.gz\nappender.index_search_slowlog_rolling.policies.type = Policies\nappender.index_search_slowlog_rolling.policies.size.type = SizeBasedTriggeringPolicy\nappender.index_search_slowlog_rolling.policies.size.size = 1GB\nappender.index_search_slowlog_rolling.strategy.type = DefaultRolloverStrategy\nappender.index_search_slowlog_rolling.strategy.max = 4\n\nlogger.index_search_slowlog_rolling.name = index.search.slowlog\nlogger.index_search_slowlog_rolling.level = trace\nlogger.index_search_slowlog_rolling.appenderRef.index_search_slowlog_rolling.ref = index_search_slowlog_rolling\nlogger.index_search_slowlog_rolling.additivity = false\n"],["body","\n"],["headingLink","identifying-search-slow-log-origin"],["heading","Identifying search slow log origin"],["body","\n"],["body","It is often useful to identify what triggered a slow running query. If a call was initiated with an X-Opaque-ID header, then the user ID is included in Search Slow logs as an additional id field (scroll to the right)."],["body","\n"],["body","识别是什么触发了运行缓慢的查询通常很有用。如果使用X-Opaque-ID标头启动了调用，则该用户id将作为附加ID字段包含在 “搜索慢日志” 中。"],["body","\n"],["body","[2030-08-30T11:59:37,786][WARN ][i.s.s.query              ] [node-0] [index6][0] took[78.4micros], took_millis[0], total_hits[0 hits], stats[], search_type[QUERY_THEN_FETCH], total_shards[1], source[{\"query\":{\"match_all\":{\"boost\":1.0}}}], id[MY_USER_ID],\n"],["body","\n"],["body","用户ID也包含在JSON日志中。"],["body","\n"],["body","{\n  \"type\": \"index_search_slowlog\",\n  \"timestamp\": \"2030-08-30T11:59:37,786+02:00\",\n  \"level\": \"WARN\",\n  \"component\": \"i.s.s.query\",\n  \"cluster.name\": \"distribution_run\",\n  \"node.name\": \"node-0\",\n  \"message\": \"[index6][0]\",\n  \"took\": \"78.4micros\",\n  \"took_millis\": \"0\",\n  \"total_hits\": \"0 hits\",\n  \"stats\": \"[]\",\n  \"search_type\": \"QUERY_THEN_FETCH\",\n  \"total_shards\": \"1\",\n  \"source\": \"{\\\"query\\\":{\\\"match_all\\\":{\\\"boost\\\":1.0}}}\",\n  \"id\": \"MY_USER_ID\",\n  \"cluster.uuid\": \"Aq-c-PAeQiK3tfBYtig9Bw\",\n  \"node.id\": \"D7fUYfnfTLa2D7y-xw6tZg\"\n}\n"],["body","\n"],["headingLink","index-slow-log"],["heading","Index Slow log"],["body","\n"],["body","索引慢日志，功能类似于搜索慢日志。日志文件名以 _ index_indexing_slowlog.log结尾。"],["body","\n"],["body","日志和阈值的配置方式与搜索慢日志相同。索引慢日志样本:"],["body","\n"],["body","index.indexing.slowlog.threshold.index.warn: 10s\nindex.indexing.slowlog.threshold.index.info: 5s\nindex.indexing.slowlog.threshold.index.debug: 2s\nindex.indexing.slowlog.threshold.index.trace: 500ms\nindex.indexing.slowlog.level: info\nindex.indexing.slowlog.source: 1000\n"],["body","\n"],["body","All of the above settings are dynamic and can be set for each index using the update indices settings API. For example:"],["body","\n"],["body","PUT /my-index-000001/_settings\n{\n  \"index.indexing.slowlog.threshold.index.warn\": \"10s\",\n  \"index.indexing.slowlog.threshold.index.info\": \"5s\",\n  \"index.indexing.slowlog.threshold.index.debug\": \"2s\",\n  \"index.indexing.slowlog.threshold.index.trace\": \"500ms\",\n  \"index.indexing.slowlog.level\": \"info\",\n  \"index.indexing.slowlog.source\": \"1000\"\n}\n"],["body","\n\n"],["body","默认情况下，Elasticsearch将记录慢日志中 _source的前1000个字符。"],["body","\n"],["body","You can change that with index.indexing.slowlog.source."],["body","\n"],["body","Setting it to false or 0 will skip logging the source entirely,"],["body","\n"],["body","while setting it to true will log the entire source regardless of size."],["body","\n"],["body","默认情况下，原始 _source会重新格式化，以确保它适合单个日志行。"],["body","\n"],["body","If preserving the original document format is important, you can turn off reformatting by setting index.indexing.slowlog.reformat to false, which will cause the source to be logged \"as is\" and can potentially span multiple log lines."],["body","\n"],["body","如果保留原始文档格式很重要，则可以通过将index.indexing.slowlog.reformat设置为false来关闭重新格式化，这将导致源 “按原样” 记录，并且可能跨越多个日志行。"],["body","\n"],["body","索引慢日志文件默认配置在log4j2.properties文件中:"],["body","\n\n"],["body","appender.index_indexing_slowlog_rolling.type = RollingFile\nappender.index_indexing_slowlog_rolling.name = index_indexing_slowlog_rolling\nappender.index_indexing_slowlog_rolling.fileName = ${sys:es.logs.base_path}${sys:file.separator}${sys:es.logs.cluster_name}_index_indexing_slowlog.log\nappender.index_indexing_slowlog_rolling.layout.type = PatternLayout\nappender.index_indexing_slowlog_rolling.layout.pattern = [%d{ISO8601}][%-5p][%-25c] [%node_name]%marker %.-10000m%n\nappender.index_indexing_slowlog_rolling.filePattern = ${sys:es.logs.base_path}${sys:file.separator}${sys:es.logs.cluster_name}_index_indexing_slowlog-%i.log.gz\nappender.index_indexing_slowlog_rolling.policies.type = Policies\nappender.index_indexing_slowlog_rolling.policies.size.type = SizeBasedTriggeringPolicy\nappender.index_indexing_slowlog_rolling.policies.size.size = 1GB\nappender.index_indexing_slowlog_rolling.strategy.type = DefaultRolloverStrategy\nappender.index_indexing_slowlog_rolling.strategy.max = 4\n\nlogger.index_indexing_slowlog.name = index.indexing.slowlog.index\nlogger.index_indexing_slowlog.level = trace\nlogger.index_indexing_slowlog.appenderRef.index_indexing_slowlog_rolling.ref = index_indexing_slowlog_rolling\nlogger.index_indexing_slowlog.additivity = false\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/1.IndexModules/6.SimilarityModule.html"],["title","SimilarityModule.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","similarity-module"],["heading","Similarity module"],["body","\n\n"],["body","\n"],["body","相似性 (评分/排名模型) 定义了匹配文档的评分方式。"],["body","\n"],["body","\n"],["body","\n"],["body","相似性是每个字段，这意味着通过 mapping 可以定义每个字段的不同相似性。"],["body","\n"],["body","\n"],["body","\n"],["body","配置自定义相似性是 expert feature  ，并且内置相似性很可能就足够了，如 similarity. 中所述。"],["body","\n"],["body","\n\n"],["headingLink","configuring-a-similarity"],["heading","Configuring a similarity"],["body","\n\n"],["body","\n"],["body","大多数现有或自定义相似性都具有配置选项，可以通过索引设置进行配置，如下所示"],["body","\n"],["body","\n"],["body","\n"],["body","创建索引或更新索引设置时可以提供索引选项。"],["body","\n"],["body","\n\n"],["body","PUT /index\n{\n  \"settings\": {\n    \"index\": {\n      \"similarity\": {\n        \"my_similarity\": {\n          \"type\": \"DFR\",\n          \"basic_model\": \"g\",\n          \"after_effect\": \"l\",\n          \"normalization\": \"h2\",\n          \"normalization.h2.c\": \"3.0\"\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","在这里，我们配置DFR相似性，以便可以在映射中引用为my_simility，如下面的示例所示:"],["body","\n"],["body","PUT /index/_mapping\n{\n  \"properties\" : {\n    \"title\" : { \"type\" : \"text\", \"similarity\" : \"my_similarity\" }\n  }\n}\n"],["body","\n"],["headingLink","available-similarities"],["heading","Available similarities"],["body","\n"],["headingLink","bm25-similarity-default"],["heading","BM25 similarity (default)"],["body","\n"],["body","基于TF/IDF的相似性，具有内置的tf归一化，并且对短字段 (如名称) 更好地工作。See Okapi_BM25 for more details."],["body","\n"],["body","这种相似性有以下选项:"],["body","\n"],["body","\n"],["body","k1"],["body","Controls non-linear term frequency normalization (saturation). The default value is 1.2."],["body","\n"],["body","b"],["body","Controls to what degree document length normalizes tf values. The default value is 0.75."],["body","\n"],["body","discount_overlaps"],["body","Determines whether overlap tokens (Tokens with 0 position increment) are ignored when computing norm. By default this is true, meaning overlap tokens do not count when computing norms."],["body","\n\n\n"],["headingLink","dfr-similarity"],["heading","DFR similarity"],["body","\n"],["body","Similarity that implements the divergence from randomness framework. This similarity has the following options:"],["body","\n"],["body","\n"],["body","basic_model"],["body","Possible values: g, if, in and ine."],["body","\n"],["body","after_effect"],["body","Possible values: b and l."],["body","\n"],["body","normalization"],["body","Possible values: no, h1, h2, h3 and z."],["body","\n\n\n"],["body","All options but the first option need a normalization value."],["body","\n"],["body","Type name: DFR"],["body","\n"],["headingLink","dfi-similarity"],["heading","DFI similarity"],["body","\n"],["body","Similarity that implements the divergence from independence model. This similarity has the following options:"],["body","\n"],["body","independence_measure  Possible values standardized, saturated, chisquared."],["body","\n"],["body","When using this similarity, it is highly recommended not to remove stop words to get good relevance. Also beware that terms whose frequency is less than the expected frequency will get a score equal to 0."],["body","\n"],["body","Type name: DFI"],["body","\n"],["headingLink","ib-similarity"],["heading","IB similarity."],["body","\n"],["body","Information based model . The algorithm is based on the concept that the information content in any symbolic distribution sequence is primarily determined by the repetitive usage of its basic elements. For written texts this challenge would correspond to comparing the writing styles of different authors. This similarity has the following options:"],["body","\n"],["body","\n"],["body","distribution"],["body","Possible values: ll and spl."],["body","\n"],["body","lambda"],["body","Possible values: df and ttf."],["body","\n"],["body","normalization"],["body","Same as in DFR similarity."],["body","\n\n\n"],["body","Type name: IB"],["body","\n"],["headingLink","lm-dirichlet-similarity"],["heading","LM Dirichlet similarity."],["body","\n"],["body","LM Dirichlet similarity . This similarity has the following options:"],["body","\n"],["body","\n"],["body","mu"],["body","Default to 2000."],["body","\n\n\n"],["body","The scoring formula in the paper assigns negative scores to terms that have fewer occurrences than predicted by the language model, which is illegal to Lucene, so such terms get a score of 0."],["body","\n"],["body","Type name: LMDirichlet"],["body","\n"],["headingLink","lm-jelinek-mercer-similarity"],["heading","LM Jelinek Mercer similarity."],["body","\n"],["body","LM Jelinek Mercer similarity . The algorithm attempts to capture important patterns in the text, while leaving out noise. This similarity has the following options:"],["body","\n"],["body","\n"],["body","lambda"],["body","The optimal value depends on both the collection and the query. The optimal value is around 0.1 for title queries and 0.7 for long queries. Default to 0.1. When value approaches 0, documents that match more query terms will be ranked higher than those that match fewer terms."],["body","\n\n\n"],["body","Type name: LMJelinekMercer"],["body","\n"],["headingLink","scripted-similarity"],["heading","Scripted similarity"],["body","\n"],["body","一种相似性，允许您使用脚本来指定应如何计算分数。"],["body","\n"],["body","例如，下面的示例显示了如何重新实现tf-idf:"],["body","\n"],["body","PUT /index\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"similarity\": {\n      \"scripted_tfidf\": {\n        \"type\": \"scripted\",\n        \"script\": {\n          \"source\": \"double tf = Math.sqrt(doc.freq); double idf = Math.log((field.docCount+1.0)/(term.docFreq+1.0)) + 1.0; double norm = 1/Math.sqrt(doc.length); return query.boost * tf * idf * norm;\"\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"field\": {\n        \"type\": \"text\",\n        \"similarity\": \"scripted_tfidf\"\n      }\n    }\n  }\n}\n\nPUT /index/_doc/1\n{\n  \"field\": \"foo bar foo\"\n}\n\nPUT /index/_doc/2\n{\n  \"field\": \"bar baz\"\n}\n\nPOST /index/_refresh\n\nGET /index/_search?explain=true\n{\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"foo^1.7\",\n      \"default_field\": \"field\"\n    }\n  }\n}\n"],["body","\n"],["body","{\n  \"took\": 12,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": {\n        \"value\": 1,\n        \"relation\": \"eq\"\n    },\n    \"max_score\": 1.9508477,\n    \"hits\": [\n      {\n        \"_shard\": \"[index][0]\",\n        \"_node\": \"OzrdjxNtQGaqs4DmioFw9A\",\n        \"_index\": \"index\",\n        \"_type\": \"_doc\",\n        \"_id\": \"1\",\n        \"_score\": 1.9508477,\n        \"_source\": {\n          \"field\": \"foo bar foo\"\n        },\n        \"_explanation\": {\n          \"value\": 1.9508477,\n          \"description\": \"weight(field:foo in 0) [PerFieldSimilarity], result of:\",\n          \"details\": [\n            {\n              \"value\": 1.9508477,\n              \"description\": \"score from ScriptedSimilarity(weightScript=[null], script=[Script{type=inline, lang='painless', idOrCode='double tf = Math.sqrt(doc.freq); double idf = Math.log((field.docCount+1.0)/(term.docFreq+1.0)) + 1.0; double norm = 1/Math.sqrt(doc.length); return query.boost * tf * idf * norm;', options={}, params={}}]) computed from:\",\n              \"details\": [\n                {\n                  \"value\": 1.0,\n                  \"description\": \"weight\",\n                  \"details\": []\n                },\n                {\n                  \"value\": 1.7,\n                  \"description\": \"query.boost\",\n                  \"details\": []\n                },\n                {\n                  \"value\": 2,\n                  \"description\": \"field.docCount\",\n                  \"details\": []\n                },\n                {\n                  \"value\": 4,\n                  \"description\": \"field.sumDocFreq\",\n                  \"details\": []\n                },\n                {\n                  \"value\": 5,\n                  \"description\": \"field.sumTotalTermFreq\",\n                  \"details\": []\n                },\n                {\n                  \"value\": 1,\n                  \"description\": \"term.docFreq\",\n                  \"details\": []\n                },\n                {\n                  \"value\": 2,\n                  \"description\": \"term.totalTermFreq\",\n                  \"details\": []\n                },\n                {\n                  \"value\": 2.0,\n                  \"description\": \"doc.freq\",\n                  \"details\": []\n                },\n                {\n                  \"value\": 3,\n                  \"description\": \"doc.length\",\n                  \"details\": []\n                }\n              ]\n            }\n          ]\n        }\n      }\n    ]\n  }\n}\n"],["body","\n"],["body","While scripted similarities provide a lot of flexibility, there is a set of rules that they need to satisfy. Failing to do so could make Elasticsearch silently return wrong top hits or fail with internal errors at search time:"],["body","\n\n"],["body","Returned scores must be positive."],["body","\n"],["body","All other variables remaining equal, scores must not decrease when doc.freq increases."],["body","\n"],["body","All other variables remaining equal, scores must not increase when doc.length increases."],["body","\n\n"],["body","It is possible to make the above slightly more efficient by providing an weight_script which will compute the document-independent part of the score and will be available under the weight variable. When no weight_script is provided, weight is equal to 1. The weight_script has access to the same variables as the script except doc since it is supposed to compute a document-independent contribution to the score."],["body","\n"],["body","The below configuration will give the same tf-idf scores but is slightly more efficient:"],["body","\n"],["body","PUT /index\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"similarity\": {\n      \"scripted_tfidf\": {\n        \"type\": \"scripted\",\n        \"weight_script\": {\n          \"source\": \"double idf = Math.log((field.docCount+1.0)/(term.docFreq+1.0)) + 1.0; return query.boost * idf;\"\n        },\n        \"script\": {\n          \"source\": \"double tf = Math.sqrt(doc.freq); double norm = 1/Math.sqrt(doc.length); return weight * tf * norm;\"\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"field\": {\n        \"type\": \"text\",\n        \"similarity\": \"scripted_tfidf\"\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","default-similarity"],["heading","Default Similarity"],["body","\n"],["body","By default, Elasticsearch will use whatever similarity is configured as default."],["body","\n"],["body","You can change the default similarity for all fields in an index when it is created:"],["body","\n"],["body","PUT /index\n{\n  \"settings\": {\n    \"index\": {\n      \"similarity\": {\n        \"default\": {\n          \"type\": \"boolean\"\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["body","If you want to change the default similarity after creating the index you must close your index, send the following request and open it again afterwards:"],["body","\n"],["body","POST /index/_close?wait_for_active_shards=0\n\nPUT /index/_settings\n{\n  \"index\": {\n    \"similarity\": {\n      \"default\": {\n        \"type\": \"boolean\"\n      }\n    }\n  }\n}\n\nPOST /index/_open\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/1.IndexModules/12.IndexPressure.html"],["title","IndexPressure.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","indexing-pressure"],["heading","Indexing pressure"],["body","\n\n"],["body","将文档索引到Elasticsearch以内存和CPU负载的形式引入系统负载"],["body","\n"],["body","每个索引操作包括协调阶段、primary 阶段和复制阶段。"],["body","\n"],["body","这些阶段可以跨集群中的多个节点执行。"],["body","\n"],["body","索引压力可以通过外部操作 (例如索引请求) 或内部机制 (例如恢复和跨集群复制) 来建立"],["body","\n"],["body","如果在系统中引入过多的索引工作，则群集可能会变得饱和"],["body","\n"],["body","这可能会对其他操作产生不利影响，例如搜索，群集协调和后台处理。"],["body","\n"],["body","为了防止这些问题，Elasticsearch内部监控索引负载。当负载超过一定限度时，新的索引工作被拒绝"],["body","\n\n"],["headingLink","indexing-stages"],["heading","Indexing stages"],["body","\n"],["body","External indexing operations go through three stages: coordinating, primary, and replica. See Basic write model."],["body","\n"],["headingLink","memory-limits"],["heading","Memory limits"],["body","\n\n"],["body","\n"],["body","Indexing_presssure.memory.limit节点设置限制了可用于未完成索引请求的字节数。"],["body","\n"],["body","\n"],["body","\n"],["body","此设置默认为堆的10%。"],["body","\n"],["body","\n"],["body","\n"],["body","在每个索引阶段开始时，Elasticsearch会占用索引请求消耗的字节,此统计仅在索引阶段结束时发布。这意味着上游阶段将会被统计请求负载，直到所有下游阶段都完成"],["body","\n"],["body","\n"],["body","\n"],["body","例如，在完成  primary阶段和replica阶段之前，协调请求将保持统计。在每个同步副本响应（以在必要时启用副本重试之前），primary请求将会被保留"],["body","\n"],["body","\n"],["body","\n"],["body","当未完成的协调，主要和副本索引字节的数量超过配置的限制时，节点将在协调或primary 阶段开始拒绝新的索引工作。"],["body","\n"],["body","\n"],["body","\n"],["body","当未完成的副本索引字节数超过配置限制的1.5倍时，节点将在副本阶段开始拒绝新的索引工作。"],["body","\n"],["body","\n"],["body","\n"],["body","这种设计意味着，随着在节点上 索引压力的建立，它们自然会停止接受协调和主要工作，而倾向于未完成的副本工作。"],["body","\n"],["body","\n"],["body","\n"],["body","The indexing_pressure.memory.limit setting’s 10% default limit is generously sized."],["body","\n"],["body","\n"],["body","\n"],["body","你应该在仔细考虑后才改变它。只有索引请求才受限于此限制"],["body","\n"],["body","\n"],["body","\n"],["body","这意味着还有额外的索引开销 (缓冲区，侦听器等)，这也需要堆空间。Elasticsearch的其他组件也需要内存。将此限制设置得太高可能会拒绝其他操作和组件的操作内存。"],["body","\n"],["body","\n\n"],["headingLink","monitoring"],["heading","Monitoring"],["body","\n"],["body","您可以使用  node stats API  检索索引压力指标。"],["body","\n"],["headingLink","indexing-pressure-settings"],["heading","Indexing pressure settings"],["body","\n\n"],["body","indexing_pressure.memory.limit\n\n"],["body","索引请求可能消耗的未完成字节数。"],["body","\n"],["body","当达到或超过此限制时，节点将拒绝新的协调和Primary操作"],["body","\n"],["body","当副本操作消耗此限制的1.5倍时，节点将拒绝新的副本操作。"],["body","\n"],["body","默认为堆的10%。"],["body","\n\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/1.IndexModules/1.Analysis.html"],["title","Analysis.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","analysis"],["heading","Analysis"],["body","\n"],["body","索引分析模块充当分析器的可配置注册表，可用于将字符串字段转换为 独立的 terms:"],["body","\n\n"],["body","添加到倒排索引中，以使文档可搜索"],["body","\n"],["body","用于高级查询 例如  match query"],["body","\n\n"],["body","See Text analysis for configuration details."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/1.IndexModules/11.IndexSorting.html"],["title","IndexSorting.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","index-sorting"],["heading","Index Sorting"],["body","\n\n"],["body","\n"],["body","在Elasticsearch中创建新索引时，可以配置如何对每个分片中的段进行排序。"],["body","\n"],["body","\n"],["body","\n"],["body","默认情况下，Lucene不应用任何排序"],["body","\n"],["body","\n"],["body","\n"],["body","index.sort.* 设置定义了应使用哪些字段对每个段内的文档进行排序。"],["body","\n"],["body","\n"],["body","\n"],["body","嵌套字段与索引排序不兼容，因为它们依赖于嵌套文档存储在连续的doc id中的假设，这些id可以通过索引排序来破坏。如果对包含嵌套字段的索引激活了索引排序，则会引发错误。"],["body","\n"],["body","\n\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"index\": {\n      \"sort.field\": \"date\", \n      \"sort.order\": \"desc\"  \n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"date\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\n"],["body","\n\n"],["body","\n"],["body","This index is sorted by the date field"],["body","\n"],["body","\n"],["body","\n"],["body","… in descending order."],["body","\n"],["body","\n\n"],["body","也可以按多个字段对索引进行排序:"],["body","\n"],["body","PUT my-index-000001\n{\n  \"settings\": {\n    \"index\": {\n      \"sort.field\": [ \"username\", \"date\" ], \n      \"sort.order\": [ \"asc\", \"desc\" ]       \n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"username\": {\n        \"type\": \"keyword\",\n        \"doc_values\": true\n      },\n      \"date\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\n"],["body","\n\n"],["body","This index is sorted by username first then by date"],["body","\n"],["body","… in ascending order for the username field and in descending order for the date field."],["body","\n\n"],["body","索引排序支持以下设置:"],["body","\n"],["body","index.sort.field"],["body","\n"],["body","The list of fields used to sort the index. Only boolean, numeric, date and keyword fields with doc_values are allowed here."],["body","\n"],["body","index.sort.order"],["body","\n"],["body","The sort order to use for each field. The order option can have the following values:"],["body","\n\n"],["body","asc: For ascending order"],["body","\n"],["body","desc: For descending order."],["body","\n\n"],["body","index.sort.mode"],["body","\n"],["body","Elasticsearch支持按多值字段排序。模式选项控制选择什么值来对文档进行排序。"],["body","\n"],["body","模式选项可以具有以下值:"],["body","\n\n"],["body","min: Pick the lowest value."],["body","\n"],["body","max: Pick the highest value."],["body","\n\n"],["body","index.sort.missing"],["body","\n"],["body","缺少参数指定应如何处理缺少字段的文档。缺失值可以有以下值:"],["body","\n\n"],["body","_last: Documents without value for the field are sorted last."],["body","\n"],["body","_first: Documents without value for the field are sorted first."],["body","\n\n"],["body","注意"],["body","\n\n"],["body","\n"],["body","索引排序只能在索引创建时定义一次。"],["body","\n"],["body","\n"],["body","\n"],["body","不允许在现有索引上添加或更新排序"],["body","\n"],["body","\n"],["body","\n"],["body","索引排序在索引吞吐量方面也有成本，因为文档必须在刷新和合并时间进行排序。"],["body","\n"],["body","\n"],["body","\n"],["body","在激活此功能之前，您应该测试对应用程序的影响。"],["body","\n"],["body","\n\n"],["headingLink","early-termination-of-search-request"],["heading","Early termination of search request"],["body","\n"],["body","默认情况下，在Elasticsearch中，搜索请求必须访问与查询匹配的每个文档，以检索按指定排序排序的 TOP 文档。"],["body","\n"],["body","但是当索引排序和搜索排序相同时，可以限制每个段应访问的文档数量，以在全局范围内检索N个排名最高的文档。"],["body","\n"],["body","例如，假设我们有一个包含按时间戳字段排序的事件的索引:"],["body","\n"],["body","PUT events\n{\n  \"settings\": {\n    \"index\": {\n      \"sort.field\": \"timestamp\",\n      \"sort.order\": \"desc\" \n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"timestamp\": {\n        \"type\": \"date\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","此索引按时间戳按降序排序 (最近的第一个)"],["body","\n"],["body","GET /events/_search\n{\n  \"size\": 10,\n  \"sort\": [\n    { \"timestamp\": \"desc\" }\n  ]\n}\n"],["body","\n\n"],["body","\n"],["body","Elasticsearch将检测到每个段的顶部文档已经在索引中排序，并且将仅比较每个段的前N个文档。"],["body","\n"],["body","\n"],["body","\n"],["body","匹配查询的剩余的文档 被收集，以计算结果总数并构建聚合。"],["body","\n"],["body","\n\n"],["body","如果您仅查找最后10个事件，并且对与查询匹配的文档总数不感兴趣，则可以将track_total_hits设置为false:"],["body","\n"],["body","GET /events/_search\n{\n  \"size\": 10,\n  \"sort\": [ \n      { \"timestamp\": \"desc\" }\n  ],\n  \"track_total_hits\": false\n}\n"],["body","\n"],["body","索引排序将用于对顶级文档进行排名，并且每个段将在前10个匹配之后提前终止集合。"],["body","\n"],["body","这一次，Elasticsearch将不会尝试计算文档的数量，并且一旦每个段收集了N个文档，就能够终止查询。"],["body","\n"],["body","{\n  \"_shards\": ...\n   \"hits\" : {  \n      \"max_score\" : null,\n      \"hits\" : []\n  },\n  \"took\": 20,\n  \"timed_out\": false\n}\n"],["body","\n"],["body","由于提前终止，匹配查询的命中总数未知。"],["body","\n"],["body","聚合将收集与查询匹配的所有文档，而不管 “track_total_hits” 的值如何"],["body","\n"],["headingLink","use-index-sorting-to-speed-up-conjunctions"],["heading","Use index sorting to speed up conjunctions"],["body","\n\n"],["body","索引排序对于组织Lucene doc id (不与 _id合并) 以使 conjunctions  (a和b和…) 更有效的方式很有用。"],["body","\n"],["body","为了高效，conjunctions 依赖于这样一个事实，即如果任何子句不匹配，那么整个conjunction都不匹配。"],["body","\n"],["body","通过使用索引排序，我们可以将不匹配的文档放在一起，这将有助于有效地跳过与连接不匹配的大范围doc id。"],["body","\n"],["body","此技巧仅适用于低基数字段。"],["body","\n"],["body","经验法则是，您应该首先对基数较低且经常用于过滤的字段进行排序。"],["body","\n"],["body","排序顺序 (asc或desc) 并不重要，因为我们只关心将匹配相同子句的值彼此靠近。"],["body","\n\n"],["body","例如，如果您要索引要出售的汽车，则按燃料类型，车身类型，品牌，注册年份和最终里程进行分类可能会很有趣。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/1.IndexModules/10.HistoryRetention.html"],["title","HistoryRetention.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","history-retention"],["heading","History retention"],["body","\n"],["body","Elasticsearch有时需要重放在分片上执行的一些操作"],["body","\n"],["body","例如，如果副本短暂处于脱机状态，则重播脱机时错过的一些操作可能比从头开始重建要有效得多"],["body","\n"],["body","同样，跨集群复制的工作原理是在leader集群上执行操作，然后在follower集群上重放这些操作"],["body","\n"],["body","写操作\n在Lucene级别，实际上Elasticsearch对索引执行的写操作只有两个"],["body","\n\n"],["body","\n"],["body","新文档可能会被索引"],["body","\n"],["body","\n"],["body","\n"],["body","现有文档可能会被删除。"],["body","\n"],["body","\n"],["body","\n"],["body","通过原子删除旧文档，然后为新文档建立索引来实现更新。"],["body","\n"],["body","\n"],["body","\n"],["body","索引到Lucene的文档已经包含重放该索引操作所需的所有信息，但是文档删除并非如此。"],["body","\n"],["body","\n"],["body","\n"],["body","为了解决这个问题，Elasticsearch使用一种称为软删除的功能来保留Lucene索引中最近的删除，以便可以重放它们。"],["body","\n"],["body","\n"],["body","\n"],["body","Elasticsearch仅在索引中保留某些最近删除的文档，因为软删除的文档仍然会占用一些空间"],["body","\n"],["body","\n"],["body","\n"],["body","最终，Elasticsearch将完全丢弃这些软删除的文档，以释放该空间，从而使索引不会随着时间的推移而变得越来越大。"],["body","\n"],["body","\n"],["body","\n"],["body","幸运的是，Elasticsearch不需要能够重放曾经在分片上执行过的每个操作，因为始终可以在远程节点上制作分片的完整副本。"],["body","\n"],["body","\n"],["body","\n"],["body","但是，复制整个分片可能比重放一些缺少的操作要花费更长的时间，因此Elasticsearch尝试保留其将来需要重放的所有操作。"],["body","\n"],["body","\n\n"],["body","Elasticsearch使用称为shard history retention leases 的机制来跟踪将来需要重播的操作。"],["body","\n"],["body","每个可能需要重放操作的分片副本必须首先为自己创建一个历史保留租约"],["body","\n"],["body","例如，使用跨集群复制时，此分片副本可能是分片的副本，也可能是跟随索引的分片"],["body","\n"],["body","每个保留租约都会跟踪相应分片副本尚未收到的第一个操作的序列号"],["body","\n"],["body","当分片副本接收到新的操作时，它会增加其保留租约中包含的序列号，以表明将来不需要重播这些操作"],["body","\n"],["body","一旦软删除的操作没有被任何保留租约持有，Elasticsearch就会丢弃它们。"],["body","\n\n"],["body","如果分片副本失败，则它将停止更新其分片历史记录保留租约，这意味着Elasticsearch将保留所有新操作，以便在失败的分片副本恢复时可以重放它们"],["body","\n"],["body","然而，保留租约只能持续有限的时间"],["body","\n"],["body","如果分片副本恢复得不够快，则其保留租约可能会到期。"],["body","\n"],["body","如果分片副本永久失败，这可以保护Elasticsearch永远保留历史记录，"],["body","\n"],["body","因为一旦保留租约到期，Elasticsearch可以再次开始丢弃历史记录"],["body","\n"],["body","如果分片副本在保留租约到期后在恢复，则Elasticsearch将退回到复制整个索引，因为它不再可以简单地重播丢失的历史记录"],["body","\n"],["body","保留租约的到期时间默认为12小时，对于大多数合理的恢复情况，该时间应足够长。"],["body","\n"],["body","默认情况下，在最新版本中创建的索引上启用软删除，但可以在索引创建时显式启用或禁用它们"],["body","\n"],["body","如果禁用了软删除，则有时仍然可以通过仅从translog中复制丢失的操作来进行对等恢复，只要这些操作保留在那里即可（ as long as those operations are retained there.）。如果禁用软删除，跨集群复制将不起作用。"],["body","\n\n"],["headingLink","history-retention-settings"],["heading","History retention settings"],["body","\n\n"],["body","\n"],["body","index.soft_deletes.enabled"],["body","\n"],["body","[7.6.0] Deprecated in 7.6.0. Creating indices with soft-deletes disabled is deprecated and will be removed in future Elasticsearch versions.Indicates whether soft deletes are enabled on the index. Soft deletes can only be configured at index creation and only on indices created on or after Elasticsearch 6.5.0. Defaults to true."],["body","\n"],["body","\n"],["body","\n"],["body","index.soft_deletes.retention_lease.period"],["body","\n"],["body","The maximum period to retain a shard history retention lease before it is considered expired. Shard history retention leases ensure that soft deletes are retained during merges on the Lucene index. If a soft delete is merged away before it can be replicated to a follower the following process will fail due to incomplete history on the leader. Defaults to 12h."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/1.IndexModules/3.Indexblocks.html"],["title","Indexblocks.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","index-blocks"],["heading","Index blocks"],["body","\n\n"],["body","index block 限制了某个索引上可用的操作类型"],["body","\n"],["body","这些块有不同的风格，允许阻止写入、读取或元数据操作"],["body","\n"],["body","可以使用动态索引设置来设置/删除 block，或者可以使用专用的API来添加block，这也可以确保写入块一旦成功返回给用户，"],["body","\n"],["body","索引的所有分片都正确地考虑了block，例如，添加写入块后，对 所有的 in-flight 的 写入都已完成后。才会添加 write block"],["body","\n\n"],["headingLink","prerequisite"],["heading","Prerequisite"],["body","\n\n"],["body","disk-based shard allocator"],["body","\n\n"],["headingLink","index-block-settings"],["heading","Index block settings"],["body","\n"],["body","以下动态索引设置确定索引上存在的块:"],["body","\n\n"],["body","\n"],["body","index.blocks.read_only"],["body","\n\n"],["body","Set to true to make the index and index metadata read only,"],["body","\n"],["body","false to allow writes and metadata changes."],["body","\n\n"],["body","\n"],["body","\n"],["body","index.blocks.read_only_allow_delete"],["body","\n\n"],["body","Similar to index.blocks.read_only,但也允许删除索引 以使更多资源可用。"],["body","\n"],["body","The disk-based shard allocator may add and remove this block automatically."],["body","\n"],["body","从索引中删除文档以释放资源-而不是删除索引本身-可以随时间增加索引大小."],["body","\n"],["body","When index.blocks.read_only_allow_delete is set to true, 不允许删除文档."],["body","\n"],["body","但是，删除索引本身会释放只读索引块，并使资源几乎立即可用"],["body","\n"],["body","当磁盘利用率低于高水位时，Elasticsearch会自动添加和删除只读索引块，由 cluster.routing.allocation.disk.watermark.flood_stage.控制"],["body","\n\n"],["body","\n"],["body","\n"],["body","index.blocks.read"],["body","\n\n"],["body","设置为true以禁用对索引的读取操作。"],["body","\n\n"],["body","\n"],["body","\n"],["body","index.blocks.write"],["body","\n\n"],["body","设置为true以禁用针对索引的数据写入操作。"],["body","\n"],["body","Unlike read_only, this setting does not affect metadata."],["body","\n"],["body","例如，您可以用写Block 关闭索引，但是不能用read_only block 关闭索引。"],["body","\n\n"],["body","\n"],["body","\n"],["body","index.blocks.metadata"],["body","\n"],["body","Set to true to disable index metadata reads and writes."],["body","\n"],["body","\n\n"],["headingLink","add-index-block-api"],["heading","Add index block API"],["body","\n"],["body","Adds an index block to an index."],["body","\n"],["body","PUT /my-index-000001/_block/write\n"],["body","\n"],["headingLink","request"],["heading","Request"],["body","\n"],["body","PUT /<index>/_block/<block>\n"],["body","\n"],["headingLink","path-parameters"],["heading","Path parameters"],["body","\n\n"],["body","<index>\n\n"],["body","逗号分割的索引名或者GLOB模式"],["body","\n"],["body","_all or *添加所有索引"],["body","\n"],["body","要禁止将BLOCK 添加到具有 _all或通配符表达式的索引中，请将action.destructive_requires_name 群集设置更改为 'true'"],["body","\n"],["body","You can update this setting in the elasticsearch.yml file or using the cluster update settings API."],["body","\n\n"],["body","\n"],["body","<block>\n\n"],["body","(必填，字符串) 要添加到索引中的BLOCK类型。"],["body","\n\n"],["body","\n\n"],["headingLink","query-parameters"],["heading","Query parameters"],["body","\n\n"],["body","\n"],["body","allow_no_indices"],["body","\n"],["body","(Optional, Boolean) If false, the request returns an error if any wildcard expression, index alias, or _all value targets only missing or closed indices."],["body","\n\n"],["body","如果设置为FALSE 通配符表达式或者 _all 、索引别名等 没有命中 目标索引 则 返回异常。"],["body","\n"],["body","即使请求针对其他开放索引，此行为也适用。例如，foo*,bar* ，如果索引以foo开头，但没有索引以bar开头，则返回错误。默认为true。"],["body","\n\n"],["body","\n"],["body","\n"],["body","expand_wildcards"],["body","\n\n"],["body","(可选，字符串) 通配符表达式可以匹配的索引类型。"],["body","\n"],["body","如果请求可以针对数据流，则此参数确定通配符表达式是否与隐藏数据流匹配。"],["body","\n"],["body","支持以逗号分隔的值，如open、hidden。"],["body","\n"],["body","all 匹配任何数据流或索引 including hidden ones"],["body","\n"],["body","open Match open, non-hidden indices 也匹配任何非隐藏数据流"],["body","\n"],["body","closed Match closed, non-hidden indices，也匹配任何非隐藏数据流。数据流无法关闭"],["body","\n"],["body","hidden 匹配隐藏数据流和隐藏索引"],["body","\n"],["body","必须与open，closed或两者结合使用。不接受none Wildcard表达式。默认为open。"],["body","\n\n"],["body","\n"],["body","\n"],["body","ignore_unavailable"],["body","\n"],["body","(Optional, Boolean) If false, the request returns an error if it targets a missing or closed index. Defaults to false."],["body","\n"],["body","\n"],["body","\n"],["body","master_timeout"],["body","\n"],["body","(Optional, time units) Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error. Defaults to 30s."],["body","\n"],["body","\n"],["body","\n"],["body","timeout"],["body","\n"],["body","(Optional, time units) Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error. Defaults to 30s."],["body","\n"],["body","\n\n"],["headingLink","examples"],["heading","Examples"],["body","\n"],["body","The following example shows how to add an index block:"],["body","\n"],["body","PUT /my-index-000001/_block/write\n"],["body","\n"],["body","The API returns following response:"],["body","\n"],["body","{\n  \"acknowledged\" : true,\n  \"shards_acknowledged\" : true,\n  \"indices\" : [ {\n    \"name\" : \"my-index-000001\",\n    \"blocked\" : true\n  } ]\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/1.IndexModules/2.IndexShardAllocation.html"],["title","IndexShardAllocation.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","index-shard-allocation"],["heading","Index Shard Allocation"],["body","\n"],["body","This module provides per-index settings to control the allocation of shards to nodes:"],["body","\n\n"],["body","Shard allocation filtering: Controlling which shards are allocated to which nodes."],["body","\n"],["body","Delayed allocation: Delaying allocation of unassigned shards caused by a node leaving."],["body","\n"],["body","Total shards per node: A hard limit on the number of shards from the same index per node."],["body","\n"],["body","Data tier allocation: Controls the allocation of indices to data tiers."],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/高可用专题/弹性设计.html"],["title","弹性设计.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","designing-for-resilience"],["heading","Designing for resilience"],["body","\n\n"],["body","像Elasticsearch这样的分布式系统被设计为：即使它们的某些组件发生故障也可以继续工作。"],["body","\n"],["body","只要有足够的连接良好的节点来接管其职责，如果Elasticsearch集群的某些节点不可用或断开连接，它就可以继续正常运行。"],["body","\n\n"],["body","弹性集群的大小是有限度的。所有的Elasticsearch集群都要求:"],["body","\n\n"],["body","One elected master node node：一个选举的主节点"],["body","\n"],["body","At least one node for each role.  每个 role至少一个 节点"],["body","\n"],["body","At least one copy of every shard. 每个分片至少一个备份"],["body","\n\n"],["body","一个弹性的集群 需要 为每一个 必要的集群组件 提供冗余，这意味着一个弹性集群需要"],["body","\n\n"],["body","At least three master-eligible nodes：至少三个 有资格的 master"],["body","\n"],["body","At least two nodes of each role：每个 role至少两个节点"],["body","\n"],["body","At least two copies of each shard (one primary and one or more replicas, unless the index is a searchable snapshot index)"],["body","\n\n\n"],["body","弹性集群需要三个  可以当选主节点的节点。这样 一旦其中一个宕机，剩下的两个 任然可以组成大多数，可以成功获得选举"],["body","\n"],["body","同样的，每个role的 节点冗余 意味着某个节点宕机了，另一个节点可以顶替其职责"],["body","\n"],["body","最后每个分片至少 需要 两个 copy 。如果其中一个 copy失败了，另一个则顶上"],["body","\n"],["body","并且，es会自动 在剩余的节点 上 重新构建 副本分片，以确保 能及时恢复 集群的健康"],["body","\n"],["body","故障会暂时降低 集群的总容量，故障后，群集必须执行额外的后台活动以使其恢复健康。即使某些节点出现故障，您也应确保群集具有处理工作负载的能力。"],["body","\n\n"],["body","根据您的需求和预算，Elasticsearch集群可以由单个节点、数百个节点或两者之间的任意数量组成。在设计较小的集群时，通常应该专注于使其能够适应单节点故障。较大集群的设计者还必须考虑多个节点同时发生故障的情况。以下页面给出了构建各种规模的弹性集群的一些建议:"],["body","\n\n"],["body","Resilience in small clusters"],["body","\n"],["body","Resilience in larger clusters"],["body","\n\n"],["headingLink","resilience-in-small-clusters"],["heading","Resilience in small clusters"],["body","\n"],["body","在较小的集群中，最重要的是要对单节点故障具有弹性。本节提供了一些指导，以使您的群集对单个节点的故障具有尽可能的弹性。"],["body","\n"],["headingLink","one-node-clusters"],["heading","One-node clusters"],["body","\n\n"],["body","如果您的集群由一个节点组成，则该单个节点必须执行所有操作。"],["body","\n"],["body","为了适应这一点，Elasticsearch默认为节点分配每个角色。"],["body","\n"],["body","单个节点群集没有弹性。如果节点出现故障，集群将停止工作"],["body","\n"],["body","默认情况下：对于一个 绿色的集群来说，至少需要一个 副本分片"],["body","\n"],["body","可以通过  设置 index.number_of_replicas to 0 来修改默认副本的数"],["body","\n"],["body","如果节点失败了，则唯一的办法是 从 快照中恢复"],["body","\n"],["body","由于它们无法抵御任何故障，因此我们不建议在生产中使用单节点群集。"],["body","\n\n"],["headingLink","two-node-clusters"],["heading","Two-node clusters"],["body","\n\n"],["body","\n"],["body","如果是双节点，则两个节点最后都是 数据节点"],["body","\n"],["body","\n"],["body","\n"],["body","还要确保 每个 索引的副本数为1   index.number_of_replicas"],["body","\n"],["body","\n"],["body","\n"],["body","副本数可能会被  index template. Auto-expand replicas（小集群不要使用）"],["body","\n"],["body","\n"],["body","\n"],["body","给其中一个节点 设置  node.master: false  ，这就意味着可以 清楚的知道 谁是主节点"],["body","\n"],["body","\n"],["body","\n"],["body","如果任一节点不可用，则选举将失败，因此您的群集无法可靠地容忍任一节点的丢失。"],["body","\n"],["body","\n"],["body","\n"],["body","默认情况下，每个节点都会被分配所有角色。我们建议您为两个节点分配除主资格之外的所有其他角色。如果一个节点发生故障，则另一个节点可以处理其任务。"],["body","\n"],["body","\n"],["body","\n"],["body","您可以使用弹性负载均衡器来平衡群集中节点之间的客户端请求。"],["body","\n"],["body","\n"],["body","\n"],["body","因为它对故障没有弹性，所以我们不建议在生产中部署两个节点的集群。"],["body","\n"],["body","\n\n"],["headingLink","two-node-clusters-with-a-tiebreaker"],["heading","Two-node clusters with a tiebreaker"],["body","\n\n"],["body","\n"],["body","由于主节点的选举是基于大多数的，因此上述双节点集群可以容忍其一个节点的丢失，而另一个节点则不能容忍丢失"],["body","\n"],["body","\n"],["body","\n"],["body","您不能将  两个节点的群集，配置成 可以容忍任何一个节点的丢失，因为这在理论上是不可能的。"],["body","\n"],["body","\n"],["body","\n"],["body","您可能会期望，如果任一节点出现故障，则Elasticsearch可以选择其余节点作为主节点，但是无法分辨"],["body","\n\n"],["body","节点的故障  或者"],["body","\n"],["body","节点之间仅失去连接 这两者 的区别。"],["body","\n"],["body","如果双方都能 进行独立的选举，这将可能导致脑裂 split-brain problem"],["body","\n\n"],["body","\n"],["body","\n"],["body","Elasticsearch  直到该节点可以确定它具有最新的群集状态并且群集中没有其他主节点 才会选举成主节点，这可能会导致集群中没有主节点 直到 连接恢复"],["body","\n"],["body","\n"],["body","\n"],["body","解决办法是 增加第三个节点 ，使得这三个节点都可以选举主节点"],["body","\n"],["body","\n"],["body","\n"],["body","A master election 需要2/3投票， 这意味着集群可以容忍任何单个节点的丢失"],["body","\n"],["body","\n"],["body","\n"],["body","在两个原始节点彼此断开连接的情况下，第三个节点充当决胜局"],["body","\n"],["body","\n"],["body","\n"],["body","您可以通过使此节点成为 dedicated voting-only master-eligible node, 也称为 dedicated tiebreaker.因为它没有其他角色，所以投票节点不需要很高的性能配置 它不会执行任何搜索，也不会协调任何客户端请求，并且不能被选为群集的主服务器。"],["body","\n"],["body","\n"],["body","\n"],["body","如果您的三个节点中有两个是仅具有投票资格的主节点，则当选的主节点必须是第三个节点。然后，此节点成为单点故障。"],["body","\n"],["body","\n"],["body","\n"],["body","我们建议 分配 非 dedicated tiebreaker 节点 分配所有其他角色。这确保群集中的任何任务都可以由任一节点处理来创建冗余。"],["body","\n"],["body","\n"],["body","\n"],["body","您不应该向专用的tiebreaker节点发送任何客户端请求，也不能只向 其中一个或者连个节点发送请求，理想情况是 在非 tiebreaker 节点中保持负载均衡"],["body","\n"],["body","\n\n"],["body","带有额外的tiebreaker节点的两节点群集是适合生产部署的最小可能群集。"],["body","\n"],["headingLink","three-node-clusters"],["heading","Three-node clusters"],["body","\n\n"],["body","\n"],["body","如果你有三个节点，推荐 都设置为 数据节点 data nodes ，每个索引 至少有一个 副本"],["body","\n"],["body","\n"],["body","\n"],["body","您可能希望某些索引具有两个副本，以便每个节点在这些索引中具有每个分片的副本"],["body","\n"],["body","\n"],["body","\n"],["body","三个节点都是可当选主节点的，默认如此"],["body","\n"],["body","\n\n"],["headingLink","clusters-with-more-than-three-nodes"],["heading","Clusters with more than three nodes"],["body","\n\n"],["body","如果有超过三个节点以上的集群，可以考虑 根据不同的职责 将分配不同的角色，这可以按需 扩缩资源"],["body","\n"],["body","You can have as many data nodes, ingest nodes, machine learning nodes"],["body","\n"],["body","建议每个角色 都有一部分 专有的 节点，这使您可以独立地扩展每个任务的资源。"],["body","\n"],["body","但是，将群集中 可以当选主节点的数量限制为三个是很好的做法。因为 主节点不会 像其他节点一样 扩缩，因为 集群总是选举他们其中的一个当做主节点"],["body","\n"],["body","如果主节点数太多，导致选举时间耗费太长。"],["body","\n"],["body","推荐专用的主节点，不要往主节点上发请求。因为如果主节点因为 其他任务 过载 会导致集群变得不稳定"],["body","\n"],["body","也可以配置 其中一个 可当选主节点的节点 为 voting-only node  ，这样它就永远不能当选主节点，从而可以 担当 数据节点等其他节点功能，而它自己只是作为选举过程中的 tiebreaker"],["body","\n\n"],["headingLink","summary"],["heading","Summary"],["body","\n"],["body","The cluster will be resilient to the loss of any node as long as:"],["body","\n\n"],["body","The cluster health status is green."],["body","\n"],["body","There are at least two data nodes. 至少有两个数据节点"],["body","\n"],["body","Every index that is not a searchable snapshot index has at least one replica of each shard, in addition to the primary. 分片至少有一个副本"],["body","\n"],["body","The cluster has at least three master-eligible nodes, as long as at least two of these nodes are not voting-only master-eligible nodes. 至少有三个可当选主节点的节点"],["body","\n"],["body","Clients are configured to send their requests to more than one node or are configured to use a load balancer that balances the requests across an appropriate set of nodes. The Elastic Cloud service provides such a load balancer. 客户端的负载均衡"],["body","\n\n"],["headingLink","resilience-in-larger-clusters"],["heading","Resilience in larger clusters"],["body","\n\n"],["body","节点共享一些公共基础设施 (例如电源或网络路由器) 并不罕见"],["body","\n"],["body","如果是这样，您应该为此基础架构的故障进行计划，并确保此类故障不会影响太多节点。"],["body","\n"],["body","通常的做法是将共享某些基础结构的所有节点分组为区域，并立即计划任何整个区域的故障。"],["body","\n"],["body","您集群的区域都应包含在单个数据中心内"],["body","\n"],["body","Elasticsearch期望其节点到节点的连接是可靠的，并且具有低延迟和高带宽。数据中心之间的连接通常不符合这些期望"],["body","\n"],["body","尽管Elasticsearch在不可靠或缓慢的网络上表现正确，但它不一定表现最佳"],["body","\n"],["body","群集从网络分区完全恢复可能需要相当长的时间，因为它必须重新同步任何丢失的数据，并在分区恢复后重新平衡群集。"],["body","\n"],["body","如果您希望您的数据在多个数据中心可用，请在每个数据中心部署一个单独的集群，并  使用  cross-cluster search or cross-cluster replication  将集群连接在一起"],["body","\n"],["body","即使群集到群集的连接比每个群集内的网络可靠性差或速度慢，这些功能也被设计为性能良好。"],["body","\n"],["body","在失去整个区域的节点之后，设计正确的群集可能会起作用，但运行时容量会大大降低。在处理此类故障时，您可能需要配置额外的节点以恢复群集中可接受的性能。"],["body","\n"],["body","为了抵御全区域故障，重要的是在多个区域中存在每个分片的副本，这可以通过将数据节点放置在多个区域中并配置分片分配意识( shard allocation awareness.)来实现。您还应确保将客户端请求发送到多个区域中的节点。"],["body","\n"],["body","您应该考虑所有节点角色，并确保每个角色在两个或多个区域中冗余地拆分。"],["body","\n"],["body","例如, 使用 ingest pipelines 或 machine learning, 您应该在两个或多个区域中有多个 ingest 或 machine learning  节点。"],["body","\n"],["body","但是，主控合格节点的放置需要更多的注意，因为弹性集群需要三个主控合格节点中的至少两个才能起作用。以下各节将探讨跨多个区域放置主控合格节点的选项。"],["body","\n\n"],["headingLink","two-zone-clusters"],["heading","Two-zone clusters"],["body","\n\n"],["body","\n"],["body","如果您有两个区域，您应该在每个区域中有不同数量的主控合格节点，以便具有更多节点的区域将包含其中的大多数，并且能够在另一个区域的丢失中幸存下来。"],["body","\n"],["body","\n"],["body","\n"],["body","例如，如果您有三个主控合格的节点，那么您可以将它们全部放在一个区域中，或者可以将两个放在一个区域中，第三个放在另一个区域中。"],["body","\n"],["body","\n"],["body","\n"],["body","您不应该在每个区域中放置相等数量的符合主条件的节点，如果在每个区域中放置相同数量的主控合格节点，则两个区域都没有自己的大部分。因此，群集可能无法幸免于任何一个区域的丢失。"],["body","\n"],["body","\n\n"],["headingLink","two-zone-clusters-with-a-tiebreaker"],["heading","Two-zone clusters with a tiebreaker"],["body","\n\n"],["body","\n"],["body","上述双区部署可以容忍其中一个区的丧失，但不能容忍另一个区的丧失，因为主选举是基于多数的"],["body","\n"],["body","\n"],["body","\n"],["body","您可能会想，如果任一区域失败，则Elasticsearch可以从其余区域中选择一个节点作为主节点，但是无法分辨出，远程区域的故障 和 区域之间仅仅失去连接这两者之间的区别。"],["body","\n"],["body","\n"],["body","\n"],["body","如果两个区域都独立运行 选举，那么 丧失连接 这可能会导致 split-brain problem  从而导致数据丢失"],["body","\n"],["body","\n"],["body","\n"],["body","Elasticsearch避免了这种情况，并通过不选择来自任一区域的节点作为主节点来保护您的数据，直到该节点可以确定它具有最新的群集状态并且群集中没有其他主节点。这可能意味着在恢复连接之前根本没有master。"],["body","\n"],["body","\n"],["body","\n"],["body","您可以通过在两个区域中的每个区域中放置一个主控合格的节点，并在独立的第三区域中添加一个额外的主控合格的节点 来解决此问题。"],["body","\n"],["body","\n"],["body","\n"],["body","在两个原始区域彼此断开连接的情况下，额外的主控合格节点 充当tiebreaker ， The extra tiebreaker node should be a dedicated voting-only master-eligible node,"],["body","\n"],["body","\n"],["body","\n"],["body","您应该使用  shard allocation awareness  来确保每个区域中都有每个分片的副本。这意味着如果另一个区域出现故障，则任何一个区域都保持完全可用。"],["body","\n"],["body","\n"],["body","\n"],["body","所有主控合格节点 (包括仅投票节点) 都在发布群集状态更新的关键路径上。因此，这些节点需要合理快速的持久性存储以及与群集其他节点的可靠，低延迟的网络连接。如果在第三个独立区域中添加了tiebreaker节点，则必须确保它具有足够的资源并且与群集的其余部分具有良好的连接性。"],["body","\n"],["body","\n\n"],["headingLink","clusters-with-three-or-more-zones"],["heading","Clusters with three or more zones"],["body","\n\n"],["body","如果您有三个区域，那么每个区域中应该有一个主控合格节点"],["body","\n"],["body","如果您有三个以上的区域，则应选择三个区域，并在这三个区域中的每个区域中放置一个符合主条件的节点。"],["body","\n"],["body","这将意味着即使其中一个区域失败，群集仍然可以选择主服务器。"],["body","\n"],["body","与往常一样，您的索引应该至少有一个副本，以防节点出现故障"],["body","\n"],["body","使用  shard allocation awareness  来限制 每个分片在 每个 区域的 副本分片数"],["body","\n"],["body","例如，如果您有一个配置了一个或两个副本的索引，则 allocation awareness 将确保碎片的副本与主副本位于不同的区域中"],["body","\n"],["body","这意味着如果一个区域失败，每个分片的副本仍然可用。这种碎片的可用性不会受到这种故障的影响。"],["body","\n\n"],["headingLink","summary-1"],["heading","Summary"],["body","\n"],["body","The cluster will be resilient to the loss of any zone as long as:"],["body","\n\n"],["body","The cluster health status is green. 集群状态为绿色"],["body","\n"],["body","There are at least two zones containing data nodes. 至少两个包含数据节点的区域"],["body","\n"],["body","Every index that is not a searchable snapshot index has at least one replica of each shard, in addition to the primary.：每个主分片 有 额外的副本 分片"],["body","\n"],["body","Shard allocation awareness is configured to avoid concentrating all copies of a shard within a single zone. Shard allocation awareness 避免将 所有副本集中在 单个区域"],["body","\n"],["body","The cluster has at least three master-eligible nodes. At least two of these nodes are not voting-only master-eligible nodes, and they are spread evenly across at least three zones."],["body","\n"],["body","Clients are configured to send their requests to nodes in more than one zone or are configured to use a load balancer that balances the requests across an appropriate set of nodes. The Elastic Cloud service provides such a load balancer."],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/高可用专题/集群备份.html"],["title","集群备份.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","back-up-a-cluster"],["heading","Back up a cluster"],["body","\n\n"],["body","\n"],["body","备份群集的唯一可靠且受支持的方法是拍摄快照。"],["body","\n"],["body","\n"],["body","\n"],["body","您不能通过复制Elasticsearch集群节点的数据目录来备份它。没有支持的方法可以从文件系统级备份中还原任何数据。如果您尝试从这样的备份中恢复群集，它可能会因损坏或丢失文件或其他数据不一致的报告而失败，或者它似乎已经成功地无声地丢失了一些数据。"],["body","\n"],["body","\n\n"],["headingLink","备份"],["heading","备份"],["body","\n\n"],["body","Back up the data"],["body","\n"],["body","Back up the cluster configuration"],["body","\n"],["body","Back up the security configuration"],["body","\n\n"],["headingLink","还原"],["heading","还原"],["body","\n\n"],["body","Restore the data"],["body","\n"],["body","Restore the security configuration"],["body","\n\n"],["headingLink","备份数据"],["heading","备份数据"],["body","\n"],["body","使用 snapshot API  备份数据"],["body","\n"],["body","可以支持 本地仓库、远程仓库"],["body","\n"],["body","远程仓库包括 Amazon S3, HDFS, Microsoft Azure, Google Cloud Storage"],["body","\n"],["body","以及 repository plugin  中所支持的仓库"],["body","\n"],["headingLink","备份集群配置"],["heading","备份集群配置"],["body","\n"],["body","\n"],["body","除了备份集群中的数据之外，备份其配置也很重要。特别是 集群变得很大 而且很难重建的时候"],["body","\n"],["body","\n\n"],["body","\n"],["body","配置信息保存在每个集群节点的  regular text files"],["body","\n"],["body","\n"],["body","\n"],["body","敏感信息例如 Watcher notification 密码 被存放在二进制的 安全容器 ：the elasticsearch.keystore file."],["body","\n"],["body","\n"],["body","\n"],["body","Some setting values are file paths to the associated configuration data, such as the ingest geo ip database."],["body","\n"],["body","\n"],["body","\n"],["body","All these files are contained inside the ES_PATH_CONF directory."],["body","\n"],["body","\n\n"],["body","注意"],["body","\n\n"],["body","\n"],["body","对配置文件的所有更改都是通过手动编辑文件或使用命令行实用程序来完成的，而不是通过api来完成的。实际上，这些更改在初始设置后很少发生。"],["body","\n"],["body","\n"],["body","\n"],["body","推荐使用 第三方备份软件 备份 $ES_PATH_CONF 目录"],["body","\n"],["body","\n"],["body","\n"],["body","推荐有一个 配置管理计划。加入到版本控制系统中去。或者通过您选择的配置管理工具来配置它们。"],["body","\n"],["body","\n"],["body","\n"],["body","密码敏感信息需要自行加密"],["body","\n"],["body","\n\n"],["body","配置覆盖"],["body","\n\n"],["body","一些配置可以被覆盖，通过  cluster settings"],["body","\n"],["body","这些配置可以通过 数据备份  include_global_state: true"],["body","\n"],["body","可以通过使用 集群 get settings API: 来获取文本配置"],["body","\n\n"],["body","GET _cluster/settings?pretty&flat_settings&filter_path=persistent\n"],["body","\n"],["body","注意"],["body","\n\n"],["body","Transient settings are not considered for backup."],["body","\n"],["body","Elasticsearch security features store configuration data such as role definitions and API keys inside a dedicate special index. This \"system\" data, complements the security settings configuration and should be backed up as well."],["body","\n"],["body","Other Elastic Stack components, like Kibana and Machine learning, store their configuration data inside other dedicated indices. From the Elasticsearch perspective these are just data so you can use the regular data backup process."],["body","\n\n"],["headingLink","备份安全配置"],["heading","备份安全配置"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/高可用专题/跨集群复制.html"],["title","跨集群复制.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/8.SearchYourData/6.PaginateSearchResults.html"],["title","PaginateSearchResults.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","paginate-search-results"],["heading","Paginate search results"],["body","\n"],["body","普通的 size、from 仅适用于 1000条以内的数据、深分页会导致搜索消耗的内存过多"],["body","\n"],["body","By default, you cannot use from and size to page through more than 10,000 hits. This limit is a safeguard set by the index.max_result_window index setting. If you need to page through more than 10,000 hits, use the search_after parameter instead."],["body","\n"],["body","（默认情况下，使用from+size 不会超过1W条数据）"],["body","\n"],["headingLink","分片副本的id不一致导致的相同排序值的排序不一致"],["heading","分片副本的ID不一致导致的相同排序值的排序不一致"],["body","\n\n"],["body","\n"],["body","Elasticsearch uses Lucene’s internal doc IDs as tie-breakers. （使用 internal doc ID 做我最终的排序选择）"],["body","\n"],["body","\n"],["body","\n"],["body","These internal doc IDs can be completely different across replicas of the same data.（相同数据、不同副本的文档ID可能不一样）"],["body","\n"],["body","\n"],["body","\n"],["body","When paging search hits, you might occasionally see that documents with the same sort values are not ordered consistently.（可能会观察到 具有相同排序值的 文档 的排序 不一致，因为可能检索到不同副本的同一个数据）"],["body","\n"],["body","\n\n"],["headingLink","search-after"],["heading","Search after"],["body","\n\n"],["body","可以通过指定 上一页的最后一条记录的sort values 来获取下一页"],["body","\n"],["body","Search after requests have optimizations that make them faster when the sort order is _shard_doc and total hits are not tracked. If you want to iterate over all documents regardless of the order, this is the most efficient option.（如果 想迭代访问所有数据，不关系顺序，可以 指定 sort_order 为 _shard_doc 、不记录 total hits。此时 这是最佳性能的选项）"],["body","\n"],["body","If the sort field is a date in some target data streams or indices but a date_nanos field in other targets, use the numeric_type parameter to convert the values to a single resolution and the format parameter to specify a date format for the sort field. Otherwise, Elasticsearch won’t interpret the search after parameter correctly in each request.  （如果排序字段 在为日期字段，但是有的是 date、有的是 date_nanos、则使用 numeric_type  参数来 转换成统一的个数。 ）"],["body","\n\n"],["body","GET /_search\n{\n  \"size\": 10000,\n  \"query\": {\n    \"match\" : {\n      \"user.id\" : \"elkbee\"\n    }\n  },\n  \"pit\": {\n    \"id\":  \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\", \n    \"keep_alive\": \"1m\"\n  },\n  \"sort\": [\n    {\"@timestamp\": {\"order\": \"asc\", \"format\": \"strict_date_optional_time_nanos\"}}\n  ],\n  \"search_after\": [                                \n    \"2021-05-20T05:30:04.832Z\",\n    4294967298\n  ],\n  \"track_total_hits\": false                        \n}\n"],["body","\n"],["headingLink","searchafter-with-pit"],["heading","SearchAfter with PIT"],["body","\n"],["body","如果在 request请求之间 refresh过。则排序结果可能会改变。会导致分页数据的不一致。使用 PIT 可以避免这个问题 point in time (PIT)"],["body","\n"],["body","POST /my-index-000001/_pit?keep_alive=1m\n"],["body","\n"],["body","{\n  \"id\": \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\"\n}\n"],["body","\n\n"],["body","All PIT search requests add an implicit sort tiebreaker field called _shard_doc, which can also be provided explicitly. If you cannot use a PIT, we recommend that you include a tiebreaker field in your sort. This tiebreaker field should contain a unique value for each document. If you don’t include a tiebreaker field, your paged results could miss or duplicate hits. （PIT search 会 隐式的添加 _shard_doc 排序字段。也可以显示提供，最好提供一个 唯一性的 tiebreaker字段，如果不是则很可能会导致分页结果重复、丢失）"],["body","\n\n"],["body","{\n  \"pit_id\" : \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\", \n  \"took\" : 17,\n  \"timed_out\" : false,\n  \"_shards\" : ...,\n  \"hits\" : {\n    \"total\" : ...,\n    \"max_score\" : null,\n    \"hits\" : [\n      ...\n      {\n        \"_index\" : \"my-index-000001\",\n        \"_id\" : \"FaslK3QBySSL_rrj9zM5\",\n        \"_score\" : null,\n        \"_source\" : ...,\n        \"sort\" : [                                \n          \"2021-05-20T05:30:04.832Z\",\n          4294967298                              \n        ]\n      }\n    ]\n  }\n}\n"],["body","\n"],["body","搜索的返回值 会包含最新的 pit_id 。搜索需要总是使用 最新的PIT"],["body","\n"],["body","from必须置0或者1"],["body","\n"],["body","删除PIT"],["body","\n"],["body","DELETE /_pit\n{\n    \"id\" : \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\"\n}\n"],["body","\n"],["headingLink","scroll-search-results"],["heading","Scroll search results"],["body","\n"],["body","不推荐使用"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/8.SearchYourData/3.Highlight.html"],["title","Highlight.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","highlighting"],["heading","Highlighting"],["body","\n\n"],["body","\n"],["body","高亮显示，会返回 命中的字段、以及命中的片段"],["body","\n"],["body","\n"],["body","\n"],["body","对于复杂的Bool 表达式。highlight 可能返回的命中不正常"],["body","\n"],["body","\n"],["body","\n"],["body","highligh需要 字段的实际内容。如果store=false 则加载实际的 _source并从 _source中提取相关字段。"],["body","\n\n"],["body","\n"],["body","例如 highlight->fields->fieldName"],["body","\n"],["body","{\n  \"query\": {\n    \"match\": { \"content\": \"kimchy\" }\n  },\n  \"highlight\": {\n    \"fields\": {\n      \"content\": {}\n    }\n  }\n}\n"],["body","\n"],["body","\n\n"],["body","\n"],["body","\n"],["body","Elasticsearch支持三种荧光笔(highlighter): unified、plain和fvh (快速矢量荧光笔)。您可以为每个字段指定要使用的荧光笔类型。"],["body","\n"],["body","\n\n"],["headingLink","unified-highlighter"],["heading","Unified highlighter"],["body","\n\n"],["body","The unified highlighter uses the Lucene Unified Highlighter."],["body","\n"],["body","This highlighter breaks the text into sentences and uses the BM25 algorithm to score individual sentences as if they were documents in the corpus.\n\n"],["body","它将文本 分解成 语句。使用 BM25 算法 对每个 语句算分，就好像它们是语料库中的文档一样。"],["body","\n\n"],["body","\n"],["body","It also supports accurate phrase and multi-term (fuzzy, prefix, regex) highlighting. This is the default highlighter."],["body","\n\n"],["headingLink","plain-highlighter"],["heading","Plain highlighter"],["body","\n\n"],["body","\n"],["body","The plain highlighter uses the standard Lucene highlighter."],["body","\n"],["body","\n"],["body","\n"],["body","It attempts to reflect the query matching logic in terms of understanding word importance and any word positioning criteria in phrase queries."],["body","\n\n"],["body","它试图在理解  单词重要性 和短语查询中的任何单词定位标准方面  影响 查询匹配逻辑。"],["body","\n\n"],["body","\n"],["body","\n"],["body","The plain highlighter works best for highlighting simple query matches in a single field. To accurately reflect query logic, it creates a tiny in-memory index and re-runs the original query criteria through Lucene’s query execution planner to get access to low-level match information for the current document. This is repeated for every field and every document that needs to be highlighted. If you want to highlight a lot of fields in a lot of documents with complex queries, we recommend using the unified highlighter on postings or term_vector fields."],["body","\n\n"],["body","plain highlighter  适用于 单字段的 简单查询匹配的 高亮。"],["body","\n"],["body","它会创建一个很小的内存索引，并通过Lucene的查询执行计划器重新运行原始查询条件，以访问当前文档的低级匹配信息。对于每个需要突出显示的字段和每个文档，都会重复此操作"],["body","\n"],["body","如果要突出显示许多具有复杂查询的文档中的许多字段，我们建议在 postings 或 term_vector 字段上使用 unified 荧光笔。"],["body","\n\n"],["body","\n\n"],["headingLink","fast-vector-highlighter"],["heading","Fast vector highlighter"],["body","\n\n"],["body","The fvh highlighter uses the Lucene Fast Vector highlighter."],["body","\n"],["body","This highlighter can be used on fields with term_vector set to with_positions_offsets in the mapping. The fast vector highlighter:\n\n"],["body","Can be customized with a boundary_scanner."],["body","\n"],["body","Requires setting term_vector to with_positions_offsets which increases the size of the index"],["body","\n"],["body","Can combine matches from multiple fields into one result. See matched_fields"],["body","\n"],["body","Can assign different weights to matches at different positions allowing for things like phrase matches being sorted above term matches when highlighting a Boosting Query that boosts phrase matches over term matches\n\n"],["body","可以为不同位置的匹配分配不同的权重，允许在突出显示提升短语匹配超过术语匹配的提升查询时，短语匹配在术语匹配之上排序"],["body","\n\n"],["body","\n\n"],["body","\n"],["body","The fvh highlighter does not support span queries. If you need support for span queries, try an alternative highlighter, such as the unified highlighter."],["body","\n\n"],["headingLink","offsets-strategy"],["heading","Offsets strategy"],["body","\n"],["body","To create meaningful search snippets from the terms being queried, the highlighter needs to know the start and end character offsets of each word in the original text. These offsets can be obtained from:"],["body","\n\n"],["body","为了从被查询的术语中创建有意义的搜索片段，荧光笔（highlighter）需要知道原始文本中每个单词的开始和结束字符偏移。这些偏移可以从以下位置获得:"],["body","\n\n\n"],["body","The postings list\n\n"],["body","If index_options is set to offsets in the mapping, the unified highlighter uses this information to highlight documents without re-analyzing the text. It re-runs the original query directly on the postings and extracts the matching offsets from the index, limiting the collection to the highlighted documents. This is important if you have large fields because it doesn’t require reanalyzing the text to be highlighted. It also requires less disk space than using term_vectors.\n\n"],["body","index_options=offsets unified highlighter 会使用这个信息 去高亮文档。而不用重新分析文本。它直接在postings 重新运行原始查询，并从索引中提取匹配的偏移量。这个操作只会在 要 高亮的文档集合。如果你有很大的字段，这一点很重要，因为它不需要重新分析文本来高亮。同样 比  term_vectors 消耗更少的空间"],["body","\n\n"],["body","\n\n"],["body","\n"],["body","Term vectors.\n\n"],["body","term_vector = with_positions_offsets  unified highlighter 会自动使用 term_vector 高亮字段. It’s fast especially for large fields (> 1MB) and for highlighting multi-term queries like prefix or wildcard because it can access the dictionary of terms for each document. The fvh highlighter always uses term vectors."],["body","\n"],["body","对于大字段（>1MB） 多词项 的 查询，例如：prefix、wildcard 它很快。因为 它能 访问每个文档 词项的  字典。"],["body","\n\n"],["body","\n"],["body","Plain highlighting.\n\n"],["body","This mode is used by the unified when there is no other alternative. （没有可用选项的时候，unified highter会使用  Plain highlighting.）"],["body","\n"],["body","It creates a tiny in-memory index and re-runs the original query criteria through Lucene’s query execution planner to get access to low-level match information on the current document. This is repeated for every field and every document that needs highlighting. The plain highlighter always uses plain highlighting.（"],["body","\n\n"],["body","\n\n"],["body","\n"],["body","Plain highlighting for large texts may require substantial amount of time and memory. To protect against this, the maximum number of text characters that will be analyzed has been limited to 1000000. This default limit can be changed for a particular index with the index setting index.highlight.max_analyzed_offset."],["body","\n\n"],["body","大文本的 Plain highlighting 可能需要大量的时间和内存。为了防止这种情况，将被分析的文本字符的最大数量被限制为100_0000个。"],["body","\n"],["body","可以使用索引设置 [index.highlight.max_analyzed_offset](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/index-modules.html # index-max-analyzed-offset) 为特定索引更改此默认限制。"],["body","\n\n"],["body","\n"],["headingLink","highlighting-settings"],["heading","Highlighting settings"],["body","\n"],["body","\n"],["body","Highlighting settings can be set on a global level and overridden at the field level.（Highlighting settings 可以在全局级别上设置，并在字段级别上覆盖。）"],["body","\n"],["body","\n"],["headingLink","boundary_chars"],["heading","boundary_chars"],["body","\n"],["body","A string that contains each boundary character. Defaults to .,!? \\t\\n.（边界字符）"],["body","\n"],["headingLink","boundary_max_scan"],["heading","boundary_max_scan"],["body","\n"],["body","How far to scan for boundary characters. Defaults to 20."],["body","\n"],["headingLink","boundary_scanner"],["heading","boundary_scanner"],["body","\n\n"],["body","Specifies how to break the highlighted fragments: chars, sentence, or word. （如何 分解 highlighted fragments: ）"],["body","\n"],["body","Only valid for the unified and fvh highlighters. （只对 unified、fvh生效）"],["body","\n"],["body","Defaults to sentence for the unified highlighter. Defaults to chars for the fvh highlighter."],["body","\n"],["body","**chars**Use the characters specified by boundary_chars as highlighting boundaries.  （chars 使用  boundary_chars 来指定 highlighting boundaries.  ）"],["body","\n"],["body","The boundary_max_scan setting controls how far to scan for boundary characters. Only valid for the fvh highlighter.**sentenceBreak highlighted fragments at the next sentence boundary, as determined by Java’s BreakIterator. You can specify the locale to use with boundary_scanner_locale.When used with the unified highlighter, the sentence scanner splits sentences bigger than fragment_size at the first word boundary next to fragment_size. You can set fragment_size to 0 to never split any sentence.word**Break highlighted fragments at the next word boundary, as determined by Java’s BreakIterator. You can specify the locale to use with boundary_scanner_locale.\n\n"],["body","控制 boundary characters.扫描距离、只针对 fvh生效"],["body","\n"],["body","sentence 将 高亮片段 中段在 下一个 sentence 边界。这个边界是由 Java’s BreakIterator 定义的。可以指定 boundary_scanner_locale"],["body","\n"],["body","When used with the unified highlighter, the sentence scanner splits sentences bigger than fragment_size at the first word boundary next to fragment_size.  ？ 没看懂"],["body","\n\n"],["body","\n\n\n"],["body","\n"],["body","boundary_scanner_locale"],["body","\n"],["body","（本地化）"],["body","\n"],["body","Controls which locale is used to search for sentence and word boundaries. This parameter takes a form of a language tag, e.g. \"en-US\", \"fr-FR\", \"ja-JP\". More info can be found in the Locale Language Tag documentation. The default value is Locale.ROOT."],["body","\n"],["body","\n"],["body","\n"],["body","encoder（HTML encoder）"],["body","\n"],["body","Indicates if the snippet should be HTML encoded: default (no encoding) or html (HTML-escape the snippet text and then insert the highlighting tags)"],["body","\n"],["body","\n"],["body","\n"],["body","fields"],["body","\n"],["body","Specifies the fields to retrieve highlights for. You can use wildcards to specify fields. For example, you could specify comment_* to get highlights for all text, match_only_text, and keyword fields that start with comment_.Only text, match_only_text, and keyword fields are highlighted when you use wildcards. If you use a custom mapper and want to highlight on a field anyway, you must explicitly specify that field name."],["body","\n\n"],["body","指定要 高亮显示的字段。可以指定 wildcards"],["body","\n"],["body","comment_*"],["body","\n\n"],["body","\n"],["body","\n"],["body","force_source"],["body","\n"],["body","Highlight based on the source even if the field is stored separately. Defaults to false.（根据来源来高亮显示）"],["body","\n"],["body","\n"],["body","\n"],["body","fragmenter"],["body","\n"],["body","Specifies how text should be broken up in highlight snippets: simple or span. Only valid for the plain highlighter. Defaults to span.**simpleBreaks up text into same-sized fragments.span**Breaks up text into same-sized fragments, but tries to avoid breaking up text between highlighted terms. This is helpful when you’re querying for phrases. Default."],["body","\n"],["body","(plain highlighter 用来 分解 需要高亮的文本片段，simple or span。simple会分解成同大小字段、span 也会分解成同大小字段。但会避免  分解 highlighted terms )"],["body","\n"],["body","\n"],["body","\n"],["body","fragment_offset"],["body","\n"],["body","Controls the margin from which you want to start highlighting. Only valid when using the fvh highlighter."],["body","\n"],["body","\n"],["body","\n"],["body","fragment_size"],["body","\n"],["body","The size of the highlighted fragment in characters. Defaults to 100.（高亮的片段大小）"],["body","\n"],["body","\n"],["body","\n"],["body","highlight_query"],["body","\n"],["body","Highlight matches for a query other than the search query. This is especially useful if you use a rescore query because those are not taken into account by highlighting by default.Elasticsearch does not validate that highlight_query contains the search query in any way so it is possible to define it so legitimate query results are not highlighted. Generally, you should include the search query as part of the highlight_query. （使用这个query来进行高亮匹配）（场景之一 rescore query 没有考虑高亮。同时，如果 highlight_query 如果  定义不当，可能会使得合法结果没有高亮）"],["body","\n"],["body","\n"],["body","\n"],["body","matched_fields"],["body","\n"],["body","Combine matches on multiple fields to highlight a single field. This is most intuitive for multifields that analyze the same string in different ways. All matched_fields must have term_vector set to with_positions_offsets, but only the field to which the matches are combined is loaded so only that field benefits from having store set to yes. Only valid for the fvh highlighter."],["body","\n"],["body","（不清楚）"],["body","\n"],["body","\n"],["body","\n"],["body","no_match_size"],["body","\n"],["body","The amount of text you want to return from the beginning of the field if there are no matching fragments to highlight. Defaults to 0 (nothing is returned)."],["body","\n"],["body","\n"],["body","\n"],["body","number_of_fragments"],["body","\n"],["body","The maximum number of fragments to return. If the number of fragments is set to 0, no fragments are returned. Instead, the entire field contents are highlighted and returned. This can be handy when you need to highlight short texts such as a title or address, but fragmentation is not required. If number_of_fragments is 0, fragment_size is ignored. Defaults to 5."],["body","\n"],["body","\n"],["body","\n"],["body","order"],["body","\n"],["body","Sorts highlighted fragments by score when set to score. By default, fragments will be output in the order they appear in the field (order: none). Setting this option to score will output the most relevant fragments first. Each highlighter applies its own logic to compute relevancy scores. See the document How highlighters work internally for more details how different highlighters find the best fragments."],["body","\n"],["body","\n"],["body","\n"],["body","phrase_limit"],["body","\n"],["body","Controls the number of matching phrases in a document that are considered. Prevents the fvh highlighter from analyzing too many phrases and consuming too much memory. When using matched_fields, phrase_limit phrases per matched field are considered. Raising the limit increases query time and consumes more memory. Only supported by the fvh highlighter. Defaults to 256."],["body","\n"],["body","\n"],["body","\n"],["body","pre_tags"],["body","\n"],["body","Use in conjunction with post_tags to define the HTML tags to use for the highlighted text. By default, highlighted text is wrapped in <em> and </em> tags. Specify as an array of strings."],["body","\n"],["body","\n"],["body","\n"],["body","post_tags"],["body","\n"],["body","Use in conjunction with pre_tags to define the HTML tags to use for the highlighted text. By default, highlighted text is wrapped in <em> and </em> tags. Specify as an array of strings."],["body","\n"],["body","\n"],["body","\n"],["body","require_field_match"],["body","\n"],["body","By default, only fields that contains a query match are highlighted. Set require_field_match to false to highlight all fields. Defaults to true."],["body","\n"],["body","\n"],["body","\n"],["body","max_analyzed_offset"],["body","\n"],["body","By default, the maximum number of characters analyzed for a highlight request is bounded by the value defined in the index.highlight.max_analyzed_offset setting, and when the number of characters exceeds this limit an error is returned. If this setting is set to a non-negative value, the highlighting stops at this defined maximum limit, and the rest of the text is not processed, thus not highlighted and no error is returned. The max_analyzed_offset query setting does not override the index.highlight.max_analyzed_offset which prevails when it’s set to lower value than the query setting."],["body","\n"],["body","\n"],["body","\n"],["body","tags_schema"],["body","\n"],["body","Set to styled to use the built-in tag schema. The styled schema defines the following pre_tags and defines post_tags as </em>.<em class=\"hlt1\">, <em class=\"hlt2\">, <em class=\"hlt3\">, <em class=\"hlt4\">, <em class=\"hlt5\">, <em class=\"hlt6\">, <em class=\"hlt7\">, <em class=\"hlt8\">, <em class=\"hlt9\">, <em class=\"hlt10\">"],["body","\n"],["body","\n"],["body","\n"],["body","type"],["body","\n"],["body","The highlighter to use: unified, plain, or fvh. Defaults to unified."],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/8.SearchYourData/4.LongRunningSearches.html"],["title","LongRunningSearches.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","long-running-searches"],["heading","Long-running searches"],["body","\n"],["body","Elasticsearch generally allows you to quickly search across big amounts of data. There are situations where a search executes on many shards, possibly against frozen indices and spanning multiple remote clusters, for which results are not expected to be returned in milliseconds. When you need to execute long-running searches, synchronously waiting for its results to be returned is not ideal. Instead, Async search lets you submit a search request that gets executed asynchronously, monitor the progress of the request, and retrieve results at a later stage. You can also retrieve partial results as they become available but before the search has completed.（针对 冷冻索引搜索、横跨多个远程集群搜索、可能会需要很长时间，这时候可以使用 异步搜索）"],["body","\n"],["body","You can submit an async search request using the submit async search API. The get async search API allows you to monitor the progress of an async search request and retrieve its results. An ongoing async search can be deleted through the delete async search API."],["body","\n"],["headingLink","async-search"],["heading","Async search"],["body","\n"],["headingLink","submit-async-search-api提交异步搜索api"],["heading","Submit async search API（提交异步搜索API）"],["body","\n"],["body","POST /sales*/_async_search?size=0\n{\n  \"sort\": [\n    { \"date\": { \"order\": \"asc\" } }\n  ],\n  \"aggs\": {\n    \"sale_date\": {\n      \"date_histogram\": {\n        \"field\": \"date\",\n        \"calendar_interval\": \"1d\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","{\n  \"id\" : \"FmRldE8zREVEUzA2ZVpUeGs2ejJFUFEaMkZ5QTVrSTZSaVN3WlNFVmtlWHJsdzoxMDc=\",  //任务ID\n  \"is_partial\" : true, //true 是否所有分片都执行成功\n  \"is_running\" : true,  //是否正在执行\n  \"start_time_in_millis\" : 1583945890986,\n  \"expiration_time_in_millis\" : 1584377890986,\n  \"response\" : {\n    \"took\" : 1122,\n    \"timed_out\" : false,\n    \"num_reduce_phases\" : 0,\n    \"_shards\" : {\n      \"total\" : 562, //要执行的分片个数\n      \"successful\" : 3,  //成功的分片个数\n      \"skipped\" : 0,\n      \"failed\" : 0\n    },\n    \"hits\" : {\n      \"total\" : {\n        \"value\" : 157483,  //目前已返回的匹配的文档数\n        \"relation\" : \"gte\"\n      },\n      \"max_score\" : null,\n      \"hits\" : [ ]\n    }\n  }\n}\n"],["body","\n"],["body","It is possible to block and wait until the search is completed up to a certain timeout by providing the wait_for_completion_timeout parameter, which defaults to 1 second. When the async search completes within such timeout, the response won’t include the ID as the results are not stored in the cluster. （等待完成秒数，如果在等待期间就完成了，则直接返回结果，不返回任务ID，也不会保存任务结果）"],["body","\n"],["body","The keep_on_completion parameter, which defaults to false, can be set to true to request that results are stored for later retrieval also when the search completes within the wait_for_completion_timeout.（上述场景，仍会保存任务结果，返回任务ID，以下次取用）"],["body","\n"],["body","You can also specify how long the async search needs to be available through the keep_alive parameter, which defaults to 5d (five days). Ongoing async searches and any saved search results are deleted after this period.（异步搜索的结果的保存天数，默认5天）"],["body","\n"],["body","When the primary sort of the results is an indexed field, shards get sorted based on minimum and maximum value that they hold for that field, hence partial results become available following the sort criteria that was requested.（ 当主要排序是索引字段时，分片之间会根据 持有的最大值、最小值排序。因此部分 满足条件的 结果会立即可用 ）"],["body","\n"],["body","The submit async search API supports the same parameters as the search API, though some have different default values:（与searchAPI相同，但有以下参数）"],["body","\n\n"],["body","batched_reduce_size defaults to 5: this affects how often partial results become available, which happens whenever shard results are reduced. A partial reduction is performed every time the coordinating node has received a certain number of new shard responses (5 by default).（ 这会影响 部分结果多久变得可用，每当减少分片结果时，就会发生这种情况。每次协调节点收到一定数量的新分片响应时，都会执行部分减少 ）"],["body","\n"],["body","request_cache defaults to true"],["body","\n"],["body","pre_filter_shard_size defaults to 1 and cannot be changed: this is to enforce the execution of a pre-filter roundtrip to retrieve statistics from each shard so that the ones that surely don’t hold any document matching the query get skipped.（  用来获取 分片的统计信息，以过滤某些肯定不包含数据的分片 ）"],["body","\n"],["body","ccs_minimize_roundtrips defaults to false, which is also the only supported value"],["body","\n\n"],["body","\n"],["body","Async search does not support scroll nor search requests that only include the suggest section. Cross-cluster search is supported only with ccs_minimize_roundtrips set to false."],["body","\n"],["body","\n"],["body","\n"],["body","In 7.x Elasticsearch, by default, doesn’t limit the size of a stored async search response. Storing huge async responses can destabilize a cluster. If you want to set the limit for the maximum allowed size, change search.max_async_search_response_size cluster level setting. After that, an attempt to store an async response larger than this setting will result in an error.（限制异步检索的结果集大小）"],["body","\n"],["body","\n"],["headingLink","get-async-search"],["heading","Get async search"],["body","\n"],["body","The get async search API retrieves the results of a previously submitted async search request given its id. If the Elasticsearch security features are enabled, the access to the results of a specific async search is restricted to the user or API key that submitted it.（需要鉴权）"],["body","\n"],["body","GET /_async_search/FmRldE8zREVEUzA2ZVpUeGs2ejJFUFEaMkZ5QTVrSTZSaVN3WlNFVmtlWHJsdzoxMDc=\n"],["body","\n"],["body","{\n  \"id\" : \"FmRldE8zREVEUzA2ZVpUeGs2ejJFUFEaMkZ5QTVrSTZSaVN3WlNFVmtlWHJsdzoxMDc=\",\n  \"is_partial\" : true, \n  \"is_running\" : true, \n  \"start_time_in_millis\" : 1583945890986,\n  \"expiration_time_in_millis\" : 1584377890986, \n  \"response\" : {\n    \"took\" : 12144,\n    \"timed_out\" : false,\n    \"num_reduce_phases\" : 46, \n    \"_shards\" : {\n      \"total\" : 562,\n      \"successful\" : 188, \n      \"skipped\" : 0,\n      \"failed\" : 0\n    },\n    \"hits\" : {\n      \"total\" : {\n        \"value\" : 456433,\n        \"relation\" : \"eq\"\n      },\n      \"max_score\" : null,\n      \"hits\" : [ ]\n    },\n    \"aggregations\" : { \n      \"sale_date\" :  {\n        \"buckets\" : []\n      }\n    }\n  }\n}\n"],["body","\n"],["body","The wait_for_completion_timeout parameter can also be provided when calling the Get Async Search API, in order to wait for the search to be completed up until the provided timeout. Final results will be returned if available before the timeout expires, otherwise the currently available results will be returned once the timeout expires. By default no timeout is set meaning that the currently available results will be returned without any additional wait."],["body","\n"],["body","The keep_alive parameter specifies how long the async search should be available in the cluster. When not specified, the keep_alive set with the corresponding submit async request will be used. Otherwise, it is possible to override such value and extend the validity of the request. When this period expires, the search, if still running, is cancelled. If the search is completed, its saved results are deleted."],["body","\n"],["headingLink","get-async-search-status"],["heading","Get async search status"],["body","\n"],["body","The get async search status API, without retrieving search results, shows only the status of a previously submitted async search request given its id. If the Elasticsearch security features are enabled, the access to the get async search status API is restricted to the monitoring_user role.（获取异步搜索的结果）"],["body","\n"],["body","GET /_async_search/status/FmRldE8zREVEUzA2ZVpUeGs2ejJFUFEaMkZ5QTVrSTZSaVN3WlNFVmtlWHJsdzoxMDc=\n"],["body","\n"],["body","{\n  \"id\" : \"FmRldE8zREVEUzA2ZVpUeGs2ejJFUFEaMkZ5QTVrSTZSaVN3WlNFVmtlWHJsdzoxMDc=\",\n  \"is_running\" : true,\n  \"is_partial\" : true,\n  \"start_time_in_millis\" : 1583945890986,\n  \"expiration_time_in_millis\" : 1584377890986,\n  \"_shards\" : {\n      \"total\" : 562,\n      \"successful\" : 188, \n      \"skipped\" : 0,\n      \"failed\" : 0\n  }\n}\n"],["body","\n"],["headingLink","delete-async-search"],["heading","Delete async search"],["body","\n"],["body","DELETE /_async_search/FmRldE8zREVEUzA2ZVpUeGs2ejJFUFEaMkZ5QTVrSTZSaVN3WlNFVmtlWHJsdzoxMDc=\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/8.SearchYourData/README.html"],["title","SearchYourData - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","run-a-search"],["heading","Run a search"],["body","\n"],["body","可以使用  search API 、 aggregate API 检索数据。query 请求体参数 参照  Query DSL"],["body","\n"],["body","GET /my-index-000001/_search\n{\n  \"query\": {\n    \"match\": {\n      \"user.id\": \"kimchy\"\n    }\n  }\n}\n"],["body","\n"],["body","API响应返回与 hits.hits 属性中的查询匹配的前10个文档。"],["body","\n"],["body","{\n  \"took\": 5,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\": 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": {\n      \"value\": 1,\n      \"relation\": \"eq\"\n    },\n    \"max_score\": 1.3862942,\n    \"hits\": [\n      {\n        \"_index\": \"my-index-000001\",\n        \"_type\": \"_doc\",\n        \"_id\": \"kxWFcnMByiguvud1Z8vC\",\n        \"_score\": 1.3862942,\n        \"_source\": {\n          \"@timestamp\": \"2099-11-15T14:12:12\",\n          \"http\": {\n            \"request\": {\n              \"method\": \"get\"\n            },\n            \"response\": {\n              \"bytes\": 1070000,\n              \"status_code\": 200\n            },\n            \"version\": \"1.1\"\n          },\n          \"message\": \"GET /search HTTP/1.1 200 1070000\",\n          \"source\": {\n            \"ip\": \"127.0.0.1\"\n          },\n          \"user\": {\n            \"id\": \"kimchy\"\n          }\n        }\n      }\n    ]\n  }\n}\n"],["body","\n"],["headingLink","define-fields-that-exist-only-in-a-query"],["heading","Define fields that exist only in a query"],["body","\n\n"],["body","您可以定义 [运行时字段](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/runtime-search-request.html “在搜索请求中定义运行时字段”)，而不是索引数据然后搜索它，这些字段仅作为搜索查询的一部分存在"],["body","\n"],["body","您可以在搜索请求中指定一个 runtime_mappings 部分来定义运行时字段，该字段可以选择包含一个 Painless Script。"],["body","\n"],["body","例如，以下查询定义了一个名为 day_of_week 的运行时字段。包含的脚本根据 @timestamp 字段的值计算星期几，并使用 emit 返回计算的值。"],["body","\n\n"],["body","GET /my-index-000001/_search\n{\n  \"runtime_mappings\": {\n    \"day_of_week\": {\n      \"type\": \"keyword\",\n      \"script\": {\n        \"source\":\n        \"\"\"emit(doc['@timestamp'].value.dayOfWeekEnum\n        .getDisplayName(TextStyle.FULL, Locale.ROOT))\"\"\"\n      }\n    }\n  },\n  \"aggs\": {\n    \"day_of_week\": {\n      \"terms\": {\n        \"field\": \"day_of_week\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","        useEsClient {\n            kSearch(ScaRecording::class.java) {\n                index(\"96_sca_recording_aliyun\")\n                kFields(\"dayOfWeek2\") {\n                    format(\"xxxx\")\n                    includeUnmapped(true)\n                }\n                kFields(\"dayOfWeek1\", \"dayOfWeek\")\n                kSource {\n                    fetch(false)\n                    includes(\"\")\n                    fetch(true)\n                }\n                kQuery {\n                    kBool {\n                        filterRange {\n                            field(\"createTime\")\n                            gt(\n                                JsonData.of(\n                                    LocalDateTime.now().minusYears(1L)\n                                        .format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\n                                )\n                            )\n                            to(\"2022-12-01 00:00:00\")\n                        }\n                    }\n                }\n                //runtimeMappings->key->type->valueScript\n                runtimeMappings(\"dayOfWeek\") { runTimeField ->\n                    runTimeField.type(RuntimeFieldType.Keyword).script { script ->\n                        script.inline { inlineScript ->\n                            inlineScript.source(\n                                \"\"\"\n                                    emit(doc['createTime'].value.dayOfWeekEnum.getDisplayName(TextStyle.FULL, Locale.ROOT))\n                                \"\"\".trimIndent()\n                            )\n                        }\n                    }\n                }\n                kRuntimeMappings(\"dayOfWeek1\", RuntimeFieldType.Keyword) {\n                    inlineScript(\n                        \"\"\"\n                        emit(doc['createTime'].value.dayOfWeekEnum.getDisplayName(TextStyle.FULL, Locale.ROOT));\n                        emit(doc['updateTime'].value.dayOfWeekEnum.getDisplayName(TextStyle.FULL, Locale.ROOT))\n                    \"\"\".trimIndent()\n                    )\n                }\n\n                aggregations(\"dayOfWeek\") {\n                    it.terms {\n                        it.field(\"dayOfWeek\")\n                    }\n                }\n                kAggregations(\"dayOfWeek1\") {\n                    kTerms(\"dayOfWeek\")\n                }\n            }.also {\n                println(it.aggregations())\n            }.hits().hits().forEach {\n                println(it.fields())\n                println(it.source())\n            }\n        }\n\n"],["body","\n"],["body","可以对RunTimeMapping字段实时聚合"],["body","\n"],["headingLink","common-search-options"],["heading","Common search options"],["body","\n"],["body","\n"],["body","通用搜索条件"],["body","\n"],["body","\n"],["body","您可以使用以下选项自定义搜索。"],["body","\n\n"],["body","QueryDSL：查询"],["body","\n"],["body","Aggreations：聚合"],["body","\n"],["body","Search multiple data streams and indices：多索引查询"],["body","\n"],["body","Paginate search results：分页"],["body","\n"],["body","Retrieve selected fields：只检索指定列"],["body","\n"],["body","Sort search results：对结果排序"],["body","\n"],["body","Run an async search：异步查询"],["body","\n\n"],["body","Query DSL"],["body","\n"],["body","\n"],["body","Query DSL 支持多种查询类型，您可以混合匹配以获得所需的结果。查询类型包括"],["body","\n"],["body","\n\n"],["body","Boolean and other compound queries, which let you combine queries and match results based on multiple criteria"],["body","\n"],["body","Term-level queries for filtering and finding exact matches"],["body","\n"],["body","Full text queries, which are commonly used in search engines"],["body","\n"],["body","Geo and spatial queries"],["body","\n\n"],["body","Aggregations"],["body","\nYou can use search aggregations to get statistics and other analytics for your search results. Aggregations help you answer questions like:"],["body","\n\n"],["body","平均值"],["body","\n"],["body","TopHit"],["body","\n\n"],["body","Search multiple data streams and indices"],["body","\n"],["body","可以使用 逗号分隔的值或者 grep-like的 index pattern 查询多个索引，您甚至可以提高特定索引的搜索结果。"],["body","\n"],["body","See Search multiple data streams and indices."],["body","\n"],["body","Paginate search results"],["body","\nBy default, searches return only the top 10 matching hits. To retrieve more or fewer documents, see Paginate search results."],["body","\n"],["body","Retrieve selected fields"],["body","\nThe search response’s hit.hits property includes the full document _source for each hit. To retrieve only a subset of the _source or other fields, see Retrieve selected fields."],["body","\n"],["body","Sort search results"],["body","\nBy default, search hits are sorted by _score, a relevance score that measures how well each document matches the query. To customize the calculation of these scores, use the script_score query. To sort search hits by other field values, see Sort search results."],["body","\n"],["body","Run an async search"],["body","\nElasticsearch searches are designed to run on large volumes of data quickly, often returning results in milliseconds. For this reason, searches are synchronous by default. The search request waits for complete results before returning a response."],["body","\n"],["body","However, complete results can take longer for searches across frozen indices or multiple clusters."],["body","\n"],["body","To avoid long waits, you can run an asynchronous, or async, search instead. An async search lets you retrieve partial results for a long-running search now and get complete results later."],["body","\n"],["headingLink","search-timeout"],["heading","Search timeout"],["body","\n\n"],["body","默认情况下，搜索请求不会暂停。在返回响应之前，该请求等待每个分片的完整结果"],["body","\n"],["body","虽然 [异步搜索](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/async-search-intro.html “长时间运行的搜索”) 是为长时间运行的搜索而设计的，同样可以使用 timeout 参数，指定每个分片最多等待的时长。每个分片收集指定时间段内的，如果期间结束时收集尚未完成，Elasticsearch仅使用到该点为止累积的Hits，搜索请求的总体延迟取决于搜索所需的分片数和并发分片请求数。"],["body","\n\n"],["body","GET /my-index-000001/_search\n{\n  \"timeout\": \"2s\",\n  \"query\": {\n    \"match\": {\n      \"user.id\": \"kimchy\"\n    }\n  }\n}\n"],["body","\n\n"],["body","要为所有搜索请求设置集群范围的默认超时时间，"],["body","\n"],["body","configure search.default_search_timeout using the cluster settings API."],["body","\n"],["body","如果没有指定 timeout 参数，则使用全局timeout参数"],["body","\n"],["body","If the global search timeout expires before the search request finishes, the request is cancelled using task cancellation. The search.default_search_timeout setting defaults to -1 (no timeout)."],["body","\n\n"],["headingLink","search-cancellation"],["heading","Search cancellation"],["body","\n\n"],["body","可以通过 task management API管理异步任务"],["body","\n"],["body","当客户端的HTTP连接关闭时，Elasticsearch还会自动取消搜索请求。我们建议您将客户端设置为在搜索请求中止或超时时关闭HTTP连接。"],["body","\n\n"],["headingLink","track-total-hits"],["heading","Track total hits"],["body","\n\n"],["body","通常，如果不访问所有匹配项，则无法准确计算总命中数，这对于匹配大量文档的查询而言成本很高。"],["body","\n"],["body","track_total_hits 参数允许您控制应如何跟踪命中总数。\n\n"],["body","鉴于通常具有命中次数的下限 (例如 至少有10000次命中) 就足够了，默认设置为 10,000。"],["body","\n"],["body","这意味着请求将精确地计算总命中数，直到 10,000 命中"],["body","\n"],["body","如果您在一定阈值后不需要准确的点击次数，则可以加快搜索速度，这是一个很好的选择"],["body","\n\n"],["body","\n\n"],["body","当设置为 true 时，搜索响应将始终跟踪与查询准确匹配的命中数 (例如，当 track_total_hits 设置为true时，total.Relation 将始终等于 “eq”)。"],["body","\n"],["body","否则，在搜索响应中的 total 对象中返回的 total.relation 确定应如何解释 total.value。值 gte 表示 total.value 是匹配查询的总命中的下限，值 eq 表示 total.value 是准确的计数。"],["body","\n"],["body","GET my-index-000001/_search\n{\n  \"track_total_hits\": true,\n  \"query\": {\n    \"match\" : {\n      \"user.id\" : \"elkbee\"\n    }\n  }\n}\n"],["body","\n"],["headingLink","quickly-check-for-matching-docs"],["heading","Quickly check for matching docs"],["body","\n\n"],["body","如果您只想知道是否有任何文档与特定查询匹配，您可以将 size 设置为 “0”，以表明我们对搜索结果不感兴趣。您还可以将 terminate_fter 设置为 1，以指示只要找到第一个匹配文档(每个分片)，就可以终止查询执行。"],["body","\n\n"],["body","GET /_search?q=user.id:elkbee&size=0&terminate_after=1"],["body","\n"],["body","terminate_after 始终在  post_filter 之后应用  并在分片上收集到足够的命中后停止查询以及聚合执行。尽管聚合的文档计数可能不会反映响应中的 `hits.Total”，因为聚合在 Post filtering之前应用"],["body","\n"],["body","响应将不包含任何Hints，因为 size 设置为 “0”。Hits.Total 将等于 0，表示没有匹配的文档，或者大于 0，这意味着当查询提前终止时，至少有与查询匹配的文档。同样，如果查询提前终止，则响应中的 “terminated_early” 标志将设置为 “true”。"],["body","\n"],["body","{\n  \"took\": 3,\n  \"timed_out\": false,\n  \"terminated_early\": true,\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"skipped\" : 0,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\" : {\n        \"value\": 1,\n        \"relation\": \"eq\"\n    },\n    \"max_score\": null,\n    \"hits\": []\n  }\n}\n"],["body","\n"],["body","响应中的 took 时间包含此请求处理所需的毫秒，在节点收到查询后迅速开始，直到完成所有与搜索相关的工作，并且在上述JSON返回给客户端之前。这意味着它包括在线程池中等待的时间，在整个集群中执行分布式搜索并收集所有结果。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/8.SearchYourData/5.NearReaTimeSearch.html"],["title","NearReaTimeSearch.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","near-real-time-search"],["heading","Near real-time search"],["body","\n"],["body","近实时搜索"],["body","\n\n"],["body","为什么是叫：近实时"],["body","\n"],["body","如果做到近实时"],["body","\n"],["body","ES 文件是如何组织的\n\n"],["body","segement+commit point = luence index = Es shard = Es Index"],["body","\n\n"],["body","\n"],["body","ES是如何更新的"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/8.SearchYourData/1.CollapseSearchResults.html"],["title","CollapseSearchResults.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","collapse-search-results"],["heading","Collapse search results"],["body","\n\n"],["body","您可以使用 collapse 参数根据字段值折叠搜索结果。折叠是通过每个折叠键仅选择最顶部的一个排序的文档来完成的。"],["body","\n"],["body","例如，下面的搜索按 user.id 折叠结果，并按 http.response.bytes 排序。"],["body","\n\n"],["body","GET my-index-000001/_search\n{\n  \"query\": {\n    \"match\": {\n      \"message\": \"GET /search\"\n    }\n  },\n  \"collapse\": {\n    \"field\": \"user.id\"\n  },\n  \"sort\": [\n    {\n      \"http.response.bytes\": {\n        \"order\": \"desc\"\n      }\n    }\n  ],\n  \"from\": 0              \n}\n"],["body","\n\n"],["body","响应中的命中总数表示不折叠的匹配文档数。去重后的组的总数未知。"],["body","\n"],["body","The field used for collapsing must be a single valued keyword or numeric field with doc_values activated."],["body","\n"],["body","折叠仅适用于 top hits，不会影响聚合。"],["body","\n\n"],["headingLink","expand-collapse-results"],["heading","Expand collapse results"],["body","\n"],["body","还可以使用inner_hits 选项扩展每个折叠的TopHits。"],["body","\n"],["body","GET /my-index-000001/_search\n{\n  \"query\": {\n    \"match\": {\n      \"message\": \"GET /search\"\n    }\n  },\n  \"collapse\": {\n    \"field\": \"user.id\",\n    \"inner_hits\": {\n      \"name\": \"most_recent\",\n      \"size\": 5,\n      \"sort\": [ { \"@timestamp\": \"desc\" } ]\n    },\n    \"max_concurrent_group_searches\": 4\n  },\n  \"sort\": [\n    {\n      \"http.response.bytes\": {\n        \"order\": \"desc\"\n      }\n    }\n  ]\n}\n"],["body","\n\n"],["body","\n"],["body","See inner hits for the complete list of supported options and the format of the response."],["body","\n"],["body","\n"],["body","\n"],["body","也可以为每个折叠的命中请求多个 inner_hits。当您想要获得折叠命中的多个表示时，这可能很有用。"],["body","\n"],["body","\n\n"],["body","GET /my-index-000001/_search\n{\n  \"query\": {\n    \"match\": {\n      \"message\": \"GET /search\"\n    }\n  },\n  \"collapse\": {\n    \"field\": \"user.id\",\n    \"inner_hits\": [\n      {\n        \"name\": \"largest_responses\", \n        \"size\": 3,\n        \"sort\": [\n          {\n            \"http.response.bytes\": {\n              \"order\": \"desc\"\n            }\n          }\n        ]\n      },\n      {\n        \"name\": \"most_recent\",\n        \"size\": 3,\n        \"sort\": [\n          {\n            \"@timestamp\": {\n              \"order\": \"desc\"\n            }\n          }\n        ]\n      }\n    ]\n  },\n  \"sort\": [\n    \"http.response.bytes\"\n  ]\n}\n"],["body","\n"],["body","通过为响应中返回的每个折叠命中发送每个 `inner_hit' 请求的额外查询来完成组的扩展"],["body","\n"],["body","如果您有太多的组或 inner_hit 请求，这可能会大大减慢您的搜索速度。"],["body","\n"],["body","max_concurrent_group_searches 请求参数可用于控制此阶段允许的最大并发搜索次数。默认值基于数据节点数和默认的搜索线程池大小。"],["body","\n"],["body","collapse cannot be used in conjunction with scroll or rescore."],["body","\n"],["headingLink","collapsing-with-search_after"],["heading","Collapsing with search_after"],["body","\n"],["body","仅当在同一字段上进行排序和折叠时，才支持使用 search_after，二级排序也不允许"],["body","\n"],["body","GET /my-index-000001/_search\n{\n  \"query\": {\n    \"match\": {\n      \"message\": \"GET /search\"\n    }\n  },\n  \"collapse\": {\n    \"field\": \"user.id\"\n  },\n  \"sort\": [ \"user.id\" ],\n  \"search_after\": [\"dd5ce1ad\"]\n}\n"],["body","\n"],["headingLink","second-level-of-collapsing"],["heading","Second level of collapsing"],["body","\n\n"],["body","支持二级折叠，并将其应用于 inner_hits。"],["body","\n"],["body","第二级折叠不允许 inner_hits。"],["body","\n\n"],["body","GET /my-index-000001/_search\n{\n  \"query\": {\n    \"match\": {\n      \"message\": \"GET /search\"\n    }\n  },\n  \"collapse\": {\n    \"field\": \"geo.country_name\",\n    \"inner_hits\": {\n      \"name\": \"by_location\",\n      \"collapse\": { \"field\": \"user.id\" },\n      \"size\": 3\n    }\n  }\n}\n"],["body","\n"],["body","{\n  \"hits\" : {\n    \"hits\" : [\n      {\n        \"_index\" : \"my-index-000001\",\n        \"_id\" : \"oX9uXXoB0da05OCR3adK\",\n        \"_type\" : \"_doc\",\n        \"_score\" : 0.5753642,\n        \"_source\" : {\n          \"@timestamp\" : \"2099-11-15T14:12:12\",\n          \"geo\" : {\n            \"country_name\" : \"Amsterdam\"\n          },\n          \"http\" : {\n            \"request\" : {\n              \"method\" : \"get\"\n            },\n            \"response\" : {\n              \"bytes\" : 1070000,\n              \"status_code\" : 200\n            },\n            \"version\" : \"1.1\"\n          },\n          \"message\" : \"GET /search HTTP/1.1 200 1070000\",\n          \"source\" : {\n            \"ip\" : \"127.0.0.1\"\n          },\n          \"user\" : {\n            \"id\" : \"kimchy\"\n          }\n        },\n        \"fields\" : {\n          \"geo.country_name\" : [\n            \"Amsterdam\"\n          ]\n        },\n        \"inner_hits\" : {\n          \"by_location\" : {\n            \"hits\" : {\n              \"total\" : {\n                \"value\" : 1,\n                \"relation\" : \"eq\"\n              },\n              \"max_score\" : null,\n              \"hits\" : [\n                {\n                  \"_index\" : \"my-index-000001\",\n                  \"_type\" : \"_doc\",\n                  \"_id\" : \"oX9uXXoB0da05OCR3adK\",\n                  \"_score\" : 0.5753642,\n                  \"_source\" : {\n                    \"@timestamp\" : \"2099-11-15T14:12:12\",\n                    \"geo\" : {\n                      \"country_name\" : \"Amsterdam\"\n                    },\n                    \"http\" : {\n                      \"request\" : {\n                        \"method\" : \"get\"\n                      },\n                      \"response\" : {\n                        \"bytes\" : 1070000,\n                        \"status_code\" : 200\n                      },\n                      \"version\" : \"1.1\"\n                    },\n                    \"message\" : \"GET /search HTTP/1.1 200 1070000\",\n                    \"source\" : {\n                      \"ip\" : \"127.0.0.1\"\n                    },\n                    \"user\" : {\n                      \"id\" : \"kimchy\"\n                    }\n                  },\n                  \"fields\" : {\n                    \"user.id\" : [\n                      \"kimchy\"\n                    ]\n                  }\n                }\n              ]\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/8.SearchYourData/2.FilterSearchResults.html"],["title","FilterSearchResults.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","filter-search-results"],["heading","Filter search results"],["body","\n"],["body","您可以使用两种方法来过滤搜索结果:"],["body","\n\n"],["body","带有 filter子句的 boolean query，Search requests apply boolean filters to both search hits and aggregations"],["body","\n"],["body","使用 search API的 post_filter 参数。Search requests 只在 searchHits 应用  post filters ，而不对 aggregations。你可以使用 post filter 来计算聚合，基于更加宽泛的数据集 ，然后进一步缩小结果，也可以在 post filter 之后  rescore  来提高相关性"],["body","\n\n"],["headingLink","post-filter"],["heading","Post filter"],["body","\n"],["body","post_filter 应用于 aggregations之后"],["body","\n"],["body","example"],["body","\n"],["body","PUT /shirts\n{\n  \"mappings\": {\n    \"properties\": {\n      \"brand\": { \"type\": \"keyword\"},\n      \"color\": { \"type\": \"keyword\"},\n      \"model\": { \"type\": \"keyword\"}\n    }\n  }\n}\n"],["body","\n"],["body","PUT /shirts/_doc/1?refresh\n{\n  \"brand\": \"gucci\",\n  \"color\": \"red\",\n  \"model\": \"slim\"\n}\n"],["body","\n"],["body","想象一个用户已经指定了两个过滤器:"],["body","\n"],["body","color:red and brand:gucci.\nYou only want to show them red shirts made by Gucci in the search results."],["body","\n"],["body","GET /shirts/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"filter\": [\n        { \"term\": { \"color\": \"red\"   }},\n        { \"term\": { \"brand\": \"gucci\" }}\n      ]\n    }\n  }\n}\n"],["body","\n"],["headingLink","rescore-filtered-search-results"],["heading","Rescore filtered search results"],["body","\n"],["body","......"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/KQL.html"],["title","KQL.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","kibana-query-language"],["heading","Kibana Query Language"],["body","\n"],["body","Kibana查询语言 (KQL) 是一种简单的语法，用于使用自由文本搜索或基于字段的搜索来过滤Elasticsearch数据。KQL仅用于过滤数据，没有对数据进行排序或聚合的作用。"],["body","\n"],["body","KQL能够在您键入时提示字段名称、值和运算符。"],["body","\n"],["body","提示的性能由Kibana设置控制。"],["body","\n"],["body","KQL具有与Lucene查询语法不同的功能集。KQL能够查询嵌套字段和脚本字段。"],["body","\n"],["body","KQL不支持正则表达式或用模糊项搜索。要使用旧版Lucene语法，请单击搜索字段旁边的KQL，然后关闭KQL。"],["body","\n"],["headingLink","terms-query"],["heading","Terms query"],["body","\n\n"],["body","\n"],["body","术语查询使用精确的搜索词。空格分隔每个搜索词"],["body","\n"],["body","\n"],["body","\n"],["body","并且只需要一个词就可以匹配文档。"],["body","\n"],["body","\n"],["body","\n"],["body","使用引号表示短语匹配。"],["body","\n"],["body","\n\n"],["body","要使用精确的搜索词进行查询，请输入字段名称，后跟冒号，然后输入以空格分隔的值:"],["body","\n"],["body","http.response.status_code:400 401 404\n"],["body","\n"],["body","对于文本字段，无论顺序如何，这都将匹配任何值:"],["body","\n"],["body","http.response.body.content.text:quick brown fox\n"],["body","\n"],["body","要查询确切的短语，请在值周围使用引号:"],["body","\n"],["body","http.response.body.content.text:\"quick brown fox\"\n"],["body","\n"],["body","字段名称不是KQL所必需的。如果未提供字段名称，则术语将与索引设置中的默认字段匹配。要跨字段搜索:"],["body","\n"],["body","\"quick brown fox\"\n"],["body","\n"],["headingLink","boolean-queries"],["heading","Boolean queries"],["body","\n"],["body","KQL支持or，and，not。"],["body","\n"],["body","默认情况下，and 具有比 or 更高的优先级。"],["body","\n"],["body","要覆盖默认优先级，请在括号中对运算符进行分组。这些运算符可以是大写或小写的。"],["body","\n"],["body","response:200 or extension:php\n"],["body","\n"],["body","response:(200 or 404)\n"],["body","\n"],["body","response:200 and (extension:php or extension:css)\n"],["body","\n"],["body","response:200 and extension:php or extension:css\n"],["body","\n"],["body","not response:200\n"],["body","\n"],["body","response:200 and not (extension:php or extension:css)\n"],["body","\n"],["body","tags:(success and info and security)\n"],["body","\n"],["headingLink","range-queries"],["heading","Range queries"],["body","\n"],["body","KQL supports >, >=, <, and <= on numeric and date types."],["body","\n"],["body","account_number >= 100 and items_sold <= 200\n"],["body","\n"],["headingLink","date-range-queries"],["heading","Date range queries"],["body","\n"],["body","@timestamp < \"2021-01-02T21:55:59\"\n"],["body","\n"],["body","@timestamp < \"2021-01\"\n"],["body","\n"],["body","@timestamp < \"2021\"\n"],["body","\n"],["headingLink","exist-queries"],["heading","Exist queries"],["body","\n"],["body","response:*\n"],["body","\n"],["headingLink","wildcard-queries"],["heading","Wildcard queries"],["body","\n"],["body","通配符查询可用于按术语前缀搜索或搜索多个字段。"],["body","\n"],["body","The default settings of Kibana prevent leading wildcards for performance reasons, but this can be allowed with an advanced setting."],["body","\n"],["body","To match documents where machine.os starts with win, such as \"windows 7\" and \"windows 10\":"],["body","\n"],["body","machine.os:win*\n"],["body","\n"],["body","machine.os*:windows 10\n"],["body","\n"],["body","当您具有字段的文本和关键字版本时，此语法非常方便。该查询检查术语为windows 10的machine.os和machine.os.关键字。"],["body","\n"],["headingLink","nested-field-queries"],["heading","Nested field queries"],["body","\n"],["body","查询嵌套字段的主要考虑因素是如何将嵌套查询的部分与单个嵌套文档进行匹配。您可以:"],["body","\n\n"],["body","仅将查询的部分与单个嵌套文档匹配。这是大多数用户在嵌套字段上查询时想要的。"],["body","\n"],["body","将查询的部分与不同的嵌套文档进行匹配。这就是常规对象字段的工作方式。此查询通常不如匹配单个文档有用。"],["body","\n\n"],["body","在下面的文档中，items是一个嵌套字段。嵌套字段中的每个文档都包含名称，股票和类别。"],["body","\n"],["body","{\n  \"grocery_name\": \"Elastic Eats\",\n  \"items\": [\n    {\n      \"name\": \"banana\",\n      \"stock\": \"12\",\n      \"category\": \"fruit\"\n    },\n    {\n      \"name\": \"peach\",\n      \"stock\": \"10\",\n      \"category\": \"fruit\"\n    },\n    {\n      \"name\": \"carrot\",\n      \"stock\": \"9\",\n      \"category\": \"vegetable\"\n    },\n    {\n      \"name\": \"broccoli\",\n      \"stock\": \"5\",\n      \"category\": \"vegetable\"\n    }\n  ]\n}\n"],["body","\n"],["headingLink","match-a-single-document"],["heading","Match a single document"],["body","\n"],["body","To match stores that have more than 10 bananas in stock:"],["body","\n"],["body","items:{ name:banana and stock > 10 }\n"],["body","\n"],["body","items是嵌套路径。花括号 (嵌套组) 内的所有内容都必须与单个嵌套文档匹配。"],["body","\n"],["body","以下查询不返回任何匹配项，因为没有单个嵌套文档具有库存为9的香蕉。"],["body","\n"],["body","items:{ name:banana and stock:9 }\n"],["body","\n"],["headingLink","match-different-documents"],["heading","Match different documents"],["body","\n"],["body","以下子查询位于单独的嵌套组中，可以匹配不同的嵌套文档:"],["body","\n"],["body","items:{ name:banana } and items:{ stock:9 }\n"],["body","\n"],["body","名称: banana匹配数组中的第一个文档，stock:9匹配数组中的第三个文档。"],["body","\n"],["headingLink","match-single-and-different-documents"],["heading","Match single and different documents"],["body","\n"],["body","items:{ name:banana and stock > 10 } and items:{ category:vegetable }\n"],["body","\n"],["body","The first nested group (name:banana and stock > 10) must match a single document, but the category:vegetables subquery can match a different nested document because it is in a separate group."],["body","\n"],["headingLink","nested-fields-inside-other-nested-fields"],["heading","Nested fields inside other nested fields"],["body","\n"],["body","KQL支持其他嵌套字段内部的嵌套字段-您必须指定完整路径。在本文档中，level1和level2是嵌套字段:"],["body","\n"],["body","{\n  \"level1\": [\n    {\n      \"level2\": [\n        {\n          \"prop1\": \"foo\",\n          \"prop2\": \"bar\"\n        },\n        {\n          \"prop1\": \"baz\",\n          \"prop2\": \"qux\"\n        }\n      ]\n    }\n  ]\n}\n"],["body","\n"],["body","level1.level2:{ prop1:foo and prop2:bar }\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/9.html"],["title","语义化检索 - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/BackUpACluster.html"],["title","BackUpACluster.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","back-up-a-cluster"],["heading","Back up a cluster"],["body","\n\n"],["body","\n"],["body","备份群集的唯一可靠且受支持的方法是拍摄快照。您不能通过复制Elasticsearch集群节点的数据目录来备份它。没有支持的方法可以从文件系统级备份中还原任何数据。"],["body","\n"],["body","\n"],["body","\n"],["body","如果您尝试从这样的备份中恢复群集，它可能会因损坏或丢失文件或其他数据不一致的报告而失败，或者它似乎已经成功地无声地丢失了一些数据。"],["body","\n"],["body","\n\n"],["body","To have a complete backup for your cluster:"],["body","\n\n"],["body","Back up the data"],["body","\n"],["body","Back up the cluster configuration"],["body","\n"],["body","Back up the security configuration"],["body","\n\n"],["body","To restore your cluster from a backup:"],["body","\n\n"],["body","Restore the data"],["body","\n"],["body","Restore the security configuration"],["body","\n\n"],["headingLink","back-up-a-clusters-data"],["heading","Back up a cluster’s data"],["body","\n"],["headingLink","back-up-the-cluster-configuration"],["heading","Back up the cluster configuration"],["body","\n"],["headingLink","back-up-the-security-configuration"],["heading","Back up the security configuration"],["body","\n"],["headingLink","restore-the-security-configuration"],["heading","Restore the security configuration"],["body","\n"],["headingLink","restore-the-data"],["heading","Restore the data"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/集群内原理.html"],["title","集群内原理.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","集群内的原理"],["heading","集群内的原理"],["body","\n"],["headingLink","主旨"],["heading","主旨"],["body","\n"],["body","ElasticSearch 的主旨是随时可用和按需扩容。 而扩容可以通过购买性能更强大（ 垂直扩容 ，或 纵向扩容 ） 或者数量更多的服务器（ 水平扩容 ，或 横向扩容 ）来实现。"],["body","\n"],["body","垂直扩容与水平扩容"],["body","\n"],["body","虽然 Elasticsearch 可以获益于更强大的硬件设备，但是垂直扩容是有极限的。 真正的扩容能力是来自于水平扩容—为集群添加更多的节点，并且将负载压力和稳定性分散到这些节点中。"],["body","\n"],["headingLink","集群"],["heading","集群"],["body","\n"],["body","集群组成"],["body","\n\n"],["body","\n"],["body","一个运行中的 Elasticsearch 实例称为一个节点"],["body","\n"],["body","\n"],["body","\n"],["body","而集群是由一个或者多个拥有相同 cluster.name 配置的节点组成， 它们共同承担数据和负载的压力"],["body","\n"],["body","\n"],["body","\n"],["body","当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。"],["body","\n"],["body","\n\n"],["body","主节点"],["body","\n\n"],["body","当一个节点被选举成为 主 节点时， 它将负责管理集群范围内的所有变更，例如增加、删除索引或者增加、删除节点等"],["body","\n"],["body","而主节点并不需要涉及到文档级别的变更和搜索等操作："],["body","\n\n"],["body","所以当集群只拥有一个主节点的情况下，即使流量的增加它也不会成为瓶颈"],["body","\n\n"],["body","任何节点都可以成为主节点"],["body","\n\n"],["body","集群中的请求转发"],["body","\n\n"],["body","\n"],["body","作为用户，我们可以将请求发送到 集群中的任何节点 ，包括主节点"],["body","\n"],["body","\n"],["body","\n"],["body","每个节点都知道任意文档所处的位置，并且能够将我们的请求直接转发到存储我们所需文档的节点"],["body","\n"],["body","\n"],["body","\n"],["body","无论我们将请求发送到哪个节点，它都能负责从各个包含我们所需文档的节点收集回数据，并将最终结果返回給客户端"],["body","\n"],["body","\n\n"],["headingLink","集群健康"],["heading","集群健康"],["body","\n"],["body","Elasticsearch 的集群监控信息中包含了许多的统计数据，其中最为重要的一项就是 集群健康"],["body","\n"],["body","它在 status 字段中展示为 green 、 yellow 或者 red 。"],["body","\n"],["body","GET /_cluster/health\n"],["body","\n"],["body","在一个不包含任何索引的空集群中，它将会有一个类似于如下所示的返回内容："],["body","\n"],["body","status 字段指示着当前集群在总体上是否工作正常。它的三种颜色含义如下："],["body","\n\n"],["body","\n"],["body","green"],["body","\n"],["body","所有的主分片和副本分片都正常运行。"],["body","\n"],["body","\n"],["body","\n"],["body","yellow"],["body","\n"],["body","所有的主分片都正常运行，但不是所有的副本分片都正常运行。"],["body","\n"],["body","\n"],["body","\n"],["body","red"],["body","\n"],["body","有主分片没能正常运行。"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/IngestPipeline.html"],["title","IngestPipeline - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/dataStream.html"],["title","dataStream.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","疑问"],["heading","疑问"],["body","\n\n"],["body","没有 data stream 的时候，如何管理时序型数据？"],["body","\n"],["body","什么是 data stream？"],["body","\n"],["body","data stream 的特点有哪些？"],["body","\n"],["body","为什么要有 data stream？"],["body","\n"],["body","data stream 能做什么？"],["body","\n"],["body","data stream 应用场景？"],["body","\n"],["body","data stream 和 索引 index 的关系？"],["body","\n"],["body","data stream 和 索引生命周期管理 ILM 的关系？"],["body","\n"],["body","data stream 实操有哪些注意事项？"],["body","\n\n"],["headingLink","没有-data-stream-的时候如何管理时序型数据"],["heading","没有 data stream 的时候，如何管理时序型数据？"],["body","\n"],["headingLink","基于-rollover-滚动索引机制管理时序数据"],["heading","基于 rollover 滚动索引机制管理时序数据"],["body","\n"],["body","PUT mylogs-2021.07.24-1\n{\n  \"aliases\": {\n    \"mylogs_write\": {}\n  }\n}\n\nGET mylogs-2021.07.24-1\n\nPUT mylogs_write/_doc/1\n{\n  \"message\": \"a dummy log\"\n}\n\nPOST mylogs_write/_bulk\n{\"index\":{\"_id\":4}}\n{\"title\":\"test 04\"}\n{\"index\":{\"_id\":2}}\n{\"title\":\"test 02\"}\n{\"index\":{\"_id\":3}}\n{\"title\":\"test 03\"}\n\n\nPOST  mylogs_write/_rollover\n{\n  \"conditions\": {\n    \"max_docs\":   3\n  }\n}\n\n\n再次导入批量数据\n\nPOST mylogs_write/_doc/14\n{\"title\":\"test 14\"}\n\nPOST mylogs_write/_bulk\n{\"index\":{\"_id\":5}}\n{\"title\":\"test 05\"}\n{\"index\":{\"_id\":6}}\n{\"title\":\"test 06\"}\n{\"index\":{\"_id\":7}}\n{\"title\":\"test 07\"}\n{\"index\":{\"_id\":8}}\n{\"title\":\"test 08\"}\n{\"index\":{\"_id\":9}}\n{\"title\":\"test 09\"}\n{\"index\":{\"_id\":10}}\n{\"title\":\"test 10\"}\n"],["body","\n"],["body","早期生产环境使用 rollover，有个比较麻烦的地方就在于——需要自己结合滚动的三条件，在给定的时间点（比如凌晨0:00）定时脚本执行一下 rollover，滚动才能生效。"],["body","\n"],["body","看似脚本处理很简单，实际会有这样那样的问题，用过你就知道有多苦。"],["body","\n\n"],["body","rollover 优点：实现了最原始的索引滚动。"],["body","\n"],["body","rollover 缺点：需要手动或者脚本定时 rollover 非常麻烦。"],["body","\n\n"],["headingLink","ilm-索引生命周期管理时序数据"],["heading","ILM 索引生命周期管理时序数据"],["body","\n"],["body","ILM 是模板、别名、生命周期 policy 的综合体。"],["body","\n\n"],["body","ILM 优点：一次配置，索引生命周期全自动化。"],["body","\n"],["body","ILM 适用场景：更适合和冷热集群架构结合的业务场景。"],["body","\n"],["body","ILM 缺点：ILM是普适的概念，强调大而全，不是专门针对时序数据特点的方案，且需要为 ilm 配置 index.lifecycle.rollover_alias 设置（对时序数据场景，这非常麻烦）。"],["body","\n\n"],["body","官方强调：别名在 Elasticsearch 中的实现方式存在一些不足（官方没有细说哪些不足。我实战环境发现：一个别名对应多个索引，一个索引对应多个别名，索引滚动关联别名也可能滚动，开发者可能很容易出错和混淆），使用起来很混乱。"],["body","\n"],["body","相比于别名具有广泛的用途，而数据流将是针对时序数据的解决方案。"],["body","\n"],["headingLink","什么是-data-stream"],["heading","什么是 data stream？"],["body","\n"],["body","存储时序数据的多个索引的抽象集合，简称为：数据流（data stream）"],["body","\n"],["body","数据流可以跨多个后备索引存储仅追加（append-only，下文有详细解释）的时间序列数据，同时对外提供一个同一访问入口。"],["body","\n"],["body","所以，它是索引、模板、rollover、ilm 基于时序性数据的综合产物。"],["body","\n"],["headingLink","data-stream-的特点有哪些"],["heading","data stream 的特点有哪些？"],["body","\n"],["headingLink","关联后备支撑索引backing-indices"],["heading","关联后备支撑索引（backing indices）"],["body","\n"],["headingLink","timestamp-字段不可缺"],["heading","@timestamp 字段不可缺"],["body","\n\n"],["body","每个写入到 dataSteam 的文档必须包含 @timestamp 字段。"],["body","\n"],["body","@timestamp 字段必须是：date 类型（若不指定，默认：date 类型）或者 date_nanos 类型。"],["body","\n\n"],["headingLink","data-stream-后备索引规范"],["heading","data stream 后备索引规范"],["body","\n"],["body",".ds-<data-stream>-<yyyy.MM.dd>-<generation>"],["body","\n"],["body","举例索引真实名称：data-stream-2021.07.25-000001。"],["body","\n\n"],["body",".ds：前缀开头不可少。"],["body","\n"],["body","data-stream： 自定义的数据流的名称。"],["body","\n"],["body","yyyy.MM.dd：日期格式"],["body","\n"],["body","generation：rollover 累积值：—— 默认从：000001 开始。"],["body","\n\n"],["headingLink","append-only-仅追加"],["heading","Append-only 仅追加"],["body","\n"],["body","仅追加：指只支持 op_type=create 的索引请求，我理解的是仅支持向后追加（区别于对历史数据的删除、更新操作）。"],["body","\n"],["body","数据流只支持：update_by_query 和 delete_by_query 实现批量操作，单条文档的更新和删除操作只能通过指定后备索引的方式实现。"],["body","\n"],["body","对于频繁更新或者删除文档的业务场景，用 data stream 不合适，而相反的，使用：模板+别名+ILM更为合适。"],["body","\n"],["headingLink","为什么要有-data-stream"],["heading","为什么要有 data stream？"],["body","\n"],["body","原有实现由于别名的缺陷实现不了时序数据的管理或实现起来会繁琐、麻烦，data stream 是更为纯粹的存储仅追加时序数据的方式。"],["body","\n"],["headingLink","data-stream-能做什么"],["heading","data stream 能做什么？"],["body","\n\n"],["body","data stream 支持直接的写入、查询请求。"],["body","\n"],["body","data stream 会自动将客户端请求路由至关联索引，以用来存储流式数据。"],["body","\n"],["body","可以使用索引生命周期管理 ILM 自动管理这些关联索引。"],["body","\n\n"],["headingLink","data-stream-的适用场景"],["heading","data stream 的适用场景"],["body","\n"],["body","日志（logs）、事件（events）、指标（metrics）和其他持续生成的数据。"],["body","\n"],["headingLink","data-stream-和-模板的关系"],["heading","data stream 和 模板的关系？"],["body","\n"],["body","相同的索引模板可以用来支撑多个 data streams。可以类比为：1：N 关系。"],["body","\n"],["headingLink","data-stream--和-ilm-的关系"],["heading","data stream  和 ilm 的关系？"],["body","\n"],["body","ILM 在 data stream 中起到索引生命周期管理的作用。"],["body","\n"],["body","data stream 操作时序数据优势体现在：不再需要为 ilm 配置 index.lifecycle.rollover_alias。"],["body","\n"],["headingLink","data-stream-实操指南"],["heading","data stream 实操指南"],["body","\n"],["headingLink","创建索引生命周期-policy"],["heading","创建索引生命周期 policy。"],["body","\n"],["body","\nPUT _ilm/policy/my-lifecycle-policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"actions\": {\n          \"rollover\": {\n            \"max_primary_shard_size\": \"50gb\",\n            \"max_docs\": 10\n          }\n        }\n      },\n      \"warm\": {\n        \"min_age\": \"1m\",\n        \"actions\": {\n          \"shrink\": {\n            \"number_of_shards\": 1\n          },\n          \"forcemerge\": {\n            \"max_num_segments\": 1\n          },\n          \"allocate\": {\n            \"include\": {\n              \"nodeType\":\"warm\"\n            }\n          }\n        }\n      },\n      \"cold\": {\n        \"min_age\": \"2m\",\n        \"actions\": {\n          \"allocate\": {\n            \"include\": {\n              \"nodeType\":\"cold\"\n            }\n          }\n        }\n      },\n      \"delete\": {\n        \"min_age\": \"4m\",\n        \"actions\": {\n          \"delete\": {}\n        }\n      }\n    }\n  }\n}\n\n"],["body","\n"],["headingLink","创建模板"],["heading","创建模板"],["body","\n"],["body","PUT _component_template/my-mappings\n{\n  \"template\": {\n    \"mappings\": {\n      \"properties\": {\n        \"@timestamp\": {\n          \"type\": \"date\",\n          \"format\": \"date_optional_time||epoch_millis\"\n        },\n        \"message\": {\n          \"type\": \"wildcard\"\n        }\n      }\n    }\n  }\n}\n\n# Creates a component template for index settings\nPUT _component_template/my-settings\n{\n  \"template\": {\n    \"settings\": {\n      \"index.lifecycle.name\": \"my-lifecycle-policy\"\n    }\n  }\n}\n\n\nPUT _index_template/my-index-template\n{\n  \"index_patterns\": [\"my-data-stream*\"],\n  \"data_stream\": { },\n  \"composed_of\": [ \"my-mappings\", \"my-settings\" ],\n  \"priority\": 500\n}\n"],["body","\n"],["headingLink","创建-data-stream"],["heading","创建 data stream"],["body","\n\n"],["body","方式一：直接创建数据流 my-data-stream。"],["body","\n\n"],["body","PUT _data_stream/my-data-stream"],["body","\n\n"],["body","方式二：直接批量或者逐个导入数据（会间接生成 data stream 的创建）。"],["body","\n\n"],["body","PUT my-data-stream/_bulk\n{ \"create\":{ } }\n{ \"@timestamp\": \"2099-05-06T16:21:15.000Z\", \"message\": \"192.0.2.42 - - [06/May/2099:16:21:15 +0000] \\\"GET /images/bg.jpg HTTP/1.0\\\" 200 24736\" }\n{ \"create\":{ } }\n{ \"@timestamp\": \"2099-05-06T16:25:42.000Z\", \"message\": \"192.0.2.255 - - [06/May/2099:16:25:42 +0000] \\\"GET /favicon.ico HTTP/1.0\\\" 200 3638\" }\n\nPOST my-data-stream/_doc\n{\n  \"@timestamp\": \"2099-05-06T16:21:15.000Z\",\n  \"message\": \"192.0.2.42 - - [06/May/2099:16:21:15 +0000] \\\"GET /images/bg.jpg HTTP/1.0\\\" 200 24736\"\n}\n"],["body","\n\n"],["body","第一：批量 bulk 操作，必须使用：create 指令，而非 index（使用 index 不会报错， 会把流当做索引处理了）。"],["body","\n"],["body","第二：文档必须包含：@timestamp  时间戳字段。"],["body","\n\n"],["body","如果不包含 @timestamp 会报错如下："],["body","\n"],["body","\"reason\" : \"data stream timestamp field [@timestamp] is missing\""],["body","\n"],["headingLink","data-stream-删"],["heading","data stream 删"],["body","\n"],["body","DELETE _data_stream/my-data-stream\n"],["body","\n"],["headingLink","单条删除文档"],["heading","单条删除文档"],["body","\n"],["body","DELETE data-stream-2021.07.25-000001/_doc/1 \n"],["body","\n"],["headingLink","批量删除文档"],["heading","批量删除文档"],["body","\n"],["body","POST /my-data-stream/_delete_by_query\n{\n  \"query\": {\n    \"match\": {\n      \"user.id\": \"vlb44hny\"\n    }\n  }\n}\n"],["body","\n"],["headingLink","data-stream-改"],["heading","data stream 改"],["body","\n"],["body","# 插入一条数据\nPOST my-data-stream/_bulk\n{\"create\":{\"_id\":1}}\n{\"@timestamp\":\"2099-05-06T16:21:15.000Z\",\"message\":\"192.0.2.42 - - [06/May/2099:16:21:15 +0000] \\\"GET /images/bg.jpg HTTP/1.0\\\" 200 24736\"}\n\n# 获取数据流关联索引\nGET /_data_stream/my-data-stream\n\n# 执行更新\nPUT .ds-my-data-stream-2021.07.25-000001/_doc/1?if_seq_no=1&if_primary_term=1\n{\n  \"@timestamp\": \"2099-03-08T11:06:07.000Z\",\n  \"user\": {\n    \"id\": \"8a4f500d\"\n  },\n  \"message\": \"Login successful\"\n}\n\n# 查看验证是否已经更新（已经验证，可以更新）\nGET .ds-my-data-stream-2021.07.25-000001/_doc/1\n"],["body","\n"],["headingLink","批量更新"],["heading","批量更新"],["body","\n"],["body","POST /my-data-stream/_update_by_query\n{\n  \"query\": {\n    \"match\": {\n      \"user.id\": \"l7gk7f82\"\n    }\n  },\n  \"script\": {\n    \"source\": \"ctx._source.user.id = params.new_id\",\n    \"params\": {\n      \"new_id\": \"XgdX0NoX\"\n    }\n  }\n}\n"],["body","\n"],["headingLink","data-stream-查"],["heading","data stream 查"],["body","\n"],["body","GET _data_stream/my-data-stream"],["body","\n"],["body","{\n  \"data_streams\" : [\n    {\n      \"name\" : \"my-data-stream\",\n      \"timestamp_field\" : {\n        \"name\" : \"@timestamp\"\n      },\n      \"indices\" : [\n        {\n          \"index_name\" : \".ds-my-data-stream-2021.07.25-000001\",\n          \"index_uuid\" : \"Akg3-bWgStiKG_39Tk5PRw\"\n        }\n      ],\n      \"generation\" : 1,\n      \"status\" : \"GREEN\",\n      \"template\" : \"my-index-template\",\n      \"ilm_policy\" : \"my-lifecycle-policy\",\n      \"hidden\" : false\n    }\n  ]\n}\n"],["body","\n"],["headingLink","reindex-操作"],["heading","reindex 操作"],["body","\n"],["body","POST /_reindex\n{\n  \"source\": {\n    \"index\": \"archive\"\n  },\n  \"dest\": {\n    \"index\": \"my-data-stream\",\n    \"op_type\": \"create\"\n  }\n}\n"],["body","\n"],["headingLink","滚动操作"],["heading","滚动操作"],["body","\n"],["body","POST my-data-stream/_rollover \n"],["body","\n"],["headingLink","查看-data-stream-基础信息"],["heading","查看 data stream 基础信息"],["body","\n"],["body","GET /_data_stream/my-data-stream \n"],["body","\n"],["headingLink","data-streams"],["heading","Data streams"],["body","\n\n"],["body","\n"],["body","数据流使您可以跨多个索引存储仅追加的时间序列数据，同时为您提供用于请求的单个命名资源。数据流非常适合日志、事件、度量和其他连续生成的数据。"],["body","\n"],["body","\n"],["body","\n"],["body","您可以直接向数据流提交索引和搜索请求"],["body","\n"],["body","\n"],["body","\n"],["body","流自动将请求路由到存储流数据的支持索引。"],["body","\n"],["body","\n"],["body","\n"],["body","您可以使用索引生命周期管理 (ILM) 来自动化这些支持索引的管理。"],["body","\n"],["body","\n"],["body","\n"],["body","例如，您可以使用ILM自动将较旧的备份索引移动到较便宜的硬件并删除不需要的索引。随着数据的增长，ILM可以帮助您降低成本和开销。"],["body","\n"],["body","\n\n"],["headingLink","backing-indices"],["heading","Backing indices"],["body","\n\n"],["body","数据流由一个或多个隐藏的自动生成的后备索引组成。"],["body","\n"],["body","数据流需要匹配一个索引模板。模板包含用于配置 后备索引的mappings and settings 。"],["body","\n"],["body","索引到数据流的每个文档都必须包含一个 @ timestamp字段，映射为日期或date_nanos字段类型。如果索引模板没有为 @ timestamp字段指定映射，Elasticsearch将 @ timestamp映射为具有默认选项的日期字段。"],["body","\n"],["body","相同的索引模板可以用于多个数据流。不能删除数据流使用的索引模板。"],["body","\n\n"],["headingLink","read-requests"],["heading","Read requests"],["body","\n"],["body","当您向数据流提交读取请求时，该流将请求路由到其所有的后备索引。"],["body","\n"],["headingLink","write-index"],["heading","Write index"],["body","\n"],["body","最近创建的备份索引是数据流的写入索引。流仅将新文档添加到此索引。"],["body","\n"],["body","即使直接向索引发送请求，也无法将新文档添加到其他支持索引。"],["body","\n"],["body","您也不能对可能阻碍索引的写索引执行操作，例如:·"],["body","\n\n"],["body","Clone"],["body","\n"],["body","Delete"],["body","\n"],["body","Freeze"],["body","\n"],["body","Shrink"],["body","\n"],["body","Split"],["body","\n\n"],["headingLink","rollover"],["heading","Rollover"],["body","\n\n"],["body","\n"],["body","A rollover creates a new backing index that becomes the stream’s new write index."],["body","\n"],["body","\n"],["body","\n"],["body","We recommend using ILM to automatically roll over data streams when the write index reaches a specified age or size."],["body","\n"],["body","\n\n"],["body","If needed, you can also manually roll over a data stream."],["body","\n"],["headingLink","generation"],["heading","Generation"],["body","\n"],["body","每个数据流跟踪其生成: 一个六位数、零填充的整数，作为流的滚动的累积计数，从000001开始。"],["body","\n"],["body","创建支持索引时，索引将使用以下约定命名:"],["body","\n"],["body",".ds-<data-stream>-<yyyy.MM.dd>-<generation>\n"],["body","\n"],["body","<yyyy.MM.dd> 是支持索引的创建日期。具有较高的代的支持索引包含较新的数据。例如， web-server-logs 数据流具有生成34。2099年3月7日创建的流的最新后备索引被命名。ds-web-server-logs-2099.03.07-000034。"],["body","\n"],["body","某些操作 (例如收缩或还原) 可以更改支持索引的名称。这些名称更改不会从其数据流中删除后备索引。"],["body","\n"],["headingLink","append-only"],["heading","Append-only"],["body","\n"],["body","Data streams are designed for use cases where existing data is rarely, if ever, updated. You cannot send update or deletion requests for existing documents directly to a data stream. Instead, use the update by query and delete by query APIs."],["body","\n"],["body","If needed, you can update or delete documents by submitting requests directly to the document’s backing index."],["body","\n"],["body","注意"],["body","\n"],["body","If you frequently update or delete existing documents, use an index alias and index template instead of a data stream. You can still use ILM to manage indices for the alias."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/高可用专题.html"],["title","高可用专题 - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/索引和分片.html"],["title","索引和分片.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","概念定义"],["heading","概念定义"],["body","\n\n"],["body","集群(cluster):由一个或多个节点组成, 并通过集群名称与其他集群进行区分"],["body","\n"],["body","节点(node):单个ElasticSearch实例. 通常一个节点运行在一个隔离的容器或虚拟机中"],["body","\n"],["body","索引(index):数据的 逻辑命名空间，分片的逻辑组合"],["body","\n"],["body","分片(shard):存储数据的最小物理单元"],["body","\n"],["body","副本(replica):主分片的一个副本"],["body","\n\n"],["headingLink","副本"],["heading","副本"],["body","\n"],["body","本文中不会对ElasticSearch的副本做详细阐述. 如果想单独了解可参考这篇文章."],["body","\n"],["body","副本对搜索性能非常重要, 同时用户也可在任何时候添加或删除副本. 正如另篇文章所述, 额外的副本能给你带来更大的容量, 更高的呑吐能力及更强的故障恢复能力."],["body","\n"],["headingLink","分片"],["heading","分片"],["body","\n"],["body","一个分片 shard 就是 es中的最小工作单元"],["body","\n"],["body","它只是保存了索引中的所有数据的一部分"],["body","\n"],["body","每个分片就是一个Lucene实例，并且它本身就是一个完整的搜索引擎"],["body","\n"],["headingLink","分片是es在进群中分发数据的关键"],["heading","分片是ES在进群中分发数据的关键"],["body","\n\n"],["body","\n"],["body","可以把分片想想成数据的容器。文档存储在分片中"],["body","\n"],["body","\n"],["body","\n"],["body","然后分片分配到集群中的节点上。当集群扩容或缩小，ES将会自动在节点间迁移分片，以使集群保持平衡"],["body","\n"],["body","\n"],["body","\n"],["body","分片可以是主分片，或者是副本分片"],["body","\n"],["body","\n"],["body","\n"],["body","ES默认为一个索引创建5个主分片, 并分别为其创建一个副本分片. 也就是说每个索引都由5个主分片成本, 而每个主分片都相应的有一个copy.  如果磁盘空间不足 15%，则不分配 replica shard。磁盘空间不足 5%，则不再分配任何的 primary shard。"],["body","\n"],["body","\n"],["body","\n"],["body","索引的每个文档属于一个单独的主分片，所以主分片的数量决定了索引最多能存储多少数据。"],["body","\n"],["body","\n"],["body","\n"],["body","复制分片只是主分片的一个副本，它可以防止硬件故障导致的数据丢失，同时可以提供请求，比如搜索或者从别的shard取回文档。"],["body","\n"],["body","\n\n"],["headingLink","主分片数无法修改"],["heading","主分片数无法修改"],["body","\n\n"],["body","当索引创建完成的时候，主分片的数量就固定了，但是复制分片的数量可以随时调整"],["body","\n"],["body","在集群运行中你无法调整分片设置. 既便以后你发现需要调整分片数量, 你也只能新建创建并对数据进行重新索引(reindex)(虽然reindex会比较耗时, 但至少能保证你不会停机)."],["body","\n"],["body","主分片的配置与硬盘分区很类似, 在对一块空的硬盘空间进行分区时, 会要求用户先进行数据备份, 然后配置新的分区, 最后把数据写到新的分区上."],["body","\n\n"],["headingLink","过度分配"],["heading","过度分配"],["body","\n\n"],["body","稍有富余是好的, 但过度分配分片却是大错特错. 具体定义多少分片很难有定论, 取决于用户的数据量和使用方式"],["body","\n"],["body","每个分片都是有额外的成本的:\n\n"],["body","每个分片本质上就是一个Lucene索引, 因此会消耗相应的文件句柄, 内存和CPU资源"],["body","\n"],["body","每个搜索请求会调度到索引的每个分片中. 如果分片分散在不同的节点倒是问题不太. 但当分片开始竞争相同的硬件资源时, 性能便会逐步下降"],["body","\n"],["body","ES使用词频统计来计算相关性. 当然这些统计也会分配到各个分片上. 如果在大量分片上只维护了很少的数据, 则将导致最终的文档相关性较差"],["body","\n"],["body","尽量保证同类数据 分布到 相同的分片"],["body","\n\n"],["body","\n"],["body","如果你真的担心数据的快速增长, 我们建议你多关心这条限制 ElasticSearch推荐的最大JVM堆空间是30~32G,\n\n"],["body","所以把你的分片最大容量限制为30GB, 然后再对分片数量做合理估算. 例如, 你认为你的数据能达到200GB, 我们推荐你最多分配7到8个分片."],["body","\n\n"],["body","\n"],["body","总之, 不要现在就为你可能在三年后才能达到的10TB数据做过多分配. 如果真到那一天, 你也会很早感知到性能变化的."],["body","\n"],["body","对大数据集, 我们非常鼓励你为索引多分配些分片--当然也要在合理范围内. 上面讲到的每个分片最好不超过30GB的原则依然使用"],["body","\n\n"],["body","在开始阶段, 一个好的方案是根据你的节点数量按照1.5~3倍的原则来创建分片"],["body","\n"],["body","例如,如果你有3个节点, 则推荐你创建的分片数最多不超过9(3x3)个"],["body","\n"],["body","随着数据量的增加,如果你通过[集群状态API](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-stats.html?q=cluster stat)发现了问题,或者遭遇了性能退化,则只需要增加额外的节点即可. ES会自动帮你完成分片在不同节点上的分布平衡."],["body","\n"],["body","再强调一次, 虽然这里我们暂未涉及副本节点的介绍, 但上面的指导原则依然使用: 是否有必要在每个节点上只分配一个索引的分片."],["body","\n"],["body","另外, 如果给每个分片分配1个副本, 你所需的节点数将加倍. 如果需要为每个分片分配2个副本, 则需要3倍的节点数. 更多详情可以参考基于副本的集群."],["body","\n"],["headingLink","logstash"],["heading","Logstash"],["body","\n"],["body","日志场景就是 基于日期的索引需求, 并且对索引数据的搜索场景非常少."],["body","\n"],["body","也许这些索引量将达到成百上千, 但每个索引的数据量只有1GB甚至更小."],["body","\n"],["body","对于这种类似场景, 我建议你只需要为索引分配1个分片."],["body","\n"],["body","如果使用ES的默认配置(5个分片, 并且使用Logstash按天生成索引, 那么6个月下来, 你拥有的分片数将达到890个"],["body","\n"],["body","再多的话, 你的集群将难以工作--除非你提供了更多(例如15个或更多)的节点."],["body","\n"],["body","想一下, 大部分的Logstash用户并不会频繁的进行搜索, 甚至每分钟都不会有一次查询. 所以这种场景, 推荐更为经济使用的设置. 在这种场景下, 搜索性能并不是第一要素, 所以并不需要很多副本. 维护单个副本用于数据冗余已经足够. 不过数据被不断载入到内存的比例相应也会变高."],["body","\n"],["body","如果你的索引只需要一个分片, 那么使用Logstash的配置可以在3节点的集群中维持运行6个月. 当然你至少需要使用4GB的内存, 不过建议使用8GB, 因为在多数据云平台中使用8GB内存会有明显的网速以及更少的资源共享."],["body","\n"],["body","再次声明, 数据分片也是要有相应资源消耗,并且需要持续投入"],["body","\n"],["body","当索引拥有较多分片时, 为了组装查询结果, ES必须单独查询每个分片(当然并行的方式)并对结果进行合并. 所以高性能IO设备(SSDs)和多核处理器无疑对分片性能会有巨大帮助. 尽管如此, 你还是要多关心数据本身的大小,更新频率以及未来的状态. 在分片分配上并没有绝对的答案, 只希望你能从本文的讨论中受益."],["body","\n"],["body","一个分片只能存放 Integer.MAX_VALUE - 128 = 2,147,483,519 个 docs"],["body","\n"],["body","参考链接"],["body","\n"],["headingLink","分片设计推荐"],["heading","分片设计推荐"],["body","\n\n"],["body","每一个分片数据文件小于30GB"],["body","\n"],["body","每一个索引中的一个分片对应一个节点"],["body","\n"],["body","节点数大于等于分片数"],["body","\n\n"],["headingLink","分片数量计算"],["heading","分片数量计算"],["body","\n"],["body","假如需要300G文件大小"],["body","\n\n"],["body","\n"],["body","至少需要 10个分片，每个分片位于独立的节点 则至少需要10个节点"],["body","\n"],["body","SN(分片数) = IS(索引大小) / 30"],["body","\n"],["body","\n"],["body","\n"],["body","NN(节点数) = SN(分片数) + MNN(主节点数[无数据]) + NNN(负载节点数)"],["body","\n"],["body","\n\n"],["headingLink","分片查询"],["heading","分片查询"],["body","\n"],["headingLink","randomizeacross-shards"],["heading","randomizeacross shards"],["body","\n"],["body","随机选择分片查询数据，es的默认方式"],["body","\n"],["headingLink","_local"],["heading","_local"],["body","\n"],["body","优先在本地节点上的分片查询数据然后再去其他节点上的分片查询，本地节点没有IO问题但有可能造成负载不均问题。数据量是完整的。"],["body","\n"],["headingLink","_primary"],["heading","_primary"],["body","\n"],["body","只在主分片中查询不去副本查，一般数据完整。"],["body","\n"],["headingLink","_primary_first"],["heading","_primary_first"],["body","\n"],["body","优先在主分片中查，如果主分片挂了则去副本查，一般数据完整。"],["body","\n"],["headingLink","_only_node"],["heading","_only_node"],["body","\n"],["body","只在指定id的节点中的分片中查询，数据可能不完整。"],["body","\n"],["headingLink","_prefer_node"],["heading","_prefer_node"],["body","\n"],["body","优先在指定你给节点中查询，一般数据完整。"],["body","\n"],["headingLink","_shards"],["heading","_shards"],["body","\n"],["body","在指定分片中查询，数据可能不完整。"],["body","\n"],["headingLink","_only_nodes"],["heading","_only_nodes"],["body","\n"],["body","可以自定义去指定的多个节点查询，es不提供此方式需要改源码。"],["body","\n"],["body"," /** \n         * 指定分片 查询 \n         */  \n        @Test  \n        public void testPreference()  \n        {  \n            SearchResponse searchResponse = transportClient.prepareSearch(index)  \n                    .setTypes(\"add\")  \n                    //.setPreference(\"_local\")  \n                    //.setPreference(\"_primary\")  \n                    //.setPreference(\"_primary_first\")  \n                    //.setPreference(\"_only_node:ZYYWXGZCSkSL7QD0bDVxYA\")  \n                    //.setPreference(\"_prefer_node:ZYYWXGZCSkSL7QD0bDVxYA\")  \n                    .setPreference(\"_shards:0,1,2\")  \n                    .setQuery(QueryBuilders.matchAllQuery()).setExplain(true).get();  \n\n            SearchHits hits = searchResponse.getHits();  \n            System.out.println(hits.getTotalHits());  \n            SearchHit[] hits2 = hits.getHits();  \n            for(SearchHit h : hits2)  \n            {  \n                System.out.println(h.getSourceAsString());  \n            }  \n        }  \n"],["body","\n"],["headingLink","分片复制过程"],["heading","分片复制过程"],["body","\n"],["body","我们能够发送请求给集群中任意一个节点。每个节点都有能力处理任意请求。每个节点都知道任意文档所在的节点"],["body","\n"],["body","新建索引和删除请求都是写操作，它们必须在主分片上成功完成才能赋值到相关的复制分片上"],["body","\n"],["body","在主分片和复制分片上成功新建、索引或删除一个文档必要的顺序步骤："],["body","\n\n"],["body","客户端给Node1 发送新建、索引或删除请求。"],["body","\n"],["body","节点使用文档的_id确定文档属于分片0.转发请求到Node3，分片0位于这个节点上。"],["body","\n"],["body","Node3在主分片上执行请求，如果成功，它转发请求到相应的位于Node1和Node2的复制节点上"],["body","\n"],["body","当所有的复制节点报告成功，Node3报告成功到请求的节点，请求的节点再报告给客户端。"],["body","\n"],["body","客户端接收到成功响应的时候，文档的修改已经被用于主分片和所有的复制分片，修改生效了。"],["body","\n\n"],["body","ES分片复制"],["body","\n\n"],["body","复制默认的值是sync。这将导致主分片得到复制分片的成功响应后才返回。"],["body","\n"],["body","如果你设置replication为async,请求在主分片上被执行后就会返回给客户端。它依旧会转发给复制节点，但你将不知道复制节点成功与否。"],["body","\n\n"],["headingLink","节点类型"],["heading","节点类型"],["body","\n\n"],["body","master 节点： 集群中的一个节点会被选为 master 节点，它将负责管理集群范畴的变更，例如创建或删除索引，添加节点到集群或从集群中删除节点。master 节点无需参与文档层面的变更和搜索，这意味着仅有一个 master 节点并不会因流量增长而成为瓶颈。任意一个节点都可以成为 master 节点。"],["body","\n"],["body","data 节点： 持有数据和倒排索引。默认情况下，每个节点都可以通过设定配置文件 elasticsearch.yml 中的 node.data 属性为 true (默认) 成为数据节点。如果需要一个专门的主节点 (一个节点既可以是 master 节点，同时也可以是 data 节点)，应将其 node.data 属性设置为 false。"],["body","\n"],["body","client 节点： 如果将 node.master 属性和 node.data 属性都设置为 false，那么该节点就是一个客户端节点，扮演一个负载均衡的角色，将到来的请求路由到集群中的各个节点。"],["body","\n\n"],["headingLink","分片与索引实战"],["heading","分片与索引实战"],["body","\n"],["headingLink","新建索引分片实例"],["heading","新建索引分片实例"],["body","\n"],["body","PUT /testIndex\n{\n\t\"settings\":{\n\t\t\"number_of_shards\":12,\n\t\t\"number_of_replicas\":1\n\t}\n}\n"],["body","\n"],["headingLink","调整分片数"],["heading","调整分片数"],["body","\n"],["body","PUT /testIndex/_settings\n{\n\t\"number_of_replicas\":2\n}\n"],["body","\n"],["headingLink","检查分片信息"],["heading","检查分片信息"],["body","\n"],["body","GET _cat/shards?v\n"],["body","\n"],["headingLink","检查索引信息"],["heading","检查索引信息"],["body","\n"],["body","GET _cat/indices\n"],["body","\n"],["headingLink","设置磁盘水位"],["heading","设置磁盘水位"],["body","\n"],["body","PUT _cluster/settings \n{ \n\t\"transient\": { \n\t\t\"cluster.routing.allocation.disk.watermark.low\": \"90%\", \n\t\t\"cluster.routing.allocation.disk.watermark.high\": \"5gb\"\n\t }\n }\n\n"],["body","\n\n"],["body","ES 默认当磁盘空间不足 15%时，会禁止分配 replica shard。可以动态调整 ES 对磁盘空间的要求限制；"],["body","\n"],["body","配置磁盘空间限制的时候，要求low必须比 high 大，可以使用百分比或 gb 的方式设置，且ES要求low至少满足磁盘 95%的容量。"],["body","\n"],["body","low - 对磁盘空闲容量的最低限制（默认85%）；"],["body","\n"],["body","high - 对磁盘空闲容量的最高限制（默认90%，极限值95%）"],["body","\n"],["body","low 为 50gb。high 为 10gb。则当磁盘空闲容量不足 50gb 时停止分配 replica shard。 当磁盘空闲容量不足 10gb 时，停止分配 shard，并将应该在当前结点中分配的 shard 分配 到其他结点中；"],["body","\n\n"],["headingLink","查看集群健康状态"],["heading","查看集群健康状态"],["body","\n"],["body","GET _cluster/health\n\n{\n  \"cluster_name\" : \"elasticsearch\",\n  \"status\" : \"green\",\n  \"timed_out\" : false,\n  \"number_of_nodes\" : 2,\n  \"number_of_data_nodes\" : 2,\n  \"active_primary_shards\" : 8,\n  \"active_shards\" : 16,\n  \"relocating_shards\" : 0,\n  \"initializing_shards\" : 0,\n  \"unassigned_shards\" : 0,\n  \"delayed_unassigned_shards\" : 0,\n  \"number_of_pending_tasks\" : 0,\n  \"number_of_in_flight_fetch\" : 0,\n  \"task_max_waiting_in_queue_millis\" : 0,\n  \"active_shards_percent_as_number\" : 100.0\n}\n"],["body","\n"],["headingLink","索引未分配原因"],["heading","索引未分配原因"],["body","\n\n"],["body","INDEX_CREATED：由于创建索引的API导致未分配。"],["body","\n"],["body","CLUSTER_RECOVERED：由于完全集群恢复导致未分配。"],["body","\n"],["body","INDEX_REOPENED ：由于打开open或关闭close一个索引导致未分配。"],["body","\n"],["body","DANGLING_INDEX_IMPORTED ：由于导入dangling索引的结果导致未分配。"],["body","\n"],["body","NEW_INDEX_RESTORED ：由于恢复到新索引导致未分配。"],["body","\n"],["body","EXISTING_INDEX_RESTORED：由于恢复到已关闭的索引导致未分配。"],["body","\n"],["body","REPLICA_ADDED：由于显式添加副本分片导致未分配。"],["body","\n"],["body","ALLOCATION_FAILED ：由于分片分配失败导致未分配。"],["body","\n"],["body","NODE_LEFT ：由于承载该分片的节点离开集群导致未分配。"],["body","\n"],["body","REINITIALIZED ：由于当分片从开始移动到初始化时导致未分配（例如，使用影子shadow副本分片）。"],["body","\n"],["body","REROUTE_CANCELLED ：作为显式取消重新路由命令的结果取消分配。"],["body","\n"],["body","REALLOCATED_REPLICA：确定更好的副本位置被标定使用，导致现有的副本分配被取消，出现未分配。"],["body","\n\n"],["headingLink","查看具体未分配原因"],["heading","查看具体未分配原因"],["body","\n"],["body","GET /_cluster/allocation/explain\n\nGET _cat/indices?v&health=red\n\n\nGET /_cat/shards?v&h=n,index,shard,prirep,state,sto,sc,unassigned.reason,unassigned.details\n"],["body","\n"],["headingLink","尝试重新分配失败的分片"],["heading","尝试重新分配失败的分片"],["body","\n"],["body","POST /_cluster/reroute?retry_failed=true\n"],["body","\n"],["body","默认索引的尝试次数为5"],["body","\n"],["body","PUT /indexname/_settings\n{\n  \"index\": {\n    \"allocation\": {\n      \"max_retries\": 20\n    }\n  }\n}\n"],["body","\n"],["headingLink","将副本分片提升为主分片"],["heading","将副本分片提升为主分片"],["body","\n"],["body","如果确定了主分片已经损坏，可以尝试将副本分片提升为主(会丢部分数据)："],["body","\n"],["body","POST /_cluster/reroute?pretty\n{\n  \"commands\": [\n    {\n      \"allocate_stale_primary\": {\n        \"index\": \"indexname\",//索引名\n        \"shard\": 3,//操作的分片id\n        \"node\": \"node1\",//此分片副本位于的节点\n        \"accept_data_loss\": true//提示数据可能会丢失\n      }\n    }\n  ]\n}\n"],["body","\n"],["body","此方案存在一个问题是需要提前知道此分片的副本位于哪个节点用以指定，可以通过如果api获取副本分片位置："],["body","\n"],["body","GET _shard_stores?pretty\nGET indexname/_shard_stores?pretty\n"],["body","\n"],["headingLink","手动判断目录"],["heading","手动判断目录"],["body","\n"],["body","判断当前es进程使用的数据目录:通过pid和yml配置的目录去匹配，如data"],["body","\n"],["body","ll /proc/pid/fd |grep data\n"],["body","\n"],["body","如果索引损坏导致api失效，则需要人工去数据目录进行查找副本分片位置,目录结构如下:"],["body","\n"],["body","data/nodes/0/indices/Z60wvPOWSP6Qbk79i757Vg/0\n"],["body","\n"],["body","数据目录下为节点号 -> 索引文件夹 -> 索引ID -> 分片号"],["body","\n"],["headingLink","将此分片置为空分片"],["heading","将此分片置为空分片"],["body","\n\n"],["body","\n"],["body","如果此分片的主副都已经损坏，则可将此分片置为空以保留索引其他分片数据："],["body","\n"],["body","\n"],["body","\n"],["body","{\n  \"commands\": [\n    {\n      \"allocate_empty_primary\": {\n        \"index\": \"indexname\",//索引名\n        \"shard\": 3,//操作的分片id\n        \"node\": \"node1\",//空分片要分配的节点\n        \"accept_data_loss\": true//提示数据可能会丢失\n      }\n    }\n  ]\n}\n"],["body","\n"],["body","\n"],["body","\n"],["body","如果集群存在大量索引分片无法恢复，则可以使用脚本将全部分片置空,可以基于下面的脚本修改："],["body","\n"],["body","\n\n"],["body","#!/bin/bash\nmaster=$(curl -s 'http://localhost:9200/_cat/master?v' | grep -v ' ip ' | awk '{print $1}')\nfor index in $(curl  -s 'http://localhost:9200/_cat/shards' | grep UNASSIGNED | awk '{print $1}' | sort | uniq); do\n    for shard in $(curl  -s 'http://localhost:9200/_cat/shards' | grep UNASSIGNED | grep $index | awk '{print $2}' | sort | uniq); do\n        echo  $index $shard\n        curl -XPOST -H 'Content-Type: application/json'  'http://localhost:9200/_cluster/reroute' -d '{\n            \"commands\" : [ {\n                  \"allocate_empty_primary\" : {\n                      \"index\" : \"'$index'\",\n                      \"shard\" : \"'$shard'\",\n                      \"node\" : \"'$master'\",\n                  \"accept_data_loss\" : true\n                  }\n                }\n            ]\n        }'\n        sleep 1\n    done\ndone\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/数据分片.html"],["title","数据分片.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","分片路由"],["heading","分片路由"],["body","\n"],["body","当索引一个文档的时候，文档会被存储到一个主分片中"],["body","\n"],["body","Elasticsearch 如何知道一个文档应该存放到哪个分片中呢？"],["body","\n"],["body","shard = hash(routing) % number_of_primary_shards\n"],["body","\n"],["body","routing是一个可变值，默认是文档的_id，也可以设置成一个自定义的值"],["body","\n"],["body","routing 通过 hash 函数生成一个数字，然后这个数字再除以 number_of_primary_shards （主分片的数量）后得到 余数"],["body","\n"],["body","主分片的数量不可变"],["body","\n"],["body","这就解释了为什么我们要在创建索引的时候就确定好主分片的数量 并且永远不会改变这个数量：因为如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。"],["body","\n"],["body","自定义routing字段"],["body","\n"],["body","所有的文档 API（ get 、 index 、 delete 、 bulk 、 update 以及 mget ）都接受一个叫做 routing 的路由参数 ，通过这个参数我们可以自定义文档到分片的映射。一个自定义的路由参数可以用来确保所有相关的文档——例如所有属于同一个用户的文档——都被存储到同一个分片中。我们也会在扩容设计这一章中详细讨论为什么会有这样一种需求。"],["body","\n"],["headingLink","主分片和副本分片如何交互"],["heading","主分片和副本分片如何交互"],["body","\n"],["headingLink","新建索引和删除单个文档"],["heading","新建、索引和删除单个文档"],["body","\n\n"],["body","客户端向 任意一个节点 发送新建、索引或者删除请求，该节点称为协调节点"],["body","\n"],["body","节点使用文档的 _id 确定文档属于哪个分片。请求会被转发到 该分片所处的节点"],["body","\n"],["body","该节点 在该文档的主分片上面执行请求。如果成功了，它将请求并行转发到该索引其他的副本分片上。一旦所有的副本分片都报告成功,将向协调节点报告成功，协调节点向客户端报告成功。"],["body","\n\n"],["body","在客户端收到成功响应时，文档变更已经在主分片和所有副本分片执行完成，变更是安全的。"],["body","\n"],["body","有一些可选的请求参数允许您影响这个过程，可能以数据安全为代价提升性能。这些选项很少使用，因为Elasticsearch已经很快，但是为了完整起见，在这里阐述如下："],["body","\n"],["body","consistency"],["body","\n"],["body","一致性：要求 大多数 副本分片处于活跃状态下才 会执行 写操作，是为了避免在发生网络分区故障（network partition）的时候进行_写_操作，进而导致数据不一致"],["body","\n"],["body","规定数量："],["body","\n"],["body","int( (primary + number_of_replicas) / 2 ) + 1\n"],["body","\n"],["body","取值"],["body","\n"],["body","one:只要主分片状态 ok 就允许执行_写_操作"],["body","\n"],["body","all:必须要主分片和所有副本分片的状态没问题才允许执行_写_操作"],["body","\n"],["body","quorum: 即大多数的分片副本状态没问题就允许执行_写_操作。"],["body","\n"],["body","timeout"],["body","\n"],["body","如果没有足够的副本分片会发生什么？ Elasticsearch会等待，希望更多的分片出现。默认情况下，它最多等待1分钟。 如果你需要，你可以使用 timeout 参数 使它更早终止： 100 100毫秒，30s 是30秒。"],["body","\n"],["headingLink","取回文档"],["heading","取回文档"],["body","\n\n"],["body","\n"],["body","客户端向 任意节点 发送获取请求。（即协调节点）"],["body","\n"],["body","\n"],["body","\n"],["body","节点使用文档的 _id 来确定文档属于分片 0 。由于分片存在副本，轮询到其中一个节点的分片上"],["body","\n"],["body","\n\n"],["body","3、该节点 将文档返回给 协调节点 ，然后将文档返回给客户端。"],["body","\n"],["body","在处理读取请求时，协调结点在每次请求的时候都会通过轮询所有的副本分片来达到负载均衡。"],["body","\n"],["body","在文档被检索时，已经被索引的文档可能已经存在于主分片上但是还没有复制到副本分片。"],["body","\n"],["body","在这种情况下，副本分片可能会报告文档不存在，但是主分片可能成功返回文档。 一旦索引请求成功返回给用户，文档在主分片和副本分片都是可用的。"],["body","\n"],["headingLink","局部更新文档"],["heading","局部更新文档"],["body","\n\n"],["body","客户端向 任意节点 发送更新请求。（即协调节点）"],["body","\n"],["body","它将请求转发到主分片所在的 处理节点 。"],["body","\n"],["body","处理节点 从主分片检索文档，修改 _source 字段中的 JSON ，并且尝试重新索引主分片的文档。 如果文档已经被另一个进程修改，它会重试步骤 3 ，超过 retry_on_conflict 次后放弃。"],["body","\n"],["body","如果 处理节点 成功地更新文档，它将新版本的文档并行转发到 其他节点上的副本分片，重新建立索引。 一旦所有副本分片都返回成功，处理节点 向协调节点也返回成功，协调节点向客户端返回成功。"],["body","\n\n"],["body","基于文档的复制"],["body","\n"],["body","当主分片把更改转发到副本分片时， 它不会转发更新请求。"],["body","\n"],["body","相反，它转发完整文档的新版本。请记住，这些更改将会异步转发到副本分片，并且不能保证它们以发送它们相同的顺序到达。 如果Elasticsearch仅转发更改请求，则可能以错误的顺序应用更改，导致得到损坏的文档。"],["body","\n"],["headingLink","多文档模式"],["heading","多文档模式"],["body","\n"],["body","mget 和 bulk API 的模式类似于单文档模式。区别在于协调节点知道每个文档存在于哪个分片中。 它将整个多文档请求分解成 每个分片 的多文档请求，并且将这些请求并行转发到每个参与节点。"],["body","\n"],["body","协调节点一旦收到来自每个节点的应答，就将每个节点的响应收集整理成单个响应，返回给客户端"],["body","\n"],["headingLink","使用-mget-取回多个文档"],["heading","使用 mget 取回多个文档"],["body","\n\n"],["body","客户端向 任意某个节点 发送 mget 请求。（即协调节点）"],["body","\n"],["body","协调节点 分析批量操作中每个请求的所位于的分片，并为每个分片构建多文档获取请求，然后并行转发这些请求到托管在每个所需的主分片或者副本分片的节点上。一旦收到所有答复，协调节点 构建响应并将其返回给客户端。"],["body","\n\n"],["headingLink","使用-bulk-修改多个文档"],["heading","使用 bulk 修改多个文档"],["body","\n\n"],["body","客户端向 任意某个节点 发送 bulk 请求。（即协调节点）"],["body","\n"],["body","协调节点分析批量操作中每个请求的所位于的分片，并将这些请求并行转发到每个包含主分片的节点主机"],["body","\n"],["body","主分片一个接一个按顺序执行每个操作。当每个操作成功时，主分片并行转发新文档（或删除）到副本分片，然后执行下一个操作。 一旦所有的副本分片报告所有操作成功，该节点将向协调节点报告成功，协调节点将这些响应收集整理并返回给客户端。"],["body","\n\n"],["body","bulk API 还可以在整个批量请求的最顶层使用 consistency 参数，以及在每个请求中的元数据中使用 routing 参数。"],["body","\n"],["headingLink","多文档模式中的换行符"],["heading","多文档模式中的换行符"],["body","\n"],["body","为什么 bulk API 需要有换行符的有趣格式，而不是发送包装在 JSON 数组中的请求，例如 mget API？"],["body","\n"],["body","在批量请求中引用的每个文档可能属于不同的主分片， 每个文档可能被分配给集群中的任何节点。这意味着批量请求 bulk 中的每个 操作 都需要被转发到正确节点上的正确分片。"],["body","\n"],["body","如果单个请求被包装在 JSON 数组中，那就意味着我们需要执行以下操作："],["body","\n\n"],["body","将 JSON 解析为数组（包括文档数据，可以非常大）"],["body","\n"],["body","查看每个请求以确定应该去哪个分片"],["body","\n"],["body","为每个分片创建一个请求数组"],["body","\n"],["body","将这些数组序列化为内部传输格式"],["body","\n"],["body","将请求发送到每个分片"],["body","\n\n"],["body","这是可行的，但需要大量的 RAM 来存储原本相同的数据的副本，并将创建更多的数据结构，Java虚拟机（JVM）将不得不花费时间进行垃圾回收"],["body","\n"],["body","相反，Elasticsearch可以直接读取被网络缓冲区接收的原始数据。 它使用换行符字符来识别和解析小的 action/metadata 行来决定哪个分片应该处理每个请求。"],["body","\n"],["body","这些原始请求会被直接转发到正确的分片。没有冗余的数据复制，没有浪费的数据结构。整个请求尽可能在最小的内存中处理。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/聚合.html"],["title","聚合 - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/5.Aliases/README.html"],["title","Aliases - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","aliases"],["heading","Aliases"],["body","\n\n"],["body","别名是一组数据流或索引的辅助名称"],["body","\n"],["body","大多数Elasticsearch api接受别名来代替数据流或索引名称。"],["body","\n"],["body","您可以随时更改别名的数据流或索引"],["body","\n"],["body","如果您在应用程序的Elasticsearch请求中使用别名，则可以在不停机或不更改应用程序代码的情况下重新索引数据"],["body","\n\n"],["headingLink","alias-types"],["heading","Alias types"],["body","\n"],["body","别名有两种类型:"],["body","\n\n"],["body","A data stream alias points to one or more data streams."],["body","\n"],["body","An index alias points to one or more indices."],["body","\n\n"],["body","别名不能同时指向数据流和索引。您也不能将数据流的后备索引添加到索引别名中。"],["body","\n"],["headingLink","add-an-alias"],["heading","Add an alias"],["body","\n"],["body","To add an existing data stream or index to an alias, use the aliases API's add action. If the alias doesn’t exist, the request creates it."],["body","\n"],["body","POST _aliases\n{\n  \"actions\": [\n    {\n      \"add\": {\n        \"index\": \"logs-nginx.access-prod\",\n        \"alias\": \"logs\"\n      }\n    }\n  ]\n}\n"],["body","\n"],["body","The API’s index and indices parameters support wildcards (*). Wildcard patterns that match both data streams and indices return an error."],["body","\n"],["body","POST _aliases\n{\n  \"actions\": [\n    {\n      \"add\": {\n        \"index\": \"logs-*\",\n        \"alias\": \"logs\"\n      }\n    }\n  ]\n}\n"],["body","\n"],["headingLink","remove-an-alias"],["heading","Remove an alias"],["body","\n"],["body","To remove an alias, use the aliases API’s remove action."],["body","\n"],["body","POST _aliases\n{\n  \"actions\": [\n    {\n      \"remove\": {\n        \"index\": \"logs-nginx.access-prod\",\n        \"alias\": \"logs\"\n      }\n    }\n  ]\n}\n"],["body","\n"],["headingLink","multiple-actions"],["heading","Multiple actions"],["body","\n\n"],["body","\n"],["body","您可以使用别名API在单个原子操作中执行多个操作。"],["body","\n"],["body","\n"],["body","\n"],["body","For example, the logs alias points to a single data stream. The following request swaps the stream for the alias. During this swap, the logs alias has no downtime and never points to both streams at the same time."],["body","\n"],["body","\n\n"],["body","POST _aliases\n{\n  \"actions\": [\n    {\n      \"remove\": {\n        \"index\": \"logs-nginx.access-prod\",\n        \"alias\": \"logs\"\n      }\n    },\n    {\n      \"add\": {\n        \"index\": \"logs-my_app-default\",\n        \"alias\": \"logs\"\n      }\n    }\n  ]\n}\n"],["body","\n"],["headingLink","add-an-alias-at-index-creation"],["heading","Add an alias at index creation"],["body","\n"],["body","You can also use a component or index template to add index or data stream aliases when they are created."],["body","\n\n"],["body","索引创建时添加别名"],["body","\n"],["body","可以配置在 索引模板中、或者 组件模板中"],["body","\n\n"],["body","# Component template with index aliases\nPUT _component_template/my-aliases\n{\n  \"template\": {\n    \"aliases\": {\n      \"my-alias\": {}\n    }\n  }\n}\n\n# Index template with index aliases\nPUT _index_template/my-index-template\n{\n  \"index_patterns\": [\n    \"my-index-*\"\n  ],\n  \"composed_of\": [\n    \"my-aliases\",\n    \"my-mappings\",\n    \"my-settings\"\n  ],\n  \"template\": {\n    \"aliases\": {\n      \"yet-another-alias\": {}\n    }\n  }\n}\n"],["body","\n"],["body","You can also specify index aliases in create index API requests."],["body","\n"],["body","# PUT <my-index-{now/d}-000001>\nPUT %3Cmy-index-%7Bnow%2Fd%7D-000001%3E\n{\n  \"aliases\": {\n    \"my-alias\": {}\n  }\n}\n"],["body","\n"],["headingLink","view-aliases"],["heading","View aliases"],["body","\n"],["body","To get a list of your cluster’s aliases, use the get alias API with no argument."],["body","\n"],["body","GET _alias\n"],["body","\n"],["body","Specify a data stream or index before _alias to view its aliases."],["body","\n"],["body","GET my-data-stream/_alias\n"],["body","\n"],["body","GET _alias/logs\n"],["body","\n"],["headingLink","write-index"],["heading","Write index"],["body","\n\n"],["body","您可以使用’is_write_index‘ 为别名指定写索引或写数据流"],["body","\n"],["body","Elasticsearch将别名的任何写请求路由到该索引或数据流。"],["body","\n\n"],["body","POST _aliases\n{\n  \"actions\": [\n    {\n      \"add\": {\n        \"index\": \"logs-nginx.access-prod\",\n        \"alias\": \"logs\"\n      }\n    },\n    {\n      \"add\": {\n        \"index\": \"logs-my_app-default\",\n        \"alias\": \"logs\",\n        \"is_write_index\": true\n      }\n    }\n  ]\n}\n"],["body","\n\n"],["body","如果别名指向多个索引或数据流，并且未设置 “is_write_index”，则该别名会拒绝写入请求"],["body","\n"],["body","如果索引别名指向一个索引并且未设置 “is_write_index”，则该索引会自动充当写入索引"],["body","\n"],["body","数据流别名不会自动设置写数据流，即使别名指向一个数据流"],["body","\n"],["body","我们建议使用数据流来存储仅追加的时间序列数据。如果您经常更新或删除现有的时间序列数据，请使用带有写索引的索引别名，See Manage time series data without data streams."],["body","\n\n"],["headingLink","filter-an-alias"],["heading","Filter an alias"],["body","\n"],["body","The filter option uses Query DSL to limit the documents an alias can access."],["body","\n"],["body","POST _aliases\n{\n  \"actions\": [\n    {\n      \"add\": {\n        \"index\": \"my-index-2099.05.06-000001\",\n        \"alias\": \"my-alias\",\n        \"filter\": {\n          \"bool\": {\n            \"filter\": [\n              {\n                \"range\": {\n                  \"@timestamp\": {\n                    \"gte\": \"now-1d/d\",\n                    \"lt\": \"now/d\"\n                  }\n                }\n              },\n              {\n                \"term\": {\n                  \"user.id\": \"kimchy\"\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  ]\n}\n"],["body","\n"],["headingLink","routing"],["heading","Routing"],["body","\n\n"],["body","\n"],["body","Use the routing option to route requests for an alias to a specific shard."],["body","\n"],["body","\n"],["body","\n"],["body","This lets you take advantage of shard caches to speed up searches. Data stream aliases do not support routing options."],["body","\n"],["body","\n\n"],["body","POST _aliases\n{\n  \"actions\": [\n    {\n      \"add\": {\n        \"index\": \"my-index-2099.05.06-000001\",\n        \"alias\": \"my-alias\",\n        \"routing\": \"1\"\n      }\n    }\n  ]\n}\n"],["body","\n"],["body","Use index_routing and search_routing to specify different routing values for indexing and search. If specified, these options overwrite the routing value for their respective operations."],["body","\n"],["body","POST _aliases\n{\n  \"actions\": [\n    {\n      \"add\": {\n        \"index\": \"my-index-2099.05.06-000001\",\n        \"alias\": \"my-alias\",\n        \"search_routing\": \"1\",\n        \"index_routing\": \"2\"\n      }\n    }\n  ]\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/5.Aliases/Aliases.mm.html"],["title","Aliases.mm.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","索引别名"],["heading","索引别名"],["body","\n"],["headingLink","添加索引"],["heading","添加索引"],["body","\n"],["body","POST _aliases\n{\n  \"actions\": [\n    {\n      \"add\": {\n        \"index\": \"logs-nginx.access-prod\",\n        \"alias\": \"logs\"\n      },\n      \"add\": {\n        \"index\": \"logs-*\",\n        \"alias\": \"logs\"\n      }\n    }\n  ]\n}\n"],["body","\n"],["headingLink","移除索引"],["heading","移除索引"],["body","\n"],["body","POST _aliases\n{\n  \"actions\": [\n    {\n      \"remove\": {\n        \"index\": \"logs-nginx.access-prod\",\n        \"alias\": \"logs\"\n      }\n    }\n  ]\n}\n"],["body","\n"],["headingLink","索引创建时添加别名"],["heading","索引创建时添加别名"],["body","\n\n"],["body","组件模板中添加别名"],["body","\n\n"],["body","PUT _component_template/my-aliases\n{\n  \"template\": {\n    \"aliases\": {\n      \"my-alias\": {}\n    }\n  }\n}\n"],["body","\n\n"],["body","索引模板"],["body","\n\n"],["body","PUT _index_template/my-index-template\n{\n  \"index_patterns\": [\n    \"my-index-*\"\n  ],\n  \"composed_of\": [\n    \"my-aliases\",\n    \"my-mappings\",\n    \"my-settings\"\n  ],\n  \"template\": {\n    \"aliases\": {\n      \"yet-another-alias\": {}\n    }\n  }\n}\n"],["body","\n"],["headingLink","查看索引别名"],["heading","查看索引别名"],["body","\n\n"],["body","GET _alias/<aliasName>"],["body","\n"],["body","GET my-data-stream/_alias"],["body","\n\n"],["headingLink","write-index"],["heading","Write index"],["body","\n\n"],["body","Elasticsearch将别名的任何写请求路由到 is_write_index=true的索引或数据流。"],["body","\n"],["body","如果别名指向多个索引或数据流，并且未设置 “is_write_index”，则该别名会拒绝写入请求"],["body","\n"],["body","如果索引别名指向一个索引并且未设置 “is_write_index”，则该索引会自动充当写入索引"],["body","\n"],["body","数据流别名不会自动设置写数据流，即使别名指向一个数据流"],["body","\n\n"],["body","POST _aliases\n{\n  \"actions\": [\n    {\n      \"add\": {\n        \"index\": \"logs-nginx.access-prod\",\n        \"alias\": \"logs\"\n      }\n    },\n    {\n      \"add\": {\n        \"index\": \"logs-my_app-default\",\n        \"alias\": \"logs\",\n        \"is_write_index\": true\n      }\n    }\n  ]\n}\n"],["body","\n"],["headingLink","过滤索引"],["heading","过滤索引"],["body","\n\n"],["body","允许该别名查看的索引数据范围"],["body","\n\n"],["body","POST _aliases\n{\n  \"actions\": [\n    {\n      \"add\": {\n        \"index\": \"my-index-2099.05.06-000001\",\n        \"alias\": \"my-alias\",\n        \"filter\": {\n          \"bool\": {\n            \"filter\": [\n              {\n                \"range\": {\n                  \"@timestamp\": {\n                    \"gte\": \"now-1d/d\",\n                    \"lt\": \"now/d\"\n                  }\n                }\n              },\n              {\n                \"term\": {\n                  \"user.id\": \"kimchy\"\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  ]\n}\n"],["body","\n"],["headingLink","别名分片路由"],["heading","别名分片路由"],["body","\n"],["body","POST _aliases\n{\n  \"actions\": [\n    {\n      \"add\": {\n        \"index\": \"my-index-2099.05.06-000001\",\n        \"alias\": \"my-alias\",\n        \"search_routing\": \"1\",\n        \"index_routing\": \"2\"\n      }\n    }\n  ]\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/杂项学习.html"],["title","杂项学习.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","es数据导入导出"],["heading","es数据导入导出"],["body","\n"],["body","elasticdump \\\n  --input=http://es-test1:9201/sca_recording_aliyun_1634527720352 \\\n  --output=http://es-test:9200/sca_recording \\\n  --type=analyzer\n  \nelasticdump \\\n  --input=http://es-test1:9201/sca_recording_aliyun_1634527720352 \\\n  --output=http://es-test:9200/sca_recording \\\n  --type=mapping\nelasticdump \\\n  --input=http://es-test1:9201/sca_recording_aliyun_1634527720352 \\\n  --output=http://es-test:9200/sca_recording \\\n  --type=data\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/配置/1.重要配置.html"],["title","重要配置.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","重要配置"],["heading","重要配置"],["body","\n"],["body","Elasticsearch需要很少的配置才能开始使用，但是在生产中使用集群之前必须考虑许多项目:"],["body","\n\n"],["body","Path settings"],["body","\n"],["body","Cluster name setting"],["body","\n"],["body","Node name setting"],["body","\n"],["body","Network host settings"],["body","\n"],["body","Discovery settings"],["body","\n"],["body","Heap size settings"],["body","\n"],["body","JVM heap dump path setting"],["body","\n"],["body","GC logging settings"],["body","\n"],["body","Temporary directory settings"],["body","\n"],["body","JVM fatal error log setting"],["body","\n"],["body","Cluster backups"],["body","\n\n"],["headingLink","path-settings"],["heading","Path settings"],["body","\n"],["body","主要包括 数据文件路径 和 日志文件路径"],["body","\n"],["body","path:\n  data: /var/data/elasticsearch\n  logs: /var/log/elasticsearch\n"],["body","\n"],["headingLink","cluster-name-setting"],["heading","Cluster name setting"],["body","\n\n"],["body","\n"],["body","只有当节点与群集中的所有其他节点有相同的 cluster.name 时，节点才能加入群集。默认名称是elasticsearch，但您应该将其更改为描述群集用途的适当名称。"],["body","\n"],["body","\n"],["body","\n"],["body","不要在不同的环境中重用相同的集群名称。否则，节点可能会加入错误的集群。"],["body","\n"],["body","\n\n"],["headingLink","node-name-setting"],["heading","Node name setting"],["body","\n\n"],["body","集群中的节点名"],["body","\n"],["body","默认 主机名"],["body","\n\n"],["body","node.name: prod-data-2\n"],["body","\n"],["headingLink","network-host-setting"],["heading","Network host setting"],["body","\n"],["body","默认情况下，Elasticsearch仅绑定回送地址，例如127.0.0.1和 [::1]。这足以在单个服务器上运行一个或多个节点的群集以进行开发和测试，"],["body","\n"],["body","但是弹性生产群集必须涉及其他服务器上的节点。有许多网络设置，但通常您需要配置的只是network.host:"],["body","\n"],["body","network.host: 192.168.1.10\n"],["body","\n"],["body","当您为network.host提供值时，Elasticsearch假定您正在从开发模式转到生产模式，并将许多系统启动检查从警告升级为异常。请参阅开发模式和生产模式之间的差异。"],["body","\n"],["headingLink","discovery-and-cluster-formation-settings"],["heading","Discovery and cluster formation settings"],["body","\n"],["body","在进入生产之前，配置两个重要的发现和集群形成设置，以便集群中的节点可以相互发现并选择一个主节点。"],["body","\n"],["headingLink","discoveryseed_hosts"],["heading","discovery.seed_hosts"],["body","\n"],["body","本地集群"],["body","\n"],["body","开箱即用，无需任何网络配置，Elasticsearch将绑定到可用的环回地址，并扫描本地端口9300 9305与同一服务器上运行的其他节点连接。此行为无需进行任何配置即可提供自动群集体验。"],["body","\n"],["body","不同机器集群"],["body","\n"],["body","当您想要与其他主机上的节点组成集群时"],["body","\n"],["body","使用 discovery.seed_hosts"],["body","\n"],["body","此设置提供了集群中其他节点的列表，这些节点是符合主条件的，并且很可能是存活的，可以通信的。以便进行 发现过程"],["body","\n"],["body","discovery.seed_hosts:\n   - 192.168.1.10:9300\n   - 192.168.1.11 \n   - seeds.mydomain.com \n   - [0:0:0:0:0:ffff:c0a8:10c]:9301 \n"],["body","\n\n"],["body","The port is optional and defaults to 9300, but can be overridden."],["body","\n"],["body","If a hostname resolves to multiple IP addresses, the node will attempt to discover other nodes at all resolved addresses."],["body","\n"],["body","IPv6 addresses must be enclosed in square brackets."],["body","\n\n"],["body","如果您的主合格节点没有固定名称或地址，请使用替代主机提供程序动态查找其地址。"],["body","\n"],["headingLink","clusterinitial_master_nodes"],["heading","cluster.initial_master_nodes"],["body","\n\n"],["body","\n"],["body","第一次启动集群时，a cluster bootstrapping  步骤 用来选举主节点"],["body","\n"],["body","\n"],["body","\n"],["body","In development mode, with no discovery settings configured, this step is performed automatically by the nodes themselves."],["body","\n"],["body","\n\n"],["body","由于自动引导本质上是不安全的，因此在生产模式下启动新集群时，必须明确列出符合主条件的节点，这些节点的票数应在第一次选举中计算。您使用cluster.initial_master_nodes设置此列表。"],["body","\n"],["body","集群首次成功形成后，从每个节点的配置中删除cluster.initial_master_nodes设置。重新启动群集或将新节点添加到现有群集时，请勿使用此设置。"],["body","\n"],["body","discovery.seed_hosts:\n   - 192.168.1.10:9300\n   - 192.168.1.11\n   - seeds.mydomain.com\n   - [0:0:0:0:0:ffff:c0a8:10c]:9301\ncluster.initial_master_nodes: \n   - master-node-a\n   - master-node-b\n   - master-node-c\n"],["body","\n\n"],["body","指定master nodes 默认是 hostname ，确保 node.name 匹配  cluster.initial_master_nodes"],["body","\n"],["body","使用 FQDN 的话 这里的列表也要使用FQDN"],["body","\n\n"],["body","See bootstrapping a cluster and discovery and cluster formation settings."],["body","\n"],["headingLink","heap-size-settings"],["heading","Heap size settings"],["body","\n\n"],["body","\n"],["body","默认情况下，Elasticsearch会根据节点的角色和总内存自动设置JVM堆大小。我们建议大多数生产环境的保持默认"],["body","\n"],["body","\n"],["body","\n"],["body","Automatic heap sizing requires the bundled JDK or, if using a custom JRE location, a Java 14 or later JRE."],["body","\n"],["body","\n"],["body","\n"],["body","If needed, you can override the default sizing by manually setting the JVM heap size."],["body","\n"],["body","\n\n"],["headingLink","jvm-heap-dump-path-setting"],["heading","JVM heap dump path setting"],["body","\n"],["body","OOM 后的 heap dump 文件路径"],["body","\n"],["body","RPM Debian : /var/lib/elasticsearch"],["body","\n"],["body","Manual:  安装home目录"],["body","\n"],["body","修改： -XX:HeapDumpPath=... entry in jvm.options:"],["body","\n\n"],["body","指定目录：, the JVM will generate a filename for the heap dump based on the PID of the running instance."],["body","\n"],["body","指定文件： the file must not exist when the JVM needs to perform a heap dump on an out of memory exception. Otherwise, the heap dump will fail."],["body","\n\n"],["headingLink","gc-logging-settings"],["heading","GC logging settings"],["body","\n\n"],["body","\n"],["body","By default, Elasticsearch enables garbage collection (GC) logs. These are configured in jvm.options and output to the same default location as the Elasticsearch logs.  跟 日志目录保持 一致"],["body","\n"],["body","\n"],["body","\n"],["body","The default configuration rotates the logs every 64 MB and can consume up to 2 GB of disk space."],["body","\n"],["body","64MB ~ 2GB"],["body","\n"],["body","\n\n"],["body","使用 JEP158配置JVM日志"],["body","\n"],["body","You can reconfigure JVM logging using the command line options described in JEP 158: Unified JVM Logging. Unless you change the default jvm.options file directly, the Elasticsearch default configuration is applied in addition to your own settings. To disable the default configuration, first disable logging by supplying the -Xlog:disable option, then supply your own command line options. This disables all JVM logging, so be sure to review the available options and enable everything that you require."],["body","\n"],["body","To see further options not contained in the original JEP, see Enable Logging with the JVM Unified Logging Framework."],["body","\n"],["body","Examples"],["body","\n"],["body","# Turn off all previous logging configuratons\n-Xlog:disable\n\n# Default settings from JEP 158, but with `utctime` instead of `uptime` to match the next line\n-Xlog:all=warning:stderr:utctime,level,tags\n\n# Enable GC logging to a custom location with a variety of options\n-Xlog:gc*,gc+age=trace,safepoint:file=/opt/my-app/gc.log:utctime,pid,tags:filecount=32,filesize=64m\n"],["body","\n"],["body","docker配置"],["body","\n"],["body","Configure an Elasticsearch Docker container to send GC debug logs to standard error (stderr). This lets the container orchestrator handle the output. If using the ES_JAVA_OPTS environment variable, specify:"],["body","\n"],["body","MY_OPTS=\"-Xlog:disable -Xlog:all=warning:stderr:utctime,level,tags -Xlog:gc=debug:stderr:utctime\"\ndocker run -e ES_JAVA_OPTS=\"$MY_OPTS\" # etc\n"],["body","\n"],["headingLink","temporary-directory-settings"],["heading","Temporary directory settings"],["body","\n\n"],["body","Elasticsearch会使用到临时目录"],["body","\n"],["body","在一些Linux发行版本中，系统工具会 定时 清理  最近没有被访问过的 文件 和目录 ，这种行为会导致ESBUG"],["body","\n"],["body","通过 deb rpm 安装的 es使用的 临时目录 不会定期删除"],["body","\n"],["body","自己安装的 最后通过 $ES_TMPDIR 指定一个安全的临时目录"],["body","\n\n"],["headingLink","jvm-fatal-error-log-setting"],["heading","JVM fatal error log setting"],["body","\n"],["body","By default, Elasticsearch configures the JVM to write fatal error logs to the default logging directory. On RPM and Debian packages, this directory is /var/log/elasticsearch. On Linux and MacOS and Windows distributions, the logs directory is located under the root of the Elasticsearch installation."],["body","\n"],["body","These are logs produced by the JVM when it encounters a fatal error, such as a segmentation fault. If this path is not suitable for receiving logs, modify the -XX:ErrorFile=... entry in jvm.options."],["body","\n"],["headingLink","cluster-backups"],["heading","Cluster backups"],["body","\n"],["body","In a disaster, snapshots can prevent permanent data loss. Snapshot lifecycle management is the easiest way to take regular backups of your cluster. For more information, see Back up a cluster."],["body","\n\n"],["body","唯一可靠的集群备份方式是 snapshot"],["body","\n"],["body","没法通过 文件系统级别的 方式恢复"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/配置/README.html"],["title","配置 - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","configuring-elasticsearch"],["heading","Configuring Elasticsearch"],["body","\n\n"],["body","ElasticSearch开箱即用 配置很少"],["body","\n"],["body","大部分集群配置可以通过  Cluster update settings API"],["body","\n"],["body","特定于节点 的静态配置才需要 使用到次配置文件\n\n"],["body","cluster.name"],["body","\n"],["body","network.host"],["body","\n\n"],["body","\n\n"],["headingLink","config-files-location"],["heading","Config files location"],["body","\n"],["headingLink","配置文件"],["heading","配置文件"],["body","\n\n"],["body","elasticsearch.yml for configuring Elasticsearch"],["body","\n"],["body","jvm.options for configuring Elasticsearch JVM settings"],["body","\n"],["body","log4j2.properties for configuring Elasticsearch logging"],["body","\n\n"],["headingLink","安装位置取决于安装方式"],["heading","安装位置取决于安装方式"],["body","\n\n"],["body","\n"],["body","手动安装 取决于 $ES_HOME/config ES_PATH_CONF 可以修改"],["body","\n"],["body","\n"],["body","\n"],["body","RPM、Debian 包安装：/etc/elasticsearch 路径下"],["body","\n\n"],["body","/etc/default/elasticsearch (for the Debian package)"],["body","\n"],["body","/etc/sysconfig/elasticsearch"],["body","\n\n"],["body","修改上述文件中的 ES_PATH_CONF=/etc/elasticsearch 可以修改默认配置文件路径"],["body","\n"],["body","\n\n"],["headingLink","配置文件格式"],["heading","配置文件格式"],["body","\n"],["body","YAML"],["body","\n"],["body","path:\n    data: /var/lib/elasticsearch\n    logs: /var/log/elasticsearch\n"],["body","\n"],["body","discovery.seed_hosts:\n   - 192.168.1.10:9300\n   - 192.168.1.11\n   - seeds.mydomain.com\n"],["body","\n"],["headingLink","环境变量替换"],["heading","环境变量替换"],["body","\n"],["body","node.name:    ${HOSTNAME}\nnetwork.host: ${ES_NETWORK_HOST}\n"],["body","\n"],["body","多个值可以使用 逗号分割"],["body","\n"],["body","export HOSTNAME=“host1,host2\"\n"],["body","\n"],["headingLink","cluster-and-node-setting-types"],["heading","Cluster and node setting types"],["body","\n"],["body","群集和节点 配置 可以根据它们的配置方式进行分类:"],["body","\n"],["headingLink","dynamic"],["heading","Dynamic"],["body","\n\n"],["body","可以使用  the cluster update settings API.  运行时配置"],["body","\n"],["body","也可以在启动前配置"],["body","\n\n"],["body","使用群集更新设置API进行的更新可以是持久性的，适用于群集重新启动，"],["body","\n"],["body","也可以是瞬态的，在群集重新启动后重置。"],["body","\n"],["body","您还可以通过使用API为瞬态或持久设置分配空值来重置它们。"],["body","\n"],["body","如果使用多种方法配置相同的设置，Elasticsearch将按以下优先顺序应用设置:"],["body","\n\n"],["body","Transient setting"],["body","\n"],["body","Persistent setting"],["body","\n"],["body","elasticsearch.yml setting"],["body","\n"],["body","Default setting value"],["body","\n\n"],["body","例如，您可以应用瞬态设置来覆盖持久设置或elasticsearch.yml设置。"],["body","\n"],["body","但是，对elasticsearch.yml设置的更改不会覆盖已定义的瞬态或持久设置。"],["body","\n"],["body","最佳实践"],["body","\n\n"],["body","\n"],["body","最好使用群集更新设置API设置动态的群集范围设置，"],["body","\n"],["body","\n"],["body","\n"],["body","并仅将elasticsearch.yml用于本地配置。"],["body","\n"],["body","\n"],["body","\n"],["body","使用群集更新设置API可确保所有节点上的设置相同。"],["body","\n"],["body","\n"],["body","\n"],["body","如果您不小心在不同节点上的elasticsearch.yml中配置了不同的设置，可能会很难注意到差异。"],["body","\n"],["body","\n\n"],["headingLink","static"],["heading","Static"],["body","\n"],["body","Static settings can only be configured on an unstarted or shut down node using elasticsearch.yml."],["body","\n\n"],["body","只能未启动时配置、无法运行时配置"],["body","\n"],["body","每个节点都要配置"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/配置/2.elasticsearch.yml核心配置.html"],["title","elasticsearch.yml核心配置.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["body","cluster.name: cluster1 \nnode.name: node1\nnode.master: true \nnode.data: true \ncluster.remote.connect: false \nnetwork.host: 172.17.0.17\nhttp.port: 9200\ntransport.port: 9300\ndiscovery.seed_hosts: [\"172.17.0.17:9300\"]\ncluster.initial_master_nodes: [\"172.17.0.17:9300\"]\n"],["body","\n\n"],["body","cluster.name: 集群名称，唯一确定一个集群。"],["body","\n"],["body","node.name：节点名称，一个集群中的节点名称是唯一固定的，不同节点不能同名。"],["body","\n"],["body","node.master: 主节点属性值"],["body","\n"],["body","node.data: 数据节点属性值"],["body","\n"],["body","network.host： 本节点的ip"],["body","\n"],["body","http.port: 本节点的http端口"],["body","\n"],["body","transport.port：9300——集群之间通信的端口，若不指定默认：9300"],["body","\n"],["body","discovery.seed_hosts:节点发现需要配置一些种子节点，与7.X之前老版本：disvoery.zen.ping.unicast.hosts类似，一般配置集群中的全部节点"],["body","\n"],["body","cluster.initial_master_nodes：指定集群初次选举中用到的具有主节点资格的节点，称为集群引导，只在第一次形成集群时需要。"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/配置/3.es系统参数修改.html"],["title","es系统参数修改.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","修改文件描述符数目"],["heading","修改文件描述符数目"],["body","\n\n"],["body","Elasticsearch 在节点和 HTTP 客户端之间进行通信也使用了大量的套接字（注：sockets）。 所有这一切都需要足够的文件描述符。"],["body","\n"],["body","许多现代的 Linux 发行版本，每个进程默认允许一个微不足道的 1024 文件描述符。这对一个小的 Elasticsearch 节点来说实在是太低了，更不用说一个处理数以百计索引的节点。"],["body","\n\n"],["headingLink","设置环境变量"],["heading","设置环境变量"],["body","\n"],["body","vim /etc/profile\nulimit -n 65535\nsource /etc/profile\n"],["body","\n"],["headingLink","修改limitsconf配置文件"],["heading","修改limits.conf配置文件"],["body","\n"],["body","vim /etc/security/limits.conf\n\n\n* soft nofile 65536\n* hard nofile 65536\n"],["body","\n"],["body","ulimit -a"],["body","\n"],["headingLink","修改-最大映射数量-mmp"],["heading","修改 最大映射数量 MMP"],["body","\n"],["body","Elasticsearch 对各种文件混合使用了 NioFs（ 非阻塞文件系统）和 MMapFs （ 内存映射文件系统）。"],["body","\n"],["body","请确保你配置的最大映射数量，以便有足够的虚拟内存可用于 mmapped 文件。这可以暂时设置："],["body","\n"],["body","sysctl -w vm.max_map_count=262144\n"],["body","\n"],["body"," /etc/sysctl.conf\n \n vm.max_map_count=262144\n \n  sysctl -p \n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/分片内部原理.html"],["title","分片内部原理.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","分片内部原理"],["heading","分片内部原理"],["body","\n"],["body","分片是 最小的 工作单元 ，但是究竟什么 是 一个分片，它是如何工作的？ 在这个章节，我们回答以下问题:"],["body","\n\n"],["body","为什么搜索是 NRT （近实时）"],["body","\n"],["body","为什么 CRUD 是实时的"],["body","\n"],["body","如何保证断电数据不丢失"],["body","\n"],["body","optimize、reflush、flush API  都做了什么，在什么情况下使用"],["body","\n\n"],["headingLink","lucene-和-es"],["heading","Lucene 和 ES"],["body","\n"],["body","Lucene"],["body","\n\n"],["body","Lucene 是 Elasticsearch所基于的 Java 库，它引入了按段搜索的概念。"],["body","\n"],["body","Segment： 段，类似于倒排索引，相当于一个数据集。"],["body","\n"],["body","Commit point：提交点，记录着所有已知的段。"],["body","\n"],["body","Lucene index： a collection of segments plus a commit point。由一堆 Segment 的集合加上一个提交点组成。"],["body","\n\n"],["body","ES"],["body","\n\n"],["body","一个 Elasticsearch Index 由一个或者多个 shard （分片） 组成。"],["body","\n"],["body","而 Lucene 中的 Lucene index 相当于 ES 的一个 shard。"],["body","\n\n"],["headingLink","段写入过程"],["heading","段写入过程"],["body","\n"],["headingLink","内存-磁盘"],["heading","内存->磁盘"],["body","\n\n"],["body","不断将 Document 写入到 In-memory buffer （内存缓冲区）。"],["body","\n"],["body","当满足一定条件后内存缓冲区中的 Documents 刷新到磁盘。"],["body","\n"],["body","生成新的 segment 以及一个 Commit point 提交点。"],["body","\n\n"],["body","这个 segment 就可以像其他 segment 一样被读取了。"],["body","\n"],["body","问题：将文件刷新到磁盘是非常耗费资源的，而且在内存缓冲区和磁盘中间存在一个高速缓存（cache），一旦文件进入到 cache 就可以像磁盘上的 segment 一样被读取了。"],["body","\n"],["headingLink","内存-高速缓存区-磁盘"],["heading","内存->高速缓存区->磁盘"],["body","\n\n"],["body","不断将 Document 写入到 In-memory buffer （内存缓冲区）。"],["body","\n"],["body","当满足一定条件后内存缓冲区中的 Documents 刷新到 高速缓存（cache）。"],["body","\n"],["body","生成新的 segment ，这个 segment 还在 cache 中。"],["body","\n"],["body","这时候还没有 commit ，但是已经可以被读取了。"],["body","\n\n"],["body","准实时"],["body","\n\n"],["body","数据从 buffer 到 cache 的过程是定期每秒刷新一次。所以新写入的 Document 最慢 1 秒就可以在 cache 中被搜索到。"],["body","\n"],["body","从 内存缓存到 磁盘高速缓存 过程叫 refresh、一般是1秒1次"],["body","\n\n"],["body","使文档立即可见："],["body","\n"],["body","PUT /test/_doc/1?refresh\nPUT /test/_doc/2?refresh=true\n"],["body","\n"],["body","\n"],["headingLink","使文本可搜索"],["heading","使文本可搜索"],["body","\n"],["body","\n"],["body","第一个要解决的问题是：使文本可搜索"],["body","\n"],["body","\n"],["body","倒排索引包含一个有序列表，有序列表包含所有文档中出现的不重复的词项，每个词项都包含它所出现文档的位置列表"],["body","\n"],["body","Term  | Doc 1 | Doc 2 | Doc 3 | ...\n------------------------------------\nbrown |   X   |       |  X    | ...\nfox   |   X   |   X   |  X    | ...\nquick |   X   |   X   |       | ...\nthe   |   X   |       |  X    | ...\n"],["body","\n"],["body","\n"],["body","我们讨论倒排索引时，由于历史原因，用来针对整个非结构化文档进行标引，而elasticSearch 是存在结构化的JSON文档，所以，针对elasticSearch 的每一个索引字段 都有自己的倒排索引"],["body","\n"],["body","\n"],["headingLink","不变性"],["heading","不变性"],["body","\n"],["body","倒排索引写入磁盘后，是不可变的，不变性有价值："],["body","\n\n"],["body","无锁、没有数据竞争"],["body","\n"],["body","增加操作系统缓存性能"],["body","\n"],["body","利于其他缓存（filter）"],["body","\n"],["body","存储数据可以压缩"],["body","\n\n"],["body","当然，一个不变的索引也有不好的地方。主要事实是它是不可变的! 你不能修改它。如果你需要让一个新的文档 可被搜索，你需要重建整个索引。这要么对一个索引所能包含的数据量造成了很大的限制，要么对索引可被更新的频率造成了很大的限制。"],["body","\n"],["headingLink","动态更新索引"],["heading","动态更新索引"],["body","\n"],["body","\n"],["body","下一个要解决的问题是：如何保证在保证不变性的前提下，更新倒排索引：解决办法是使用更多索引"],["body","\n"],["body","\n"],["body","通过增加新的倒排索引 来反映 最近的变化。而不是重写整个倒排索引，每个倒排索引都会被查询到，从最早的开始，查询完后对结果集进行合并"],["body","\n"],["body","Elasticsearch 基于 Lucene, 这个 java 库引入了 按段搜索 的概念：每段本身是一个倒排索引"],["body","\n"],["body","Luence 还增加了提交点的概念：列出了所有已知段的 文件"],["body","\n"],["body","一个 Lucene 索引包含一个提交点和三个段"],["body","\n"],["body","\n"],["body","每个段跟提交点 都是一个文件"],["body","\n"],["body","逐段搜索会以如下流程进行工作："],["body","\n\n"],["body","新文档被收集到内存索引缓存"],["body","\n"],["body","不时地, 缓存被 提交 ：\n\n"],["body","一个新的段—一个追加的倒排索引—被写入磁盘。"],["body","\n"],["body","一个新的包含新段名字的 提交点 被写入磁盘。"],["body","\n"],["body","磁盘进行 同步 — 所有在文件系统缓存中等待的写入都刷新到磁盘，以确保它们被写入物理文件。"],["body","\n\n"],["body","\n"],["body","新的段被开启，让它包含的文档可见以被搜索。"],["body","\n"],["body","内存缓存被清空，等待接收新的文档。"],["body","\n\n"],["body","当一个查询被触发，所有已知的段按顺序被查询。"],["body","\n"],["body","词项统计会对所有段的结果进行聚合，以保证每个词和每个文档的关联都被准确计算。"],["body","\n"],["body","这种方式可以用相对较低的成本将新文档添加到索引。"],["body","\n"],["headingLink","删除和更新"],["heading","删除和更新"],["body","\n\n"],["body","\n"],["body","segment 不可改变，所以 docment 并不能从之前的 segment 中移除或更新。"],["body","\n"],["body","\n"],["body","\n"],["body","所以每次 commit， 生成 commit point 时，会有一个 .del 文件，里面会列出被删除的 document（逻辑删除）。 而查询时，获取到的结果在返回前会经过 .del 过滤。"],["body","\n"],["body","\n"],["body","\n"],["body","更新时，也会标记旧的 docment 被删除，写入到 .del 文件，同时会写入一个新的文件。此时查询会查询到两个版本的数据，但在返回前会被移除掉一个。"],["body","\n"],["body","\n\n"],["headingLink","近实时搜索"],["heading","近实时搜索"],["body","\n"],["body","背景"],["body","\n"],["body","按段搜索 使得一个文档 从 索引到可搜索 的延迟 显著降低，通常需要几分钟，这还不够快。"],["body","\n"],["body","原因"],["body","\n"],["body","磁盘在这里成了瓶颈："],["body","\n"],["body","commit 一个新段到磁盘 需要一次 fsync fsync代价很大，每次索引都 fsync的话会造成性能问题"],["body","\n"],["body","解决办法"],["body","\n"],["body","在elasticSearch跟磁盘之间 是 文件系统缓存"],["body","\n"],["body","在内存索引缓存区中的文档会被先写入到一个新的段中，这里的新段会被先写入到文件系统缓存，这一步代价比较低，稍后再被刷新到磁盘，这一步代价较高"],["body","\n"],["body","只要文件在缓存中，就可以像其他文件一样被打开读取了"],["body","\n"],["body","refreshAPI"],["body","\n"],["body","在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 refresh"],["body","\n"],["body","默认情况下每个分片会每秒自动刷新一次"],["body","\n"],["body","这就是为什么我们说 Elasticsearch 是 近 实时搜索"],["body","\n"],["body","：文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。"],["body","\n"],["body","# 刷新（Refresh）所有的索引。\nPOST /_refresh \n# 只刷新（Refresh） `blogs` 索引\nPOST /blogs/_refresh \n"],["body","\n"],["body","当你在索引日志上，不关注实时性，但比较关注索引的速度，可以降低 刷新的频率"],["body","\n"],["body","PUT /my_logs\n{\n  \"settings\": {\n    \"refresh_interval\": \"30s\" \n  }\n}\n"],["body","\n"],["body","也可以在建立一个大型索引时，先关闭自动刷新"],["body","\n"],["body","PUT /my_logs/_settings\n{ \"refresh_interval\": -1 } \n\nPUT /my_logs/_settings\n{ \"refresh_interval\": \"1s\" } \n"],["body","\n"],["headingLink","持久化变更"],["heading","持久化变更"],["body","\n\n"],["body","\n"],["body","如果没有 fsync 将文件系统缓存刷到磁盘， 则不能保证数据在断电甚至是程序正常退出之后仍然存在。"],["body","\n"],["body","\n"],["body","\n"],["body","一次完整的提交 会将段刷到磁盘、并写入一个包含所有段列表的提交点，elasticSearch 在启动或重新打开一个索引的过程根据 这个提交点文件 判断哪些段属于当前分片"],["body","\n"],["body","\n\n"],["body","ElasticSearch 增加了 Translog 的概念，每次操作都会记录TransLog"],["body","\n"],["body","流程如下："],["body","\n\n"],["body","文档被索引、添加到内存缓存区中，并写入 translog"],["body","\n"],["body","分片每秒刷新（refresh）\n\n"],["body","内存缓存区的文档写入到新段中，且没有进行 fsync 操作"],["body","\n"],["body","段被打开、可搜索"],["body","\n"],["body","内存缓存区被清除"],["body","\n\n"],["body","\n"],["body","更多的文档被添加到 内存缓冲区和追加到事务日志中"],["body","\n"],["body","每隔一段时间 索引被 flush\n\n"],["body","所有内存缓冲区的文档被写入一个新的段"],["body","\n"],["body","缓冲区被清空"],["body","\n"],["body","提交点被写入硬盘"],["body","\n"],["body","文件系统 通过 fsync 刷新"],["body","\n"],["body","老的 translog 被删除"],["body","\n\n"],["body","\n\n"],["body","translog 提供所有还没有被刷到磁盘的操作的一个持久化纪录。"],["body","\n"],["body","当elasticSearch 启动时，会从磁盘中最后一个提交点去恢复已知的段：它会重放 translog 中 在这个提交点后发生的变更操作"],["body","\n"],["headingLink","translog事务日志写入过程"],["heading","Translog事务日志写入过程"],["body","\n\n"],["body","Document 不断写入到 In-memory buffer，此时也会追加 translog。"],["body","\n"],["body","当 buffer 中的数据每秒 refresh 到 cache 中时，translog 并没有进入到刷新到磁盘，是持续追加的。"],["body","\n"],["body","translog 每隔 5s 会 fsync 到磁盘。"],["body","\n"],["body","translog 会继续累加变得越来越大，当 translog 大到一定程度或者每隔一段时间，会执行 flush。"],["body","\n\n"],["headingLink","实时crud"],["heading","实时CRUD"],["body","\n"],["body","translog也提供 实时CRUD"],["body","\n"],["body","当你试着通过ID查询、更新、删除一个文档 它会先尝试 检查 translog 任何最近的变更"],["body","\n"],["headingLink","flushapi"],["heading","flushAPI"],["body","\n"],["body","执行一次提交、并截断translog 的行为在 elasticsearch 中被称作 一次 flush"],["body","\n"],["body","分片每30分钟自动刷新（flush）"],["body","\n"],["body","# 刷新（flush） blogs 索引。\nPOST /blogs/_flush \n# 刷新（flush）所有的索引并且并且等待所有刷新在返回前完成。\nPOST /_flush?wait_for_ongoing \n"],["body","\n"],["body","你很少需要自己手动执行 flush 操作；通常情况下，自动刷新就足够了。"],["body","\n"],["body","在重启节点或关闭索引之前执行 flush 有益于你的索引"],["body","\n"],["body","当 Elasticsearch 尝试恢复或重新打开一个索引， 它需要重放 translog 中所有的操作，所以如果日志越短，恢复越快。"],["body","\n"],["headingLink","translog-有多安全"],["heading","Translog 有多安全?"],["body","\n"],["body","translog 的目的是保证操作不会丢失。这引出了这个问题： Translog 有多安全？"],["body","\n"],["body","translog的持久化"],["body","\n"],["body","默认 translog 是每 5 秒被 fsync 刷新到硬盘， 或者在每次写请求完成之后执行(e.g. index, delete, update, bulk)"],["body","\n"],["body","这意味着在整个请求被 fsync 到主分片和复制分片的translog之前，你的客户端不会得到一个 200 OK 响应。"],["body","\n"],["body","在每次请求后都执行一个 fsync 会带来一些性能损失，尽管实践表明这种损失相对较小（特别是bulk导入，它在一次请求中平摊了大量文档的开销）。"],["body","\n"],["body","但是对于一些大容量的偶尔丢失几秒数据问题也并不严重的集群，使用异步的 fsync 还是比较有益的。比如，写入的数据被缓存到内存中，再每5秒执行一次 fsync 。"],["body","\n"],["body","这个行为可以通过设置 durability 参数为 async 来启用："],["body","\n"],["body","PUT /my_index/_settings\n{\n    \"index.translog.durability\": \"async\",\n    \"index.translog.sync_interval\": \"5s\"\n}\n"],["body","\n"],["body","这个选项可以针对索引单独设置，并且可以动态进行修改。如果你决定使用异步 translog 的话，你需要 保证 在发生crash时，丢失掉 sync_interval 时间段的数据也无所谓。请在决定前知晓这个特性。"],["body","\n"],["body","如果你不确定这个行为的后果，最好是使用默认的参数（ \"index.translog.durability\": \"request\" ）来避免数据丢失。"],["body","\n"],["headingLink","段合并"],["heading","段合并"],["body","\n\n"],["body","每 1s 执行一次 refresh 都会将内存中的数据创建一个 segment。"],["body","\n"],["body","segment 数目太多会带来较大的麻烦。 每一个 segment 都会消耗文件句柄、内存和cpu运行周期。更重要的是，每个搜索请求都必须轮流检查每个 segment ；所以 segment 越多，搜索也就越慢。"],["body","\n\n"],["body","在 ES 后台会有一个线程进行 segment 合并。"],["body","\n\n"],["body","refresh操作会创建新的 segment 并打开以供搜索使用。"],["body","\n"],["body","合并进程选择一小部分大小相似的 segment，并且在后台将它们合并到更大的 segment 中。这并不会中断索引和搜索。"],["body","\n"],["body","当合并结束，老的 segment 被删除 说明合并完成时的活动："],["body","\n"],["body","新的 segment 被刷新（flush）到了磁盘。 写入一个包含新 segment 且排除旧的和较小的 segment的新 commit point。"],["body","\n"],["body","新的 segment 被打开用来搜索。"],["body","\n"],["body","老的 segment 被删除。"],["body","\n\n"],["body","Elasticsearch通过在后台进行段合并来解决这个问题。小的段被合并到大的段，然后这些大的段再被合并到更大的段。"],["body","\n"],["body","段合并的时候会将那些旧的已删除文档从文件系统中清除。被删除的文档（或被更新文档的旧版本）不会被拷贝到新的大段中。"],["body","\n"],["body","合并大的段需要消耗大量的I/O和CPU资源，如果任其发展会影响搜索性能。Elasticsearch在默认情况下会对合并流程进行资源限制"],["body","\n"],["headingLink","optimize-api"],["heading","optimize API"],["body","\n"],["body","optimize API大可看做是 强制合并 API。它会将一个分片强制合并到 max_num_segments 参数指定大小的段数目。 这样做的意图是减少段的数量（通常减少到一个），来提升搜索性能。"],["body","\n"],["body","在特定情况下，使用 optimize API 颇有益处。例如在日志这种用例下，每天、每周、每月的日志被存储在一个索引中。 老的索引实质上是只读的；它们也并不太可能会发生变化。"],["body","\n"],["body","在这种情况下，使用optimize优化老的索引，将每一个分片合并为一个单独的段就很有用了；这样既可以节省资源，也可以使搜索更加快速："],["body","\n"],["body","POST /logstash-2014-10/_optimize?max_num_segments=1 \n"],["body","\n"],["body","请注意，使用 optimize API 触发段合并的操作不会受到任何资源上的限制。这可能会消耗掉你节点上全部的I/O资源, 使其没有余裕来处理搜索请求，从而有可能使集群失去响应。 如果你想要对索引执行 optimize，你需要先使用分片分配（查看 迁移旧索引）把索引移到一个安全的节点，再执行。"],["body","\n"],["headingLink","总结"],["heading","总结"],["body","\n"],["body","主要介绍了内部写入和删除的过程，需要了解 refresh、fsync、flush、.del、segment merge 等名词的具体含义"],["body","\n"],["body","\n"],["body","参考"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/9.语义化检索/ES8.X语义检索.html"],["title","ES8.X语义检索.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","es8x语义检索"],["heading","ES8.X语义检索"],["body","\n"],["headingLink","dense-vector-field-type"],["heading","Dense vector field type"],["body","\n\n"],["body","\n"],["body","dense_vector 字段类型存储稠密的数值向量。稠密向量字段主要用于k最近邻（kNN）搜索。"],["body","\n"],["body","\n"],["body","\n"],["body","dense_vector类型不支持聚合或排序。"],["body","\n"],["body","\n"],["body","\n"],["body","您可以将dense_vector字段添加为基于float类型的数值数组，默认情况下为element_type。"],["body","\n"],["body","\n\n"],["body","PUT my-index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 3\n      },\n      \"my_text\" : {\n        \"type\" : \"keyword\"\n      }\n    }\n  }\n}\n\nPUT my-index/_doc/1\n{\n  \"my_text\" : \"text1\",\n  \"my_vector\" : [0.5, 10, 6]\n}\n\nPUT my-index/_doc/2\n{\n  \"my_text\" : \"text2\",\n  \"my_vector\" : [-0.5, 10, 10]\n}\n\n"],["body","\n"],["body","与大多数其他数据类型不同，稠密向量始终是单值的。在一个稠密向量字段中不可能存储多个数值。"],["body","\n"],["headingLink","index-vectors-for-knn-search"],["heading","Index vectors for kNN search"],["body","\n\n"],["body","\n"],["body","k最近邻（kNN）搜索会找到与查询向量最相似的k个向量。"],["body","\n"],["body","\n"],["body","\n"],["body","稠密向量字段可用于在script_score查询中对文档进行排名。这样你就可以通过扫描所有文档并根据相似度对它们进行排名来执行蛮力kNN(force kNN)搜索。"],["body","\n"],["body","\n"],["body","\n"],["body","在许多情况下，蛮力kNN搜索效率不够高。因此，dense_vector类型支持将向量索引到专门的数据结构中，以通过搜索API中的knn选项支持快速kNN检索。"],["body","\n"],["body","\n"],["body","\n"],["body","浮点元素大小在128到4096之间的未映射数组字段会动态映射为带有默认余弦相似度的dense_vector。您可以通过将字段显式映射为具有所需相似度的dense_vector来覆盖默认相似度。"],["body","\n"],["body","\n\n"],["body","稠密向量字段默认启用索引。启用索引后，您可以定义在kNN搜索中使用的向量相似度。"],["body","\n"],["body","PUT my-index-2\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 3,\n        \"similarity\": \"dot_product\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","为了进行近似k最近邻搜索的向量索引是一个昂贵的过程。对包含启用索引的向量字段的文档进行摄取可能需要相当长的时间。了解更多关于内存需求的内容，请参阅k最近邻（kNN）搜索。"],["body","\n"],["body","关闭索引向量"],["body","\n"],["body","PUT my-index-2\n{\n  \"mappings\": {\n    \"properties\": {\n      \"my_vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 3,\n        \"index\": false\n      }\n    }\n  }\n}\n"],["body","\n"],["body","Elasticsearch uses the HNSW algorithm to support efficient kNN search. Like most kNN algorithms, HNSW is an approximate method that sacrifices result accuracy for improved speed."],["body","\n"],["headingLink","parameters-for-dense-vector-fields"],["heading","Parameters for dense vector fields"],["body","\n"],["headingLink","element_type"],["heading","element_type："],["body","\n"],["body","（可选，字符串）用于编码向量的数据类型。支持的数据类型为float（默认）和byte。float为每个维度索引一个4字节的浮点值。byte为每个维度索引一个1字节的整数值。使用byte可以显著减小索引大小，但牺牲了精度。使用byte的向量需要在-128到127之间具有整数值的维度，包括索引和搜索。"],["body","\n"],["headingLink","dims"],["heading","dims："],["body","\n"],["body","（可选，整数）向量维度数。不能超过4096。如果未指定dims，它将被设置为添加到字段的第一个向量的长度。"],["body","\n"],["headingLink","index"],["heading","index："],["body","\n"],["body","是否索引"],["body","\n"],["headingLink","similarity"],["heading","similarity："],["body","\n"],["body","相似度计算方式"],["body","\n"],["body","（可选*，字符串）在kNN搜索中使用的向量相似度度量。文档根据它们的向量字段与查询向量的相似度进行排名。每个文档的_score将根据相似度推导出来，以确保分数为正，并且较大的分数对应较高的排名。默认为余弦相似度。"],["body","\n"],["body","l2_norm：基于向量之间的L2距离（也称为欧氏距离）计算相似度。文档的_score计算公式为1 / (1 + l2_norm(query, vector)^2)。"],["body","\n"],["body","dot_product：计算两个单位向量的点积。此选项提供了执行余弦相似度的优化方法。约束和计算得分由element_type定义。"],["body","\n"],["body","当element_type为float时，所有向量必须是单位长度，包括文档和查询向量。文档的_score计算公式为(1 + dot_product(query, vector)) / 2。"],["body","\n"],["body","当element_type为byte时，所有向量必须具有相同的长度，包括文档和查询向量，否则结果将不准确。文档的_score计算公式为0.5 + (dot_product(query, vector) / (32768 * dims))，其中dims是每个向量的维数。"],["body","\n"],["body","cosine：计算余弦相似度。请注意，执行余弦相似度的最有效方法是将所有向量归一化为单位长度，然后使用点积。只有在需要保留原始向量并且无法提前归一化它们时，才应使用余弦相似度。文档的_score计算公式为(1 + cosine(query, vector)) / 2。余弦相似度不允许具有零大小的向量，因为在这种情况下余弦值未定义。"],["body","\n"],["body","max_inner_product：计算两个向量的最大内积。这类似于点积，但不要求向量归一化。这意味着每个向量的大小可以显著影响得分。文档的_score经过调整以防止出现负值。对于max_inner_product值小于0的情况，_score为1 / (1 + -1 * max_inner_product(query, vector))。对于非负max_inner_product的结果，_score计算为max_inner_product(query, vector) + 1。"],["body","\n"],["body","注意：虽然它们在概念上相关，但相似度参数与文本字段相似度不同，并接受一组不同的选项。"],["body","\n"],["headingLink","index_options"],["heading","index_options"],["body","\n"],["body","配置kNN索引算法的可选部分。HNSW算法有两个内部参数，影响数据结构的构建方式。可以调整这些参数以提高结果的准确性，但会降低索引速度。提供index_options时，必须定义其所有属性。"],["body","\n"],["body","type："],["body","\n"],["body","**m：**knn算法的类型、只有 hnsw支持"],["body","\n"],["headingLink","k-nearest-neighbor-knn-search"],["heading","k-nearest neighbor (kNN) search"],["body","\n"],["body","k-最近邻（kNN）搜索会找出与查询向量最接近的k个向量，通过相似度度量来衡量。"],["body","\n"],["body","kNN的常见用途包括："],["body","\n\n"],["body","基于自然语言处理（NLP）算法的相关性排名"],["body","\n"],["body","产品推荐和推荐引擎"],["body","\n"],["body","图像或视频的相似度搜索"],["body","\n\n"],["headingLink","prerequisites"],["heading","Prerequisites"],["body","\n"],["body","要运行kNN搜索，您必须能够将数据转换为有意义的向量值。您可以使用Elasticsearch中的自然语言处理（NLP）模型创建这些向量，或者在Elasticsearch之外生成它们。向量可以作为dense_vector字段值添加到文档中。查询被表示为具有相同维度的向量。"],["body","\n"],["body","设计您的向量，使得文档向量根据相似度度量越接近查询向量，匹配度就越好。"],["body","\n"],["headingLink","knn-methods"],["heading","kNN methods"],["body","\n"],["body","Elasticsearch支持两种kNN搜索方法："],["body","\n\n"],["body","使用knn搜索选项的近似kNN"],["body","\n"],["body","使用带有向量函数的script_score查询的精确的暴力kNN"],["body","\n\n"],["body","在大多数情况下，您会希望使用近似kNN。近似kNN提供更低的延迟，但索引速度较慢并且准确度不完美。"],["body","\n"],["body","精确的暴力kNN保证准确的结果，但在处理大型数据集时扩展性不佳。采用这种方法，script_score查询必须扫描每个匹配的文档来计算向量函数，这可能导致搜索速度较慢。但是，您可以通过使用查询来限制传递给函数的匹配文档数量来改善延迟。如果将数据过滤到文档的一个小子集，您可以通过这种方法获得良好的搜索性能。"],["body","\n"],["headingLink","approximate-knn"],["heading","Approximate kNN"],["body","\n"],["body","临近KNN算法有资源要求、特别的是：所有向量数据必须 合适 节点的 page cache 以使之高效：请参考： approximate kNN search tuning guide 调优"],["body","\n"],["body","要运行近似kNN搜索，请使用knn选项来搜索一个或多个启用了索引的dense_vector字段。"],["body","\n"],["headingLink","使用knn搜索"],["heading","使用KNN搜索"],["body","\n"],["body","明确定义一个或多个dense_vector字段。近似kNN搜索需要以下映射选项："],["body","\n"],["body","PUT image-index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"image-vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 3,\n        \"index\": true,\n        \"similarity\": \"l2_norm\"\n      },\n      \"title-vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 5,\n        \"index\": true,\n        \"similarity\": \"l2_norm\"\n      },\n      \"title\": {\n        \"type\": \"text\"\n      },\n      \"file-type\": {\n        \"type\": \"keyword\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","POST image-index/_bulk?refresh=true\n{ \"index\": { \"_id\": \"1\" } }\n{ \"image-vector\": [1, 5, -20], \"title-vector\": [12, 50, -10, 0, 1], \"title\": \"moose family\", \"file-type\": \"jpg\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"image-vector\": [42, 8, -15], \"title-vector\": [25, 1, 4, -12, 2], \"title\": \"alpine lake\", \"file-type\": \"png\" }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"image-vector\": [15, 11, 23], \"title-vector\": [1, 5, 25, 50, 20], \"title\": \"full moon\", \"file-type\": \"jpg\" }\n...\n"],["body","\n"],["body","Run the search using the knn option."],["body","\n"],["body","POST image-index/_search\n{\n  \"knn\": {\n    \"field\": \"image-vector\",\n    \"query_vector\": [-5, 9, -12],\n    \"k\": 10,\n    \"num_candidates\": 100\n  },\n  \"fields\": [ \"title\", \"file-type\" ]\n}\n"],["body","\n"],["body","注意：在8.0版本中添加了对近似kNN搜索的支持。在此之前，dense_vector字段不支持在映射中启用索引。如果在8.0版本之前创建了包含dense_vector字段的索引，那么为了支持近似kNN搜索，数据必须使用设置index: true的新字段映射进行重新索引。"],["body","\n"],["headingLink","tune-approximate-knn-for-speed-or-accuracy"],["heading","Tune approximate kNN for speed or accuracy"],["body","\n"],["body","KNN调优"],["body","\n"],["body","为了收集结果，kNN搜索API在每个分片上找到了num_candidates个近似最近邻候选。搜索计算这些候选向量与查询向量的相似性，从每个分片中选择k个最相似的结果。然后搜索合并每个分片的结果，返回全局前k个最近邻。"],["body","\n"],["body","您可以增加num_candidates以获得更准确的结果，但搜索速度会变慢。num_candidates值较高的搜索考虑了每个分片中更多的候选。这需要更多时间，但搜索有更高的概率找到真正的前k个最近邻。"],["body","\n"],["body","同样，您可以减少num_candidates以获得更快的搜索，但结果可能不太准确。"],["body","\n"],["headingLink","approximate-knn-using-byte-vectors"],["heading","Approximate kNN using byte vectors"],["body","\n"],["body","字节向量KNN搜索"],["body","\n"],["body","近似kNN搜索API支持字节值向量和浮点值向量。使用knn选项来搜索一个element_type设置为byte且启用索引的dense_vector字段。"],["body","\n"],["body","创建Mapping"],["body","\n"],["body","PUT byte-image-index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"byte-image-vector\": {\n        \"type\": \"dense_vector\",\n        \"element_type\": \"byte\",\n        \"dims\": 2,\n        \"index\": true,\n        \"similarity\": \"cosine\"\n      },\n      \"title\": {\n        \"type\": \"text\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","索引数据"],["body","\n"],["body","POST byte-image-index/_bulk?refresh=true\n{ \"index\": { \"_id\": \"1\" } }\n{ \"byte-image-vector\": [5, -20], \"title\": \"moose family\" }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"byte-image-vector\": [8, -15], \"title\": \"alpine lake\" }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"byte-image-vector\": [11, 23], \"title\": \"full moon\" }\n"],["body","\n"],["body","向量检索"],["body","\n"],["body","POST byte-image-index/_search\n{\n  \"knn\": {\n    \"field\": \"byte-image-vector\",\n    \"query_vector\": [-5, 9],\n    \"k\": 10,\n    \"num_candidates\": 100\n  },\n  \"fields\": [ \"title\" ]\n}\n"],["body","\n"],["headingLink","filtered-knn-search"],["heading","Filtered kNN search"],["body","\n"],["body","kNN搜索API支持使用过滤器限制搜索。搜索将返回与过滤器查询匹配的前k个文档。"],["body","\n"],["body","以下请求执行了一个由文件类型字段进行筛选的近似kNN搜索："],["body","\n"],["body","POST image-index/_search\n{\n  \"knn\": {\n    \"field\": \"image-vector\",\n    \"query_vector\": [54, 10, -2],\n    \"k\": 5,\n    \"num_candidates\": 50,\n    \"filter\": {\n      \"term\": {\n        \"file-type\": \"png\"\n      }\n    }\n  },\n  \"fields\": [\"title\"],\n  \"_source\": false\n}\n"],["body","\n"],["body","注意："],["body","\n"],["body","在进行近似kNN搜索时应用过滤器，以确保返回k个匹配的文档。这与后过滤方法形成对比，后过滤是在近似kNN搜索完成后应用过滤器。后过滤的缺点是有时会返回少于k个结果，即使有足够的匹配文档。"],["body","\n"],["headingLink","combine-approximate-knn-with-other-features"],["heading","Combine approximate kNN with other features"],["body","\n"],["body","KNN混合检索"],["body","\n"],["body","POST image-index/_search\n{\n  \"query\": {\n    \"match\": {\n      \"title\": {\n        \"query\": \"mountain lake\",\n        \"boost\": 0.9\n      }\n    }\n  },\n  \"knn\": {\n    \"field\": \"image-vector\",\n    \"query_vector\": [54, 10, -2],\n    \"k\": 5,\n    \"num_candidates\": 50,\n    \"boost\": 0.1\n  },\n  \"size\": 10\n}\n"],["body","\n"],["body","这个搜索找到全局前k=5个向量匹配，将它们与匹配查询的结果结合起来，最后返回前10个得分最高的结果。knn和查询匹配通过一个分离操作进行组合，就好像你对它们进行布尔或操作一样。前k个向量结果代表了所有索引分片中全局最近的邻居。"],["body","\n"],["body","每个命中的得分是knn和查询得分的总和。您可以指定一个提升值来为总和中的每个得分赋予权重。在上面的例子中，得分将被计算为"],["body","\n"],["body","score = 0.9 * match_score + 0.1 * knn_score\n"],["body","\n"],["body","knn选项也可以与聚合一起使用。一般来说，Elasticsearch会计算所有匹配搜索条件的文档的聚合。因此，对于近似kNN搜索，聚合是在前k个最近的文档上计算的。如果搜索还包括查询，那么聚合将在knn和查询匹配的组合集上进行计算。"],["body","\n"],["headingLink","perform-semantic-search"],["heading","Perform semantic search"],["body","\n"],["body","\n"],["body","语义化检索"],["body","\n"],["body","\n\n"],["body","\n"],["body","kNN搜索使您能够使用先前部署的文本嵌入模型  text embedding model. 执行语义搜索。与在搜索词上进行字面匹配不同，语义搜索是根据搜索查询的意图和上下文含义检索结果。"],["body","\n"],["body","\n"],["body","\n"],["body","在内部，文本嵌入NLP模型会从您提供的输入查询字符串（称为model_text）生成一个密集向量。然后，它会与包含使用相同文本嵌入机器学习模型创建的密集向量的索引进行匹配。搜索结果是根据模型学习到的语义相似性。"],["body","\n"],["body","\n\n"],["body","注意"],["body","\n"],["body","要执行语义搜索："],["body","\n\n"],["body","您需要一个包含输入数据的密集向量表示的索引进行搜索，"],["body","\n"],["body","您必须使用与创建输入数据的密集向量相同的文本嵌入模型进行搜索，"],["body","\n"],["body","文本嵌入NLP模型部署必须已经启动。"],["body","\n\n"],["body","在query_vector_builder对象中引用已部署的文本嵌入模型或模型部署，并将搜索查询作为model_text提供。"],["body","\n"],["body","{\n  \"knn\": {\n    \"field\": \"dense-vector-field\",\n    \"k\": 10,\n    \"num_candidates\": 100,\n    \"query_vector_builder\": {\n      \"text_embedding\": { \n        \"model_id\": \"my-text-embedding-model\", \n        \"model_text\": \"The opposite of blue\" \n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","search-multiple-knn-fields"],["heading","Search multiple kNN fields"],["body","\n"],["body","多字段KNN检索"],["body","\n"],["body","POST image-index/_search\n{\n  \"query\": {\n    \"match\": {\n      \"title\": {\n        \"query\": \"mountain lake\",\n        \"boost\": 0.9\n      }\n    }\n  },\n  \"knn\": [ {\n    \"field\": \"image-vector\",\n    \"query_vector\": [54, 10, -2],\n    \"k\": 5,\n    \"num_candidates\": 50,\n    \"boost\": 0.1\n  },\n  {\n    \"field\": \"title-vector\",\n    \"query_vector\": [1, 20, -52, 23, 10],\n    \"k\": 10,\n    \"num_candidates\": 10,\n    \"boost\": 0.5\n  }],\n  \"size\": 10\n}\n"],["body","\n"],["body","该搜索找到图像向量的全局前k = 5个匹配项，以及标题向量的全局前k = 10个匹配项。然后将这些 TopValues 与匹配查询的匹配项组合，返回前10个文档。多个knn条目和查询匹配通过一个分离(disjunction)进行组合，就好像在它们之间进行了布尔或操作。前k个向量结果代表了所有索引分片中的全局最近邻居。"],["body","\n"],["body","具有上述配置的文档的评分为："],["body","\n"],["body","score = 0.9 * match_score + 0.1 * knn_score_image-vector + 0.5 * knn_score_title-vector"],["body","\n"],["headingLink","search-knn-with-expected-similarity"],["heading","Search kNN with expected similarity"],["body","\n"],["body","\n"],["body","避免经过filter过滤后，没有相似度匹配的文档。而导致  返回的K个文档相似度不高"],["body","\n"],["body","\n"],["body","尽管kNN是一个强大的工具，它总是试图返回k个最近的邻居。因此，当使用带有过滤器的knn时，您可能会过滤掉所有相关文档，只剩下一些无关的文档进行搜索。在这种情况下，knn仍会尽力返回k个最近的邻居，即使这些邻居在向量空间中可能相距甚远。"],["body","\n"],["body","为了缓解这种担忧，在knn子句中提供了一个相似度参数。该值是被视为匹配的向量所需的最小相似度。具有此参数的knn搜索流程如下："],["body","\n\n"],["body","\n"],["body","应用任何用户提供的过滤查询"],["body","\n"],["body","\n"],["body","\n"],["body","探索向量空间以获取k个向量"],["body","\n"],["body","\n"],["body","\n"],["body","不返回超出配置相似度的任何向量"],["body","\n"],["body","\n\n"],["body","这里有一个例子。在这个例子中，我们搜索给定的query_vector的k个最近邻居。但是，在应用过滤器并要求找到的向量之间至少具有提供的相似度的情况下。"],["body","\n"],["body","POST image-index/_search\n{\n  \"knn\": {\n    \"field\": \"image-vector\",\n    \"query_vector\": [1, 5, -20],\n    \"k\": 5,\n    \"num_candidates\": 50,\n    \"similarity\": 36,\n    \"filter\": {\n      \"term\": {\n        \"file-type\": \"png\"\n      }\n    }\n  },\n  \"fields\": [\"title\"],\n  \"_source\": false\n}\n"],["body","\n"],["body","在我们的数据集中，唯一具有PNG文件类型的文档具有向量[42, 8, -15]。[42, 8, -15]与[1, 5, -20]之间的L2范数距离为41.412，大于配置的相似度36。这意味着此搜索将不会返回任何结果。"],["body","\n"],["headingLink","nested-knn-search"],["heading","Nested kNN Search"],["body","\n"],["body","嵌套的KNN检索"],["body","\n"],["body","通常文本会超出特定模型的 token 限制，并需要在构建各个块的嵌入（向量）之前进行分块。在使用嵌套结构和dense_vector时，可以实现最近段落检索，而无需复制顶层文档元数据。"],["body","\n"],["body","PUT passage_vectors\n{\n    \"mappings\": {\n        \"properties\": {\n            \"full_text\": {\n                \"type\": \"text\"\n            },\n            \"creation_time\": {\n                \"type\": \"date\"\n            },\n            \"paragraph\": {\n                \"type\": \"nested\",\n                \"properties\": {\n                    \"vector\": {\n                        \"type\": \"dense_vector\",\n                        \"dims\": 2,\n                        \"index\": true,\n                        \"similarity\": \"cosine\"\n                    },\n                    \"text\": {\n                        \"type\": \"text\",\n                        \"index\": false\n                    }\n                }\n            }\n        }\n    }\n}\n"],["body","\n"],["headingLink","indexing-considerations"],["heading","Indexing considerations"],["body","\n"],["body","对于近似的 kNN 搜索，Elasticsearch 将每个段的密集向量值存储为 HNSW 图。由于构建这些图的成本很高，为了进行近似的 kNN 搜索，索引向量可能需要花费大量时间。您可能需要增加索引和批量请求的客户端请求超时时间。approximate kNN tuning guide  包含了关于索引性能和索引配置如何影响搜索性能的重要指导。"],["body","\n"],["body","除了其  搜索时调整参数之外，HNSW 算法还具有在索引时间的参数，可以在构建图的成本、搜索速度和准确性之间进行权衡。在设置密集向量映射时，您可以使用 index_options 参数来调整这些参数。"],["body","\n"],["body","PUT image-index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"image-vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 3,\n        \"index\": true,\n        \"similarity\": \"l2_norm\",\n        \"index_options\": {\n          \"type\": \"hnsw\",\n          \"m\": 32,\n          \"ef_construction\": 100\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","limitations-for-approximate-knn-search"],["heading","Limitations for approximate kNN search"],["body","\n\n"],["body","When using kNN search in cross-cluster search, the ccs_minimize_roundtrips option is not supported."],["body","\n"],["body","Elasticsearch uses the HNSW algorithm to support efficient kNN search. Like most kNN algorithms, HNSW is an approximate method that sacrifices result accuracy for improved search speed. This means the results returned are not always the true k closest neighbors."],["body","\n\n"],["body","**注意：**近似的 kNN 搜索总是使用 dfs_query_then_fetch 搜索类型，以便在各个分片上收集全局前 k 个匹配项。在运行 kNN 搜索时，您不能显式设置 search_type。"],["body","\n"],["headingLink","exact-knn"],["heading","Exact kNN"],["body","\n"],["body","\n"],["body","确定的KNN搜搜算法"],["body","\n"],["body","\n"],["body","要运行精确的 kNN 搜索，请使用带有向量函数的 script_score 查询。"],["body","\n\n"],["body","明确定义一个或多个 dense_vector 字段。如果您不打算将该字段用于近似 kNN 搜索，则省略索引映射选项或将其设置为 false。这样可以显著提高索引速度。"],["body","\n\n"],["body","使用流畅"],["body","\n"],["body","构建Mapping"],["body","\n"],["body","PUT product-index\n{\n  \"mappings\": {\n    \"properties\": {\n      \"product-vector\": {\n        \"type\": \"dense_vector\",\n        \"dims\": 5,\n        \"index\": false\n      },\n      \"price\": {\n        \"type\": \"long\"\n      }\n    }\n  }\n}\n"],["body","\n"],["body","索引数据"],["body","\n"],["body","POST product-index/_bulk?refresh=true\n{ \"index\": { \"_id\": \"1\" } }\n{ \"product-vector\": [230.0, 300.33, -34.8988, 15.555, -200.0], \"price\": 1599 }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"product-vector\": [-0.5, 100.0, -13.0, 14.8, -156.0], \"price\": 799 }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"product-vector\": [0.5, 111.3, -13.0, 14.8, -156.0], \"price\": 1099 }\n...\n"],["body","\n"],["body","Use the search API to run a script_score query containing a vector function."],["body","\n"],["body","为了限制传递给向量函数的匹配文档数量，我们建议您在 script_score.query 参数中指定一个过滤查询。如果需要，您可以在该参数中使用 match_all 查询来匹配所有文档。但是，匹配所有文档可能会显著增加搜索延迟。"],["body","\n"],["body","POST product-index/_search\n{\n  \"query\": {\n    \"script_score\": {\n      \"query\" : {\n        \"bool\" : {\n          \"filter\" : {\n            \"range\" : {\n              \"price\" : {\n                \"gte\": 1000\n              }\n            }\n          }\n        }\n      },\n      \"script\": {\n        \"source\": \"cosineSimilarity(params.queryVector, 'product-vector') + 1.0\",\n        \"params\": {\n          \"queryVector\": [-0.5, 90.0, -10, 14.8, -156.0]\n        }\n      }\n    }\n  }\n}\n"],["body","\n"],["headingLink","近似knn调优"],["heading","近似KNN调优"],["body","\n"],["body","Elasticsearch支持近似的k最近邻搜索，可以高效地找到查询向量的k个最近向量。由于近似kNN搜索与其他查询的工作方式不同，所以在性能方面有特殊考虑。"],["body","\n"],["body","这些建议中的许多都有助于提高搜索速度。对于近似kNN，索引算法在后台运行搜索以创建向量索引结构。因此，这些相同的建议也有助于提高索引速度。"],["body","\n"],["headingLink","prefer-dot_product-over-cosine"],["heading","Prefer dot_product over cosine"],["body","\n"],["body","\n"],["body","点积优先于cosin"],["body","\n"],["body","\n"],["body","在为近似kNN搜索索引向量时，您需要指定用于比较向量的相似性函数。如果您想通过余弦相似性比较向量，有两个选项。"],["body","\n"],["body","余弦选项接受任何浮点向量并计算余弦相似性。虽然这对测试很方便，但不是最有效的方法。相反，我们建议使用dot_product选项来计算相似性。要使用dot_product，所有向量都需要提前归一化为长度为1。dot_product选项要快得多，因为它在搜索过程中避免了执行额外的向量长度计算。"],["body","\n"],["headingLink","ensure-data-nodes-have-enough-memory"],["heading","Ensure data nodes have enough memory"],["body","\n"],["body","\n"],["body","确保节点有足够多的内存"],["body","\n"],["body","\n\n"],["body","\n"],["body","Elasticsearch使用HNSW算法进行近似kNN搜索。HNSW是一种基于图的算法，只有在大多数向量数据保存在内存中时才能有效工作。"],["body","\n"],["body","\n"],["body","\n"],["body","您应确保数据节点至少有足够的RAM来保存向量数据和索引结构。要检查向量数据的大小，您可以使用Analyze index disk usage API。作为一个粗略的经验法则，并假设默认的HNSW选项，使用的字节数将是  num_vectors * 4 * (num_dimensions + 12)。当使用字节element_type时，所需的空间将更接近于num_vectors * (num_dimensions + 12)。请注意，所需的RAM是用于文件系统缓存的，这与Java堆是分开的。"],["body","\n"],["body","\n"],["body","\n"],["body","数据节点还应为RAM需要的其他方式留出一些缓冲区。例如，您的索引可能还包括文本字段和数字字段，这些字段也受益于使用文件系统缓存。建议使用您的特定数据集运行基准测试，以确保有足够的内存来提供良好的搜索性能。您可以在这里和这里找到我们用于每晚基准测试的数据集和配置的一些示例。"],["body","\n"],["body","\n\n"],["headingLink","warm-up-the-filesystem-cache"],["heading","Warm up the filesystem cache"],["body","\n"],["body","\n"],["body","预热文件系统缓存"],["body","\n"],["body","\n"],["body","如果运行Elasticsearch的机器重新启动，文件系统缓存将会被清空，因此在操作系统将索引的热点区域加载到内存中以便搜索操作变得快速之前会需要一些时间。您可以使用index.store.preload  设置显式地告诉操作系统应该根据文件扩展名急切地加载哪些文件到内存中。"],["body","\n"],["body","在太多索引或太多文件上急切地将数据加载到文件系统缓存中，如果文件系统缓存不足以容纳所有数据，将会使搜索变慢。请谨慎使用。"],["body","\n"],["body","以下文件扩展名用于近似kNN搜索：\"vec\"（用于向量值），\"vex\"（用于HNSW图），\"vem\"（用于元数据）。"],["body","\n"],["headingLink","reduce-vector-dimensionality"],["heading","Reduce vector dimensionality"],["body","\n"],["body","\n"],["body","缩减向量维度"],["body","\n"],["body","\n"],["body","kNN搜索的速度与向量维度的数量成正比，因为每个相似性计算都考虑两个向量中的每个元素。在可能的情况下，最好使用维度较低的向量。一些嵌入模型有不同的“大小”，提供了维度较低和较高的选项。您还可以尝试使用主成分分析等降维技术。在尝试不同方法时，重要的是要衡量对相关性的影响，以确保搜索质量仍然可接受。"],["body","\n"],["headingLink","exclude-vector-fields-from-_source"],["heading","Exclude vector fields from _source"],["body","\n"],["body","\n"],["body","不保存 向量的_source"],["body","\n"],["body","\n"],["body","Elasticsearch在索引时会将原始的JSON文档存储在_source字段中。默认情况下，搜索结果中的每个命中都包含完整的文档_source。当文档包含高维稠密向量字段时，_source可能会相当大且加载成本高。这可能会显著降低kNN搜索的速度。"],["body","\n"],["body","您可以通过excludes映射参数禁用在_source中存储dense_vector字段。这可以防止在搜索期间加载和返回大向量，并减少索引大小。已从_source中省略的向量仍然可以用于kNN搜索，因为它依赖于单独的数据结构来执行搜索。在使用excludes参数之前，请确保审查省略_source字段的缺点。"],["body","\n"],["body","另一个选择是如果您的所有索引字段都支持，则使用合成的_source。"],["body","\n"],["headingLink","reduce-the-number-of-index-segments"],["heading","Reduce the number of index segments"],["body","\n"],["body","\n"],["body","减小段大小"],["body","\n"],["body","\n"],["body","Elasticsearch分片由段组成，段是索引中的内部存储元素。对于近似kNN搜索，Elasticsearch将每个段的向量值存储为单独的HNSW图，因此kNN搜索必须检查每个段。最近kNN搜索的并行化使得在多个段之间进行搜索变得更快，但如果段较少，kNN搜索仍然可能快几倍。默认情况下，Elasticsearch会定期通过后台合并过程将较小的段合并为较大的段。如果这不够用，您可以采取显式步骤来减少索引段的数量。"],["body","\n"],["headingLink","force-merge-to-one-segment"],["heading","Force merge to one segment"],["body","\n"],["body","force merge操作会强制执行索引合并。如果您将force merge到一个段，kNN搜索只需检查一个包含所有内容的HNSW图。强制合并dense_vector字段是一项昂贵的操作，可能需要较长时间才能完成。"],["body","\n"],["body","我们建议只对只读索引（即索引不再接收写入）执行force merge操作。当文档被更新或删除时，旧版本不会立即被移除，而是被软删除并标记为“墓碑”。这些软删除的文档会在常规段合并期间自动清理。但是force merge可能会导致产生非常大（> 5GB）的段，这些段不符合常规合并的条件。因此，软删除文档的数量可能会迅速增加，导致磁盘使用量增加和搜索性能变差。如果您经常对接收写入的索引执行force merge操作，这也会使快照变得更加昂贵，因为新文档无法进行增量备份。"],["body","\n"],["headingLink","create-large-segments-during-bulk-indexing"],["heading","Create large segments during bulk indexing"],["body","\n"],["body","\n"],["body","在批量构建过程中给ES索引创建大段"],["body","\n"],["body","\n"],["body","一个常见的模式是首先执行初始的批量上传，然后使索引可用于搜索。您可以调整索引设置，鼓励Elasticsearch创建更大的初始段，而不是进行force merge操作。"],["body","\n\n"],["body","确保在批量上传过程中没有搜索活动，并通过将index.refresh_interval设置为-1来禁用它。这可以防止刷新操作并避免创建额外的段。"],["body","\n"],["body","给Elasticsearch一个大的索引缓冲区，这样它在刷新之前可以接受更多的文档。默认情况下，indices.memory.index_buffer_size设置为堆大小的10%。对于像32GB这样大的堆大小，这通常足够了。为了允许完整的索引缓冲区被使用，您还应该增加限制  index.translog.flush_threshold_size。"],["body","\n\n"],["headingLink","avoid-heavy-indexing-during-searches"],["heading","Avoid heavy indexing during searches"],["body","\n"],["body","\n"],["body","避免在重度索引时，检索"],["body","\n"],["body","\n"],["body","主动索引文档可能会对近似kNN搜索性能产生负面影响，因为索引线程会从搜索中夺取计算资源。当同时进行索引和搜索时，Elasticsearch也会频繁刷新，这会创建几个小的段。这也会影响搜索性能，因为当有更多的段时，近似kNN搜索会变慢。"],["body","\n"],["body","如果可能的话，在进行近似kNN搜索时最好避免进行大量的索引操作。如果需要重新索引所有数据，也许是因为向量嵌入模型发生了变化，那么最好将新文档重新索引到一个单独的索引中，而不是就地更新它们。这有助于避免上述的减速，并且可以避免由于频繁的文档更新而导致昂贵的合并操作。"],["body","\n"],["headingLink","avoid-page-cache-thrashing-by-using-modest-readahead-values-on-linux"],["heading","Avoid page cache thrashing by using modest readahead values on Linux"],["body","\n"],["body","在Linux上使用适度的预读取数值来避免页面缓存抖动。"],["body","\n"],["body","搜索可能导致大量随机读取I/O。当底层块设备具有较高的预读取值时，可能会进行大量不必要的读取I/O，特别是在使用内存映射访问文件时（(see storage types).）。"],["body","\n"],["body","大多数Linux发行版对于单个普通设备使用了合理的预读取值，为128KiB，然而，当使用软件RAID、LVM或dm-crypt时，生成的块设备（支持Elasticsearch path.data）可能会具有非常大的预读取值（在几MiB的范围内）。这通常会导致严重的页面（文件系统）缓存抖动，从而对搜索（或更新）性能产生不利影响。"],["body","\n"],["body","你可以使用 lsblk -o NAME,RA,MOUNTPOINT,TYPE,SIZE 命令来检查当前的值（以KiB为单位）。请参考您所使用的发行版的文档，了解如何修改此值（例如，使用udev规则以在重新启动后保持持续性，或者使用 blockdev --setra 作为临时设置）。我们建议将预读取值设为128KiB。"],["body","\n"],["body","blockdev命令期望以512字节扇区为单位的值，而lsblk命令报告的值是以KiB为单位的。例如，要临时将/dev/nvme0n1的预读取设置为128KiB，可以指定blockdev --setra 256 /dev/nvme0n1。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","elasticSearch/评分标准与相关性.html"],["title","评分标准与相关性.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","elasticsearch相似度算法"],["heading","elasticSearch相似度算法"],["body","\n"],["body","检索词频率"],["body","\n\n"],["body","检索词 在 某个字段中 出现的频率，频率越高、相关性越高"],["body","\n\n"],["body","反向文档频率"],["body","\n\n"],["body","每个检索词 在 文档库中出现的频率，频率越低 相关性越高，频率越高相关性越低"],["body","\n\n"],["body","字段长度准则"],["body","\n\n"],["body","字段的长度越长、相关性越低"],["body","\n\n"],["headingLink","es的评分标准review"],["heading","es的评分标准review"],["body","\n"],["body","GET /_search?explain \n{\n   \"query\"   : { \"match\" : { \"tweet\" : \"honeymoon\" }}\n}\n"],["body","\n"],["body","\"_explanation\": { \n   \"description\": \"weight(tweet:honeymoon in 0)\n                  [PerFieldSimilarity], result of:\",\n   \"value\":       0.076713204, //总结\n   \"details\": [\n      {\n         \"description\": \"fieldWeight in 0, product of:\",\n         \"value\":       0.076713204,\n         \"details\": [\n            {  \n               \"description\": \"tf(freq=1.0), with freq of:\", //tf\n               \"value\":       1,\n               \"details\": [\n                  {\n                     \"description\": \"termFreq=1.0\",\n                     \"value\":       1\n                  }\n               ]\n            },\n            { \n               \"description\": \"idf(docFreq=1, maxDocs=1)\", //idf\n               \"value\":       0.30685282\n            },\n            { \n               \"description\": \"fieldNorm(doc=0)\", //字段长度准则\n               \"value\":        0.25,\n            }\n         ]\n      }\n   ]\n}\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","mysql/mysqlDataFile.html"],["title","mysqlDataFile.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","load-data-infile语法"],["heading","LOAD DATA INFILE（语法）"],["body","\n"],["body","\n"],["body","语法"],["body","\n"],["body","\n"],["body","LOAD DATA\n    [LOW_PRIORITY | CONCURRENT] [LOCAL]\n    INFILE 'file_name'\n    [REPLACE | IGNORE]\n    INTO TABLE tbl_name\n    [PARTITION (partition_name [, partition_name] ...)]\n    [CHARACTER SET charset_name]\n    [{FIELDS | COLUMNS}\n        [TERMINATED BY 'string']\n        [[OPTIONALLY] ENCLOSED BY 'char']\n        [ESCAPED BY 'char']\n    ]\n    [LINES\n        [STARTING BY 'string']\n        [TERMINATED BY 'string']\n    ]\n    [IGNORE number {LINES | ROWS}]\n    [(col_name_or_user_var\n        [, col_name_or_user_var] ...)]\n    [SET col_name={expr | DEFAULT}\n        [, col_name={expr | DEFAULT}] ...]\n"],["body","\n\n"],["body","LOAD DATA INFILE 语句以非常高的速度从文本文件中读取行到表中。"],["body","\n"],["body","可以从服务器主机或客户端主机读取文件，具体取决于是否给出了 local 修饰符。"],["body","\n"],["body","LOCAL also affects data interpretation and error handling.\n\n"],["body","也会影响数据解释 和 错误处理。"],["body","\n\n"],["body","\n"],["body","load_data 是   SELECT ... INTO OUTFILE.  的补充"],["body","\n\n"],["body","The mysqlimport utility provides another way to load data files; it operates by sending a LOAD DATA statement to the server. See Section 4.5.5, “mysqlimport — A Data Import Program”."],["body","\n"],["body","For information about the efficiency of INSERT versus LOAD DATA and speeding up LOAD DATA, see Section 8.2.4.1, “Optimizing INSERT Statements”."],["body","\n\n"],["body","Non-LOCAL Versus LOCAL Operation"],["body","\n"],["body","Input File Character Set"],["body","\n"],["body","Input File Location"],["body","\n"],["body","Security Requirements"],["body","\n"],["body","Duplicate-Key and Error Handling"],["body","\n"],["body","Index Handling"],["body","\n"],["body","Field and Line Handling"],["body","\n"],["body","Column List Specification"],["body","\n"],["body","Input Preprocessing"],["body","\n"],["body","Column Value Assignment"],["body","\n"],["body","Partitioned Table Support"],["body","\n"],["body","Concurrency Considerations"],["body","\n"],["body","Statement Result Information"],["body","\n"],["body","Replication Considerations"],["body","\n"],["body","Miscellaneous Topics"],["body","\n\n"],["headingLink","non-local-versus-local-operation本地与非本地"],["heading","Non-LOCAL Versus LOCAL Operation（本地与非本地）"],["body","\n"],["body","\n"],["body","本地非本地选项"],["body","\n"],["body","\n"],["body","The LOCAL modifier affects these aspects of LOAD DATA, compared to non-LOCAL operation:（local 非local 影响下面几个方面）"],["body","\n\n"],["body","\n"],["body","It changes the expected location of the input file; see Input File Location. （影响输入文件）"],["body","\n"],["body","\n"],["body","\n"],["body","It changes the statement security requirements; see Security Requirements.（影响安全策略）"],["body","\n"],["body","\n"],["body","\n"],["body","It has the same effect as the IGNORE modifier on the interpretation of input file contents and error handling; see Duplicate-Key and Error Handling, and Column Value Assignment.（影响 IGNORE 修饰符对 输入文件内容的解释和错误处理）"],["body","\n"],["body","\n"],["body","\n"],["body","LOCAL works only if the server and your client both have been configured to permit it. (服务端、客户端都必须要配置才能启用local)"],["body","\n"],["body","\n"],["body","\n"],["body","For example, if mysqld was started with the local_infile system variable disabled, LOCAL produces an error. See Section 6.1.6, “Security Considerations for LOAD DATA LOCAL”. （如果 mysqld 以 local_infile 禁用启动。则不能使用local）"],["body","\n"],["body","\n\n"],["headingLink","input-file-character-set输入文件的字符集"],["heading","Input File Character Set（输入文件的字符集）"],["body","\n"],["body","\n"],["body","输入文件的字符集"],["body","\n"],["body","\n\n"],["body","The file name must be given as a literal string. On Windows, specify backslashes in path names as forward slashes or doubled backslashes. The server interprets the file name using the character set indicated by the character_set_filesystem system variable.（服务端使用 character_set_filesystem 判断windows or Linux 来解析文件名）"],["body","\n"],["body","By default, the server interprets the file contents using the character set indicated by the character_set_database system variable. If the file contents use a character set different from this default, it is a good idea to specify that character set by using the CHARACTER SET clause. A character set of binary specifies no conversion\n\n"],["body","默认使用 character_set_database 解析文件。也可以使用 CHARACTER SET  字句指定字符集。"],["body","\n"],["body","binary 表示 无转换"],["body","\n\n"],["body","\n"],["body","SET NAMES and the setting of character_set_client do not affect interpretation of file contents."],["body","\n"],["body","LOAD DATA interprets all fields in the file as having the same character set, regardless of the data types of the columns into which field values are loaded. For proper interpretation of the file, you must ensure that it was written with the correct character set. For example, if you write a data file with mysqldump -T or by issuing a SELECT ... INTO OUTFILE statement in mysql, be sure to use a --default-character-set option to write output in the character set to be used when the file is loaded with LOAD DATA.\n\n"],["body","dump 与 load_data 要保持一致的字符集"],["body","\n\n"],["body","\n"],["body","It is not possible to load data files that use the ucs2, utf16, utf16le, or utf32 character set."],["body","\n\n"],["headingLink","input-file-location输入文件路径"],["heading","Input File Location（输入文件路径）"],["body","\n"],["body","\n"],["body","输入文件路径"],["body","\n"],["body","\n"],["body","These rules determine the LOAD DATA input file location:（load data 的输入文件 规则）"],["body","\n\n"],["body","\n"],["body","If LOCAL is not specified, the file must be located on the server host. The server reads the file directly, locating it as follows:（非local）"],["body","\n\n"],["body","If the file name is an absolute path name, the server uses it as given.（绝对路径名）"],["body","\n"],["body","If the file name is a relative path name with leading components, the server looks for the file relative to its data directory.（data 相对路径+ leading components）"],["body","\n"],["body","If the file name has no leading components, the server looks for the file in the database directory of the default database.（相对路径+database directory）"],["body","\n\n"],["body","\n"],["body","\n"],["body","If LOCAL is specified, the file must be located on the client host. The client program reads the file, locating it as follows:（local）"],["body","\n\n"],["body","If the file name is an absolute path name, the client program uses it as given.（client直接使用绝对路径）"],["body","\n"],["body","If the file name is a relative path name, the client program looks for the file relative to its invocation directory.（运行客户端的工作目录的相对地址）"],["body","\n"],["body","When LOCAL is used, the client program reads the file and sends its contents to the server. The server creates a copy of the file in the directory where it stores temporary files. See Section B.3.3.5, “Where MySQL Stores Temporary Files”. Lack of sufficient space for the copy in this directory can cause the LOAD DATA LOCAL statement to fail.\n\n"],["body","服务端会将本地的文件Copy一份。存在临时文件中。缺少空间则会导致 load data报错"],["body","\n\n"],["body","\n\n"],["body","\n"],["body","\n"],["body","The non-LOCAL rules mean that the server reads a file named as ./myfile.txt relative to its data directory, whereas it reads a file named as myfile.txt from the database directory of the default database. For example, if the following LOAD DATA statement is executed while db1 is the default database, the server reads the file data.txt from the database directory for db1, even though the statement explicitly loads the file into a table in the db2 database:"],["body","\n\n"],["body","非local 的 文件从默认数据库的 data目录取文件"],["body","\n\n"],["body","LOAD DATA INFILE 'data.txt' INTO TABLE db2.my_table;\n"],["body","\n"],["body","\n\n"],["headingLink","security-requirements安全考量"],["heading","Security Requirements（安全考量）"],["body","\n"],["body","\n"],["body","安全考量"],["body","\n"],["body","\n"],["body","For a non-LOCAL load operation, the server reads a text file located on the server host, so these security requirements must be satisified:"],["body","\n"],["body","（非本地加载，在服务机上读取文件，需要有安全措施）"],["body","\n\n"],["body","You must have the FILE privilege. See Section 6.2.2, “Privileges Provided by MySQL”."],["body","\n"],["body","The operation is subject to the secure_file_priv system variable setting:（受 secure_file_priv 控制）\n\n"],["body","If the variable value is a nonempty directory name, the file must be located in that directory.（值不为空，则文件必须在那个目录）"],["body","\n"],["body","If the variable value is empty (which is insecure), the file need only be readable by the server.（值为空，则需要文件对于MySQL服务器只读）"],["body","\n\n"],["body","\n"],["body","For a LOCAL load operation, the client program reads a text file located on the client host. Because the file contents are sent over the connection by the client to the server, using LOCAL is a bit slower than when the server accesses the file directly. On the other hand, you do not need the FILE privilege, and the file can be located in any directory the client program can access.（local操作 不需要文件权限，由于需要网络传输，可能会慢一点 ）"],["body","\n\n"],["headingLink","duplicate-key-and-error-handling唯一性key重复错误处理"],["heading","Duplicate-Key and Error Handling（唯一性Key重复、错误处理）"],["body","\n"],["body","\n"],["body","唯一性Key重复、错误处理"],["body","\n"],["body","\n"],["body","The REPLACE and IGNORE modifiers control handling of new (input) rows that duplicate existing table rows on unique key values (PRIMARY KEY or UNIQUE index values):"],["body","\n\n"],["body","With REPLACE, new rows that have the same value as a unique key value in an existing row replace the existing row. See Section 13.2.8, “REPLACE Statement”.（如果存在重复，则直接replace）"],["body","\n"],["body","With IGNORE, new rows that duplicate an existing row on a unique key value are discarded. For more information, see The Effect of IGNORE on Statement Execution.（如果存在重复，则直接丢弃）"],["body","\n\n"],["body","The LOCAL modifier has the same effect as IGNORE. This occurs because the server has no way to stop transmission of the file in the middle of the operation.（local 选项 也会直接忽略，IGNORE  。发生这种情况是 因为 服务器无法在操作过程中停止文件的传输 ）"],["body","\n"],["body","In addition to affecting duplicate-key handling as just described, IGNORE and LOCAL also affect error handling:（IGNORE、LOCAL 除了会影响 重复Key的处理之外，还会影响下列方面）"],["body","\n\n"],["body","With neither IGNORE nor LOCAL, data-interpretation errors terminate the operation.：不是IGNORE、LOCAL的情况下，数据解析错误会终止操作"],["body","\n"],["body","With IGNORE or LOCAL, data-interpretation errors become warnings and the load operation continues, even if the SQL mode is restrictive. For examples, see Column Value Assignment.\n\n"],["body","IGNORE or LOCAL 的情况下，数据解析错误 只会出现警告"],["body","\n\n"],["body","\n\n"],["headingLink","index-handling索引处理"],["heading","Index Handling（索引处理）"],["body","\n"],["body","\n"],["body","索引处理"],["body","\n"],["body","\n\n"],["body","To ignore foreign key constraints during the load operation, execute a SET foreign_key_checks = 0 statement before executing LOAD DATA.\n\n"],["body","通过执行 SET foreign_key_checks = 0 在 load过程中 忽略外键"],["body","\n\n"],["body","\n"],["body","If you use LOAD DATA on an empty MyISAM table, all nonunique indexes are created in a separate batch (as for REPAIR TABLE). Normally, this makes LOAD DATA much faster when you have many indexes. In some extreme cases, you can create the indexes even faster by turning them off with ALTER TABLE ... DISABLE KEYS before loading the file into the table and re-creating the indexes with ALTER TABLE ... ENABLE KEYS after loading the file. See Section 8.2.4.1, “Optimizing INSERT Statements”.\n\n"],["body","在空的MyISAM 存储引擎 的表上执行load data。 所有非唯一性索引  are created in a separate batch"],["body","\n"],["body","甚至可以 先禁用 索引创建。文件载入完成之后，在启用索引"],["body","\n\n"],["body","\n\n"],["headingLink","field-and-line-handling字段和行处理"],["heading","Field and Line Handling（字段和行处理）"],["body","\n"],["body","\n"],["body","字段和行处理"],["body","\n"],["body","\n"],["headingLink","概述"],["heading","概述"],["body","\n"],["body","For both the LOAD DATA and SELECT ... INTO OUTFILE statements, the syntax of the FIELDS and LINES clauses is the same. Both clauses are optional, but FIELDS must precede LINES if both are specified. （load data 跟   select into outfile 语句来说， fields、lines语法是一样的。fields在lines前面）"],["body","\n"],["body","If you specify a FIELDS clause, each of its subclauses (TERMINATED BY, [OPTIONALLY] ENCLOSED BY, and ESCAPED BY) is also optional, except that you must specify at least one of them. Arguments to these clauses are permitted to contain only ASCII characters."],["body","\n"],["body","（fields子句 terminated by 、 [optionally] enclosed by 、escaped by 。子句的参数 只能包含 ASCII字符)"],["body","\n"],["body","FIELDS TERMINATED BY '\\t' ENCLOSED BY '' ESCAPED BY '\\\\'\nLINES TERMINATED BY '\\n' STARTING BY ''\n"],["body","\n"],["headingLink","selecttodatafile-和-loaddata的默认行为"],["heading","SelectToDataFile 和 loadData的默认行为"],["body","\n"],["body","Backslash is the MySQL escape character within strings in SQL statements. Thus, to specify a literal backslash, you must specify two backslashes for the value to be interpreted as a single backslash. The escape sequences '\\t' and '\\n' specify tab and newline characters, respectively.（反斜杆是 MySQL SQL语句转义字符。需要字面量的反斜杠，则需要 双反斜杆）"],["body","\n"],["body","In other words, the defaults cause LOAD DATA to act as follows when reading input:"],["body","\n"],["body","（以下是load data的默认行为）"],["body","\n\n"],["body","Look for line boundaries at newlines. （换行符）"],["body","\n"],["body","Do not skip any line prefix.（不跳过任何行）"],["body","\n"],["body","Break lines into fields at tabs.（制表符分割字段）"],["body","\n"],["body","Do not expect fields to be enclosed within any quoting characters.（无任何引号字符串）"],["body","\n"],["body","Interpret characters preceded by the escape character \\ as escape sequences. For example, \\t, \\n, and \\\\ signify tab, newline, and backslash, respectively. See the discussion of FIELDS ESCAPED BY later for the full list of escape sequences."],["body","\n\n"],["body","Conversely, the defaults cause SELECT ... INTO OUTFILE to act as follows when writing output:（select into outfile 的默认行为，也是相应的）"],["body","\n\n"],["body","Write tabs between fields.（字段以制表符分隔）"],["body","\n"],["body","Do not enclose fields within any quoting characters.（没有任何引号字符）"],["body","\n"],["body","Use \\ to escape instances of tab, newline, or \\ that occur within field values.（使用反斜杆 转义 制表符。使用 双反斜杠转义反斜杆）"],["body","\n"],["body","Write newlines at the ends of lines.（换行符分割记录）"],["body","\n\n"],["body","\n"],["body","For a text file generated on a Windows system, proper file reading might require LINES TERMINATED BY '\\r\\n' because Windows programs typically use two characters as a line terminator. Some programs, such as WordPad, might use \\r as a line terminator when writing files. To read such files, use LINES TERMINATED BY '\\r'.（对于windows使用 \\r\\n、wordPad使用 \\r。 应该使用 lines terminated by '\\r'）"],["body","\n"],["body","\n"],["headingLink","前缀过滤符"],["heading","前缀过滤符"],["body","\n"],["body","If all the input lines have a common prefix that you want to ignore, you can use LINES STARTING BY 'prefix_string' to skip the prefix and anything before it. If a line does not include the prefix, the entire line is skipped. Suppose that you issue the following statement:（lines starting by 'xxxx'，忽略 指定字符串之前的 字符，并从下一个字符串开始计算。如果不包含前缀，则整行忽略）"],["body","\n"],["body","LOAD DATA INFILE '/tmp/test.txt' INTO TABLE test\n  FIELDS TERMINATED BY ','  LINES STARTING BY 'xxx';\n"],["body","\n"],["body","xxx\"abc\",1\nsomething xxx\"def\",2\n\"ghi\",3\n"],["body","\n"],["body","The resulting rows are (\"abc\",1) and (\"def\",2). The third row in the file is skipped because it does not contain the prefix."],["body","\n"],["body","The IGNORE number LINES clause can be used to ignore lines at the start of the file. For example, you can use IGNORE 1 LINES to skip an initial header line containing column names:）(忽略前几行)"],["body","\n"],["body","LOAD DATA INFILE '/tmp/test.txt' INTO TABLE test IGNORE 1 LINES;\n\n"],["body","\n"],["headingLink","多字符与单字符"],["heading","多字符与单字符"],["body","\n"],["body","Any of the field- or line-handling options can specify an empty string (''). If not empty, the FIELDS [OPTIONALLY] ENCLOSED BY and FIELDS ESCAPED BY values must be a single character. The FIELDS TERMINATED BY, LINES STARTING BY, and LINES TERMINATED BY values can be more than one character. For example, to write lines that are terminated by carriage return/linefeed pairs, or to read a file containing such lines, specify a LINES TERMINATED BY '\\r\\n' clause.（enclosed by 、escaped by 必须是一个字符。terminated by、starting by 可以是多字符）"],["body","\n"],["body","To read a file containing jokes that are separated by lines consisting of %%, you can do this"],["body","\n"],["body","CREATE TABLE jokes\n  (a INT NOT NULL AUTO_INCREMENT PRIMARY KEY,\n  joke TEXT NOT NULL);\n  \nLOAD DATA INFILE '/tmp/jokes.txt' INTO TABLE jokes\n  FIELDS TERMINATED BY ''\n  LINES TERMINATED BY '\\n%%\\n' (joke);\n"],["body","\n"],["body","If you specify OPTIONALLY, the ENCLOSED BY character is used only to enclose values from columns that have a string data type (such as CHAR, BINARY, TEXT, or ENUM):（optionally enclosed by 。只会包裹string data type）"],["body","\n"],["body","1,\"a string\",100.20\n2,\"a string containing a , comma\",102.20\n3,\"a string containing a \\\" quote\",102.20\n4,\"a string containing a \\\", quote and comma\",102.20\n"],["body","\n\n"],["body","\n"],["body","Occurrences of the ENCLOSED BY character within a field value are escaped by prefixing them with the ESCAPED BY character.（double 转义字符 则代表转义字符本身 ）"],["body","\n"],["body","\n"],["body","\n"],["body","Also, if you specify an empty ESCAPED BY value, it is possible to inadvertently generate output that cannot be read properly by LOAD DATA. For example, the preceding output just shown would appear as follows if the escape character is empty. Observe that the second field in the fourth line contains a comma following the quote, which (erroneously) appears to terminate the field:（没有转义符。会导致被数据呗错误截断）"],["body","\n\n"],["body","\n"],["body","1,\"a string\",100.20\n2,\"a string containing a , comma\",102.20\n3,\"a string containing a \" quote\",102.20\n4,\"a string containing a \", quote and comma\",102.20\n"],["body","\n"],["body","\n\n"],["body","\n"],["body","\n"],["body","For input, the ENCLOSED BY character, if present, is stripped from the ends of field values. (This is true regardless of whether OPTIONALLY is specified; OPTIONALLY has no effect on input interpretation.) Occurrences of the ENCLOSED BY character preceded by the ESCAPED BY character are interpreted as part of the current field value.（指定 enclosed by之后，首先会删除字段后面的 封闭字符串。optionally 对输入值的解析不起作用。双封闭字符串 被当做封闭字符串本身。作为当前字段值的一部分）"],["body","\n\n"],["body","\n"],["body","If the field begins with the ENCLOSED BY character, instances of that character are recognized as terminating a field value only if followed by the field or line TERMINATED BY sequence. To avoid ambiguity, occurrences of the ENCLOSED BY character within a field value can be doubled and are interpreted as a single instance of the character. For example, if ENCLOSED BY '\"' is specified, quotation marks are handled as shown here:）(双封闭字符串 被当做封闭字符串本身。作为当前字段值的一部分)"],["body","\n"],["body","\n"],["body","\n"],["body","\"The \"\"BIG\"\" boss\"  -> The \"BIG\" boss\nThe \"BIG\" boss      -> The \"BIG\" boss\nThe \"\"BIG\"\" boss    -> The \"\"BIG\"\" boss\n"],["body","\n"],["body","\n\n"],["body","\n\n"],["headingLink","字段转义符"],["heading","字段转义符"],["body","\n"],["body","FIELDS ESCAPED BY controls how to read or write special characters:（fields escaped by 控制如何处理特殊字符）"],["body","\n\n"],["body","\n"],["body","For input, if the FIELDS ESCAPED BY character is not empty, occurrences of that character are stripped and the following character is taken literally as part of a field value. Some two-character sequences that are exceptions, where the first character is the escape character. These sequences are shown in the following table (using \\ for the escape character). The rules for NULL handling are described later in this section.（字段转义）"],["body","\n"],["body","\n"],["body","Character"],["body","Escape Sequence"],["body","\n"],["body","\\0"],["body","An ASCII NUL (X'00') character"],["body","\n"],["body","\\b"],["body","A backspace character"],["body","\n"],["body","\\n"],["body","A newline (linefeed) character"],["body","\n"],["body","\\r"],["body","A carriage return character"],["body","\n"],["body","\\t"],["body","A tab character."],["body","\n"],["body","\\Z"],["body","ASCII 26 (Control+Z)"],["body","\n"],["body","\\N"],["body","NULL"],["body","\n\n"],["body","\n"],["body","\n"],["body","If the FIELDS ESCAPED BY character is empty, escape-sequence interpretation does not occur."],["body","\n"],["body","\n"],["body","\n"],["body","For output, if the FIELDS ESCAPED BY character is not empty, it is used to prefix the following characters on output:"],["body","\n\n"],["body","The FIELDS ESCAPED BY character."],["body","\n"],["body","The FIELDS [OPTIONALLY] ENCLOSED BY character."],["body","\n"],["body","The first character of the FIELDS TERMINATED BY and LINES TERMINATED BY values, if the ENCLOSED BY character is empty or unspecified."],["body","\n"],["body","ASCII 0 (what is actually written following the escape character is ASCII 0, not a zero-valued byte)."],["body","\n\n"],["body","\n"],["body","\n"],["body","If the FIELDS ESCAPED BY character is empty, no characters are escaped and NULL is output as NULL, not \\N. It is probably not a good idea to specify an empty escape character, particularly if field values in your data contain any of the characters in the list just given."],["body","\n"],["body","\n\n"],["headingLink","字段与行处理选项互影响"],["heading","字段与行处理选项互影响"],["body","\n"],["body","In certain cases, field- and line-handling options interact:"],["body","\n\n"],["body","If LINES TERMINATED BY is an empty string and FIELDS TERMINATED BY is nonempty, lines are also terminated with FIELDS TERMINATED BY.（字段分隔符存在，行分隔符不存在。则行分隔符取字段分隔符）"],["body","\n"],["body","If the FIELDS TERMINATED BY and FIELDS ENCLOSED BY values are both empty (''), a fixed-row (nondelimited) format is used. With fixed-row format, no delimiters are used between fields (but you can still have a line terminator). Instead, column values are read and written using a field width wide enough to hold all values in the field. For TINYINT, SMALLINT, MEDIUMINT, INT, and BIGINT, the field widths are 4, 6, 8, 11, and 20, respectively, no matter what the declared display width is.\n\n"],["body","如果 字段分隔符，字段封闭符都是空字符‘’ 则使用固定列格式。字段间没有分隔符。每个字段都是定宽的。"],["body","\n\n"],["body","\n"],["body","LINES TERMINATED BY is still used to separate lines. If a line does not contain all fields, the rest of the columns are set to their default values. If you do not have a line terminator, you should set this to ''. In this case, the text file must contain all fields for each row."],["body","\n"],["body","Fixed-row format also affects handling of NULL values, as described later.\n\n"],["body","Fixed-size format does not work if you are using a multibyte character set."],["body","\n\n"],["body","\n\n"],["body","An attempt to load NULL into a NOT NULL column produces either a warning or an error according to the rules described in Column Value Assignment.（将空值导入 非空字段。会导致警告或报错）"],["body","\n"],["headingLink","不支持的场景"],["heading","不支持的场景"],["body","\n"],["body","Some cases are not supported by LOAD DATA:"],["body","\n\n"],["body","\n"],["body","Fixed-size rows (FIELDS TERMINATED BY and FIELDS ENCLOSED BY both empty) and BLOB or TEXT columns. 固定列 和 Blob Text数据结构"],["body","\n"],["body","\n"],["body","\n"],["body","If you specify one separator that is the same as or a prefix of another, LOAD DATA cannot interpret the input properly. For example, the following FIELDS clause would cause problems: （换行符跟 字段封闭符 不能一样）"],["body","\n\n"],["body","\n"],["body","FIELDS TERMINATED BY '\"' ENCLOSED BY '\"'\n"],["body","\n"],["body","\n\n"],["body","\n"],["body","\n"],["body","If FIELDS ESCAPED BY is empty, a field value that contains an occurrence of FIELDS ENCLOSED BY or LINES TERMINATED BY followed by the FIELDS TERMINATED BY value causes LOAD DATA to stop reading a field or line too early. This happens because LOAD DATA cannot properly determine where the field or line value ends."],["body","\n\n"],["body","（字段转义符没有。但是字段内容中包含了 字段封闭符、换行符的内容。可能会导致解析异常）"],["body","\n\n"],["body","\n\n"],["headingLink","column-list-specification字段列表"],["heading","Column List Specification（字段列表）"],["body","\n"],["body","The following example loads all columns of the persondata table:"],["body","\n"],["body","LOAD DATA INFILE 'persondata.txt' INTO TABLE persondata;\n\n"],["body","\n"],["body","By default, when no column list is provided at the end of the LOAD DATA statement, input lines are expected to contain a field for each table column. If you want to load only some of a table's columns, specify a column list:（如果不指定 列名列表。则期望所有的字段都要有）"],["body","\n"],["body","LOAD DATA INFILE 'persondata.txt' INTO TABLE persondata\n(col_name_or_user_var [, col_name_or_user_var] ...);\n"],["body","\n"],["body","You must also specify a column list if the order of the fields in the input file differs from the order of the columns in the table. Otherwise, MySQL cannot tell how to match input fields with table columns.（必须要指定 字段列表。）"],["body","\n"],["headingLink","input-preprocessing输入处理"],["heading","Input Preprocessing（输入处理）"],["body","\n"],["body","Each instance of col_name_or_user_var in LOAD DATA syntax is either a column name or a user variable. With user variables, the SET clause enables you to perform preprocessing transformations on their values before assigning the result to columns.（col_name_or_user_var要么是字段名。要么是变量。可以通过 set子句  进行预处理装换）"],["body","\n"],["body","User variables in the SET clause can be used in several ways. The following example uses the first input column directly for the value of t1.column1, and assigns the second input column to a user variable that is subjected to a division operation before being used for the value of t1.column2:"],["body","\n"],["body","LOAD DATA INFILE 'file.txt'\n  INTO TABLE t1\n  (column1, @var1)\n  SET column2 = @var1/100;\n"],["body","\n"],["body","The SET clause can be used to supply values not derived from the input file. The following statement sets column3 to the current date and time:"],["body","\n"],["body","LOAD DATA INFILE 'file.txt'\n  INTO TABLE t1\n  (column1, column2)\n  SET column3 = CURRENT_TIMESTAMP;\n"],["body","\n"],["body","You can also discard an input value by assigning it to a user variable and not assigning the variable to any table column: （丢弃字段输入）"],["body","\n"],["body","LOAD DATA INFILE 'file.txt'\n  INTO TABLE t1\n  (column1, @dummy, column2, @dummy, column3);\n"],["body","\n"],["body","Use of the column/variable list and SET clause is subject to the following restrictions:（限制）"],["body","\n\n"],["body","Assignments in the SET clause should have only column names on the left hand side of assignment operators.（列名只能出现在左边）"],["body","\n"],["body","You can use subqueries in the right hand side of SET assignments. A subquery that returns a value to be assigned to a column may be a scalar subquery only. Also, you cannot use a subquery to select from the table that is being loaded.（子查询可以出现在set右边。子查询只能返回标量数据。不能对正在loadData的进行子查询）"],["body","\n"],["body","Lines ignored by an IGNORE *number* LINES clause are not processed for the column/variable list or SET clause.（忽略的数据 不参与变量处理）"],["body","\n"],["body","User variables cannot be used when loading data with fixed-row format because user variables do not have a display width.（用户变量不能用来加载 定宽文件。）"],["body","\n\n"],["headingLink","column-value-assignment列赋值"],["heading","Column Value Assignment（列赋值）"],["body","\n"],["body","To process an input line, LOAD DATA splits it into fields and uses the values according to the column/variable list and the SET clause, if they are present. Then the resulting row is inserted into the table. If there are BEFORE INSERT or AFTER INSERT triggers for the table, they are activated before or after inserting the row, respectively.（会触发 before insert、after insert 触发器）"],["body","\n"],["headingLink","限制性与非限制性数据解析"],["heading","限制性与非限制性数据解析"],["body","\n"],["body","Interpretation of field values and assignment to table columns depends on these factors:(影响字段值的解析的因素)"],["body","\n\n"],["body","The SQL mode (the value of the sql_mode system variable). The mode can be nonstrictive, or restrictive in various ways. For example, strict SQL mode can be enabled, or the mode can include values such as NO_ZERO_DATE or NO_ZERO_IN_DATE."],["body","\n"],["body","Presence or absence of the IGNORE and LOCAL modifiers."],["body","\n\n"],["body","Those factors combine to produce restrictive or nonrestrictive data interpretation by LOAD DATA:（根据上述因素，也就地址了 限制性或非限制性数据解析）"],["body","\n\n"],["body","Data interpretation is restrictive if the SQL mode is restrictive and neither the IGNORE nor the LOCAL modifier is specified. Errors terminate the load operation. （限制性 错误会立即中断操作）"],["body","\n"],["body","Data interpretation is nonrestrictive if the SQL mode is nonrestrictive or the IGNORE or LOCAL modifier is specified. (In particular, either modifier if specified overrides a restrictive SQL mode.) Errors become warnings and the load operation continues. （遇到错误后继续执行）"],["body","\n\n"],["body","Restrictive data interpretation uses these rules:"],["body","\n\n"],["body","Too many or too few fields results an error. 字段过多、多少导致错误"],["body","\n"],["body","Assigning NULL (that is, \\N) to a non-NULL column results in an error. 空赋值非空导致错误"],["body","\n"],["body","A value that is out of range for the column data type results in an error.超限超宽 导致错误"],["body","\n"],["body","Invalid values produce errors. For example, a value such as 'x' for a numeric column results in an error, not conversion to 0.格式不合法导致错误"],["body","\n\n"],["body","By contrast, nonrestrictive data interpretation uses these rules:（非限制性数据解析）"],["body","\n\n"],["body","If an input line has too many fields, the extra fields are ignored and the number of warnings is incremented.（字段过多。忽略多余字段）"],["body","\n"],["body","If an input line has too few fields, the columns for which input fields are missing are assigned their default values. Default value assignment is described in Section 11.6, “Data Type Default Values”.（字段多少，则赋值默认值）"],["body","\n"],["body","Assigning NULL (that is, \\N) to a non-NULL column results in assignment of the implicit default value for the column data type. Implicit default values are described in Section 11.6, “Data Type Default Values”. 空赋值非空  导致 隐式的字段类型默认值"],["body","\n"],["body","Invalid values produce warnings rather than errors, and are converted to the “closest” valid value for the column data type. Examples: 数据类型不对。赋值最接近（closest）的值\n\n"],["body","A value such as 'x' for a numeric column results in conversion to 0. 数值类型的x 转换为0"],["body","\n"],["body","An out-of-range numeric or temporal value is clipped to the closest endpoint of the range for the column data type. 超范围截断"],["body","\n"],["body","An invalid value for a DATETIME, DATE, or TIME column is inserted as the implicit default value, regardless of the SQL mode NO_ZERO_DATE setting. The implicit default is the appropriate “zero” value for the type ('0000-00-00 00:00:00', '0000-00-00', or '00:00:00'). See Section 11.2, “Date and Time Data Types”.（非法日期格式 赋0日期值 0000-00-00 00:00:00）"],["body","\n\n"],["body","\n"],["body","LOAD DATA interprets an empty field value differently from a missing field: 少字段的默认值\n\n"],["body","For string types, the column is set to the empty string.  空字符串"],["body","\n"],["body","For numeric types, the column is set to 0. 0"],["body","\n"],["body","For date and time types, the column is set to the appropriate “zero” value for the type. See Section 11.2, “Date and Time Data Types”. 0日期值"],["body","\n\n"],["body","\n\n"],["headingLink","timestamp数据处理"],["heading","Timestamp数据处理"],["body","\n"],["body","These are the same values that result if you assign an empty string explicitly to a string, numeric, or date or time type explicitly in an INSERT or UPDATE statement."],["body","\n"],["body","TIMESTAMP columns are set to the current date and time only if there is a NULL value for the column (that is, \\N) and the column is not declared to permit NULL values, or if the TIMESTAMP column default value is the current timestamp and it is omitted from the field list when a field list is specified."],["body","\n"],["body","LOAD DATA regards all input as strings, so you cannot use numeric values for ENUM or SET columns the way you can with INSERT statements. All ENUM and SET values must be specified as strings."],["body","\n"],["headingLink","bit数据处理"],["heading","BIT数据处理"],["body","\n"],["body","BIT values cannot be loaded directly using binary notation (for example, b'011010'). To work around this, use the SET clause to strip off the leading b' and trailing ' and perform a base-2 to base-10 conversion so that MySQL loads the values into the BIT column properly:"],["body","\n"],["body","$> cat /tmp/bit_test.txt\nb'10'\nb'1111111'\n$> mysql test\nmysql> LOAD DATA INFILE '/tmp/bit_test.txt'\n       INTO TABLE bit_test (@var1)\n       SET b = CAST(CONV(MID(@var1, 3, LENGTH(@var1)-3), 2, 10) AS UNSIGNED);\nQuery OK, 2 rows affected (0.00 sec)\nRecords: 2  Deleted: 0  Skipped: 0  Warnings: 0\n\nmysql> SELECT BIN(b+0) FROM bit_test;\n+----------+\n| BIN(b+0) |\n+----------+\n| 10       |\n| 1111111  |\n+----------+\n2 rows in set (0.00 sec)\n"],["body","\n"],["body","For BIT values in 0b binary notation (for example, 0b011010), use this SET clause instead to strip off the leading 0b:"],["body","\n"],["body","SET b = CAST(CONV(MID(@var1, 3, LENGTH(@var1)-2), 2, 10) AS UNSIGNED)\n"],["body","\n"],["headingLink","partitioned-table-support分区表支持"],["heading","Partitioned Table Support（分区表支持）"],["body","\n"],["body","LOAD DATA supports explicit partition selection using the PARTITION clause with a list of one or more comma-separated names of partitions, subpartitions, or both. When this clause is used, if any rows from the file cannot be inserted into any of the partitions or subpartitions named in the list, the statement fails with the error Found a row not matching the given partition set. For more information and examples, see Section 22.5, “Partition Selection”."],["body","\n"],["body","For partitioned tables using storage engines that employ table locks, such as MyISAM, LOAD DATA cannot prune any partition locks. This does not apply to tables using storage engines that employ row-level locking, such as InnoDB. For more information, see Section 22.6.4, “Partitioning and Locking”."],["body","\n"],["headingLink","concurrency-considerations允许并发"],["heading","Concurrency Considerations（允许并发）"],["body","\n"],["body","With the LOW_PRIORITY modifier, execution of the LOAD DATA statement is delayed until no other clients are reading from the table. This affects only storage engines that use only table-level locking (such as MyISAM, MEMORY, and MERGE)."],["body","\n"],["body","With the CONCURRENT modifier and a MyISAM table that satisfies the condition for concurrent inserts (that is, it contains no free blocks in the middle), other threads can retrieve data from the table while LOAD DATA is executing. This modifier affects the performance of LOAD DATA a bit, even if no other thread is using the table at the same time."],["body","\n"],["headingLink","statement-result-information结果统计"],["heading","Statement Result Information（结果统计）"],["body","\n"],["body","When the LOAD DATA statement finishes, it returns an information string in the following format:"],["body","\n"],["body","Records: 1  Deleted: 0  Skipped: 0  Warnings: 0\n"],["body","\n"],["body","Warnings occur under the same circumstances as when values are inserted using the INSERT statement (see Section 13.2.5, “INSERT Statement”), except that LOAD DATA also generates warnings when there are too few or too many fields in the input row."],["body","\n"],["body","You can use SHOW WARNINGS to get a list of the first max_error_count warnings as information about what went wrong. See Section 13.7.5.40, “SHOW WARNINGS Statement”."],["body","\n"],["body","If you are using the C API, you can get information about the statement by calling the mysql_info() function. See mysql_info()."],["body","\n"],["headingLink","replication-considerations副本"],["heading","Replication Considerations（副本）"],["body","\n"],["body","For information about LOAD DATA in relation to replication, see Section 16.4.1.18, “Replication and LOAD DATA”."],["body","\n"],["headingLink","miscellaneous-topics杂项主题"],["heading","Miscellaneous Topics（杂项主题）"],["body","\n"],["body","On Unix, if you need LOAD DATA to read from a pipe, you can use the following technique (the example loads a listing of the / directory into the table db1.t1):（使用管道）"],["body","\n"],["body","mkfifo /mysql/data/db1/ls.dat\nchmod 666 /mysql/data/db1/ls.dat\nfind / -ls > /mysql/data/db1/ls.dat &\nmysql -e \"LOAD DATA INFILE 'ls.dat' INTO TABLE t1\" db1\n"],["body","\n"],["body","Here you must run the command that generates the data to be loaded and the mysql commands either on separate terminals, or run the data generation process in the background (as shown in the preceding example). If you do not do this, the pipe blocks until data is read by the mysql process."],["body","\n"],["body","/usr/local/alibaba/mysql/bin/mysql  -h127.0.0.1 -u alibbpoc -pxxxxx -A qualitycheck  -e \"load data local infile 'tmp.csv' into table hit_result_copy FIELDS TERMINATED by ',' LINES TERMINATED by '\\n' (rid, hit_id, rule_hit, create_time, check_time);\"\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","mysql/空值处理函数.html"],["title","空值处理函数.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["body","函数"],["body","说明"],["body","\n"],["body","IF(expr1,expr2,expr3)"],["body","expr1为true 则返回 expr2 否则expr3"],["body","\n"],["body","IFNULL(expr1,expr2)"],["body","判断第一个表达式是否为 NULL，如果为 NULL 则返回第二个参数的值，如果不为 NULL 则返回第一个参数的值。"],["body","\n"],["body","NULLIF(expr1,expr2)"],["body","如果两个参数相等则返回NULL，否则返回第一个参数的值expr1"],["body","\n"],["body","coalesce(expr1,expr2....)"],["body","返回第一个不为null的值"],["body","\n"],["body","ISNULL(expr)"],["body","判断是否为null"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","mysql/mysql执行计划.html"],["title","mysql执行计划.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","优化策略"],["heading","优化策略"],["body","\n\n"],["body","出现了 Using temporary；"],["body","\n"],["body","rows 过多，或者几乎是全表的记录数；"],["body","\n"],["body","filtered 太低"],["body","\n"],["body","key 是 (NULL)；"],["body","\n"],["body","possible_keys 出现过多（待选）索引。"],["body","\n\n"],["headingLink","explain"],["heading","explain"],["body","\n"],["headingLink","id"],["heading","ID"],["body","\n"],["body","\n"],["body","表示表的读取顺序"],["body","\n"],["body","\n\n"],["body","\n"],["body","id 相同的情况下:执行顺序由上至下"],["body","\n"],["body","\n"],["body","\n"],["body","id 不同的情况下: id 越大,越先被执行( 例如最里面的越先被执行)"],["body","\n"],["body","\n\n"],["headingLink","select_type"],["heading","select_type"],["body","\n"],["body","\n"],["body","数据是以何种方式读取的"],["body","\n"],["body","\n"],["body","simple"],["body","\n"],["body","简单的子查询,不包含子查询或者union"],["body","\n"],["body","primary"],["body","\n"],["body","查询中若包含多层子查询,最外层的查询为primary查询"],["body","\n"],["body","subquery"],["body","\n"],["body","子查询"],["body","\n"],["body","derived"],["body","\n\n"],["body","子查询衍生的虚表的表名格式为 derived${id},"],["body","\n"],["body","其中 id 为 explan 表 的 id,表示由这个 id 代表的某步骤产生的临时表"],["body","\n"],["body","用于 from 子句里有子查询的情况。MySQL 会递归执行这些子查询，把结果放在临时表里"],["body","\n\n"],["body","union"],["body","\n\n"],["body","若第二个 select 出现在 union 之后,则被标记为 union"],["body","\n"],["body","若 union 包含在 from 子句的 子查询中,则外层的 select 将被标记为 drived"],["body","\n\n"],["body","union result"],["body","\n\n"],["body","从 union 表获取结果的 select"],["body","\n\n"],["headingLink","table"],["heading","table"],["body","\n"],["body","\n"],["body","表名"],["body","\n"],["body","\n"],["headingLink","type"],["heading","type"],["body","\n"],["body","\n"],["body","实际索引使用方式,性能升序增加"],["body","\n"],["body","\n"],["body","ALL"],["body","\n"],["body","\n"],["body","全表扫描"],["body","\n"],["body","\n"],["body","index （带索引的全表扫描）"],["body","\n"],["body","这种连接类型只是另外一种形式的全表扫描，只不过它的扫描顺序是按照索引的顺序。这种扫描根据索引然后回表取数据，和 all 相比，他们都是取得了全表的数据，而且 index 要先读索引而且要回表随机取数据，因此 index 不可能会比 all 快（取同一个表数据），但为什么官方的手册将它的效率说的比 all 好，唯一可能的场景在于，按照索引扫描全表的数据是有序的。这样一来使用该索引排序 时 比 ALL 要好"],["body","\n"],["body","mysql> explain select * from employee order by `no` ;\n+----+-------------+----------+------+---------------+------+---------+------+------+----------------+\n| id | select_type | table    | type | possible_keys | key  | key_len | ref  | rows | Extra          |\n+----+-------------+----------+------+---------------+------+---------+------+------+----------------+\n|  1 | SIMPLE      | employee | ALL  | NULL          | NULL | NULL    | NULL |    5 | Using filesort |\n+----+-------------+----------+------+---------------+------+---------+------+------+----------------+\nmysql> explain select * from employee order by rec_id ;\n+----+-------------+----------+-------+---------------+---------+---------+------+------+-------+\n| id | select_type | table    | type  | possible_keys | key     | key_len | ref  | rows | Extra |\n+----+-------------+----------+-------+---------------+---------+---------+------+------+-------+\n|  1 | SIMPLE      | employee | index | NULL          | PRIMARY | 4       | NULL |    5 | NULL  |\n+----+-------------+----------+-------+---------------+---------+---------+------+------+-------+\n"],["body","\n"],["body","range"],["body","\n\n"],["body","只检索给定范围的行,使用一个索引来选择行,索引开始于某一点,结束于某一点"],["body","\n"],["body","例如 between in,> < , in, or 也是索引扫描"],["body","\n\n"],["body","ref"],["body","\n\n"],["body","非唯一性索引扫描,返回匹配某个单独值得所有行"],["body","\n\n"],["body","eq_ref"],["body","\n\n"],["body","唯一性索引扫描,对于每个索引键 只有一条记录与之匹配,常见于主键或唯一扫描"],["body","\n\n"],["body","const"],["body","\n\n"],["body","只有一条记录匹配,通常常见于 主键和唯一性索引"],["body","\n\n"],["body","system"],["body","\n"],["body","表只有一行记录,等于系统表,是 const 类型的特例"],["body","\n"],["body","system>const>eq_ref>ref>fulltext>ref_or_null>index_merge>unique_subquery>index_subquery>rang>index>all\n保证达到 range级别 即可,最好能达到ref\n"],["body","\n"],["headingLink","possiblekeys"],["heading","possibleKeys"],["body","\n"],["body","\n"],["body","可能用到的索引,因为一张表索引只能使用一个"],["body","\n"],["body","\n"],["headingLink","keys"],["heading","keys"],["body","\n"],["body","\n"],["body","实际使用到的索引"],["body","\n"],["body","\n\n"],["body","\n"],["body","覆盖索引"],["body","\n"],["body","查询的字段 刚好建立了索引,且查询顺序一致"],["body","\n"],["body","\n\n"],["headingLink","key_len"],["heading","key_len"],["body","\n\n"],["body","表示索引中使用的字节数,可通过该计算查询中使用的索引的长度,长度越短越好"],["body","\n"],["body","根据表定义计算得出得 索引字节数"],["body","\n\n"],["headingLink","ref"],["heading","ref"],["body","\n"],["body","\n"],["body","显示该次查询 使用的索引的值 是 引用得哪个地方的, 一般有两种引用"],["body","\n"],["body","\n\n"],["body","const 引用的常量"],["body","\n"],["body","test.t1.id 引用 某个库的某个表的某个字段 作为索引值的来源"],["body","\n\n"],["headingLink","rows"],["heading","rows"],["body","\n\n"],["body","找到记录大致要读取的行数"],["body","\n\n"],["headingLink","extra"],["heading","extra"],["body","\n"],["body","\n"],["body","表示不适合在其他列中显示,但十分重要的额外信息"],["body","\n"],["body","\n\n"],["body","\n"],["body","using filesort 没有用到索引的排序,因为联合索引 字段顺序问题"],["body","\n"],["body","example\nindex(col1,col2,col3)\ncol1='ac' order by col2,col3\n这种情况是会用到索引\n如何建索引 , 就 如何按照索引走\n"],["body","\n"],["body","\n"],["body","\n"],["body","using temporary:使用了临时表,保存中间结果,常见于 order by,group by"],["body","\n\n"],["body","对于联合索引 请按照建立的顺序使用"],["body","\n\n"],["body","\n"],["body","\n"],["body","using index"],["body","\n\n"],["body","select 操作中使用了覆盖索引,效率不错"],["body","\n"],["body","如果同时出现了usingwhere 表明索引被用来 执行索引键值的查找"],["body","\n"],["body","如果没有出现 using where 表明索引用来读取数据而非执行查找"],["body","\n\n"],["body","\n"],["body","\n"],["body","using where"],["body","\n\n"],["body","使用where过滤"],["body","\n\n"],["body","\n"],["body","\n"],["body","using join buffer"],["body","\n\n"],["body","使用了连接缓存"],["body","\n\n"],["body","\n"],["body","\n"],["body","impossiable where"],["body","\n\n"],["body","不可能的 where 过滤条件"],["body","\n\n"],["body","\n"],["body","\n"],["body","select table optimized away"],["body","\n\n"],["body","在没有 groupby 子句的情况下,基于索引优化 MIN/MAX 操作"],["body","\n"],["body","或 对于 MyISAM 存储引擎 优化 count(*) 操作,不必等到执行阶段计算"],["body","\n\n"],["body","\n"],["body","\n"],["body","distinct"],["body","\n"],["body","\n\n"],["headingLink","using-temporaryusing-filesort"],["heading","Using temporary/Using filesort"],["body","\n"],["headingLink","using-temporary"],["heading","Using temporary"],["body","\n"],["body","\n"],["body","表示由于排序没有走索引、使用union、子查询连接查询、使用某些视图等原因（详见internal-temporary-tables），因此创建了一个内部临时表。"],["body","\n"],["body","注意这里的临时表可能是内存上的临时表，也有可能是硬盘上的临时表"],["body","\n"],["body","\n"],["body","内存临时表 or 硬盘临时表"],["body","\n"],["body","查看 sql 执行时使用的是内存临时表还是硬盘临时表，需要使用如下命令："],["body","\n"],["body","mysql> show global status like '%tmp%';\n+-------------------------+-------+\n| Variable_name           | Value |\n+-------------------------+-------+\n| Created_tmp_disk_tables | 0     |\n| Created_tmp_files       | 5     |\n| Created_tmp_tables      | 11    |\n+-------------------------+-------+\n3 rows in set\n"],["body","\n"],["body","Created_tmp_tables 表示 mysql 创建的内部临时表的总数（包括内存临时表和硬盘临时表）；"],["body","\n"],["body","Created_tmp_disk_tables 表示 mysql 创建的硬盘临时表的总数。"],["body","\n"],["body","与临时表有关的参数"],["body","\n"],["body","当 mysql 需要创建临时表时，选择内存临时表还是硬盘临时表取决于参数tmp_table_size和max_heap_table_size，"],["body","\n"],["body","当临时表的容量 > Min(tmp_table_size ,max_heap_table_size ) , mysql 就会使用硬盘临时表存放数据。"],["body","\n"],["body","用户可以在 mysql 的配置文件里修改该两个参数的值，两者的默认值均为 16M。"],["body","\n"],["body","tmp_table_size = 16M\nmax_heap_table_size = 16M\n12\n"],["body","\n"],["body","查看tmp_table_size和max_heap_table_size值："],["body","\n"],["body","mysql> show global variables like 'max_heap_table_size' or 'tmp_table_size';\n"],["body","\n"],["headingLink","using-filesort"],["heading","Using filesort"],["body","\n"],["body","\n"],["body","Using filesort仅仅表示没有使用索引的排序,filesort与文件无关。消除Using filesort的方法就是让查询 sql 的排序走索引"],["body","\n"],["body","\n"],["body","简介"],["body","\n"],["body","filesort使用的算法是QuickSort，即对需要排序的记录生成元数据进行分块排序，然后再使用 mergesort 方法合并块。其中filesort可以使用的内存空间大小为参数sort_buffer_size的值，默认为 2M。当排序记录太多sort_buffer_size不够用时，mysql 会使用临时文件来存放各个分块，然后各个分块排序后再多次合并分块最终全局完成排序。"],["body","\n"],["body","mysql> show global variables like 'sort_buffer_size';\n+------------------+--------+\n| Variable_name    | Value  |\n+------------------+--------+\n| sort_buffer_size | 262144 |\n+------------------+--------+\n1 row in set\n"],["body","\n"],["body","Sort_merge_passes表示filesort执行过的文件分块合并次数的总和，如果该值比较大，建议增大sort_buffer_size的值。"],["body","\n"],["body","使用算法"],["body","\n"],["body","filesort使用的排序方法有两种："],["body","\n"],["body","rowid 回表排序"],["body","\n"],["body","第一种方法是对需要排序的记录生成<sort_key,rowid>的元数据进行排序，该元数据仅包含排序字段和 rowid。排序完成后只有按字段排序的 rowid，因此还需要通过 rowid 进行回表操作获取所需要的列的值，可能会导致大量的随机 IO 读消耗；"],["body","\n"],["body","带元数据排序"],["body","\n"],["body","第二种方法是是对需要排序的记录生成<sort_key,additional_fields>的元数据，该元数据包含排序字段和需要返回的所有列。排序完后不需要回表，但是元数据要比第一种方法长得多，需要更多的空间用于排序。"],["body","\n"],["body","参数max_length_for_sort_data字段用于控制filesort使用的排序方法，当所有需要排序记录的字段数量总和小于max_length_for_sort_data时使用第二种算法，否则会用第一种算法。该值的默认值为 1024"],["body","\n"],["headingLink","小表驱动大表"],["heading","小表驱动大表"],["body","\n"],["headingLink","重要知识点"],["heading","重要知识点"],["body","\n\n"],["body","EXPLAIN 结果中，第一行出现的表就是驱动表"],["body","\n"],["body","对驱动表可以直接排序，对非驱动表（的字段排序）需要对循环查询的合并结果（临时表）进行排序**（Important!）**"],["body","\n"],["body","永远用小结果集驱动大结果集(mysql 中)"],["body","\n\n"],["headingLink","nested-loop-join"],["heading","Nested Loop Join"],["body","\n"],["body","以驱动表的结果集作为循环的基础数据，然后将结果集中的数据作为过滤条件一条条地到下一个表中查询数据，最后合并结果；此时还有第三个表，则将前两个表的 Join 结果集作为循环基础数据，再一次通过循环查询条件到第三个表中查询数据，如此反复。"],["body","\n"],["headingLink","驱动表的定义"],["heading","驱动表的定义"],["body","\n"],["body","进行多表连接查询时， [驱动表] 的定义为：\n1）指定了联接条件时，满足查询条件的记录行数少的表为[驱动表]；"],["body","\n"],["body","2）未指定联接条件时，行数少的表为[驱动表]"],["body","\n\n"],["body","left join 一定程度上会 将 左表设置为 [驱动表] (除非 右表的查询足够小)"],["body","\n\n"],["headingLink","keylen-的计算"],["heading","KeyLen 的计算"],["body","\n"],["body","key_len 表示索引使用的字节数，根据这个值可以判断索引的使用情况,特别是在组合索引的时候,判断该索引有多少部分被使用到\n在计算 key_len 时，下面是一些需要考虑的点:"],["body","\n\n"],["body","**索引字段的附加信息:**可以分为变长和定长数据类型讨论\n\n"],["body","当索引字段为定长数据类型时,如 char，int，datetime,需要有是否为空的标记,这个标记占用 1 个字节(对于 not null 的字段来说,则不需要这 1 字节);"],["body","\n"],["body","对于变长数据类型,比如 varchar,除了是否为空的标记外,还需要有长度信息,需要占用两个字节。"],["body","\n\n"],["body","\n"],["body","对于,char、varchar、blob、text 等字符集来说，key len 的长度还和字符集有关\n\n"],["body","latin1 一个字符占用 1 个字节"],["body","\n"],["body","gbk 一个字符占用 2 个字节"],["body","\n"],["body","utf8 一个字符占用 3 个字节。"],["body","\n\n"],["body","\n\n"],["body","综上，下面来看一些例子:"],["body","\n"],["body","列类型"],["body","KEY_LEN"],["body","备注"],["body","\n"],["body","id int"],["body","key_len = 4+1"],["body","int 为 4bytes,允许为 NULL,加 1byte"],["body","\n"],["body","id bigint not null"],["body","key_len=8"],["body","bigint 为 8bytes"],["body","\n"],["body","user char(30) utf8"],["body","key_len=30*3+1"],["body","utf8 每个字符为 3bytes,允许为 NULL,加 1byte"],["body","\n"],["body","user varchar(30) not null utf8"],["body","key_len=30*3+2"],["body","utf8 每个字符为 3bytes,变长数据类型,加 2bytes"],["body","\n"],["body","user varchar(30) utf8"],["body","key_len=30*3+2+1"],["body","utf8 每个字符为 3bytes,允许为 NULL,加 1byte,变长数据类型,加 2bytes"],["body","\n"],["body","detail text(10) utf8"],["body","key_len=30*3+2+1"],["body","TEXT 截取部分,被视为动态列类型。"],["body","\n\n\n"],["body","key_len 只指示了where 中用于条件过滤时被选中的索引列，是不包含 order by/group by 这一部分被选中的索引列的\n例如,有个联合索引 idx(c1,c2,c3),3 列均是 int not null,那么下面的 SQL 执行计划中"],["body","\n"],["body","//key_len的值是8而不是12:\nselect ... from tb where c1=? and c2=? order by c1;\n"],["body","\n"],["headingLink","indexmerge"],["heading","indexMerge"],["body","\n"],["body","\n"],["body","对多个索引分别进行条件扫描，然后将它们各自的结果进行合并(intersect/union) 同一个表的多个索引的范围扫描可以对结果进行合并"],["body","\n"],["body","\n"],["headingLink","示例"],["heading","示例"],["body","\n"],["body","SELECT * FROM tbl_name WHERE key1 = 10 OR key2 = 20;\nSELECT * FROM tbl_name WHERE (key1 = 10 OR key2 = 20) AND non_key=30;\nSELECT * FROM t1, t2 WHERE (t1.key1 IN (1,2) OR t1.key2 LIKE 'value%') AND t2.key1=t1.some_col;\nSELECT * FROM t1, t2 WHERE t1.key1=1 AND (t2.key1=t1.some_col OR t2.key2=t1.some_col2);\n"],["body","\n"],["body","1,SIMPLE,a,,index_merge,\"index_shift_results_worker_id,index_shift_results_org_code\",\"index_shift_results_worker_id,index_shift_results_org_code\",\"51,81\",,61,100,\"Using union(index_shift_results_worker_id,index_shift_results_org_code); Using where\n"],["body","\n"],["headingLink","using-intersectindex_1index_2"],["heading","Using intersect(index_1,index_2...)"],["body","\n"],["body","//取交集\nSELECT * FROM tbl_name WHERE key1 = 10 and key2 = 20;\n"],["body","\n"],["headingLink","using-unionindex_1index_2"],["heading","Using union(index_1,index_2)"],["body","\n"],["body","//取并集\nSELECT * FROM tbl_name WHERE key1 = 10 or key2 = 20;\n"],["body","\n"],["body","以及它们的组合(先内部 intersect 然后在外面 union)。"],["body","\n"],["headingLink","using-sort_unionindex_1index_2"],["heading","Using sort_union(index_1,index_2)"],["body","\n"],["body","select id,worker_id,org_code from shift_results a where worker_id between '10197' and '102000' or org_code between  '100101100' and '100201100'\n"],["body","\n"],["body","两个结果集进行并集 运算 需要排序去重"],["body","\n"],["headingLink","ref_or_null"],["heading","ref_or_null"],["body","\n"],["body","https://dev.mysql.com/doc/refman/8.0/en/is-null-optimization.html"],["body","\n"],["body","select id,worker_id,org_code from shift_results a where\nworker_id = '10197' or worker_id is null\n"],["body","\n"],["headingLink","mysql-子查询"],["heading","mysql 子查询"],["body","\n"],["headingLink","子查询定义"],["heading","子查询定义"],["body","\n"],["body","SUBQUERY"],["body","\n"],["body","子查询中的第一个 SELECT"],["body","\n"],["body","DEPENDENT SUBQUERY"],["body","\n"],["body","子查询中的第一个 SELECT，取决于外面的查询 。"],["body","\n"],["body","换句话说，就是 子查询对 g2 的查询方式依赖于外层 g1 的查询。\n\n\n第一步，MySQL 根据 select gid,count(id) from shop_goods where status=0 group by gid; 得到一个大结果集 t1，其数据量就是上图中的 rows=850672 了。\n\n第二步，上面的大结果集 t1 中的每一条记录，都将与子查询 SQL 组成新的查询语句：select gid from shop_goods where sid in (15...blabla..29) and gid=%t1.gid%。等于说，子查询要执行85万次……即使这两步查询都用到了索引，但不慢才怪。\n\n如此一来，子查询的执行效率居然受制于外层查询的记录数，那还不如拆成两个独立查询顺序执行呢。\n"],["body","\n"],["body","**优化策略 **"],["body","\n"],["body","子查询转临时表 做 join关联"],["body","\n"],["headingLink","where条件分析"],["heading","where条件分析"],["body","\n"],["body","所有SQL的where条件，均可归纳为3大类"],["body","\n"],["body","Index Key (First Key & Last Key)"],["body","\n"],["body","Index Filter"],["body","\n"],["body","Table Filter"],["body","\n"],["body","对 where 中过滤条件的处理 ,根据索引使用情况分成了三种：index key, index filter, table filter"],["body","\n"],["body","index key"],["body","\n"],["body","用于确定SQL查询在索引中的连续范围(起始范围+结束范围)的查询条件，被称之为Index Key。由于一个范围，至少包含一个起始与一个终止，因此Index Key也被拆分为Index First Key和Index Last Key，分别用于定位索引查找的起始，以及索引查询的终止条件。也就是说根据索引来确定扫描的范围。"],["body","\n"],["body","index filter"],["body","\n"],["body","在使用 index key 确定了起始范围和介绍范围之后，在此范围之内，还有一些记录不符合where 条件，如果这些条件可以使用索引进行过滤，那么就是 index filter。也就是说用索引来进行where条件过滤。"],["body","\n"],["body","table filter"],["body","\n"],["body","where 中的条件不能使用索引进行处理的，只能访问table，进行条件过滤了。"],["body","\n"],["headingLink","什么是icp"],["heading","什么是ICP？"],["body","\n"],["body","即所索引条件下推（index condition pushdown）"],["body","\n"],["body","它能减少在使用 二级索引 过滤where条件时的回表次数 和 减少MySQL server层和引擎层的交互次数。在索引组织表中，使用二级索引进行回表的代价相比堆表中是要高一些的。"],["body","\n"],["body","也就是说各种各样的 where 条件，在进行处理时，分成了上面三种情况，一种条件会使用索引确定扫描的范围；一种条件可以在索引中进行过滤；一种必须回表进行过滤；"],["body","\n"],["body","在 MySQL5.6 之前，并不区分Index Filter与Table Filter，统统将Index First Key与Index Last Key范围内的索引记录，回表读取完整记录，然后返回给MySQL Server层进行过滤。"],["body","\n"],["body","而在MySQL 5.6之后，Index Filter与Table Filter分离，Index Filter下降到InnoDB的索引层面进行过滤，减少了回表与返回MySQL Server层的记录交互开销，提高了SQL的执行效率。"],["body","\n"],["body","所以所谓的 ICP 技术，其实就是 index filter 技术而已。只不过因为MySQL的架构原因，分成了server层和引擎层，才有所谓的“下推”的说法。所以ICP其实就是实现了index filter技术，将原来的在server层进行的table filter中可以进行index filter的部分，在引擎层面使用index filter进行处理，不再需要回表进行table filter。"],["body","\n"],["body","using index"],["body","\n"],["body","using index;using where"],["body","\n"],["body","using where"],["body","\n"],["body","using index condition"],["body","\n"],["headingLink","示例-1"],["heading","示例"],["body","\n"],["body","// 索引`INDEX(zipcode, lastname, firstname)`\n\nSELECT * FROM people\nWHERE zipcode='95054'\nAND lastname LIKE '%etrunia%'\nAND address LIKE '%Main Street%';\n"],["body","\n"],["body","MySQL可以使用索引去定位那些zipcode='95054'的信息，但是第二个条件lastname LIKE '%etrunia%'却没法用于减少必须要扫描的表行，所以如果没有ICP优化，在执行此查询时必须读取所有zipcode='95054'的表行数据。"],["body","\n"],["body","如果启用了ICP优化，因为MySQL使用了索引INDEX(zipcode, lastname, firstname)，并且WHERE语句中的第二部分lastname LIKE '%etrunia%'仅仅使用了索引中的列lastname，所以在读取完整表行数据前可以基于此过滤那些索引中lastname不符合条件的索引数据，这样就能避免对那些满足zipcode='95054'但是不满足条件lastname LIKE '%etrunia%'表行数据的访问。"],["body","\n"],["body","ICP可以通过系统变量optimizer_switch中的index_condition_pushdown进行启用和关闭："],["body","\n"],["body","SET optimizer_switch = 'index_condition_pushdown=off';\nSET optimizer_switch = 'index_condition_pushdown=on';\n"],["body","\n"],["headingLink","物化表"],["heading","物化表"],["body","\n"],["body","\n"],["body","MySQL 引入了Materialization（物化）这一关键特性用于子查询（比如在 IN/NOT IN 子查询以及 FROM 子查询）优化。"],["body","\n"],["body","\n"],["body","具体方式是"],["body","\n\n"],["body","\n"],["body","在 SQL 执行过程中，第一次需要子查询结果时执行子查询并将子查询的结果保存为临时表 ，后续对子查询结果集的访问将直接通过临时表获得。"],["body","\n"],["body","\n"],["body","\n"],["body","与此同时，优化器还具有延迟物化子查询的能力，先通过其它条件判断子查询是否真的需要执行。"],["body","\n"],["body","\n"],["body","\n"],["body","物化子查询优化 SQL 执行的关键点在于对子查询只需要执行一次。 与之相对的执行方式是对外表的每一行都对子查询进行调用，其执行计划中的查询类型为“DEPENDENT SUBQUERY”。"],["body","\n"],["body","\n\n"],["headingLink","查询优化开关"],["heading","查询优化开关"],["body","\n"],["body","show variables  like 'optimizer_switch'\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","mysql/mysql官方文档/客户端.html"],["title","客户端.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","连接"],["heading","连接"],["body","\n"],["body","mysql -h host -u user -p\nmysql -u user -p\n"],["body","\n"],["headingLink","查询输入"],["heading","查询输入"],["body","\n"],["body","Prompt"],["body","Meaning"],["body","\n"],["body","mysql>"],["body","Ready for new query"],["body","\n"],["body","->"],["body","Waiting for next line of multiple-line query"],["body","\n"],["body","'>"],["body","Waiting for next line, waiting for completion of a string that began with a single quote (')"],["body","\n"],["body","\">"],["body","Waiting for next line, waiting for completion of a string that began with a double quote (\")"],["body","\n"],["body","``>`"],["body","Waiting for next line, waiting for completion of an identifier that began with a backtick (```)"],["body","\n"],["body","/*>"],["body","Waiting for next line, waiting for completion of a comment that began with /*"],["body","\n\n\n"],["headingLink","数据库查看操作"],["heading","数据库查看操作"],["body","\n"],["body","SHOW DATABASES;\nUSE test\nshow tables\nLOAD DATA LOCAL INFILE '/path/pet.txt' INTO TABLE pet;\nLOAD DATA LOCAL INFILE '/path/pet.txt' INTO TABLE pet\n       LINES TERMINATED BY '\\r\\n';\n"],["body","\n"],["headingLink","杂项"],["heading","杂项"],["body","\n"],["body","赋权"],["body","\n"],["body","GRANT ALL ON menagerie.* TO 'your_mysql_name'@'your_client_host';\n"],["body","\n"],["body","空值处理"],["body","\n"],["body","IS NULL and IS NOT NULL\n任何与空值作比较 都为null\n0或者null为 false\nnull在分组中会被当做一个组\n"],["body","\n"],["body","模式匹配"],["body","\n"],["body","使用  REGEXP and NOT REGEXP  操作符, 模式匹配"],["body","\n"],["body","RLIKE and NOT RLIKE 同义词"],["body","\n"],["body","匹配规则"],["body","\n\n"],["body",".  匹配单字符"],["body","\n\n\n"],["body","[]  匹配任意字符"],["body","\n\n\n"],["body","* 匹配0个或者多个字符"],["body","\n"],["body","正则匹配只要部分匹配成功就 满足, 而 like是 整体匹配"],["body","\n"],["body","匹配整个字符 串 使用 ^$"],["body","\n"],["body","大小写敏感"],["body","\n\n"],["body","SELECT * FROM pet WHERE name REGEXP BINARY '^b';\n"],["body","\n"],["body","SELECT DATABASE();"],["body","\n"],["body","mysql 批处理模式"],["body","\n"],["body","mysql < batch-file\nmysql -e \"source batch-file\"\nmysql -h host -u user -p < batch-file\nmysql < batch-file | more\nmysql < batch-file > mysql.out\nsource filename;\n"],["body","\n"],["body","使用自定义变量"],["body","\n"],["body","mysql> SELECT @min_price:=MIN(price),@max_price:=MAX(price) FROM shop;\nmysql> SELECT * FROM shop WHERE price=@min_price OR price=@max_price;\n+---------+--------+-------+\n| article | dealer | price |\n+---------+--------+-------+\n|    0003 | D      |  1.25 |\n|    0004 | D      | 19.95 |\n+---------+--------+-------+\n"],["body","\n"],["body","外键的使用"],["body","\n"],["body","mysql innnodb 引擎  将外键 当作一种注释,"],["body","\n"],["body","不会进行 外键约束检查"],["body","\n"],["body","不会级联删除"],["body","\n"],["body","不会索引"],["body","\n"],["body","多键值搜索"],["body","\n"],["body","使用union all 代替"],["body","\n"],["body","SELECT field1_index, field2_index FROM test_table\nWHERE field1_index = '1' OR  field2_index = '1'\n\n\nSELECT field1_index, field2_index\n    FROM test_table WHERE field1_index = '1'\nUNION\nSELECT field1_index, field2_index\n    FROM test_table WHERE field2_index = '1';\n"],["body","\n"],["body","BIT_COUNT/BIT_OR"],["body","\n"],["body","CREATE TABLE t1 (year YEAR, month INT UNSIGNED,\n             day INT UNSIGNED);\nINSERT INTO t1 VALUES(2000,1,1),(2000,1,20),(2000,1,30),(2000,2,2),\n            (2000,2,23),(2000,2,23);\n            \n            \nSELECT year,month,BIT_COUNT(BIT_OR(1<<day)) AS days FROM t1\n       GROUP BY year,month;\n\n"],["body","\n"],["body","AUTO_INCREMENT"],["body","\n"],["body","NO_AUTO_VALUE_ON_ZERO"],["body","\n"],["body","如果已启用"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","mysql/mysql官方文档/mysql程序/README.html"],["title","mysql程序 - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","mysql-server-启停脚本"],["heading","mysql server 启停脚本"],["body","\n"],["headingLink","mysqld"],["heading","mysqld"],["body","\n"],["body","服务程序,主要进程"],["body","\n"],["headingLink","mysqld_safe"],["heading","mysqld_safe"],["body","\n"],["body","启动脚本"],["body","\n"],["body","mysqld_safe attempts to start mysqld"],["body","\n"],["body","See Section 4.3.2, “mysqld_safe — MySQL Server Startup Script”."],["body","\n"],["headingLink","mysqlserver"],["heading","mysql.server"],["body","\n"],["body","启动脚本"],["body","\n"],["body","使用在 system-V 风格的 通过目录脚本划分 系统特定服务等级"],["body","\n"],["body","它调用了 mysqld_safe to start the MySQL server."],["body","\n"],["body","See Section 4.3.3, “mysql.server — MySQL Server Startup Script”."],["body","\n"],["headingLink","mysqld_multi"],["heading","mysqld_multi"],["body","\n"],["body","多服务启停"],["body","\n"],["body","See Section 4.3.4, “mysqld_multi — Manage Multiple MySQL Servers”."],["body","\n"],["headingLink","安装升级脚本"],["heading","安装升级脚本"],["body","\n"],["headingLink","comp_err"],["heading","comp_err"],["body","\n"],["body","在编译构建过程中, 从错误文件中 解析出错误消息"],["body","\n"],["body","See Section 4.4.1, “comp_err — Compile MySQL Error Message File”."],["body","\n"],["headingLink","mysql_install_db"],["heading","mysql_install_db"],["body","\n"],["body","初始化 mysql data 目录"],["body","\n"],["body","创建 mysql 数据库"],["body","\n"],["body","初始化默认表权限"],["body","\n"],["body","初始化 innodb 表空间"],["body","\n"],["body","when first installing MySQL on a system. See Section 4.4.2, “mysql_install_db — Initialize MySQL Data Directory”, and Section 2.10, “Postinstallation Setup and Testing”."],["body","\n"],["headingLink","mysql_plugin"],["heading","mysql_plugin"],["body","\n"],["body","This program configures MySQL server plugins. See Section 4.4.3, “mysql_plugin — Configure MySQL Server Plugins”."],["body","\n"],["headingLink","mysql_secure_installation"],["heading","mysql_secure_installation"],["body","\n"],["body","This program enables you to improve the security of your MySQL installation. See Section 4.4.4, “mysql_secure_installation — Improve MySQL Installation Security”."],["body","\n"],["headingLink","mysql_ssl_rsa_setup"],["heading","mysql_ssl_rsa_setup"],["body","\n"],["body","This program creates the SSL certificate and key files and RSA key-pair files required to support secure connections, if those files are missing. Files created by mysql_ssl_rsa_setup can be used for secure connections using SSL or RSA. See Section 4.4.5, “mysql_ssl_rsa_setup — Create SSL/RSA Files”."],["body","\n"],["headingLink","mysql_tzinfo_to_sql"],["heading","mysql_tzinfo_to_sql"],["body","\n"],["body","This program loads the time zone tables in the mysql database using the contents of the host system zoneinfo database (the set of files describing time zones). See Section 4.4.6, “mysql_tzinfo_to_sql — Load the Time Zone Tables”"],["body","\n"],["headingLink","mysql_upgrade"],["heading","mysql_upgrade"],["body","\n"],["body","This program is used after a MySQL upgrade operation. It updates the grant tables with any changes that have been made in newer versions of MySQL, and checks tables for incompatibilities and repairs them if necessary. See Section 4.4.7, “mysql_upgrade — Check and Upgrade MySQL Tables”."],["body","\n"],["headingLink","mysql-客户端程序"],["heading","mysql 客户端程序"],["body","\n"],["headingLink","mysql"],["heading","mysql"],["body","\n"],["body","mysql 命令行工具,从文件中执行批量 sql"],["body","\n"],["body","See Section 4.5.1, “mysql — The MySQL Command-Line Client”."],["body","\n"],["headingLink","mysqladmin"],["heading","mysqladmin"],["body","\n"],["body","执行管理员操作"],["body","\n"],["body","数据库创建"],["body","\n"],["body","重新加载授权表"],["body","\n"],["body","重开日志文件"],["body","\n"],["body","查询版本,进程,状态信息,"],["body","\n"],["body","See Section 4.5.2, “mysqladmin — A MySQL Server Administration Program”."],["body","\n"],["headingLink","mysqlcheck"],["heading","mysqlcheck"],["body","\n"],["body","检查维护分析 优化表,"],["body","\n"],["body","See Section 4.5.3, “mysqlcheck — A Table Maintenance Program”."],["body","\n"],["headingLink","mysqldump"],["heading","mysqldump"],["body","\n"],["body","导出数据库为文件"],["body","\n"],["body","See Section 4.5.4, “mysqldump — A Database Backup Program”."],["body","\n"],["headingLink","mysqlimport"],["heading","mysqlimport"],["body","\n"],["body","导入程序"],["body","\n"],["body","See Section 4.5.5, “mysqlimport — A Data Import Program”."],["body","\n"],["headingLink","mysqlpump"],["heading","mysqlpump"],["body","\n"],["body","A client that dumps a MySQL database into a file as SQL. See Section 4.5.6, “mysqlpump — A Database Backup Program”."],["body","\n"],["headingLink","mysqlsh"],["heading","mysqlsh"],["body","\n"],["body","MySQL Shell is an advanced client and code editor for MySQL Server. See MySQL Shell 8.0 (part of MySQL 8.0). In addition to the provided SQL functionality, similar to mysql,"],["body","\n"],["body","see Chapter 19, Using MySQL as a Document Store."],["body","\n"],["body","see Chapter 20, InnoDB Cluster."],["body","\n"],["headingLink","mysqlshow"],["heading","mysqlshow"],["body","\n"],["body","显示数据库信息"],["body","\n"],["body","数据库"],["body","\n"],["body","表"],["body","\n"],["body","列"],["body","\n"],["body","索引"],["body","\n"],["body","See Section 4.5.7, “mysqlshow — Display Database, Table, and Column Information”."],["body","\n"],["headingLink","mysqlslap"],["heading","mysqlslap"],["body","\n"],["body","负载模拟客户端"],["body","\n"],["body","See Section 4.5.8, “mysqlslap — A Load Emulation Client”."],["body","\n"],["headingLink","管理工具"],["heading","管理工具"],["body","\n"],["headingLink","innochecksum"],["heading","innochecksum"],["body","\n"],["body","离线 Innodb 文件校验和工具"],["body","\n"],["body","See Section 4.6.1, “innochecksum — Offline InnoDB File Checksum Utility”."],["body","\n"],["headingLink","myisam_ftdump"],["heading","myisam_ftdump"],["body","\n"],["body","查看 myisam 全文索引 信息的工具"],["body","\n"],["headingLink","myisamchk"],["heading","myisamchk"],["body","\n"],["body","描述,检查,优化 修复 MyISAM 表"],["body","\n"],["body","See Section 4.6.3, “myisamchk — MyISAM Table-Maintenance Utility”."],["body","\n"],["headingLink","myisamlog"],["heading","myisamlog"],["body","\n"],["body","处理 myisam 日志文件内容"],["body","\n"],["body","See Section 4.6.4, “myisamlog — Display MyISAM Log File Contents”."],["body","\n"],["headingLink","myisampack"],["heading","myisampack"],["body","\n"],["body","A utility that compresses MyISAM tables to produce smaller read-only tables. See Section 4.6.5, “myisampack — Generate Compressed, Read-Only MyISAM Tables”."],["body","\n"],["headingLink","mysql_config_editor"],["heading","mysql_config_editor"],["body","\n"],["body","A utility that enables you to store authentication credentials in a secure, encrypted login path"],["body","\n"],["body","file named .mylogin.cnf. See Section 4.6.6, “mysql_config_editor — MySQL Configuration Utility”."],["body","\n"],["headingLink","mysqlbinlog"],["heading","mysqlbinlog"],["body","\n"],["body","A utility for reading statements from a binary log. The log of executed statements contained in the binary log files can be used to help recover from a crash."],["body","\n"],["body","See Section 4.6.7, “mysqlbinlog — Utility for Processing Binary Log Files”."],["body","\n"],["headingLink","mysqldumpslow"],["heading","mysqldumpslow"],["body","\n"],["body","慢 sql 日志"],["body","\n"],["body","See Section 4.6.8, “mysqldumpslow — Summarize Slow Query Log Files”."],["body","\n"],["body","mysql 客户端 服务器通信时 使用如下环境变量"],["body","\n"],["body","Environment Variable"],["body","Meaning"],["body","\n"],["body","MYSQL_UNIX_PORT"],["body","The default Unix socket file; used for connections to localhost"],["body","\n"],["body","MYSQL_TCP_PORT"],["body","The default port number; used for TCP/IP connections"],["body","\n"],["body","MYSQL_PWD"],["body","The default password"],["body","\n"],["body","MYSQL_DEBUG"],["body","Debug trace options when debugging"],["body","\n"],["body","TMPDIR"],["body","The directory where temporary tables and files are created"],["body","\n\n\n"],["body","For a full list of environment variables used by MySQL programs, see Section 4.9, “Environment Variables”."],["body","\n"],["body","Use of MYSQL_PWD is insecure. See Section 6.1.2.1, “End-User Guidelines for Password Security”."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","mysql/mysql官方文档/mysql程序/使用mysql相关程序.html"],["title","使用mysql相关程序.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","选项参数与非选项参数"],["heading","选项参数与非选项参数"],["body","\n"],["body","选项参数以 - 或者 --   开始的命令选项"],["body","\n"],["body","非选项参数 提供额外信息给命令行程序"],["body","\n"],["body","例如 mysql命令 的第一个非选项 参数 为数据库名"],["body","\n"],["body","使用最多的选项 连接参数"],["body","\n"],["body","--host -h"],["body","\n"],["body","--user -u"],["body","\n"],["body","--password -p"],["body","\n"],["body","--port -P"],["body","\n"],["body","--socket -S 指定 UnixSocketFile 或者windows上的具名管道"],["body","\n"],["headingLink","指定选项参数的-几种方式"],["heading","指定选项参数的 几种方式"],["body","\n"],["headingLink","命令名称后面-跟随"],["heading","命令名称后面 跟随"],["body","\n\n"],["body","后面的覆盖前面的"],["body","\n\n"],["body","mysql --column-names --skip-column-names\n"],["body","\n\n"],["body","\n"],["body","-- 为选项全拼 - 为选项缩写"],["body","\n"],["body","\n"],["body","\n"],["body","缩写没有 等于号,选项名 与选项值 直接可以有 空格"],["body","\n"],["body","-h localhost or --host=localhost\n"],["body","\n"],["body","\n"],["body","\n"],["body","大小写敏感"],["body","\n"],["body","\n"],["body","\n"],["body","- _ 一致"],["body","\n"],["body","--skip-grant-tables and --skip_grant_tables是一样的\n"],["body","\n"],["body","\n"],["body","\n"],["body","数值类型可以 有 K M G"],["body","\n"],["body","\n\n"],["body","mysqladmin --count=1K --sleep=10 ping\nping服务器 1024次 每次等10s\n"],["body","\n\n"],["body","带有空格的 选项值 带引号"],["body","\n\n"],["body","shell> mysql -u root -p -e \"SELECT VERSION();SELECT NOW()\"\nEnter password: ******\n+------------+\n| VERSION()  |\n+------------+\n| 5.7.29     |\n+------------+\n+---------------------+\n| NOW()               |\n+---------------------+\n| 2019-09-03 10:36:28 |\n+---------------------+\nshell>\n"],["body","\n"],["headingLink","选项文件"],["heading","选项文件"],["body","\n"],["body","忽略空行"],["body","\n"],["body","头尾空格自动去除"],["body","\n"],["body","非空行有以下几种形式"],["body","\n\n"],["body","\n"],["body","注释 #comment, ;comment"],["body","\n"],["body","Comment lines start with # or ;. A # comment can start in the middle of a line as well."],["body","\n"],["body","\n"],["body","\n"],["body","[group]"],["body","\n"],["body","组号,或程序的 名称, 后面跟着的是 应用与该程序的 选项"],["body","\n\n"],["body","\n"],["body","the [mysqld] and [mysql] groups apply to the mysqld server and the mysql client program, respectively."],["body","\n"],["body","\n"],["body","\n"],["body","The [client] option group is read by all client programs provided in MySQL distributions (but not by mysqld)."],["body","\n"],["body","To understand how third-party client programs that use the C API can use option files, see the C API documentation at mysql_options()."],["body","\n"],["body","\n"],["body","\n"],["body","[mysqldump] enables mysqldump-specific options to override [client] options. 可以覆盖前面的 client指定的同名选项"],["body","\n"],["body","\n"],["body","\n"],["body","指定版本"],["body","\n"],["body","[mysqld-5.7]\nsql_mode=TRADITIONAL\n"],["body","\n"],["body","\n"],["body","\n"],["body","包含其他 选项文件"],["body","\n"],["body","!include /home/mydir/myopt.cnf\n\n查找目录\n!includedir /home/mydir, 查找任何以 .cnf .ini\nAny files to be found and included using the !includedir directive on Unix operating systems must have file names ending in .cnf. On Windows, this directive checks for files with the .ini or .cnf extension.\n"],["body","\n"],["body","\n\n"],["body","\n"],["body","\n"],["body","opt_name=value"],["body","\n"],["body","选项名, 相比命令行 选项 去掉 短横杠"],["body","\n"],["body","\n"],["body","\n"],["body","转义处理"],["body","\n"],["body","\\b, \\t, \\n, \\r, \\\\ \\s\nbackspace, tab, newline, carriage return, backslash, and space characters.\nwindows路径目录特殊处理\n\n\nbasedir=\"C:\\Program Files\\MySQL\\MySQL Server 5.7\"\nbasedir=\"C:\\\\Program Files\\\\MySQL\\\\MySQL Server 5.7\"\nbasedir=\"C:/Program Files/MySQL/MySQL Server 5.7\"\nbasedir=C:\\\\Program\\sFiles\\\\MySQL\\\\MySQL\\sServer\\s5.7\n"],["body","\n"],["body","\n"],["body","\n"],["body","影响 选项文件处理 的命令行参数"],["body","\n\n"],["body","\n"],["body","--print-defaults"],["body","\n"],["body","打印所有从 文件中读取的选项,密码会被掩盖"],["body","\n"],["body","\n"],["body","\n"],["body","--no-defaults"],["body","\n"],["body","不要从文件中读取选项"],["body","\n"],["body","有一个例外是 客户端程序 从  .mylogin.cnf  文件中读取选项  不受影响"],["body","\n"],["body",".mylogin.cnf is created by the mysql_config_editor"],["body","\n"],["body","\n"],["body","\n"],["body","--login-path=name"],["body","\n"],["body","从 loginFile中读取 选项 , 其中的选项 涉及 登录信息,  mysql_config_editor 工具来编辑此文件"],["body","\n"],["body","mysql --login-path=mypath\n默认读取 [client] [mysql] 组\n还有  [mypath] 组\n"],["body","\n"],["body","指定备用登录文件组名"],["body","\n"],["body","MYSQL_TEST_LOGIN_FILE 环境变量\n"],["body","\n"],["body","\n"],["body","\n"],["body","--defaults-group-suffix=str"],["body","\n"],["body","指定其他组名前缀读取\n[client] and [mysql]\n--defaults-group-suffix=_other\nmysql also reads the [client_other] and [mysql_other] groups.\n"],["body","\n"],["body","\n"],["body","\n"],["body","--defaults-file=file_name"],["body","\n"],["body","给定 指定的 选项文件读取"],["body","\n"],["body","同样不会影响  .mylogin.cnf"],["body","\n"],["body","\n"],["body","\n"],["body","--defaults-extra-file=file_name"],["body","\n"],["body","全局选项文件 读取后 , 用户选项文件读取前,  .mylogin.cnf 读取前"],["body","\n"],["body","\n\n"],["body","\n"],["body","\n"],["body","程序选项 修饰符"],["body","\n"],["body","在查询时 不输出字段名"],["body","\n"],["body","--disable-column-names \n--skip-column-names \n--column-names=0\n"],["body","\n"],["body","--column-names\n--enable-column-names\n--column-names=1\n"],["body","\n"],["body","使用 --loose 前缀 时 如果不存在该选项 会警告 而不报错退出"],["body","\n"],["body","shell> mysql --loose-no-such-option\nmysql: WARNING: unknown option '--loose-no-such-option'\n"],["body","\n"],["body","--maximum 设置 session级别 变量 的最大值"],["body","\n"],["body"," --maximum-max_heap_table_size=32M  最大表堆的大小\n"],["body","\n"],["body","\n"],["body","\n"],["body","在执行时可以使用表达式. 在启动时使用标识符"],["body","\n"],["body"," mysql --max_allowed_packet=16M\n SET GLOBAL max_allowed_packet=16*1024*1024;\n"],["body","\n"],["body","\n\n"],["body","建立连接的命令行选项"],["body","\n"],["body","\n"],["body","Option Name"],["body","Description"],["body","Deprecated"],["body","\n"],["body","--default-auth"],["body","客户端认证所使用的插件,See Section 6.2.13, “Pluggable Authentication”."],["body","\n"],["body","--host"],["body","主机名或者IPV4 地址"],["body","\n"],["body","--password"],["body","密码,使用命令行输入密码不安全, See Section 6.1.2.1, “End-User Guidelines for Password Security”."],["body","\n"],["body","--pipe"],["body","Connect to server using named pipe (Windows only)"],["body","This option applies only if the server was started with the named_pipe system variable enabled to support named-pipe connections. In addition, the user making the connection must be a member of the Windows group specified by the named_pipe_full_access_group system variable."],["body","\n"],["body","--plugin-dir"],["body","寻找插件的目录,See Section 6.2.13, “Pluggable Authentication”."],["body","\n"],["body","--port"],["body","TCP/IP port number for connection"],["body","\n"],["body","--protocol"],["body","`TCP"],["body","SOCKET"],["body","\n"],["body","--secure-auth"],["body","Do not send passwords to server in old (pre-4.1) format"],["body","Section 6.4.1.3, “Migrating Away from Pre-4.1 Password Hashing and the mysql_old_password Plugin”."],["body","Yes"],["body","\n"],["body","--shared-memory-base-name"],["body","On Windows, the shared-memory name to use for connections made using shared memory to a local server. The default value is MYSQL. The shared-memory name is case-sensitive."],["body","This option applies only if the server was started with the shared_memory system variable enabled to support shared-memory connections."],["body","\n"],["body","--socket"],["body","Unix socket file 用来做本地连接,默认名是 /tmp/mysql.sock"],["body","在Windows是命名管道,默认名是MySQL"],["body","On Windows, this option applies only if the server was started with the named_pipe system variable enabled to support named-pipe connections. In addition, the user making the connection must be a member of the Windows group specified by the named_pipe_full_access_group system variable."],["body","\n"],["body","--user"],["body","MySQL user name to use when connecting to server"],["body","\n\n\n"],["body","协议介绍"],["body","\n"],["body","--protocol Value"],["body","Transport Protocol Used"],["body","Applicable Platforms"],["body","\n"],["body","TCP"],["body","TCP/IP transport to local or remote server"],["body","All"],["body","\n"],["body","SOCKET"],["body","Unix socket-file transport to local server"],["body","Unix and Unix-like systems"],["body","\n"],["body","PIPE"],["body","Named-pipe transport to local server"],["body","Windows"],["body","\n"],["body","MEMORY"],["body","Shared-memory transport to local server"],["body","Windows"],["body","\n\n\n"],["body","加密连接选项"],["body","\n"],["body","Option Name"],["body","Description"],["body","Introduced"],["body","\n"],["body","--get-server-public-key"],["body","Request RSA public key from server"],["body","5.7.23"],["body","\n"],["body","--server-public-key-path"],["body","Path name to file containing RSA public key"],["body","\n"],["body","--skip-ssl"],["body","Disable connection encryption"],["body","\n"],["body","--ssl"],["body","Enable connection encryption"],["body","\n"],["body","--ssl-ca"],["body","File that contains list of trusted SSL Certificate Authorities"],["body","\n"],["body","--ssl-capath"],["body","Directory that contains trusted SSL Certificate Authority certificate files"],["body","\n"],["body","--ssl-cert"],["body","File that contains X.509 certificate"],["body","\n"],["body","--ssl-cipher"],["body","Permissible ciphers for connection encryption"],["body","\n"],["body","--ssl-crl"],["body","File that contains certificate revocation lists"],["body","\n"],["body","--ssl-crlpath"],["body","Directory that contains certificate revocation-list files"],["body","\n"],["body","--ssl-key"],["body","File that contains X.509 key"],["body","\n"],["body","--ssl-mode"],["body","Desired security state of connection to server"],["body","5.7.11"],["body","\n"],["body","--ssl-verify-server-cert"],["body","Verify host name against server certificate Common Name identity"],["body","\n"],["body","--tls-version"],["body","Permissible TLS protocols for encrypted connections"],["body","5.7.10"],["body","\n\n\n"],["body","--ssl-mode=mode"],["body","\n\n"],["body","\n"],["body","disable"],["body","\n"],["body","没有加密的连接"],["body","\n"],["body"," --ssl=0 option or its synonyms (--skip-ssl, --disable-ssl).\n"],["body","\n"],["body","\n"],["body","\n"],["body","PREFERRED"],["body","\n"],["body","尝试建立加密连接,如果不能建立 则建立非加密连接"],["body","\n"],["body","\n"],["body","\n"],["body","REQUIRED"],["body","\n"],["body","需要建立加密连接,否则无法建立连接"],["body","\n"],["body","\n"],["body","\n"],["body","VERIFY_CA"],["body","\n"],["body","\n\n"],["headingLink","读环境变量"],["heading","读环境变量"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","mysql/mysql官方文档/mysql程序.html"],["title","mysql程序.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","mysql-server-启停脚本"],["heading","mysql server 启停脚本"],["body","\n"],["headingLink","mysqld"],["heading","mysqld"],["body","\n"],["body","服务程序,主要进程"],["body","\n"],["headingLink","mysqld_safe"],["heading","mysqld_safe"],["body","\n"],["body","启动脚本"],["body","\n"],["body","mysqld_safe attempts to start mysqld"],["body","\n"],["body","See Section 4.3.2, “mysqld_safe — MySQL Server Startup Script”."],["body","\n"],["headingLink","mysqlserver"],["heading","mysql.server"],["body","\n"],["body","启动脚本"],["body","\n"],["body","使用在 system-V 风格的  通过目录脚本划分 系统特定服务等级"],["body","\n"],["body","它调用了 mysqld_safe to start the MySQL server."],["body","\n"],["body","See Section 4.3.3, “mysql.server — MySQL Server Startup Script”."],["body","\n"],["headingLink","mysqld_multi"],["heading","mysqld_multi"],["body","\n"],["body","多服务启停"],["body","\n"],["body","See Section 4.3.4, “mysqld_multi — Manage Multiple MySQL Servers”."],["body","\n"],["headingLink","安装升级脚本"],["heading","安装升级脚本"],["body","\n"],["headingLink","comp_err"],["heading","comp_err"],["body","\n"],["body","在编译构建过程中, 从错误文件中 解析出错误消息"],["body","\n"],["body","See Section 4.4.1, “comp_err — Compile MySQL Error Message File”."],["body","\n"],["headingLink","mysql_install_db"],["heading","mysql_install_db"],["body","\n"],["body","初始化 mysql data目录"],["body","\n"],["body","创建mysql数据库"],["body","\n"],["body","初始化默认表权限"],["body","\n"],["body","初始化innodb 表空间"],["body","\n"],["body","when first installing MySQL on a system. See Section 4.4.2, “mysql_install_db — Initialize MySQL Data Directory”, and Section 2.10, “Postinstallation Setup and Testing”."],["body","\n"],["headingLink","mysql_plugin"],["heading","mysql_plugin"],["body","\n"],["body","This program configures MySQL server plugins. See Section 4.4.3, “mysql_plugin — Configure MySQL Server Plugins”."],["body","\n"],["headingLink","mysql_secure_installation"],["heading","mysql_secure_installation"],["body","\n"],["body","This program enables you to improve the security of your MySQL installation. See Section 4.4.4, “mysql_secure_installation — Improve MySQL Installation Security”."],["body","\n"],["headingLink","mysql_ssl_rsa_setup"],["heading","mysql_ssl_rsa_setup"],["body","\n"],["body","This program creates the SSL certificate and key files and RSA key-pair files required to support secure connections, if those files are missing. Files created by mysql_ssl_rsa_setup can be used for secure connections using SSL or RSA. See Section 4.4.5, “mysql_ssl_rsa_setup — Create SSL/RSA Files”."],["body","\n"],["headingLink","mysql_tzinfo_to_sql"],["heading","mysql_tzinfo_to_sql"],["body","\n"],["body","This program loads the time zone tables in the mysql database using the contents of the host system zoneinfo database (the set of files describing time zones). See Section 4.4.6, “mysql_tzinfo_to_sql — Load the Time Zone Tables”"],["body","\n"],["headingLink","mysql_upgrade"],["heading","mysql_upgrade"],["body","\n"],["body","This program is used after a MySQL upgrade operation. It updates the grant tables with any changes that have been made in newer versions of MySQL, and checks tables for incompatibilities and repairs them if necessary. See Section 4.4.7, “mysql_upgrade — Check and Upgrade MySQL Tables”."],["body","\n"],["headingLink","mysql客户端程序"],["heading","mysql客户端程序"],["body","\n"],["headingLink","mysql"],["heading","mysql"],["body","\n"],["body","mysql命令行工具,从文件中执行批量sql"],["body","\n"],["body","See Section 4.5.1, “mysql — The MySQL Command-Line Client”."],["body","\n"],["headingLink","mysqladmin"],["heading","mysqladmin"],["body","\n"],["body","执行管理员操作"],["body","\n"],["body","数据库创建"],["body","\n"],["body","重新加载授权表"],["body","\n"],["body","重开日志文件"],["body","\n"],["body","查询版本,进程,状态信息,"],["body","\n"],["body","See Section 4.5.2, “mysqladmin — A MySQL Server Administration Program”."],["body","\n"],["headingLink","mysqlcheck"],["heading","mysqlcheck"],["body","\n"],["body","检查维护分析 优化表,"],["body","\n"],["body","See Section 4.5.3, “mysqlcheck — A Table Maintenance Program”."],["body","\n"],["headingLink","mysqldump"],["heading","mysqldump"],["body","\n"],["body","导出数据库为文件"],["body","\n"],["body","See Section 4.5.4, “mysqldump — A Database Backup Program”."],["body","\n"],["headingLink","mysqlimport"],["heading","mysqlimport"],["body","\n"],["body","导入程序"],["body","\n"],["body","See Section 4.5.5, “mysqlimport — A Data Import Program”."],["body","\n"],["headingLink","mysqlpump"],["heading","mysqlpump"],["body","\n"],["body","A client that dumps a MySQL database into a file as SQL. See Section 4.5.6, “mysqlpump — A Database Backup Program”."],["body","\n"],["headingLink","mysqlsh"],["heading","mysqlsh"],["body","\n"],["body","MySQL Shell is an advanced client and code editor for MySQL Server. See MySQL Shell 8.0 (part of MySQL 8.0). In addition to the provided SQL functionality, similar to mysql,"],["body","\n"],["body","see Chapter 19, Using MySQL as a Document Store."],["body","\n"],["body","see Chapter 20, InnoDB Cluster."],["body","\n"],["headingLink","mysqlshow"],["heading","mysqlshow"],["body","\n"],["body","显示数据库信息"],["body","\n"],["body","数据库"],["body","\n"],["body","表"],["body","\n"],["body","列"],["body","\n"],["body","索引"],["body","\n"],["body","See Section 4.5.7, “mysqlshow — Display Database, Table, and Column Information”."],["body","\n"],["headingLink","mysqlslap"],["heading","mysqlslap"],["body","\n"],["body","负载模拟客户端"],["body","\n"],["body","See Section 4.5.8, “mysqlslap — A Load Emulation Client”."],["body","\n"],["headingLink","管理工具"],["heading","管理工具"],["body","\n"],["headingLink","innochecksum"],["heading","innochecksum"],["body","\n"],["body","离线Innodb 文件校验和工具"],["body","\n"],["body","See Section 4.6.1, “innochecksum — Offline InnoDB File Checksum Utility”."],["body","\n"],["headingLink","myisam_ftdump"],["heading","myisam_ftdump"],["body","\n"],["body","查看 myisam 全文索引 信息的工具"],["body","\n"],["headingLink","myisamchk"],["heading","myisamchk"],["body","\n"],["body","描述,检查,优化 修复MyISAM 表"],["body","\n"],["body","See Section 4.6.3, “myisamchk — MyISAM Table-Maintenance Utility”."],["body","\n"],["headingLink","myisamlog"],["heading","myisamlog"],["body","\n"],["body","处理myisam 日志文件内容"],["body","\n"],["body","See Section 4.6.4, “myisamlog — Display MyISAM Log File Contents”."],["body","\n"],["headingLink","myisampack"],["heading","myisampack"],["body","\n"],["body","A utility that compresses MyISAM tables to produce smaller read-only tables. See Section 4.6.5, “myisampack — Generate Compressed, Read-Only MyISAM Tables”."],["body","\n"],["headingLink","mysql_config_editor"],["heading","mysql_config_editor"],["body","\n"],["body","A utility that enables you to store authentication credentials in a secure, encrypted login path"],["body","\n"],["body","file named .mylogin.cnf. See Section 4.6.6, “mysql_config_editor — MySQL Configuration Utility”."],["body","\n"],["headingLink","mysqlbinlog"],["heading","mysqlbinlog"],["body","\n"],["body","A utility for reading statements from a binary log. The log of executed statements contained in the binary log files can be used to help recover from a crash."],["body","\n"],["body","See Section 4.6.7, “mysqlbinlog — Utility for Processing Binary Log Files”."],["body","\n"],["headingLink","mysqldumpslow"],["heading","mysqldumpslow"],["body","\n"],["body","慢sql日志"],["body","\n"],["body","See Section 4.6.8, “mysqldumpslow — Summarize Slow Query Log Files”."],["body","\n"],["body","mysql客户端 服务器通信时 使用如下环境变量"],["body","\n"],["body","Environment Variable"],["body","Meaning"],["body","\n"],["body","MYSQL_UNIX_PORT"],["body","The default Unix socket file; used for connections to localhost"],["body","\n"],["body","MYSQL_TCP_PORT"],["body","The default port number; used for TCP/IP connections"],["body","\n"],["body","MYSQL_PWD"],["body","The default password"],["body","\n"],["body","MYSQL_DEBUG"],["body","Debug trace options when debugging"],["body","\n"],["body","TMPDIR"],["body","The directory where temporary tables and files are created"],["body","\n\n\n"],["body","For a full list of environment variables used by MySQL programs, see Section 4.9, “Environment Variables”."],["body","\n"],["body","Use of MYSQL_PWD is insecure. See Section 6.1.2.1, “End-User Guidelines for Password Security”."],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","mysql/mysqlfunction.html"],["title","mysqlfunction.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["body","find_in_set\ngroup_concat"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","mysql/mysql公用表达式.html"],["title","mysql公用表达式.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","什么是公用表表达式cte"],["heading","什么是公用表表达式(CTE)?"],["body","\n"],["body","common table expression"],["body","\n\n"],["body","\n"],["body","公用表表达式是一个命名的临时结果集"],["body","\n"],["body","\n"],["body","\n"],["body","仅在单个SQL语句  SELECT，INSERTUPDATE或DELETE的执行范围内存在"],["body","\n"],["body","\n"],["body","\n"],["body","CTE不作为对象存储，仅在查询执行期间持续。 与派生表不同，CTE可以是自引用(递归CTE)，也可以在同一查询中多次引用。 此外，与派生表相比，CTE提供了更好的可读性和性能。"],["body","\n"],["body","\n\n"],["headingLink","mysql-cte语法"],["heading","MySQL CTE语法"],["body","\n"],["body","组成"],["body","\n"],["body","with cte_name (column_list) as (\n\tquery\n)\nSELECT * FROM cte_name\n"],["body","\n"],["body","请注意，查询中的列数必须与column_list中的列数相同。 如果省略column_list，CTE将使用定义CTE的查询的列列表"],["body","\n"],["headingLink","多个cte"],["heading","多个CTE"],["body","\n"],["body","WITH salesrep AS (\n    SELECT \n        employeeNumber,\n        CONCAT(firstName, ' ', lastName) AS salesrepName\n    FROM\n        employees\n    WHERE\n        jobTitle = 'Sales Rep'\n),\ncustomer_salesrep AS (\n    SELECT \n        customerName, salesrepName\n    FROM\n        customers\n            INNER JOIN\n        salesrep ON employeeNumber = salesrepEmployeeNumber\n)\nSELECT \n    *\nFROM\n    customer_salesrep\nORDER BY customerName;\n"],["body","\n"],["headingLink","递归查询"],["heading","递归查询"],["body","\n"],["body","\nWITH RECURSIVE test(id, name, path)\nAS\n(\n    // 初始值\nSELECT id, name, CAST(id AS CHAR(200))\nFROM emp WHERE manager_id IS NULL\nUNION ALL\n    //递归查询\nSELECT e.id, e.name, CONCAT(ep.path, ',', e.id)\nFROM test AS ep JOIN emp AS e  ON ep.id = e.manager_id\n)SELECT * FROM test ORDER BY path;\n"],["body","\n"],["headingLink","使用cte的好处"],["heading","使用CTE的好处"],["body","\n"],["body","○ 可读性：CTE提高了可读性。而不是将所有查询逻辑都集中到一个大型查询中，而是创建几个CTE，它们将在语句的后面组合。这使您可以获得所需的数据块，并将它们组合在最终的SELECT中。"],["body","\n"],["body","○ 替代视图：您可以用CTE替换视图。如果您没有创建视图对象的权限，或者您不想创建一个视图对象，因为它仅在此一个查询中使用，这很方便。"],["body","\n"],["body","○ 递归：使用CTE会创建递归查询，即可以调用自身的查询。当您需要处理组织结构图等分层数据时，这很方便。"],["body","\n"],["body","○ 限制：克服SELECT语句限制，例如引用自身（递归）或使用非确定性函数执行GROUP BY。"],["body","\n"],["body","○ 排名：每当你想使用排名函数，如ROW_NUMBER()，RANK()，NTILE()等。"],["body","\n"],["headingLink","子查询派生表临时表"],["heading","子查询,派生表,临时表"],["body","\n"],["headingLink","子查询"],["heading","子查询"],["body","\n"],["body","子查询是嵌套在另一个查询(如select、insert、update和delete)中的查询。子查询又称为内部查询，而包含子查询的查询称为外部查询。 子查询可以在使用表达式的任何地方使用，并且必须在括号中关闭。"],["body","\n"],["headingLink","派生表"],["heading","派生表"],["body","\n"],["body","派生表和子查询通常可以互换使用，但是与子查询不同的是，派生表必须具有别名"],["body","\n"],["body","\nSELECT column_list  FROM\n    ( SELECT column_list  FROM table_1) derived_table_name   --派生表\nWHERE derived_table_name.c1 > 0;\n"],["body","\n"],["body","派生表之间不可以相互引用。例如：SELECT ... FROM (SELECT ... FROM ...) AS d1, (SELECT ... FROM d1 ...) AS d2，第一个查询标记为d1，在第二个查询语句中使用d1是不允许的。"],["body","\n"],["headingLink","临时表"],["heading","临时表"],["body","\n"],["body","临时表是一种特殊类型的表，它允许您存储一个临时结果集，可以在单个会话中多次重用。"],["body","\n\n"],["body","\n"],["body","使用CREATE TEMPORARY TABLE语句创建临时表。请注意，在CREATE和TABLE关键字之间添加TEMPORARY关键字。"],["body","\n"],["body","\n"],["body","\n"],["body","当会话结束或连接终止时，MySQL会自动删除临时表。当您不再使用临时表时，也可以使用DROP TABLE语句来显式删除临时表。"],["body","\n"],["body","\n"],["body","\n"],["body","一个临时表只能由创建它的客户机访问。不同的客户端可以创建具有相同名称的临时表，而不会导致错误，因为只有创建临时表的客户端才能看到它。 但是，在同一个会话中，两个临时表不能共享相同的名称。"],["body","\n"],["body","\n"],["body","\n"],["body","临时表可以与数据库中的普通表具有相同的名称。 不推荐使用相同名称。例如，如果在示例数据库(yiibaidb)中创建一个名为employees的临时表，则现有的employees表将变得无法访问。 对employees表发出的每个查询现在都是指employees临时表。 当删除您临时表时，永久employees表可以再次访问。"],["body","\n"],["body","\n\n"],["body","\nCREATE TEMPORARY TABLE table_name (\n    name VARCHAR(10) NOT NULL,\n    value INTEGER NOT NULL\n  )\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","mysql/mysql日志/慢sql日志.html"],["title","慢sql日志.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","慢查询"],["heading","慢查询"],["body","\n"],["body","慢查询，到底多慢才叫慢？有没有统一的标准？其实呀，这并没有统一的标准，每个公司，甚至同一公司不同场景(数据库)都会有不同标准"],["body","\n"],["body","像OLTP(联机事务处理), OLAP (联机分析处理) 这两者对慢查询的标准就不一样"],["body","\n"],["body","OLAP 实时性则没那么高，对慢查询容错性也会更高些"],["body","\n"],["body","而OLTP(联机事务处理)属于事务处理型，实时性要求高，响应时间快，对慢查询几乎零容忍"],["body","\n"],["body","一个成熟系统中，有监控系统实时监控线上查询运行状态，提前将慢查询筛选出来，也是避免生产事故，降低风险的有效措施"],["body","\n"],["body","有些数据库中也内置慢查询监控，如：MySQL慢查询日志就是其一。"],["body","\n"],["headingLink","开启慢查询日志"],["heading","开启慢查询日志"],["body","\n"],["body","在 MySQL中，提供了慢查询查询日志，基于性能方面的考虑，该配置默认为OFF(关闭) 状态。那么如何开启慢日志查询呢？其步骤如下："],["body","\n"],["body","show variables like \"slow_query_log\";\nset global slow_query_log = \"ON\";\nshow variables like \"slow_query_log_file\";\n# 其中: path 表示路径， filename 表示文件名，如果不指定，其默认filename 为hostname。\nset global slow_query_log_file = ${path}/${filename}.log;\n"],["body","\n"],["body","慢查询 查询时间，当SQL执行时间超过该值时，则会记录在slow_query_log_file 文件中，其默认为 10 ，最小值为 0，(单位：秒)。"],["body","\n"],["body"," show variables like \"long_query_time\";\n set global long_query_time = 5;\n"],["body","\n"],["body","当设置值小于0时，默认为 0。"],["body","\n"],["body","通过上述设置后，退出当前会话或者开启一个新的会话，执行如下命令："],["body","\n"],["body","select sleep(11);\n"],["body","\n"],["body","# Time: 200310 13:30:57\n# User@Host: root[root] @ localhost []  Id: 21528\n# Query_time: 6.000164  Lock_time: 0.000000 Rows_sent: 1  Rows_examined: 0\nSET timestamp=1583818257;\nselect sleep(6);\n"],["body","\n"],["headingLink","慢查询日志文件"],["heading","慢查询日志文件"],["body","\n\n"],["body","慢查询日志以#作为起始符。"],["body","\n"],["body","User@Host：表示用户 和 慢查询查询的ip地址。"],["body","\n"],["body","如上所述，表示 root用户 localhost地址。"],["body","\n"],["body","Query_time: 表示SQL查询持续时间， 单位 (秒)。"],["body","\n"],["body","Lock_time: 表示获取锁的时间， 单位(秒)。"],["body","\n"],["body","Rows_sent: 表示发送给客户端的行数。"],["body","\n"],["body","Rows_examined: 表示：服务器层检查的行数。"],["body","\n"],["body","set timestamp ：表示 慢SQL 记录时的时间戳。"],["body","\n"],["body","其中 select sleep(6) 则表示慢SQL语句。"],["body","\n\n"],["headingLink","注意事项"],["heading","注意事项"],["body","\n\n"],["body","在 MySQL 中，慢查询日志中默认不记录管理语句，如："],["body","\n\n"],["body","alter table, analyze table，check table等。"],["body","\n"],["body","set global log_slow_admin_statements = \"ON\";\n"],["body","\n\n"],["body","在 MySQL 中，还可以设置将未走索引的SQL语句记录在慢日志查询文件中(默认为关闭状态)。通过下述属性即可进行设置："],["body","\n\n"],["body","set global log_queries_not_using_indexes = \"ON\";\n"],["body","\n\n"],["body","日志输出格式有支持：FILE(默认)，TABLE 两种，可进行组合使用。如下所示:\nset global log_output = \"FILE,TABLE\";"],["body","\n\n"],["body","这样设置会同时在 FILE, mysql库中的slow_log表中同时写入。"],["body","\n"],["body"," select * from slow_log;\n"],["body","\n"],["headingLink","特别注意"],["heading","特别注意"],["body","\n\n"],["body","设置该属性后，只要SQL未走索引，即使查询时间小于long_query_time值，也会记录在慢SQL日志文件中。"],["body","\n"],["body","该设置会导致慢日志快速增长，开启前建议检查慢查询日志文件所在磁盘空间是否充足。"],["body","\n"],["body","在生产环境中，不建议开启该参数。"],["body","\n\n"],["headingLink","删除慢查询日志"],["heading","删除慢查询日志"],["body","\n"],["body","mysqladmin -uroot -p flush-logs\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","mysql/mysql日志/README.html"],["title","mysql日志 - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","mysql日志"],["heading","MySQL日志"],["body","\n"],["body","任何一种数据库，都会拥有各种各样的日志，用来记录数据库的运行情况、日常操作和错误等信息，可以帮助我们诊断数据库出现的各种问题。"],["body","\n"],["body","MySQL 也不例外，它有不同类型的日志文件，各自存储了不同类型的日志。分析这些日志文件，除了可以了解 MySQL 数据库的运行情况，还可以为 MySQL 的管理和优化提供必要的信息。"],["body","\n"],["body","日志管理是维护数据库的重要步骤，所以经常需要在 MySQL 中进行日志启动、查看、停止和删除等操作。这些操作是数据库管理中最基本、最重要的操作。本章将介绍 MySQL 中各种日志的作用和使用。"],["body","\n"],["body","MySQL日志及分类"],["body","\n"],["body","MySQL错误日志（Error Log）详解"],["body","\n"],["body","MySQL二进制日志（Binary Log）详解"],["body","\n"],["body","MySQL通用查询日志（General Query Log）"],["body","\n"],["body","MySQL慢查询日志（Slow Query Log）"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","mysql/mysql日志/mysql日志及分类.html"],["title","mysql日志及分类.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","前言"],["heading","前言"],["body","\n"],["body","日志是数据库的重要组成部分，主要用来记录数据库的运行情况、日常操作和错误信息。"],["body","\n"],["body","在 MySQL 中，日志可以分为"],["body","\n\n"],["body","\n"],["body","二进制日志"],["body","\n"],["body","该日志文件会以二进制的形式记录数据库的各种操作，但不记录查询语句。"],["body","\n"],["body","\n"],["body","\n"],["body","错误日志"],["body","\n"],["body","该日志文件会记录 MySQL 服务器的启动、关闭和运行错误等信息。"],["body","\n"],["body","\n"],["body","\n"],["body","通用查询日志"],["body","\n"],["body","该日志记录 MySQL 服务器的启动和关闭信息、客户端的连接信息、更新、查询数据记录的 SQL 语句等。"],["body","\n"],["body","\n"],["body","\n"],["body","慢查询日志"],["body","\n"],["body","记录执行事件超过指定时间的操作，通过工具分析慢查询日志可以定位 MySQL 服务器性能瓶颈所在。"],["body","\n"],["body","\n\n"],["body","为了维护 MySQL 数据库，经常需要在 MySQL 中进行日志操作，包含日志文件的启动、查看、停止和删除等，这些操作都是数据库管理中最基本、最重要的操作。"],["body","\n"],["body","例如，当用户 root 登录到 MySQL 服务器后，就会在日志文件里记录该用户的登录事件、执行操作等信息。当 MySQL 服务器运行时出错，出错信息就会被记录到日志文件里。"],["body","\n"],["body","日志操作是数据库维护中最重要的手段之一"],["body","\n\n"],["body","\n"],["body","如果 MySQL 数据库系统意外停止服务，我们可以通过错误日志查看出现错误的原因"],["body","\n"],["body","\n"],["body","\n"],["body","还可以通过二进制日志文件来查看用户分别执行了哪些操作、对数据库文件做了哪些修改"],["body","\n"],["body","\n"],["body","\n"],["body","然后，还可以根据二进制日志中的记录来修复数据库。"],["body","\n"],["body","\n\n"],["body","默认只启动错误日志"],["body","\n"],["body","在 MySQL 所支持的日志文件里，除了二进制日志文件外，其它日志文件都是文本文件。默认情况下，MySQL 只会启动错误日志文件，而其它日志则需要手动启动。"],["body","\n"],["body","缺点与优点"],["body","\n"],["body","使用日志有优点也有缺点。启动日志后，虽然可以对 MySQL 服务器性能进行维护，但是会降低 MySQL 的执行速度。例如，一个查询操作比较频繁的 MySQL 中，记录通用查询日志和慢查询日志要花费很多的时间。"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","mysql/mysql日志/通用日志.html"],["title","通用日志.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","mysql通用查询日志general-query-log"],["heading","MySQL通用查询日志（General Query Log）"],["body","\n"],["body","通用查询日志（General Query Log）用来记录用户的所有操作，包括启动和关闭 MySQL 服务、更新语句和查询语句等。"],["body","\n"],["body","默认情况下，通用查询日志功能是关闭的。可以通过以下命令查看通用查询日志是否开启，命令如下："],["body","\n"],["body"," SHOW VARIABLES LIKE '%general%';\n"],["body","\n"],["body","从结果可以看出，通用查询日志是关闭的，general_log_file 变量指定了通用查询日志文件所在的位置。"],["body","\n"],["headingLink","启动和设置通用查询日志"],["heading","启动和设置通用查询日志"],["body","\n"],["body","[mysqld]\nlog=dir/filename\n"],["body","\n"],["body","其中，dir 参数指定通用查询日志的存储路径；filename 参数指定日志的文件名。如果不指定存储路径，通用查询日志将默认存储到 MySQL 数据库的数据文件夹下。如果不指定文件名，默认文件名为 hostname.log，其中 hostname 表示主机名。"],["body","\n"],["headingLink","查看通用查询日志"],["heading","查看通用查询日志"],["body","\n"],["body","如果希望了解用户最近的操作，可以查看通用查询日志。通用查询日志以文本文件的形式存储，可以使用普通文本文件查看该类型日志内容。"],["body","\n"],["body","SHOW VARIABLES LIKE '%general%';\nuse test;\nSELECT * FROM tb_student;\n"],["body","\n"],["body","执行成功后，打开通用查询日志，这里日志名称为 LAPTOP-UHQ6V8KP.log，下面是通用查询日志中的部分内容。"],["body","\n"],["body","Time                 Id Command    Argument\n2020-05-29T06:43:44.382878Z     7 Quit\n2020-05-29T06:44:10.001382Z     8 Connect root@localhost on  using SSL/TLS\n2020-05-29T06:44:10.007532Z     8 Query select @@version_comment limit 1\n2020-05-29T06:44:11.748179Z     8 Query SHOW VARIABLES LIKE '%general%'\n2020-05-29T06:44:25.487472Z     8 Query SELECT DATABASE()\n2020-05-29T06:44:25.487748Z     8 Init DB test\n2020-05-29T06:44:35.390523Z     8 Query SELECT * FROM tb_student\n"],["body","\n"],["headingLink","停止通用查询日志"],["heading","停止通用查询日志"],["body","\n"],["body"," SET GLOBAL general_log=off;\n"],["body","\n"],["headingLink","删除通用查询日志"],["heading","删除通用查询日志"],["body","\n"],["body","mysqladmin -uroot -p flush-logs\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","mysql/mysql日志/二进制日志.html"],["title","二进制日志.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","前言"],["heading","前言"],["body","\n"],["body","二进制日志（Binary Log）也可叫作变更日志（Update Log），是 MySQL 中非常重要的日志。"],["body","\n"],["body","主要用于记录数据库的变化情况，即 SQL 语句的 DDL 和 DML 语句，不包含数据记录查询操作。"],["body","\n"],["body","如果 MySQL 数据库意外停止，可以通过二进制日志文件来查看用户执行了哪些操作，对数据库服务器文件做了哪些修改，然后根据二进制日志文件中的记录来恢复数据库服务器。"],["body","\n"],["body","默认情况下，二进制日志功能是关闭的。可以通过以下命令查看二进制日志是否开启，命令如下："],["body","\n"],["body"," SHOW VARIABLES LIKE 'log_bin';\n"],["body","\n"],["headingLink","启动和设置二进制日志"],["heading","启动和设置二进制日志"],["body","\n"],["body","在 MySQL 中，可以通过在配置文件中添加 log-bin 选项来开启二进制日志，格式如下："],["body","\n"],["body","[mysqld]\nlog-bin=dir/[filename]\n"],["body","\n"],["body","其中，dir 参数指定二进制文件的存储路径；filename 参数指定二进制文件的文件名，其形式为 filename.number，number 的形式为 000001、000002 等。"],["body","\n"],["body","每次重启 MySQL 服务后，都会生成一个新的二进制日志文件，这些日志文件的文件名中 filename 部分不会改变，number 会不断递增。"],["body","\n"],["body","如果没有 dir 和 filename 参数，二进制日志将默认存储在数据库的数据目录下，"],["body","\n"],["body","默认的文件名为 hostname-bin.number，其中 hostname 表示主机名。"],["body","\n"],["body","下面在 my.ini 文件的 [mysqld] 组中添加以下语句："],["body","\n"],["body","log-bin\n"],["body","\n"],["body","重启 MySQL 服务器后，可以在 MySQL 数据库的数据目录下看到 LAPTOP-UHQ6V8KP-bin.000001 这个文件，同时还生成了 LAPTOP-UHQ6V8KP-bin.index 文件。"],["body","\n"],["body","还可以在 my.ini 文件的 [mysqld] 组中进行如下修改。语句如下："],["body","\n"],["body","log-bin=C:log\\mylog\n"],["body","\n"],["headingLink","查看二进制日志"],["heading","查看二进制日志"],["body","\n"],["headingLink","查看二进制日志文件列表"],["heading","查看二进制日志文件列表"],["body","\n"],["body"," SHOW binary logs;\n"],["body","\n"],["headingLink","查看当前正在写入的二进制日志文件"],["heading","查看当前正在写入的二进制日志文件"],["body","\n"],["body"," SHOW master status;\n"],["body","\n"],["headingLink","查看二进制日志文件内容"],["heading","查看二进制日志文件内容"],["body","\n"],["body","二进制日志使用二进制格式存储，不能直接打开查看。如果需要查看二进制日志，必须使用 mysqlbinlog 命令。"],["body","\n"],["body","mysqlbinlog filename.number\n"],["body","\n"],["body","mysqlbinlog 命令只在当前文件夹下查找指定的二进制日志，因此需要在二进制日志所在的目录下运行该命令，否则将会找不到指定的二进制日志文件。"],["body","\n"],["body","除了 filename.number 文件，MySQL 还会生成一个名为 filename.index 的文件，这个文件存储着所有二进制日志文件的列表，可以用记事本打开该文件。"],["body","\n"],["body","\n"],["body","小技巧：实际工作中，二进制日志文件与数据库的数据文件不放在同一块硬盘上，这样即使数据文件所在的硬盘被破坏，也可以使用另一块硬盘上的二进制日志来恢复数据库文件。两块硬盘同时坏了的可能性要小得多，这样可以保证数据库中数据的安全。"],["body","\n"],["body","\n"],["headingLink","删除二进制日志"],["heading","删除二进制日志"],["body","\n"],["body","二进制日志中记录着大量的信息，如果很长时间不清理二进制日志，将会浪费很多的磁盘空间。删除二进制日志的方法很多，下面介绍几种删除二进制日志的方法。"],["body","\n"],["headingLink","删除所有二进制日志"],["heading","删除所有二进制日志"],["body","\n"],["body","RESET MASTER;\n"],["body","\n"],["body","登录 MySQL 数据库后，可以执行该语句来删除所有二进制日志。删除所有二进制日志后，MySQL 将会重新创建新的二进制日志，新二进制日志的编号从 000001 开始。"],["body","\n"],["headingLink","根据编号删除二进制日志"],["heading","根据编号删除二进制日志"],["body","\n"],["body","每个二进制日志文件后面有一个 6 位数的编号，如 000001。使用 PURGE MASTER LOGS TO 语句，可以删除指定二进制日志的编号之前的日志。该语句的基本语法形式如下："],["body","\n"],["body","# 该语句将删除编号小于 filename.number 的所有二进制日志。\nPURGE MASTER LOGS TO 'filename.number';\n\n# 下面删除 mylog.000004 之前的二进制日志\nPURGE MASTER LOGS TO 'mylog.000004';\n#代码执行完后，编号为 000001、000002 和 000003 的二进制日志将被删除。\n"],["body","\n"],["headingLink","根据创建时间删除二进制日志"],["heading","根据创建时间删除二进制日志"],["body","\n"],["body","使用 PURGE MASTER LOGS TO 语句，可以删除指定时间之前创建的二进制日志，该语句的基本语法格式如下："],["body","\n"],["body","其中，“hh”为 24 制的小时。该语句将删除在指定时间之前创建的所有二进制日志。"],["body","\n"],["body","PURGE MASTER LOGS TO 'yyyy-mm-dd hh:MM:ss';\nPURGE MASTER LOGS TO '2019-12-20 15:00:00\";\n"],["body","\n"],["headingLink","暂时停止二进制日志"],["heading","暂时停止二进制日志"],["body","\n"],["body","在配置文件中设置了 log_bin 选项之后，MySQL 服务器将会一直开启二进制日志功能。删除该选项后就可以停止二进制日志功能，如果需要再次启动这个功能，需要重新添加 log_bin 选项。由于这样比较麻烦，所以 MySQL 提供了暂时停止二进制日志功能的语句。"],["body","\n"],["body","SET SQL_LOG_BIN=0/1;\n"],["body","\n"],["headingLink","其他参数"],["heading","其他参数"],["body","\n"],["body","# 定义了 MySQL 清除过期日志的时间、二进制日志自动删除的天数。默认值为 0，表示“没有自动删除”，当 MySQL 启动或刷新二进制日志时可能删除。\nexpire_logs_days = 10\n# 定义了单个文件的大小限制，如果二进制日志写入的内容大小超出给定值，日志就会发生滚动（关闭当前文件，重新打开一个新的日志文件  不能将该变量设置为大于 1GB 或小于 4096B（字节），其默认值是 1GB。\nmax_binlog_size = 100M\n"],["body","\n"],["headingLink","mysql使用二进制日志还原数据库"],["heading","MySQL使用二进制日志还原数据库"],["body","\n"],["body","数据库遭到意外损坏时，应该先使用最近的备份文件来还原数据库。另外备份之后，数据库可能进行了一些更新，这时可以使用二进制日志来还原。因为二进制日志中存储了更新数据库的语句，如 UPDATE 语句、INSERT 语句等。"],["body","\n"],["body","mysqlbinlog filename.number | mysql -u root -p\n"],["body","\n"],["body","以上命令可以理解成，先使用 mysqlbinlog 命令来读取 filename.number 中的内容，再使用 mysql 命令将这些内容还原到数据库中。"],["body","\n"],["body","因此，在备份 MySQL 数据库之后，应该删除备份之前的二进制日志。如果备份之后发生异常，造成数据库的数据损失，可以通过备份之后的二进制日志进行还原。"],["body","\n"],["body","使用 mysqlbinlog 命令进行还原操作时，必须是编号（number）小的先还原。例如，mylog.000001 必须在 mylog.000002 之前还原。"],["body","\n"],["body","mysqlbinlog mylog.000001 | mysql -u root -p\nmysqlbinlog mylog.000002 | mysql -u root -p\nmysqlbinlog mylog.000003 | mysql -u root -p\nmysqlbinlog mylog.000004 | mysql -u root -p\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","mysql/mysql日志/错误日志.html"],["title","错误日志.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","前言"],["heading","前言"],["body","\n"],["body","错误日志（Error Log）是 MySQL 中最常用的一种日志，主要记录 MySQL 服务器启动和停止过程中的信息、服务器在运行过程中发生的故障和异常情况等。"],["body","\n"],["headingLink","启动和设置错误日志"],["heading","启动和设置错误日志"],["body","\n"],["body","在 MySQL 数据库中，默认开启错误日志功能"],["body","\n"],["body","一般情况下，错误日志存储在 MySQL 数据库的数据文件夹下，通常名称为"],["body","\n"],["body","hostname.err 其中，hostname 表示 MySQL 服务器的主机名。"],["body","\n"],["body","配置文件定义错误日志"],["body","\n"],["body","在 MySQL 配置文件中，错误日志所记录的信息可以通过 log-error 和 log-warnings 来定义"],["body","\n"],["body","log-error：  定义是否启用错误日志功能和错误日志的存储位置"],["body","\n"],["body","log-warnings: 定义是否将警告信息也记录到错误日志中"],["body","\n"],["body","将 log_error 选项加入到 MySQL 配置文件的 [mysqld] 组中，形式如下："],["body","\n"],["body","# 其中，dir 参数指定错误日志的存储路径；filename 参数指定错误日志的文件名；省略参数时文件名默认为主机名，存放在 Data 目录中。\n[mysqld]\nlog-error=dir/{filename}\n"],["body","\n"],["body","重启 MySQL 服务后，参数开始生效，可以在指定路径下看到 filename.err 的文件，如果没有指定 filename，那么错误日志将直接默认为 hostname.err。"],["body","\n"],["body","注意：错误日志中记录的并非全是错误信息，例如 MySQL 如何启动 InnoDB 的表空间文件、如何初始化自己的存储引擎等，这些也记录在错误日志文件中。"],["body","\n"],["headingLink","查看错误日志"],["heading","查看错误日志"],["body","\n"],["body","错误日志中记录着开启和关闭 MySQL 服务的时间，以及服务运行过程中出现哪些异常等信息。如果 MySQL 服务出现异常，可以到错误日志中查找原因。"],["body","\n"],["body","在 MySQL 中，通过 SHOW 命令可以查看错误日志文件所在的目录及文件名信息。"],["body","\n"],["body","SHOW VARIABLES LIKE 'log_error';\n"],["body","\n"],["body","错误日志以文本文件的形式存储，直接使用普通文本工具就可以查看"],["body","\n"],["headingLink","删除错误日志"],["heading","删除错误日志"],["body","\n"],["body","# 执行该命令后，MySQL 服务器首先会自动创建一个新的错误日志，然后将旧的错误日志更名为 filename.err-old。\nmysqladmin -uroot -p flush-logs\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","mysql/mysqlzip安装.html"],["title","mysqlzip安装.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["body","配置环境变量"],["body","\n"],["body","在mysql安装路径下 建 mysql.ini,与 data目录"],["body","\n"],["body","[mysql]\n\n# 设置mysql客户端默认字符集\ndefault-character-set=utf8 \n\n[mysqld]\n\n#设置3306端口\nport = 3306 \n\n# 设置mysql的安装目录\nbasedir=F:\\mysql\\mysql-5.7.24-winx64\\mysql-5.7.24-winx64\n\n# 设置mysql数据库的数据的存放目录\ndatadir=F:\\mysql\\mysql-5.7.24-winx64\\mysql-5.7.24-winx64\\data\n\n# 允许最大连接数\nmax_connections=200\n\n# 服务端使用的字符集默认为8比特编码的latin1字符集\ncharacter-set-server=utf8\n\n# 创建新表时将使用的默认存储引擎\ndefault-storage-engine=INNODB\n"],["body","\n"],["body","初始化mysql服务"],["body","\n"],["body","mysqld --initialize-insecure --user=mysql\nmysqld install\nnet start mysql\n"],["body","\n"],["body","登录mysql修改密码"],["body","\n"],["body","mysql -u root -p\nmysqladmin -u root password\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","mysql/mysqlpdump.html"],["title","mysqlpdump.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","mysql-57-mysqlpump-备份工具说明"],["heading","MySQL 5.7 mysqlpump 备份工具说明"],["body","\n"],["body","mysqlpump和mysqldump一样，属于逻辑备份，备份以SQL形式的文本保存。逻辑备份相对物理备份的好处是不关心undo log的大小，直接备份数据即可。它最主要的特点是："],["body","\n\n"],["body","并行备份数据库和数据库中的对象的，加快备份过程。"],["body","\n"],["body","更好的控制数据库和数据库对象（表，存储过程，用户帐户）的备份。"],["body","\n"],["body","备份用户账号作为帐户管理语句（CREATE USER，GRANT），而不是直接插入到MySQL的系统数据库。"],["body","\n"],["body","备份出来直接生成压缩后的备份文件。"],["body","\n"],["body","备份进度指示（估计值）。"],["body","\n"],["body","重新加载（还原）备份文件，先建表后插入数据最后建立索引，减少了索引维护开销，加快了还原速度。"],["body","\n"],["body","备份可以排除或则指定数据库。"],["body","\n\n"],["headingLink","选项说明"],["heading","选项说明"],["body","\n"],["body","**参数：**绝大部分参数和mysqldump一致，顺便复习一下。对于mysqlpump参数会标记出来。"],["body","\n"],["headingLink","mysqldump与mysqlpump共同拥有"],["heading","mysqldump与mysqlpump共同拥有"],["body","\n"],["body","--add-drop-database：在建立库之前先执行删库操作。"],["body","\n"],["body","DROP DATABASE IF EXISTS `...`;\n"],["body","\n"],["body","--add-drop-table：在建表之前先执行删表操作。"],["body","\n"],["body","DROP TABLE IF EXISTS `...`.`...`;\n"],["body","\n"],["body","--add-locks：备份表时，使用LOCK TABLES和UNLOCK TABLES。**注意：**这个参数不支持并行备份，需要关闭并行备份功能：--default-parallelism=0"],["body","\n"],["body","LOCK TABLES `...`.`...` WRITE;\n...\nUNLOCK TABLES;\n"],["body","\n"],["body","--all-databases：备份所有库，-A。"],["body","\n"],["body","--bind-address：指定通过哪个网络接口来连接Mysql服务器（一台服务器可能有多个IP），防止同一个网卡出去影响业务。"],["body","\n"],["body","--complete-insert：dump出包含所有列的完整insert语句。"],["body","\n"],["body","--compress： 压缩客户端和服务器传输的所有的数据，-C。"],["body","\n"],["body","--databases：手动指定要备份的库，支持多个数据库，用空格分隔，-B。"],["body","\n"],["body","--default-character-set：指定备份的字符集。"],["body","\n"],["body","--events：备份数据库的事件，默认开启，关闭使用--skip-events参数。"],["body","\n"],["body","--insert-ignore：备份用insert ignore语句代替insert语句。"],["body","\n"],["body","--log-error-file：备份出现的warnings和erros信息输出到一个指定的文件。"],["body","\n"],["body","--max-allowed-packet：备份时用于client/server直接通信的最大buffer包的大小。"],["body","\n"],["body","--net-buffer-length：备份时用于client/server通信的初始buffer大小，当创建多行插入语句的时候，mysqlpump 创建行到N个字节长。"],["body","\n"],["body","--no-create-db：备份不写CREATE DATABASE语句。要是备份多个库，需要使用参数-B，而使用-B的时候会出现create database语句，该参数可以屏蔽create database 语句。"],["body","\n"],["body","--no-create-info：备份不写建表语句，即不备份表结构，只备份数据，-t。"],["body","\n"],["body","--hex-blob： 备份binary字段的时候使用十六进制计数法，受影响的字段类型有BINARY、VARBINARY、BLOB、BIT。"],["body","\n"],["body","--host ：备份指定的数据库地址，-h。"],["body","\n"],["body","--password：备份需要的密码。"],["body","\n"],["body","--port ：备份数据库的端口。"],["body","\n"],["body","--protocol={TCP|SOCKET|PIPE|MEMORY}：指定连接服务器的协议。"],["body","\n"],["body","--replace：备份出来replace into语句。"],["body","\n"],["body","--routines：备份出来包含存储过程和函数，默认开启，需要对 mysql.proc表有查看权限。生成的文件中会包含CREATE PROCEDURE 和 CREATE FUNCTION语句以用于恢复，关闭则需要用--skip-routines参数。"],["body","\n"],["body","--triggers：备份出来包含触发器，默认开启，使用--skip-triggers来关闭。"],["body","\n"],["body","--set-charset：备份文件里写SET NAMES default_character_set 到输出，此参默认开启。 -- skip-set-charset禁用此参数，不会在备份文件里面写出set names..."],["body","\n"],["body","--single-transaction：该参数在事务隔离级别设置成Repeatable Read，并在dump之前发送start transaction 语句给服务端。这在使用innodb时很有用，因为在发出start transaction时，保证了在不阻塞任何应用下的一致性状态。对myisam和memory等非事务表，还是会改变状态的，当使用此参的时候要确保没有其他连接在使用ALTER TABLE、CREATE TABLE、DROP TABLE、RENAME TABLE、TRUNCATE TABLE等语句，否则会出现不正确的内容或则失败。--add-locks和此参互斥，在mysql5.7.11之前，--default-parallelism大于1的时候和此参也互斥，必须使用--default-parallelism=0。5.7.11之后解决了--single-transaction和--default-parallelism的互斥问题。"],["body","\n"],["body","--skip-definer：忽略那些创建视图和存储过程用到的 DEFINER 和 SQL SECURITY 语句，恢复的时候，会使用默认值，否则会在还原的时候看到没有DEFINER定义时的账号而报错。"],["body","\n"],["body","--socket：对于连接到localhost，Unix使用套接字文件，在Windows上是命名管道的名称使用，-S。"],["body","\n"],["body","--ssl：--ssl参数将要被去除，用--ssl-mode取代。关于ssl相关的备份，请看官方文档。"],["body","\n"],["body","--tz-utc：备份时会在备份文件的最前几行添加SET TIME_ZONE='+00:00'。**注意：**如果还原的服务器不在同一个时区并且还原表中的列有timestamp字段，会导致还原出来的结果不一致。默认开启该参数，用 --skip-tz-utc来关闭参数。"],["body","\n"],["body","--user：备份时候的用户名，-u。"],["body","\n"],["body","--users：备份数据库用户，备份的形式是CREATE USER...，GRANT...，只备份数据库账号可以通过如下命令："],["body","\n"],["body","mysqlpump --exclude-databases=% --users    #过滤掉所有数据库\n"],["body","\n"],["body","--watch-progress：定期显示进度的完成，包括总数表、行和其他对象。该参数默认开启，用--skip-watch-progress来关闭。"],["body","\n"],["headingLink","mysqlpump新增功能"],["heading","mysqlpump新增功能"],["body","\n"],["body","--add-drop-user：在CREATE USER语句之前增加DROP USER，**注意：**这个参数需要和--users一起使用，否者不生效。"],["body","\n"],["body","DROP USER 'backup'@'192.168.123.%';\n"],["body","\n"],["body","--compress-output：默认不压缩输出，目前可以使用的压缩算法有LZ4和ZLIB。"],["body","\n"],["body","shell> mysqlpump --compress-output=LZ4 > dump.lz4\nshell> lz4_decompress dump.lz4 dump.txt\n\nshell> mysqlpump --compress-output=ZLIB > dump.zlib\nshell> zlib_decompress dump.zlib dump.txt\n"],["body","\n"],["body","--skip-dump-rows："],["body","\n"],["body","只备份表结构，不备份数据，-d。**注意：**mysqldump支持--no-data，mysqlpump不支持--no-data"],["body","\n"],["body","--default-parallelism：指定并行线程数，默认是2，如果设置成0，表示不使用并行备份。**注意：**每个线程的备份步骤是：先create table但不建立二级索引（主键会在create table时候建立），再写入数据，最后建立二级索引。"],["body","\n"],["body","--defer-table-indexes："],["body","\n"],["body","延迟创建索引，直到所有数据都加载完之后，再创建索引，默认开启。若关闭则会和mysqldump一样：先创建一个表和所有索引，再导入数据，因为在加载还原数据的时候要维护二级索引的开销，导致效率比较低。关闭使用参数：--skip--defer-table-indexes。"],["body","\n"],["body","--exclude-databases：备份排除该参数指定的数据库，多个用逗号分隔。类似的还有--exclude-events、--exclude-routines、--exclude-tables、--exclude-triggers、--exclude-users。"],["body","\n"],["body","mysqlpump --exclude-databases=mysql,sys    #备份过滤mysql和sys数据库\n\nmysqlpump --exclude-tables=rr,tt   #备份过滤所有数据库中rr、tt表\n\nmysqlpump -B test --exclude-tables=tmp_ifulltext,tt #备份过滤test库中的rr、tt表\n"],["body","\n"],["body","要是只备份数据库的账号，需要添加参数--users，并且需要过滤掉所有的数据库，如："],["body","\n"],["body","mysqlpump --users --exclude-databases=sys,mysql,db1,db2 --exclude-users=dba,backup  #备份除dba和backup的所有账号。\n"],["body","\n"],["body","--include-databases：指定备份数据库，多个用逗号分隔，类似的还有--include-events、--include-routines、--include-tables、--include-triggers、--include-users，大致方法使用同"],["body","\n"],["body","--parallel-schemas=[N:]db_list："],["body","\n"],["body","指定并行备份的库，多个库用逗号分隔，如果指定了N，将使用N个线程的地队列，如果N不指定，将由 --default-parallelism才确认N的值，可以设置多个--parallel-schemas。"],["body","\n"],["body","mysqlpump --parallel-schemas=4:vs,aa --parallel-schemas=3:pt   #4个线程备份vs和aa，3个线程备份pt。通过show processlist 可以看到有7个线程。\n\nmysqlpump --parallel-schemas=vs,abc --parallel-schemas=pt  #默认2个线程，即2个线程备份vs和abc，2个线程备份pt\n\n####当然要是硬盘IO不允许的话，可以少开几个线程和数据库进行并行备份\n"],["body","\n"],["body","官方文档"],["body","\n"],["body","参考文档"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","mysql/mysql官方文档.html"],["title","mysql官方文档 - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","mysql/mysql学习.html"],["title","mysql学习.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","mysql配置文件解析"],["heading","mysql配置文件解析"],["body","\n"],["body","查看版本 version()"],["body","\n"],["headingLink","mysql语法规范"],["heading","mysql语法规范"],["body","\n"],["body","不区分大小写"],["body","\n"],["body","建议 关键字大写 表明列名 小写"],["body","\n"],["body","sql格式可以根据需要缩进 换行"],["body","\n"],["headingLink","语法分类"],["heading","语法分类"],["body","\n"],["body","DQL"],["body","\n"],["body","DML"],["body","\n"],["body","DDL"],["body","\n"],["body","TCL"],["body","\n"],["body","DCL"],["body","\n"],["headingLink","函数"],["heading","函数"],["body","\n"],["headingLink","单行函数"],["heading","单行函数"],["body","\n"],["headingLink","字符函数"],["heading","字符函数"],["body","\n"],["body","查找"],["body","\n"],["body","替换"],["body","\n"],["body","填充"],["body","\n"],["body","去除"],["body","\n"],["headingLink","数学函数"],["heading","数学函数"],["body","\n"],["body","取整"],["body","\n"],["body","取余"],["body","\n"],["body","小数取值"],["body","\n"],["headingLink","日期函数"],["heading","日期函数"],["body","\n"],["body","时间:now()"],["body","\n"],["body","日期:curdate()"],["body","\n"],["body","时间不包含日期:curtime()"],["body","\n"],["body","year()获取日期类型或者字符串类型的年"],["body","\n"],["body","month()/monthname():获取月名"],["body","\n"],["body","str_to_date(str,date)"],["body","\n"],["body","date_format"],["body","\n"],["body","datediff"],["body","\n"],["headingLink","其他函数"],["heading","其他函数"],["body","\n"],["body","version()"],["body","\n"],["body","database()"],["body","\n"],["body","user()"],["body","\n"],["headingLink","流程控制函数"],["heading","流程控制函数"],["body","\n"],["body","select  if(10<5,'大','小')"],["body","\n"],["body","case expr when value_1 then statement; else statement end"],["body","\n"],["body","case when expr1 then statement; when expr2 then statement else statement end"],["body","\n"],["headingLink","聚合函数"],["heading","聚合函数"],["body","\n"],["body","sum,avg,max,min,count"],["body","\n"],["headingLink","连接查询"],["heading","连接查询"],["body","\n"],["body","内连接"],["body","\n\n"],["body","等值连接"],["body","\n"],["body","非等值连接"],["body","\n"],["body","自连接"],["body","\n\n"],["body","外连接"],["body","\n\n"],["body","左外"],["body","\n"],["body","右外"],["body","\n"],["body","全外"],["body","\n\n"],["body","交叉连接"],["body","\n"],["headingLink","子查询"],["heading","子查询"],["body","\n"],["headingLink","按结果集的行列数不同分类"],["heading","按结果集的行列数不同分类"],["body","\n"],["body","标量子查询:结果集只有一行一列"],["body","\n"],["body","列子查询 结果集只有一列 多行, 搭配多行操作符使用 in,any/some, all"],["body","\n"],["body","行子查询:结果集 一行多列"],["body","\n"],["body","表子查询:多行多列"],["body","\n"],["headingLink","按出现的位置"],["heading","按出现的位置"],["body","\n"],["body","select后面,只支持标量子查询"],["body","\n"],["body","from 后面:表子查询"],["body","\n"],["body","where后面:标量子查询,列子查询,行子查询"],["body","\n"],["body","having后面"],["body","\n"],["body","exists后面:表子查询"],["body","\n"],["headingLink","数据类型"],["heading","数据类型"],["body","\n"],["body","数值类型"],["body","\n"],["body","age int unsigned 无符号int类型"],["body","\n"],["body","t1 int(7) zerofill unisnged 七位格式控制,不足七位0填充"],["body","\n"],["body","枚举类型"],["body","\n"],["body","c1 enum('a','b','c')"],["body","\n"],["body","set 类型"],["body","\n"],["body","c1 set('a','b','c','d')"],["body","\n"],["body","日期类型"],["body","\n"],["body","date"],["body","\n"],["body","time"],["body","\n"],["body","datetime:"],["body","\n"],["body","timestamp:不建议使用,2038年截止"],["body","\n"],["body","日期和时间类型"],["body","字节数"],["body","最小值"],["body","最大值"],["body","\n"],["body","date"],["body","4"],["body","1000-01-01"],["body","9999-12-31"],["body","\n"],["body","datetime"],["body","8"],["body","1000-01-01 00:00:00"],["body","9999-12-31 23:59:59"],["body","\n"],["body","timestamp"],["body","4"],["body","19700101080001"],["body","2038年"],["body","\n"],["body","time"],["body","3"],["body","\n"],["body","year"],["body","1"],["body","1901"],["body","2155"],["body","\n\n\n"],["body","timestamp 取决于时区的影响"],["body","\n"],["headingLink","约束"],["heading","约束"],["body","\n"],["body","常见约束"],["body","\n\n"],["body","\n"],["body","not null"],["body","\n"],["body","\n"],["body","\n"],["body","default"],["body","\n"],["body","\n"],["body","\n"],["body","primary key"],["body","\n"],["body","\n"],["body","\n"],["body","unique 可以为空"],["body","\n"],["body","\n"],["body","\n"],["body","check 约束检查"],["body","\n"],["body","\n"],["body","\n"],["body","foreign key 外键约束"],["body","\n"],["body","\n"],["body","\n"],["body","表级约束"],["body","\n"],["body","constraint pk primary key(id)"],["body","\n"],["body","constraint pk check(gender='1'or gender='0')"],["body","\n"],["body","constraint fk_stuinfo_major foreign key(majorid) references major(id)"],["body","\n"],["body","联合主键"],["body","\n"],["body","\n"],["body","\n"],["body","约束管理"],["body","\n\n"],["body","alter table 表名 modify column 字段名 字段类型  新约束"],["body","\n"],["body","alter table 表明 add [contraint 约束名] 约束类型(字段名) [外键引用]"],["body","\n"],["body","alter table stuinfo drop primary key"],["body","\n"],["body","列级约束 不支持起名字"],["body","\n\n"],["body","\n\n"],["headingLink","标识列"],["heading","标识列"],["body","\n"],["body","系统提供的默认的序列值"],["body","\n\n"],["body","auto_increment:步长与初始值,可以在mysql环境变量查看"],["body","\n"],["body","自增长列必须时唯一的"],["body","\n"],["body","一个表只能有一个"],["body","\n"],["body","只能是数值类型"],["body","\n"],["body","可以通过 set auto_increment_increment  =3 设置步长"],["body","\n"],["body","手动给定初始值设置 初始值"],["body","\n\n"],["headingLink","tcl"],["heading","TCL"],["body","\n\n"],["body","\n"],["body","开启事务"],["body","\n"],["body","set autocommit = 0"],["body","\n"],["body","start transaction"],["body","\n"],["body","commit;"],["body","\n"],["body","rollback"],["body","\n"],["body","\n"],["body","\n"],["body","隔离级别"],["body","\n\n"],["body","脏读"],["body","\n"],["body","不可重复读:针对更新,同一个事务的多次查询 数据不一致"],["body","\n"],["body","幻读:针对插入,同一个事务中 准备更新数据时,其他事务提交了新的行"],["body","\n\n"],["body","\n"],["body","\n"],["body","mysql隔离级别设置"],["body","\n\n"],["body","select @@tx_isolation"],["body","\n"],["body","set session transaction isolation level read uncommited"],["body","\n"],["body","set names gbk"],["body","\n\n"],["body","\n"],["body","\n"],["body","mysql隔离级别"],["body","\n\n"],["body","read uncommited\n\n"],["body","可以读到别人没有提交的数据"],["body","\n\n"],["body","\n"],["body","read commited\n\n"],["body","读已提交"],["body","\n"],["body","可避免脏读"],["body","\n\n"],["body","\n"],["body","repeatable read\n\n"],["body","可以重复读,但无法避免幻读"],["body","\n\n"],["body","\n"],["body","serializable 串行化\n\n"],["body","一次只执行一个事务"],["body","\n\n"],["body","\n\n"],["body","\n\n"],["headingLink","视图"],["heading","视图"],["body","\n"],["body","create view as select"],["body","\n"],["headingLink","变量"],["heading","变量"],["body","\n"],["headingLink","系统变量"],["heading","系统变量"],["body","\n"],["headingLink","全局变量会话变量"],["heading","全局变量/会话变量"],["body","\n"],["body","查看所有某部分变量\nshow [session|global] variables  [like] \"variable_name\"\n查看系统变量的某个值\nselect @@[global|session].系统变量 ;\n系统变量赋值\nset [global|session] 系统容变量名 = 值\nset @@[global|session].系统变量名 = 值\nglobal,session 不写的话 默认为 session\n\n"],["body","\n"],["headingLink","自定义变量"],["heading","自定义变量"],["body","\n"],["headingLink","用户变量"],["heading","用户变量"],["body","\n"],["body","用户自定义变量,作用于当前会话"],["body","\n"],["body","声明\nset @用户变量名=值\nset @用户变量名 := 值\nselect @用户变量名:= 值;\n赋值\nselect 字段名 into @变量名 from 表\n使用\nselect @变量名\n"],["body","\n"],["headingLink","局部变量"],["heading","局部变量"],["body","\n"],["body","在 begin end 块之内的 变量,必须在第一行"],["body","\n"],["body","需要限定类型"],["body","\n"],["body","声明\ndeclare 变量名 类型 default 值;\n赋值\nset 局部变量名 [:]= 值\nselect 局部变量名\n"],["body","\n"],["headingLink","存储过程"],["heading","存储过程"],["body","\n"],["body","一组编译好的sql语句集合"],["body","\n"],["body","create procedure 存储过程名(参数列表)\nbeign\n\t方法体\nend\n参数列表\n模式 参数名 参数类型\nin stuname varchar(20)\nin|out|inout\n"],["body","\n\n"],["body","\n"],["body","存储过程一句话 可以省略 begin end"],["body","\n"],["body","\n"],["body","\n"],["body","每条语句必须加分号 结束符, 可以通过 delimter 重新设置"],["body","\n"],["body","\n"],["body","\n"],["body","调用: call 存储过程名(实参列表);"],["body","\n"],["body","\n"],["body","\n"],["body","out模式的参数 可以当作 用户自定义变量一样访问"],["body","\n"],["body","\n"],["body","\n"],["body","drop procedure p1"],["body","\n"],["body","\n"],["body","\n"],["body","desc p1"],["body","\n"],["body","\n"],["body","\n"],["body","show create procedure p2"],["body","\n"],["body","\n\n"],["headingLink","函数-1"],["heading","函数"],["body","\n"],["body","只能有一个返回"],["body","\n"],["body","create function 函数名 (参数列表) returns 返回类型\nbeign\n\t函数体\nend\n\nselect 函数名(参数列表)\n\n"],["body","\n"],["body","`` 区分 关键字与字段名表名"],["body","\n"],["body","mysql中的 +  号  只起到 加法作用,不是数值型变量会试图转换 成数值型"],["body","\n"],["body","对 null值 进行混合 运算时 也为null"],["body","\n"],["body","ifnull(expr1,expr2) 判断是否为空"],["body","\n"],["body","where last_name like '$_' escape '$'  申明转义字符"],["body","\n"],["body","count(*) myisam 引擎 count(*) 效率高,建议使用 count(*)"],["body","\n"],["body","order by 后面 支持 select 中引用的别名"],["body","\n"],["body","not 关键字 可以 与 between and 配合"],["body","\n"],["body","sql语言 索引都是从 1开始的"],["body","\n"],["body","trim('aa' from 'str') 去头尾的某些字符串"],["body","\n"],["body","所有分组函数都忽略null值,且null元素不参与计数"],["body","\n"],["body","a > any(select ....) 表示 a > 集合任何一个就可以, 即 a>最小值即可"],["body","\n"],["body","多表删除"],["body","\n"],["body","delete  a1,a2 from table1 a1 join table2 a2 on 连接条件 where筛选条件"],["body","\n"],["body","truncate会重置 表的 自增长序列, 而delete不会"],["body","\n"],["body","create database if not exists books"],["body","\n"],["body","alter database books character set gbk"],["body","\n"],["body","create table 表名("],["body","\n"],["body","​\t列名 列的类型(长度) 约束,"],["body","\n"],["body","​\t列名 列的类型(长度) 约束,"],["body","\n"],["body",")"],["body","\n"],["body","表的修改"],["body","\n"],["body","列名"],["body","\n"],["body","alter table book change column publishdata pubDate datetime"],["body","\n"],["body","列的类型与约束"],["body","\n"],["body","alter table book modify column pubdate"],["body","\n"],["body","添加列"],["body","\n"],["body","add column"],["body","\n"],["body","删除列"],["body","\n"],["body","drop column"],["body","\n"],["body","修改表名"],["body","\n"],["body","rename to"],["body","\n"],["body","复制表结构 create table copy like author"],["body","\n"],["body","复制表结构与数据  create table copy2 select * from author"],["body","\n"],["headingLink","流程控制"],["heading","流程控制"],["body","\n"],["headingLink","分支结构"],["heading","分支结构"],["body","\n"],["body","case"],["body","\n"],["body","delimeter $\ncreate procedure test_case(in score int)\nbegin\n\tcase \n\twhen score >= 90 and score <=100 then select '1';\n\twhen score >= 80 then select '2';\n\twhen score >= 70 then selecet '3';\n\telse select '4';\n\tend case ;\nend $\n"],["body","\n"],["body","if"],["body","\n"],["body","if 条件1 then 语句1;\nelseif 条件2 then 语句2;\nelse 语句;\nend if;\n"],["body","\n"],["body","循环结构"],["body","\n"],["body","while,loop,repeat\n循环控制\niterate:类似continue\nleave:break类似\n\n标签:while 进入循环的条件 do\n\t循环体;\nend while 标签;\n\n标签:loop\n\t循环体;\nend loop 标签\n\n标签:repeat\n\t循环体;\nuntil 结束循环的条件\nend repeat 标签\n"],["body","\n"],["body","179集"],["body","\n"],["headingLink","mysql文件组织"],["heading","mysql文件组织"],["body","\n"],["body","文件名"],["body","作用"],["body","\n"],["body","log-bin"],["body","用于主从复制"],["body","\n"],["body","log-error"],["body","用于记录mysql启停日志"],["body","\n"],["body","log"],["body","查询日志,默认不开启,记录每条查询sql的日志"],["body","\n"],["body","data目录"],["body","数据库数据存放的位置"],["body","\n"],["body","frm文件"],["body","存放数据库定数据"],["body","\n"],["body","myd文件"],["body","数据文件"],["body","\n"],["body","myi"],["body","索引文件"],["body","\n\n\n"],["body","数据文件存放路径"],["body","\n"],["body","配置文件目录:my.cnf"],["body","\n"],["body","命令目录"],["body","\n"],["body","启停脚本目录"],["body","\n"],["headingLink","mysql逻辑架构"],["heading","mysql逻辑架构"],["body","\n"],["body","\n"],["body","连接接入层:connection pool"],["body","\n\n"],["body","负责认证, 线程重用,连接限制,内存检查,缓存"],["body","\n\n"],["body","sql 接口层"],["body","\n\n"],["body","DML DDL,存储过程, 视图,触发器"],["body","\n\n"],["body","查询sql解析器"],["body","\n"],["body","优化器"],["body","\n"],["body","缓存池"],["body","\n"],["body","管理工具"],["body","\n\n"],["body","备份恢复"],["body","\n"],["body","安全副本"],["body","\n"],["body","集群"],["body","\n"],["body","配置"],["body","\n"],["body","迁移"],["body","\n"],["body","元数据"],["body","\n\n"],["body","可插拔的存储引擎"],["body","\n\n"],["body","myisam"],["body","\n"],["body","innoDB"],["body","\n"],["body","Memory"],["body","\n\n"],["body","大致分为四层"],["body","\n"],["body","第一层是 连接层, 解决谁可以连,怎么通信的问题"],["body","\n"],["body","第二层是 sql处理层,包括解析,优化等,解决要查什么的问题"],["body","\n"],["body","第三层是 存储存 , 解决 数据底层 是如何存"],["body","\n"],["body","第四层是 管理层, 负责 备份,恢复,集群等"],["body","\n"],["headingLink","存储引擎"],["heading","存储引擎"],["body","\n"],["body","查看"],["body","\n"],["body","show engine\nshow variables like '%storgeengine%'\n"],["body","\n"],["body","对比项"],["body","MyISAM"],["body","InnoDB"],["body","\n"],["body","主外键"],["body","不支持"],["body","支持"],["body","\n"],["body","事务"],["body","不支持"],["body","支持"],["body","\n"],["body","行表锁"],["body","表锁"],["body","行锁"],["body","\n"],["body","缓存"],["body","只缓存索引"],["body","可以缓存真实数据,对内存有要求"],["body","\n"],["body","表空间"],["body","小"],["body","大"],["body","\n"],["body","关注点"],["body","性能"],["body","事务"],["body","\n\n\n"],["body","XtraDB 存储引擎"],["body","\n"],["headingLink","mysql优化"],["heading","mysql优化"],["body","\n"],["body","sql性能下降的原因"],["body","\n\n"],["body","sql本身写得烂"],["body","\n"],["body","索引失效"],["body","\n"],["body","关联查询太多(设计缺陷)"],["body","\n"],["body","服务器调优 不给力"],["body","\n\n"],["body","sql执行顺序"],["body","\n"],["body","from\njoin on\nwhere\ngroup by\nhaving\nselect\ndistinct\norder by\nlimit\n"],["body","\n"],["body","七种连接理论"],["body","\n\n"],["body","join"],["body","\n"],["body","left join"],["body","\n"],["body","right join"],["body","\n"],["body","fuller join"],["body","\n"],["body","left join and b.key is null"],["body","\n"],["body","right join  and a.key is null"],["body","\n"],["body","fuller join  and a.key is null or b.key is null"],["body","\n\n"],["body","索引"],["body","\n\n"],["body","索引是一种数据结构,排好序的快速查找数据结构"],["body","\n"],["body","在数据之外,数据库系统还维护者满足特定 查找算法的数据结构,这些数据结构以某种方式引用数据"],["body","\n"],["body","索引的每个结点包含索引键值(即对应数据库索引字段),和指向对应数据记录的物理地址"],["body","\n\n"],["body","索引分类"],["body","\n\n"],["body","单值索引: 索引的key只有一个 字段"],["body","\n"],["body","唯一索引"],["body","\n"],["body","复合索引"],["body","\n\n"],["body","建立索引的建议"],["body","\n\n"],["body","主键自动建立索引"],["body","\n"],["body","频繁条件查询字段作为索引"],["body","\n"],["body","join的字段建立索引"],["body","\n"],["body","复合索引 一般好于 单值索引"],["body","\n"],["body","排序的字段 可以建立索引"],["body","\n"],["body","分组的字段 也可以建立索引"],["body","\n"],["body","字段分布均匀, 经常更新的字段 ,记录少的表 不需要建立索引"],["body","\n\n"],["headingLink","执行计划"],["heading","执行计划"],["body","\n"],["body","执行计划"],["body","\n"],["headingLink","索引优化案例"],["heading","索引优化案例"],["body","\n"],["headingLink","单表查询优化"],["heading","单表查询优化"],["body","\n"],["body","select * from table_a where a=1 and b>2 group by c;"],["body","\n"],["body","可以 建立 a,c 的联合索引,不能建立 a,b,c的联合索引"],["body","\n"],["headingLink","两表连接"],["heading","两表连接"],["body","\n"],["body","左连接 索引加右表, 因为左表无论如何都会有"],["body","\n"],["body","右连接 索引加左表"],["body","\n"],["headingLink","三表优化"],["heading","三表优化"],["body","\n"],["body","在join字段 设置索引"],["body","\n"],["headingLink","索引使用准则"],["heading","索引使用准则"],["body","\n\n"],["body","使用联合索引的过程中:最左前缀法则,且不能跳过索引的中间列"],["body","\n"],["body","不要在索引列上做任何操作,包括显示转换,隐式转换"],["body","\n"],["body","在联合索引中,使用范围之后的索引列全失效"],["body","\n"],["body","按需取字段,尽量使用覆盖索引,可以避免索引失效的问题"],["body","\n"],["body","避免判断空值"],["body","\n"],["body","groupBy order by 后的字段一般要遵守 联合索引字段顺序,除非 其中的其中为空值"],["body","\n"],["body","组合索引中,字段过滤性越好 越要 靠前"],["body","\n\n"],["headingLink","查询截取分析"],["heading","查询截取分析"],["body","\n\n"],["body","开启慢查询日志,设置阀值,超过多少s的是慢sql"],["body","\n"],["body","explain 分析该sql"],["body","\n"],["body","show profile"],["body","\n"],["body","sql服务器的参数调优"],["body","\n\n"],["headingLink","关键字优化"],["heading","关键字优化"],["body","\n"],["headingLink","小表驱动大表"],["heading","小表驱动大表"],["body","\n\n"],["body","\n"],["body","in 与exists的分析"],["body","\n\n"],["body","\n"],["body","可以把子查询过滤表等价于双层for循环,"],["body","\n"],["body","select *from a where id in (select id from b)\n等价于 \nfor (select id from b) \n\tfor select * from b where b.id= a.id\n\nselect * from a where exist (select 1 from b where exists b.id=a.id)\n等价于\nfor(select * from a)\n\tfor(select * from b where b.id=a.id\n\t)\n"],["body","\n"],["body","\n"],["body","\n"],["body","总结: 当外层表数据>内部表数据,使用 in,当外层表数据<内部表数据  使用exists"],["body","\n"],["body","\n\n"],["body","\n\n"],["headingLink","order-by关键字"],["heading","ORDER BY关键字"],["body","\n\n"],["body","排序的字段 尽量使用索引字段"],["body","\n"],["body","不要使用select *"],["body","\n"],["body","排序算法\n\n"],["body","单路排序\n\n"],["body","后出, 取出所有数据,在缓冲区中排序,"],["body","\n"],["body","如果数据超过 buffer缓冲区 ,会一部分一部分的排序,然后合并,产生多次IO"],["body","\n\n"],["body","\n"],["body","双路排序\n\n"],["body","早先,取出排序字段排序,排完序后 在读 其余字段"],["body","\n\n"],["body","\n"],["body","增大 sort_buffer_size 缓冲区"],["body","\n\n"],["body","\n\n"],["headingLink","慢查询日志"],["heading","慢查询日志"],["body","\n"],["body","long_time_query 默认10s"],["body","\n"],["body","show variables like 'slow_query_log'"],["body","\n"],["body","set global slow_query_log=1"],["body","\n"],["body","需要重开session"],["body","\n"],["body","测试"],["body","\n"],["body","select sleep(4)"],["body","\n"],["body","mysqldumpslow mysql慢查询日志分析工具"],["body","\n\n"],["body","s:何种方式排序\n\n"],["body","c:访问次数"],["body","\n"],["body","l:锁定时间"],["body","\n"],["body","r:返回记录"],["body","\n"],["body","t:查询时间"],["body","\n"],["body","al:平均锁定时间"],["body","\n"],["body","ar:平均返回记录"],["body","\n"],["body","at:平均返回时间"],["body","\n\n"],["body","\n"],["body","t:返回记录的个数"],["body","\n"],["body","g:正则"],["body","\n\n"],["headingLink","showprofile-性能分析"],["heading","showprofile 性能分析"],["body","\n\n"],["body","\n"],["body","默认情况下关闭"],["body","\n"],["body","\n"],["body","\n"],["body","保存最近15次运行的结果"],["body","\n"],["body","\n"],["body","\n"],["body","命令"],["body","\n\n"],["body","show variables like 'profiling%'"],["body","\n"],["body","show profiles 查询历史sql"],["body","\n"],["body","show profile cpu,block,io for query ${id},看到sql的生命周期"],["body","\n"],["body","以下四个现象 是比较糟糕的\n\n"],["body","converting heap to myisam 查询结果太大,内存不够用了往磁盘上搬运"],["body","\n"],["body","creating tmp table"],["body","\n"],["body","copying to tmp table on disk 把内存中临时表 复制到磁盘"],["body","\n"],["body","locked"],["body","\n\n"],["body","\n\n"],["body","\n\n"],["headingLink","全局查询日志"],["heading","全局查询日志"],["body","\n"],["body","不要在生产环境启用这个选项"],["body","\n"],["body","set global general_log =1"],["body","\n"],["body","set global log_output ='TABLE'"],["body","\n"],["body","所编写的sql语句,将会记录到mysql库里的general_log表中"],["body","\n"],["headingLink","mysql锁机制"],["heading","Mysql锁机制"],["body","\n"],["headingLink","表级锁"],["heading","表级锁"],["body","\n"],["headingLink","手动表锁"],["heading","手动表锁"],["body","\n"],["body","lock table (table_name read|write[,])\nshow open tables\n\nunlock talbes\n其中想要更新该表的 会话会阻塞\n某会话加读锁之后,只能先对该表进行读操作\nmyisam在查询时会自动给涉及的所有表加读锁,\n在修改时会自动加写锁\n"],["body","\n"],["headingLink","表锁定分析"],["heading","表锁定分析"],["body","\n\n"],["body","show \tstatus like 'table%'"],["body","\n"],["body","记录mysql内部表级锁定情况\n\n"],["body","table_locks_immediate 表示可以立即获取锁的次数"],["body","\n"],["body","table_locks_waited 表示 不能立即获取锁的次数"],["body","\n\n"],["body","\n"],["body","isam存储引擎偏向 读写"],["body","\n\n"],["headingLink","行级锁"],["heading","行级锁"],["body","\n\n"],["body","\n"],["body","行级锁升级案例"],["body","\n"],["body","索引失效,导致表锁升级成表锁"],["body","\n"],["body","\n"],["body","\n"],["body","间隙锁"],["body","\n"],["body","在区间更新时,mysql会 把 某个范围内的所有记录加锁,即使这个记录不存在"],["body","\n"],["body","\n"],["body","\n"],["body","手动加锁"],["body","\n"],["body","select * from table_name where ... for update"],["body","\n"],["body","\n"],["body","\n"],["body","行锁分析"],["body","\n"],["body","show inodb_row_lock%\ninnodb_row_lock_current_waits:当前正在等待锁定的数量\ninnodb_row_lock_time:从系统启动到现在总锁定时间\ninnodb_row_lock_time_avg:每次的等待平均时间\ninnodb_row_lock_time_max:从系统启动到现在等待最长的一次\ninnodb_row_lock_waits:系统启动总共等待的次数\n"],["body","\n"],["body","\n\n"],["headingLink","主从复制"],["heading","主从复制"],["body","\n\n"],["body","\n"],["body","slaver 会从 master读取 binlog数据 进行同步"],["body","\n\n"],["body","master改变记录到二进制日志"],["body","\n"],["body","slaver将master 的binary log events 拷贝到中继日志"],["body","\n"],["body","slaver重做 中继日志的 事件"],["body","\n"],["body","mysql复制是异步串行化的"],["body","\n\n"],["body","\n"],["body","\n"],["body","主从复制的基本原则"],["body","\n\n"],["body","每个slaver只能有一个唯一的服务器ID"],["body","\n"],["body","每个master可以有多个slaver"],["body","\n"],["body","复制的最大问题:延时"],["body","\n\n"],["body","\n"],["body","\n"],["body","等待实验"],["body","\n"],["body","\n\n"],["body","\\G 表示kv键值对显示"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","oracle/Oracle小知识点.html"],["title","Oracle小知识点.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","rownum"],["heading","rownum"],["body","\n"],["headingLink","rownum-的-产生过程"],["heading","rownum 的 产生过程"],["body","\n\n"],["body","rownum初始化为0"],["body","\n"],["body","取一条记录 判断是否满足where条件"],["body","\n"],["body","如果满足条件 则rownum+1"],["body","\n"],["body","如果不满足条件 则rownum不变"],["body","\n"],["body","继续第二步骤"],["body","\n\n"],["headingLink","关于where条件后-rownum-n-分析"],["heading","关于where条件后 rownum >n 分析"],["body","\n"],["body","由于rownum 初始为 0,所以 当n为正整数时, 该条件一开始就不成立,rownum 也永远不会增加"],["body","\n"],["headingLink","oracle排序函数"],["heading","Oracle排序函数"],["body","\n"],["headingLink","row_number"],["heading","row_number"],["body","\n"],["body","row_number() over(partition by column1 order by column2) as pxh"],["body","\n"],["headingLink","oracle分组函数"],["heading","Oracle分组函数"],["body","\n"],["headingLink","grouping-sets单排列"],["heading","grouping sets:单排列"],["body","\n"],["body","select a,b,sum(c) from table_d group by grouping sets(a,b)"],["body","\n"],["body","等价于"],["body","\n"],["body","select a,'',sum(c) from table_d  group by a"],["body","\n"],["body","union all"],["body","\n"],["body","select  '',b,sum(c) from table_d group by b"],["body","\n"],["headingLink","rollup降级排列"],["heading","rollup:降级排列"],["body","\n"],["body","select a,b,sum(c) from table_d group by rollup(a,b)"],["body","\n"],["body","等价于"],["body","\n"],["body","select a,b,sum(c) from table_d group by a,b"],["body","\n"],["body","union all"],["body","\n"],["body","select a,'',sum(c) from table_d group by a"],["body","\n"],["body","union all"],["body","\n"],["body","select '','',sum(c) from table_d group by ''"],["body","\n"],["headingLink","cube全排列"],["heading","cube:全排列"],["body","\n"],["body","select a,b,sum(c) from table_d group by cube(a,b)"],["body","\n"],["body","等价于"],["body","\n"],["body","select a,b,sum(c) from table_d group by a,b"],["body","\n"],["body","union all"],["body","\n"],["body","select a,'',sum(c) from table_d group by a"],["body","\n"],["body","union all"],["body","\n"],["body","select '',b,sum(c) from table_d group by b"],["body","\n"],["body","union all"],["body","\n"],["body","select '','',sum(c) group by ''"],["body","\n"],["headingLink","oracle行转列"],["heading","Oracle行转列"],["body","\n"],["headingLink","privot函数"],["heading","privot函数"],["body","\n"],["body","select * from\n(select yljzhdh,substr(jyrq,5,2) jyrq, jyje from lsb)\nprivot\n(\nsum(jyje) su,\navg(jyje) av,\nfor(jyrq) in (\n'01' as \"1月\",\n'02' as \"2月\",\n'03' as \"3月\",\n'04' as \"4月\"\n)\n)\n"],["body","\n"],["headingLink","mergerinto"],["heading","mergerinto"],["body","\n"],["headingLink","语法"],["heading","语法"],["body","\n"],["body","merger into table_name alias1"],["body","\n"],["body","using (table |view | subquery) alias2"],["body","\n"],["body","on (join condition)"],["body","\n"],["body","when matched then update set"],["body","\n"],["body","when not matched then insert"],["body","\n"],["headingLink","注意事项"],["heading","注意事项"],["body","\n"],["body","joinCondition必须一一对应,否则会报错"],["body","\n"],["headingLink","oracle空值处理函数"],["heading","Oracle空值处理函数"],["body","\n"],["headingLink","nvlexpr1expr2"],["heading","nvl(expr1,expr2)"],["body","\n"],["body","如果expr1为null,返回expr2,否则返回expr1,两者类型要一致"],["body","\n"],["headingLink","nvl2expr1expr2expr3"],["heading","nvl2(expr1,expr2,expr3)"],["body","\n"],["body","expr1不为null,返回expr2,否则返回expr3,类型以expr2为准"],["body","\n"],["headingLink","nullifexpr1expr2"],["heading","nullIf(expr1,expr2)"],["body","\n"],["body","expr1相等返回NULL,不相等返回expr1"],["body","\n"],["headingLink","coalseceexpr1expr2expr3"],["heading","coalsece(expr1,expr2,expr3)"],["body","\n"],["body","从左到右选择不为null的一项"],["body","\n"],["headingLink","case-when-else-end"],["heading","case when else end"],["body","\n"],["body","......"],["body","\n"],["headingLink","decodeexpr1valueresultvalueresult"],["heading","decode(expr1,value,result,value,result)"],["body","\n"],["body","根据expr1的计算值, 选择相应的result"],["body","\n"],["headingLink","oracle创建序列"],["heading","Oracle创建序列"],["body","\n"],["body","create sequence seq_users seq_name"],["body","\n"],["body","increment by 1"],["body","\n"],["body","start with 1"],["body","\n"],["body","minvalue 1"],["body","\n"],["body","max value 9999"],["body","\n"],["body","cycle / no cycle"],["body","\n"],["body","cache 20"],["body","\n"],["body","order / no order"],["body","\n"],["headingLink","oracle权限管理"],["heading","Oracle权限管理"],["body","\n"],["headingLink","系统权限"],["heading","系统权限"],["body","\n"],["body","权限名"],["body","权限操作"],["body","\n"],["body","索引权限"],["body","create any index"],["body","alter any index"],["body","drop any index"],["body","\n"],["body","\n"],["body","存储过程权限"],["body","create procedure"],["body","create any procedure"],["body","alter any procedure"],["body","execute any procedure"],["body","drop any procedure"],["body","\n"],["body","角色权限"],["body","create role"],["body","alter any role"],["body","drop any role"],["body","grant any role"],["body","\n"],["body","序列权限"],["body","create sequence"],["body","create any sequence"],["body","alter any sequence"],["body","select any sequence"],["body","drop any sequence"],["body","\n"],["body","登录数据库权限"],["body","create session"],["body","\n"],["body","表空间权限"],["body","create tablespace"],["body","alter tablespace"],["body","drop tablespace"],["body","mange tablepace"],["body","unlimited tablespace"],["body","\n"],["body","类型权限"],["body","create type"],["body","create any type"],["body","alter any type"],["body","drop any type"],["body","execute any type"],["body","under any type"],["body","\n"],["body","视图权限"],["body","create view"],["body","create any view"],["body","under any view"],["body","drop any view"],["body","flashback any table"],["body","merge any view"],["body","\n"],["body","表权限"],["body","create table"],["body","create any table"],["body","alter any table"],["body","backup any table"],["body","delete any table"],["body","drop any table"],["body","delete any table"],["body","drop any table"],["body","insert any table"],["body","lock any table"],["body","select any table"],["body","flashback any table"],["body","update any table"],["body","\n"],["body","触发器"],["body","create trigger"],["body","create any trigger"],["body","alter any trigger"],["body","drop any trigger"],["body","administer database trigger"],["body","\n"],["body","备份数据库"],["body","exp_full_database"],["body","imp_full_database"],["body","\n\n\n"],["headingLink","权限赋予与撤销"],["heading","权限赋予与撤销"],["body","\n"],["headingLink","权限赋予"],["heading","权限赋予"],["body","\n"],["body","grant select,delete,insert,update on username.table_name to username\ngrant execute on procedure_name to username\n"],["body","\n"],["headingLink","限制修改的列"],["heading","限制修改的列"],["body","\n"],["body","grant update(column1,column2) on table_name to users\n"],["body","\n"],["headingLink","收回权限"],["heading","收回权限"],["body","\n"],["body","revoke insert on table_name from username\n"],["body","\n"],["headingLink","权限查询"],["heading","权限查询"],["body","\n"],["headingLink","查询用户拥有的系统权限"],["heading","查询用户拥有的系统权限"],["body","\n"],["body","select grantee,privilege from dba_sys_privs where grantee = 'scott'\n"],["body","\n"],["headingLink","查询用户拥有的对象权限"],["heading","查询用户拥有的对象权限"],["body","\n"],["body","select grantee,table_name,privilege from dba_tab_privs where grantee='SCOTT'\n\n"],["body","\n"],["headingLink","查询用户所拥有的角色"],["heading","查询用户所拥有的角色"],["body","\n"],["body","select grantee , granted_role from dba_role_privs where grantee = 'SCOTT'\n"],["body","\n"],["headingLink","oracle优化器"],["heading","Oracle优化器"],["body","\n"],["headingLink","优化器类型"],["heading","优化器类型"],["body","\n"],["headingLink","rbo"],["heading","RBO"],["body","\n"],["body","(rule based optimizer)"],["body","\n"],["body","基于规则的优化"],["body","\n"],["headingLink","cbo"],["heading","CBO"],["body","\n"],["body","(costed based optimizer)"],["body","\n"],["body","基于成本的优化"],["body","\n"],["body","通过目标sql语句所设计的表索引,列等的统计信息 选择一条执行成本最小的执行计划"],["body","\n"],["headingLink","cardinaltity"],["heading","cardinaltity"],["body","\n"],["body","结果集的势"],["body","\n"],["body","指结果集所包含的列数"],["body","\n"],["headingLink","selectivty"],["heading","selectivty"],["body","\n"],["body","where条件筛选出来的记录数占总记录数的比率,越小越好"],["body","\n"],["headingLink","可传递性"],["heading","可传递性"],["body","\n"],["headingLink","简单谓语传递"],["heading","简单谓语传递"],["body","\n"],["body","t1.c1 = t2.c2 and t1.c1 = 10  添加 t2.c2  = 10"],["body","\n"],["headingLink","连接谓语传递"],["heading","连接谓语传递"],["body","\n"],["body","t1.c1 = t2.c1   and  t2.c1 = t3.c1  添加 t3.c1 = t1.c1"],["body","\n"],["headingLink","外连接谓语传递"],["heading","外连接谓语传递"],["body","\n"],["body","t1.c1 = t2.c1(+) and t1.c1=10  添加 t2.c1(+) = 10"],["body","\n"],["headingLink","优化器模式"],["heading","优化器模式"],["body","\n"],["headingLink","rule"],["heading","RULE"],["body","\n"],["body","基于规则的优化RBO"],["body","\n"],["headingLink","choose"],["heading","CHOOSE"],["body","\n"],["body","如果目标含有统计信息则 使用 CBO"],["body","\n"],["headingLink","first_row"],["heading","FIRST_ROW"],["body","\n"],["body","快速的返回记录,侧重响应时间"],["body","\n"],["headingLink","all_rows"],["heading","ALL_ROWS"],["body","\n"],["body","侧重最佳吞吐量,默认模式"],["body","\n"],["headingLink","数据访问方式"],["heading","数据访问方式"],["body","\n"],["headingLink","全表扫描"],["heading","全表扫描"],["body","\n"],["body","从表所占用的第一个extent 第一个块开始,一直到该表的最高水位线"],["body","\n"],["headingLink","rowid扫描"],["heading","ROWID扫描"],["body","\n"],["body","直接通过数据所在的rowID 定位到物理存储空间"],["body","\n"],["headingLink","索引扫描"],["heading","索引扫描"],["body","\n"],["headingLink","索引唯一性扫描"],["heading","索引唯一性扫描"],["body","\n"],["body","index unique scan : 适用于等值查询的目标"],["body","\n"],["headingLink","索引范围扫描"],["heading","索引范围扫描"],["body","\n"],["body","index range scan"],["body","\n"],["body","​\t当扫描的对象是唯一性索引时,此时where条件为范围条件"],["body","\n"],["body","​    当扫描的对象是非唯一性索引,where条件不做限制"],["body","\n"],["headingLink","索引全扫描"],["heading","索引全扫描"],["body","\n"],["body","index full scan"],["body","\n"],["body","​\t扫描索引的所有叶子块的所有行"],["body","\n"],["headingLink","索引快速扫描"],["heading","索引快速扫描"],["body","\n"],["body","index fast full scan"],["body","\n"],["headingLink","索引跳跃扫描"],["heading","索引跳跃扫描"],["body","\n"],["body","index skip scan"],["body","\n"],["headingLink","表连接的方式"],["heading","表连接的方式"],["body","\n"],["headingLink","排序合并"],["heading","排序合并"],["body","\n"],["body","sort meger into"],["body","\n\n"],["body","以目标sql中的谓语条件去访问表t1,对返回结果按连接列进行排序,得到结果集 s1"],["body","\n"],["body","以目标sql中的谓语条件访问 t2,对返回结果按连接列进行排序,得到结果集 s2"],["body","\n"],["body","对结果s1,s2 进行合并"],["body","\n\n"],["headingLink","嵌套循环"],["heading","嵌套循环"],["body","\n"],["body","nested loops join"],["body","\n\n"],["body","根据规则决定 t1,t2 谁是驱动表,谁是被动表,驱动表做外层循环,被驱动表做内层"],["body","\n"],["body","假设t1 是 驱动表,t2是被驱动表"],["body","\n"],["body","用目标sql的谓语条件 访问表 t1 得到结果集 s1"],["body","\n"],["body","循环取出驱动表中的每条记录 与被驱动表关联"],["body","\n\n"],["headingLink","哈希连接"],["heading","哈希连接"],["body","\n"],["body","hash join"],["body","\n\n"],["body","依据 hash_area_size, db_block_size,_hash_multiblock_io_count决定hash partition数量"],["body","\n"],["body","使用谓语条件 访问t1,t2 得到结果集 s1,s2 ,假设s1数量少与s2"],["body","\n"],["body","取s1为驱动结果集, 遍历s1的每条记录, 取两个hash函数 哈希运算计算出来的结果放在不同的partition  的不同的bucket"],["body","\n"],["body","同时构造s1,s2 的哈希table"],["body","\n\n"],["headingLink","oracle-to_char"],["heading","Oracle to_char"],["body","\n"],["headingLink","用于时间转换的格式字符"],["heading","用于时间转换的格式字符"],["body","\n"],["headingLink","用于数值格式化"],["heading","用于数值格式化"],["body","\n"],["body","9 带有指定位数的值"],["body","\n"],["body","0 前导0的值"],["body","\n"],["body",". 小数点"],["body","\n"],["body",", 分组(千) 分隔符"],["body","\n"],["headingLink","oracle表空间管理"],["heading","Oracle表空间管理"],["body","\n"],["headingLink","简介"],["heading","简介"],["body","\n\n"],["body","Oracle的逻辑结构包括表空间 段 区 块"],["body","\n"],["body","表空间是Oracle的逻辑组成部分, 一个表空间可以拥有多个数据文件"],["body","\n"],["body","在逻辑上,表空间由 段segment 组成"],["body","\n"],["body","段 segment 是由区构成"],["body","\n"],["body","块是由 Oracle数据块构成"],["body","\n\n"],["headingLink","建立表空间"],["heading","建立表空间"],["body","\n\n"],["body","\n"],["body","拥有 create tablespace 权限"],["body","\n"],["body","\n"],["body","\n"],["body","语法"],["body","\n"],["body","create tablesapce tablespacename datafile'filePath' size 20M uniform size 128K"],["body","\n"],["body","\n\n"],["headingLink","修改表空间"],["heading","修改表空间"],["body","\n"],["body","alter tablesapce tablename offline|online|read only| read write"],["body","\n"],["headingLink","查询表空间所有信息"],["heading","查询表空间所有信息"],["body","\n"],["body","select * from all_tables where tablesapce_name = ''"],["body","\n"],["headingLink","删除表空间"],["heading","删除表空间"],["body","\n"],["body","drop tablespace 'tablespacename' including contents and datafiles"],["body","\n"],["headingLink","扩展表空间"],["heading","扩展表空间"],["body","\n"],["headingLink","增加数据文件"],["heading","增加数据文件"],["body","\n"],["body","alter tablespace tablesapcename add datafile 'filepath' size 20M"],["body","\n"],["headingLink","增加数据文件大小"],["heading","增加数据文件大小"],["body","\n"],["body","alter tablespace tablespacename  'filepath' resize 20M"],["body","\n"],["headingLink","设置文件自动增长"],["heading","设置文件自动增长"],["body","\n"],["body","alter tablesapce tablespacename 'filepath' autoextend on next 10m maxsize 500M"],["body","\n"],["headingLink","oracle分区管理"],["heading","Oracle分区管理"],["body","\n"],["headingLink","概述"],["heading","概述"],["body","\n"],["body","Oracle分区是一种处理超大型表,索引等的技术"],["body","\n"],["headingLink","优缺点"],["heading","优缺点"],["body","\n"],["headingLink","优点"],["heading","优点"],["body","\n"],["body","增强可用性"],["body","\n"],["body","可维护性"],["body","\n"],["body","均衡IO"],["body","\n"],["body","改善查询性能"],["body","\n"],["headingLink","缺点"],["heading","缺点"],["body","\n"],["body","已经存在的表无法直接转换分区表"],["body","\n"],["headingLink","分区方法"],["heading","分区方法"],["body","\n"],["body","范围分区"],["body","\n"],["body","Hash分区"],["body","\n"],["body","列表分区"],["body","\n"],["body","范围-散列分区"],["body","\n"],["body","范围-列表分区"],["body","\n"],["headingLink","oracle优化"],["heading","Oracle优化"],["body","\n"],["headingLink","索引优化"],["heading","索引优化"],["body","\n\n"],["body","\n"],["body","Oracle不会索引空值 所以判断为空不为空 无法使用索引"],["body","\n"],["body","可以使用 < > 替代,或者空值用特定值替代"],["body","\n"],["body","\n"],["body","\n"],["body","不等于 <> 不会应用索引 可以改为 > <"],["body","\n"],["body","\n"],["body","\n"],["body",">= 3 与 >2 的比较"],["body","\n\n"],["body",">2  先找出 2的索引 然后作比较"],["body","\n"],["body",">=3  直接找出3 然后作比较"],["body","\n\n"],["body","\n"],["body","\n"],["body","like操作符"],["body","\n"],["body","aa like '%AAA%' 不会应用索引"],["body","\n"],["body","可以改为 aa like 'BAAA%' or aa like 'CAAA%'"],["body","\n"],["body","\n"],["body","\n"],["body","in not in 会自动转换成外连接"],["body","\n"],["body","尽量使用外连接"],["body","\n"],["body","\n"],["body","\n"],["body","union ,union all , union 会排序去重"],["body","\n"],["body","\n"],["body","\n"],["body","sql 语句书写, 是否大小写, 是否写用户名前缀,等等差异 会导致 识别为不同sql"],["body","\n"],["body","\n"],["body","\n"],["body","where 条件执行顺序 是从 右到 左 , 尽量 过滤大头的 在 右边"],["body","\n"],["body","\n"],["body","\n"],["body","表连接时, 如果有 统计信息分析, 则会自动 先小表, 后大表"],["body","\n"],["body","generate statistics on <table>"],["body","\n"],["body","\n"],["body","\n"],["body","采用函数处理的字段无法使用索引"],["body","\n\n"],["body","substr(bh,1,4) = '5400' 改成 bh like '5400'"],["body","\n"],["body","trunc(rq) = trunc(sysdate)  rq > = trunc(sysdate) and rq < trunc(sysdate+1)"],["body","\n\n"],["body","\n"],["body","\n"],["body","进行了显示或者 隐式运算的字段不能 索引"],["body","\n\n"],["body","df + 20 >50  不能用索引"],["body","\n"],["body","rq + 5  = sysdate 不能使用索引"],["body","\n"],["body","bh = 132456789 ; 不能使用索引,  bh = '132456789'"],["body","\n\n"],["body","\n"],["body","\n"],["body","条件包含了多个本表的字段运算时不能索引"],["body","\n"],["body","a > b 无法优化"],["body","\n"],["body","a||b = '123456' 优化 成 a='123' and b = '456'"],["body","\n"],["body","\n\n"],["headingLink","oracle日期函数"],["heading","Oracle日期函数"],["body","\n"],["headingLink","函数"],["heading","函数"],["body","\n"],["body","to_number(char)"],["body","\n"],["body","to_date(char,pattern)"],["body","\n"],["body","to_char(date,pattern)"],["body","\n"],["body","date'char'"],["body","\n"],["body","trunct(date,pattern)"],["body","\n"],["body","extract(day|month|year| from date)返回数字格式"],["body","\n"],["body","date + int 给日期加天数"],["body","\n"],["body","last_day(date) 最后一天"],["body","\n"],["body","round(date,'pattern') 四舍五入"],["body","\n"],["body","months_betwteen(date,date)  两个日期之间的月份"],["body","\n"],["body","next_day(sysdate,'星期三') 下个星期三的日期"],["body","\n"],["headingLink","pattern"],["heading","pattern"],["body","\n"],["body","y 年的最后一位"],["body","\n"],["body","yy 最后两位"],["body","\n"],["body","yyy 后三位"],["body","\n"],["body","yyyy 四位"],["body","\n"],["body","mm 两位月份"],["body","\n"],["body","mon 英文简写形式 11月或者nov"],["body","\n"],["body","month 全称"],["body","\n"],["body","dd 当月的第几天"],["body","\n"],["body","ddd 当年的第几天"],["body","\n"],["body","dy 当周第几天 简写星期5,fri"],["body","\n"],["body","day 全称 星期五"],["body","\n"],["body","hh 小时12进制"],["body","\n"],["body","hh24 24小时制"],["body","\n"],["body","mi 分钟"],["body","\n"],["body","ss 秒"],["body","\n"],["body","q 季度"],["body","\n"],["body","ｗw 当年第几周"],["body","\n"],["body","w 当月第几周"],["body","\n"],["headingLink","日期运算"],["heading","日期运算"],["body","\n"],["body","sysdate - interval'7' year|month|minute|second 减去七年,月,分钟,秒"],["body","\n"],["headingLink","oracle数据库导入导出"],["heading","Oracle数据库导入导出"],["body","\n"],["headingLink","sqlldr加载平面文件"],["heading","sqlldr加载平面文件"],["body","\n"],["headingLink","数据泵二进制数据导入导出"],["heading","数据泵:二进制数据导入导出"],["body","\n"],["headingLink","外部表以上两种方案的简化工具"],["heading","外部表:以上两种方案的简化工具"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","oracle/oracle多表更新方法.html"],["title","oracle多表更新方法.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","更新视图"],["heading","更新视图"],["body","\n"],["body","update\n(select  /*+BYPASS_UJVC*/  a.money,b.money1,b.money2 from salary a , person_money b where a.name=b.name )\na set  a.money1=a.money, a.money2=a.money+1;\n"],["body","\n"],["body","注意： 关联的 两个表必须是  唯一的"],["body","\n"],["headingLink","merge关键字"],["heading","merge关键字"],["body","\n"],["body","MERGE INTO person_money T1\nUSING salary T2\nON (T1.name = T2.name)\nWHEN MATCHED THEN\nUPDATE\nSET T1.money1 = T2.money ,t1.money2 = t2.money+1\nWHEN NOT MATCHED THEN NOTHING\n"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]],[["_relative_fp","oracle/oracledocker快速搭建.html"],["title","oracledocker快速搭建.md - DB笔记"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n        \n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","DB笔记"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","资源"],["heading","资源"],["body","\n"],["body","docker镜像"],["body","\n"],["body","客户端下载"],["body","\n"],["headingLink","安装oracle服务器"],["heading","安装Oracle服务器"],["body","\n"],["body","docker pull oracleinanutshell/oracle-xe-11g\ndocker run -d -p 49161:1521 -e ORACLE_ALLOW_REMOTE=true oracleinanutshell/oracle-xe-11g\n\n"],["body","\n"],["body","默认配置"],["body","\n"],["body","hostname: localhost\nport: 49161\nsid: xe\nusername: system\npassword: oracle\n"],["body","\n"],["headingLink","客户端"],["heading","客户端"],["body","\n\n"],["body","新建 ORACLE_HOME"],["body","\n"],["body","新建 TNS_ADMIN   NETWORK/ADMIN 环境变量"],["body","\n\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n\n"]]]