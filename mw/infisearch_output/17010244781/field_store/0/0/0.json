[[["_relative_fp","4.持续集成_jenkins/Jenkins.html"],["title","Jenkins - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","设置webhook"],["heading","设置webhook"],["body","\n"],["body","以 gitee为例"],["body","\n\n"],["body","\n"],["body","git生成token,生成地址"],["body","\n"],["body","\n"],["body","\n"],["body","Jenkins安装 git插件 重启"],["body","\n"],["body","\n"],["body","\n"],["body","复制其 webhookURL到gitee项目的 webhook"],["body","\n"],["body","\n"],["body","\n"],["body","在Jenkins中生成webhook密码"],["body","\n"],["body","\n"],["body","\n"],["body","测试推送是否成功"],["body","\n"],["body","\n\n"],["headingLink","为jenkins启用反向代理"],["heading","为Jenkins启用反向代理"],["body","\n\n"],["body","\n"],["body","修改Jenkins上下文路径"],["body","\n"],["body","在文件 /etc/default/jenkins "],["body","\n"],["body","JENKINS_ARGS=\"--webroot=/var/cache/$NAME/war --httpPort=$HTTP_PORT --prefix=/jenkins\"\n"],["body","\n"],["body","\n"],["body","\n"],["body","Manage Jenkins / Configure System，将Jenkins URL后添加/jenkins"],["body","\n"],["body","\n"],["body","\n"],["body","nginx配置"],["body","\n"],["body","\tserver{\n\t\tlisten 192.168.3.15:80;\n\t\tserver_name  \"\";\n\t\t#root /home/pi/gitbook/_book;\n\t\tlocation /jenkins/ {\n\t\t\t proxy_pass http://localhost:8080;\n\t\t}\n\t}\n"],["body","\n"],["body","\n\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","4.持续集成_jenkins/jenkins_pipeline.html"],["title","jenkins_pipeline - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","概述"],["heading","概述"],["body","\n"],["body","该文档覆盖了 Jenkins Pipeline功能的所有推荐的方面"],["body","\n"],["body","包括"],["body","\n\n"],["body","如何 在 UI界面、SCM、定义Pipeline "],["body","\n"],["body","创建并使用 Jenkinsfile"],["body","\n"],["body","git分支 与 pull request"],["body","\n"],["body","在Pipeline中使用Docker"],["body","\n"],["body","Pipeline中的继承"],["body","\n"],["body","使用不同的开发工具 加速 Pipeline的创建"],["body","\n"],["body","使用流水线语法 - 此页面是所有声明式流水线语法的综合参考。"],["body","\n\n"],["headingLink","什么是jenkinspipeline"],["heading","什么是JenkinsPipeline"],["body","\n"],["body","持续交付 (CD) 管道是将软件从版本控制到用户和客户的过程的自动化"],["body","\n"],["body","对您的软件（在源代码控制中提交）的每一次更改在发布之前都经历了一个复杂的过程。"],["body","\n"],["body","此过程涉及以可靠且可重复的方式构建软件，以及通过多个测试和部署阶段推进构建的软件"],["body","\n"],["body","Pipeline 提供了一组可扩展的工具，用于通过 Pipeline 域特定语言 (DSL) 语法将简单到复杂的交付管道“作为代码”建模。 "],["body","\n"],["body","Jenkins 管道的定义被写入一个文本文件（称为 Jenkinsfile），该文件又可以提交到项目的源代码控制存储库。 "],["body","\n"],["body","[2] 这是“Pipeline-as-code”的基础；"],["body","\n"],["body","将 CD 管道视为要进行版本控制和审查的应用程序的一部分，就像任何其他代码一样。"],["body","\n"],["body","创建 Jenkinsfile 并将其提交到源代码控制提供了许多直接的好处："],["body","\n\n"],["body","\n"],["body","自动为所有分支和拉取请求创建流水线构建过程。"],["body","\n"],["body","\n"],["body","\n"],["body","流水线上的代码审查/迭代（以及剩余的源代码）。"],["body","\n"],["body","\n"],["body","\n"],["body","管道的审计跟踪。"],["body","\n"],["body","\n"],["body","\n"],["body","管道的单一事实来源 [3]，可由项目的多个成员查看和编辑。"],["body","\n"],["body","\n\n"],["body","尽管在 Web UI 中或使用 Jenkinsfile 定义流水线的语法是相同的，但通常认为最佳实践是在 Jenkinsfile 中定义流水线并将其签入源代码控制。"],["body","\n"],["headingLink","声明式与脚本式-流水线语法"],["heading","声明式与脚本式 流水线语法"],["body","\n"],["body","Jenkinsfile 可以使用两种类型的语法编写 - 声明式和脚本式。"],["body","\n"],["body","声明式管道和脚本式管道的构造根本不同。"],["body","\n"],["body","声明式流水线是 Jenkins 流水线的一个更新的特性，它："],["body","\n"],["body","提供比 Scripted Pipeline 语法更丰富的语法特性，旨在使编写和阅读 Pipeline 代码更容易。"],["body","\n"],["body","然而，许多写入 Jenkinsfile 的单个语法组件（或“步骤”）对于声明式和脚本式流水线都是通用的。"],["body","\n"],["headingLink","why-pipeline"],["heading","Why Pipeline?"],["body","\n"],["body","从根本上说，Jenkins 是一个支持多种自动化模式的自动化引擎。"],["body","\n"],["body","Pipeline 在 Jenkins 上添加了一组强大的自动化工具，支持从简单的持续集成到全面的 CD 管道的用例。"],["body","\n"],["body","通过对一系列相关任务进行建模，用户可以利用 Pipeline 的许多特性："],["body","\n\n"],["body","Code: 管道在代码中实现，通常会签入源代码管理，使团队能够编辑、审查和迭代他们的交付管道"],["body","\n"],["body","Durable: 管道可以在 Jenkins 控制器的计划内和计划外重启中存活下来。"],["body","\n"],["body","Pausable: 流水线可以选择停止并等待人工输入或批准，然后再继续流水线运行。 "],["body","\n"],["body","Versatile: 管道支持复杂的现实世界 CD 要求，包括分叉/加入、循环和并行执行工作的能力。"],["body","\n"],["body","Extensible: 管道插件支持对其 DSL [1] 的自定义扩展以及与其他插件集成的多个选项。"],["body","\n\n"],["body","基于 Jenkins 可扩展性的核心价值，Pipeline 也可以由使用 Pipeline Shared Libraries 的用户和插件开发人员扩展。 "],["body","\n"],["body","下面的流程图是在 Jenkins Pipeline 中轻松建模的一个 CD 场景的示例："],["body","\n"],["headingLink","pipeline-concepts"],["heading","Pipeline concepts"],["body","\n"],["headingLink","pipeline"],["heading","Pipeline"],["body","\n"],["body","管道是用户定义的 CD 管道模型。"],["body","\n"],["body","管道的代码定义了整个构建过程，通常包括"],["body","\n\n"],["body","构建应用程序"],["body","\n"],["body","测试应用程序"],["body","\n"],["body","和交付应用程序的阶段。"],["body","\n\n"],["headingLink","node"],["heading","Node"],["body","\n"],["body","节点是一台机器，它是 Jenkins 环境的一部分，能够执行流水线。"],["body","\n"],["headingLink","stage"],["heading","Stage"],["body","\n"],["body","阶段块定义了通过整个流水线（例如“构建”、“测试”和“部署”阶段）执行的概念上不同的任务子集，许多插件使用它来可视化或呈现 Jenkins 流水线状态/进度。 "],["body","\n"],["headingLink","step"],["heading","Step"],["body","\n"],["body","一个任务。"],["body","\n"],["body","从根本上说，步骤告诉 Jenkins 在特定时间点（或过程中的“步骤”）要做什么。"],["body","\n"],["body","例如，要执行 shell 命令 make 使用 sh 步骤：sh 'make'。"],["body","\n"],["headingLink","pipeline-syntax-overview"],["heading","Pipeline syntax overview"],["body","\n"],["body","以下流水线代码框架说明了声明式流水线语法和脚本式流水线语法之间的根本区别。"],["body","\n"],["headingLink","declarative-pipeline-fundamentals"],["heading","Declarative Pipeline fundamentals"],["body","\n"],["body","Jenkinsfile (Declarative Pipeline)"],["body","\n"],["body","pipeline {\n    agent any \n    stages {\n        stage('Build') { \n            steps {\n                // \n            }\n        }\n        stage('Test') { \n            steps {\n                // \n            }\n        }\n        stage('Deploy') { \n            steps {\n                // \n            }\n        }\n    }\n}\n"],["body","\n"],["headingLink","scripted-pipeline-fundamentals"],["heading","Scripted Pipeline fundamentals"],["body","\n"],["body","在脚本化流水线语法中，一个或多个节点块在整个流水线中完成核心工作。"],["body","\n"],["body","尽管这不是 Scripted Pipeline 语法的强制性要求，但将 Pipeline 的工作限制在节点块内有两件事："],["body","\n\n"],["body","通过将项目添加到 Jenkins 队列来安排块中包含的步骤运行。一旦执行程序在节点上空闲，这些步骤就会运行。"],["body","\n"],["body","创建一个工作区（特定于该特定管道的目录），可以在其中对从源代码管理检出的文件进行工作"],["body","\n\n"],["body","\n"],["body","根据您的 Jenkins 配置，某些工作区在一段时间不活动后可能不会自动清理。"],["body","\n"],["body","\n"],["body","Jenkinsfile (Scripted Pipeline)"],["body","\n"],["body","node {  \n    stage('Build') { \n        // \n    }\n    stage('Test') { \n        // \n    }\n    stage('Deploy') { \n        // \n    }\n}\n"],["body","\n"],["headingLink","pipeline-example"],["heading","Pipeline example"],["body","\n"],["body","Jenkinsfile (Declarative Pipeline)"],["body","\n"],["body","pipeline { \n    agent any \n    options {\n        skipStagesAfterUnstable()\n    }\n    stages {\n        stage('Build') { \n            steps { \n                sh 'make' \n            }\n        }\n        stage('Test'){\n            steps {\n                sh 'make check'\n                junit 'reports/**/*.xml' \n            }\n        }\n        stage('Deploy') {\n            steps {\n                sh 'make publish'\n            }\n        }\n    }\n}\n"],["body","\n"],["body","pipeline is Declarative Pipeline-specific syntax that defines a \"block\" containing all content and instructions for executing the entire Pipeline."],["body","\n"],["body","agent is Declarative Pipeline-specific syntax that instructs Jenkins to allocate an executor (on a node) and workspace for the entire Pipeline."],["body","\n"],["body","stage is a syntax block that describes a stage of this Pipeline. Read more about stage blocks in Declarative Pipeline syntax on the Pipeline syntax page. As mentioned above, stage blocks are optional in Scripted Pipeline syntax."],["body","\n"],["body","steps is Declarative Pipeline-specific syntax that describes the steps to be run in this stage."],["body","\n"],["body","sh is a Pipeline step (provided by the Pipeline: Nodes and Processes plugin) that executes the given shell command."],["body","\n"],["body","junit is another Pipeline step (provided by the JUnit plugin) for aggregating test reports."],["body","\n"],["body","node is Scripted Pipeline-specific syntax that instructs Jenkins to execute this Pipeline (and any stages contained within it), on any available agent/node. This is effectively equivalent to agent in Declarative Pipeline-specific syntax."],["body","\n\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","8.缓存中间件_Redis/Redis应用-分布式锁.html"],["title","Redis应用-分布式锁 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","何为分布式锁"],["heading","何为分布式锁"],["body","\n"],["body","使用分布式锁，是为了解决并发情况下相同代码块被多个线程同时执行带来的数据错乱问题。使用分布式锁以保证同一个代码块只有一个线程被执行。"],["body","\n"],["body","这个代码块的执行是一个原子操作，操作一开始就会一直运行到结束，不能被线程调度机制打断，不能发生线程上线文切换。"],["body","\n"],["headingLink","redis-分布式锁的奥义"],["heading","Redis 分布式锁的奥义"],["body","\n"],["body","分布式锁本质上要实现的目标是在 redis 中占一个「坑」，当别的线程也要来占坑时，发现坑里已经有一根 “大萝卜”了，就只能放弃或重试。"],["body","\n"],["body","占坑一般使用 setnx 指令，此操作只能允许一个线程来占坑，先来先占，用完来再调用 del 指令来释放 “坑”。"],["body","\n"],["body","127.0.0.1:6379> setnx lock-test 1\n(integer) 1\n// TODO do something...\n127.0.0.1:6379> del locl-test\n(integer) 0\n"],["body","\n"],["headingLink","问题1"],["heading","问题1"],["body","\n"],["body","如果逻辑执行到中间出现异常了或机器宕机，那么就导致 del 指令没有被执行，锁就永远得不到释放。我们可以通过拿到锁后给锁加一个过期时间，这样保证如果出现异常了锁也会自动释放。"],["body","\n"],["body","127.0.0.1:6379> set lock:test 1\nOK\n127.0.0.1:6379> expire lock:test 10\n(integer) 1\n// TODO do something...\n127.0.0.1:6379> del lock:test\n(integer) 0\n"],["body","\n"],["headingLink","问题2"],["heading","问题2"],["body","\n\n"],["body","如果 set 指令和 expire 指定执行中间，服务器宕机了，expire 得不到执行，锁得不到释放，那么也会出现死锁。这是因为 set 和 expire 是两条指令，不是一个原子操作。"],["body","\n"],["body","好在 redis2.8 后，可以通过给 set 指令添加拓展参数使得 setnx 和 expire 可以一起执行。"],["body","\n\n"],["body","127.0.0.1:6379> set lock:codehole 1 ex 30 nx # 相当于 setnx lock:codehole 1 & expire lock:codehole 30\nOK\n127.0.0.1:6379> set lock:codehole 1 ex 30 nx # 已存在，setex 失败。\n(nil)\n127.0.0.1:6379> del lock:codehole\n(integer) 1\n"],["body","\n"],["headingLink","超时问题"],["heading","超时问题"],["body","\n"],["body","如果加锁的代码块执行时间太久，锁自动释放了。"],["body","\n"],["body","加锁的代码块还没执行完毕第二个线程提前拿到了锁，会导致原来加锁的代码块得不到严格的串行执行。"],["body","\n"],["body","要规避这种问题，应该尽量避免加锁代码块用于执行时间过长的任务，合理配置锁的过期时间。"],["body","\n"],["body","超时还会导致另一个问题，当前线程可能会释放另一个线程的锁。"],["body","\n"],["body","线程A由于执行时间过长锁被自动释放，线程B拿到锁还没执行完，线程A代码块执行完后调用 del 指令释放锁，然而此时释放的却是线程B的锁。"],["body","\n"],["body","要规避这种问题，我们可以设置 set 指令的 value 参数为一个随机数（或当前线程名），删除时传入这个随机数，这样用来避免当前线程的锁被其他线程释放。"],["body","\n"],["body","伪代码如下，其中 if 分支判断和 del 操作不是原子操作，如果进入 if 分支后系统挂了锁就不会执行 del 指令了。（不过当前锁已过期了，但是保证了当前线程不会释放其他线程的锁。）"],["body","\n"],["body","String ramdonValue = \"1\";\nString value = jedis.get(\"key\");\nif (ramdonValue.equals(value)) {\n    jedis.del(\"key\");\n}\n"],["body","\n"],["body","虽然在 if 分支挂掉不会误释放其他线程的锁，但是还有更优雅的方式：即使用 lua 脚本，lua 脚本中的语句会被当作一个原子操作来执行。"],["body","\n"],["body","String luaScript = \"if redis.call('get', KEYS[1]) == ARGV[1] \" +\n                    \"then\" +\n                    \"   return redis.call('del', KEYS[1])\" +\n                    \"else \" +\n                    \"   return 0 \" +\n                    \"end\";\nObject value = jedis.eval(luaScript, Collections.singletonList(\"lock:test\"), Collections.singletonList(\"randomValue\"));\nSystem.out.println(value);\n"],["body","\n"],["body","这种方案只是相对安全一点，如果真的超时了，其他线程就会乘虚而入。"],["body","\n"],["body","一种解决方案是给当前线程添加一个守护线程，间断的给当前锁「续命」，如果系统宕机，守护线程会随着主线程的死亡而死亡，一段时间后锁被自动释放。"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","8.缓存中间件_Redis/Redis应用-位图.html"],["title","Redis应用-位图 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","何为位图bitmap"],["heading","何为位图（bitmap）"],["body","\n"],["body","位图不是特殊的数据结构，它本身是一个普通字符串，即 byte 数组。一个 byte 使用 8 个二进制位存储。redis 提供一系列操作二进制位的指令操作，这些操作称为「位图操作」。"],["body","\n\n"],["body","\n"],["body","使用 getbit、setbit 可以直接对 value 的位进行操作，在处理数亿级别的业务标示位存储时能够节约很大的空间。"],["body","\n"],["body","\n"],["body","\n"],["body","redis 中一个 String 类型的 value 能存储最大的值是 512MB、那么能存储大约 40亿 个bit。"],["body","\n"],["body","\n\n"],["headingLink","基本使用"],["heading","基本使用"],["body","\n"],["body","零存零取 setbit / getbit"],["body","\n"],["body","我们可以使用 bitset 指令以「位存储」的方式存储一个 he 字符串，注意上面获取的是无符号表示，存储的时候需要添加第一位为符号位。"],["body","\n"],["body","127.0.0.1:6379> setbit he 0 0 # 添加符号位\n(integer) 0\n127.0.0.1:6379> setbit he 1 1\n(integer) 0\n127.0.0.1:6379> setbit he 2 1\n(integer) 0\n127.0.0.1:6379> setbit he 3 0\n(integer) 0\n127.0.0.1:6379> setbit he 4 1\n(integer) 0\n127.0.0.1:6379> setbit he 5 0\n(integer) 0\n127.0.0.1:6379> setbit he 6 0\n(integer) 0\n127.0.0.1:6379> setbit he 7 0\n(integer) 0\n127.0.0.1:6379> get he # “整取”\n\"h\" # 设置了第一个字符\n127.0.0.1:6379> setbit he 8 0 # 添加符号位\n(integer) 0\n127.0.0.1:6379> setbit he 9 1\n(integer) 0\n127.0.0.1:6379> setbit he 10 1\n(integer) 0\n127.0.0.1:6379> setbit he 11 0\n(integer) 0\n127.0.0.1:6379> setbit he 12 0\n(integer) 0\n127.0.0.1:6379> setbit he 13 1\n(integer) 0\n127.0.0.1:6379> setbit he 14 0\n(integer) 0\n127.0.0.1:6379> setbit he 15 1\n(integer) 0\n127.0.0.1:6379> get he # “整取”\n\"he\" # 设置了两个字符\n127.0.0.1:6379> getbit he 14 # “零取”\n(integer) 0\n"],["body","\n"],["headingLink","统计和查找-bitcount--bitpos"],["heading","统计和查找 bitcount / bitpos"],["body","\n\n"],["body","bitcount key [start end]：统计指定范围内 1 出现的次数；"],["body","\n"],["body","bitpos key bit [start] [end]：统计指定范围内 0 或 1 第一次出现的位置。"],["body","\n\n"],["body","需要注意的是，这里的索引值是以 byte 为单位的，不是以 bit 为单位。即是以字节为范围查找的，如果需要以位范围查找必须是 8 的倍数。"],["body","\n"],["body","\n"],["headingLink","批量操作-bitfield"],["heading","批量操作 bitfield"],["body","\n"],["body","bitfield 有三个子指令 set、get、incrby。他们可以对执行片段进行读写自增，最多处理 64 个连续位。"],["body","\n"],["headingLink","使用场景"],["heading","使用场景"],["body","\n"],["body","bitmap 非常合适存储标识字段，因为位存储的就是 0 和 1。假设存储一个 1 的标识字段，用 String 类型存储需要 1 个字节，八个位。如果直接用位存储则能存储 8 个标识字段。在大数据量的标示位存储下，bit 能节省很大空间。"],["body","\n"],["headingLink","场景1统计用户某个时间段的登陆次数"],["heading","【场景1】统计用户某个时间段的登陆次数"],["body","\n"],["body","key：时间段，如 1月\noffset：用户ID，必须是整数\nvalue：是否上线标识位\n"],["body","\n"],["body","前面以时间为主体，或者还可以以用户为主体，具体设计方式取决于取数据的方式。"],["body","\n"],["body","key：用户ID\noffset：日期，如 200214\nvalue：是否上线标识位\n"],["body","\n"],["body","【场景2】存储用户一年的签到次数"],["body","\n"],["body","key：用户ID\noffset：天数，1 代表今年的第一天，一年就只占用了 365 个位\nvalue：是否签到标识\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","8.缓存中间件_Redis/redis-serverAndRedis-cli.html"],["title","redis-serverAndRedis-cli - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","cli工具的类型"],["heading","CLI工具的类型"],["body","\n"],["body","可执行文件"],["body","作用"],["body","\n"],["body","redis-server"],["body","Redis Srver相关"],["body","\n"],["body","redis-cli"],["body","Redis命令行工具"],["body","\n"],["body","redis-benchmark"],["body","基准测试工具"],["body","\n"],["body","redis-check-aof"],["body","AOF持久化文件检测工具和修复工具"],["body","\n"],["body","redis-check-rdb"],["body","RDB持久化文件检测工具和修复工具"],["body","\n"],["body","redis-sentinel"],["body","Redis哨兵系统"],["body","\n\n\n"],["headingLink","redis-cli"],["heading","redis-cli"],["body","\n"],["body","命令方式"],["body","\n"],["body","# 直接得到命令的返回结果,显示在屏幕上。\nredis-cli -h {host} -p {port} {command}\n"],["body","\n"],["body","交互式命令行方式"],["body","\n"],["body","redis-cli -h {host} -p {port} \n"],["body","\n"],["body","命令"],["body","\n"],["body","选项"],["body","说明"],["body","\n"],["body","time"],["body","返回当前服务器时间"],["body","\n"],["body","eval"],["body","运行lua脚本"],["body","\n"],["body","evalsha"],["body","根据给定的 sha1 校验码，执行缓存在服务器中的脚本。"],["body","\n"],["body","script exists"],["body","查看指定的脚本是否已经被保存在缓存当中"],["body","\n"],["body","script flush"],["body","从脚本缓存中移除所有脚本"],["body","\n"],["body","script kill"],["body","杀死当前正在运行的 Lua 脚本"],["body","\n"],["body","script load"],["body","将脚本添加到脚本缓存中,并不立即执行这个脚本"],["body","\n"],["body","dbsize"],["body","返回当前数据库的 key 的数量"],["body","\n"],["body","client list"],["body","返回所有连接到服务器的客户端信息和统计数据"],["body","\n"],["body","select"],["body","切换到指定的库"],["body","\n"],["body","quit"],["body","关闭连接"],["body","\n"],["body","auth"],["body","密码认证"],["body","\n"],["body","echo"],["body","打印字符串"],["body","\n"],["body","ping"],["body","查看服务是否运行,如果Redis存活会返回pong"],["body","\n"],["body","client kill ip:port"],["body","关闭地址为 ip:port 的客户端"],["body","\n"],["body","save"],["body","将数据同步保存到磁盘"],["body","\n"],["body","bgsave"],["body","将数据异步保存到磁盘"],["body","\n"],["body","lastsave"],["body","返回上次成功将数据保存到磁盘的Unix时戳"],["body","\n"],["body","shundown"],["body","异步保存数据到硬盘，并关闭服务器"],["body","\n"],["body","info"],["body","提供服务器的信息和统计"],["body","\n"],["body","config resetstat"],["body","重置info命令中的某些统计数据"],["body","\n"],["body","config get"],["body","获取配置文件信息,CONFIG GET *获取所有配置信息"],["body","\n"],["body","config set"],["body","动态地调整 Redis 服务器的配置而无须重启"],["body","\n"],["body","config rewrite"],["body","Redis 服务器时所指定的 redis.conf 文件进行改写"],["body","\n"],["body","monitor"],["body","实时监控收到的所有请求"],["body","\n"],["body","slaveof"],["body","将当前服务器转变为指定服务器的从属服务器(slave server)"],["body","\n"],["body","role"],["body","返回主从实例所属的角色"],["body","\n"],["body","BGREWRITEAOF"],["body","异步执行一个 AOF（AppendOnly File） 文件重写操作"],["body","\n"],["body","CLIENT GETNAME"],["body","获取连接的名称"],["body","\n"],["body","CLIENT SETNAME"],["body","设置当前连接的名称"],["body","\n"],["body","CLIENT PAUSE"],["body","阻塞客户端命令一段时间（以毫秒计）"],["body","\n"],["body","CLUSTER SLOTS"],["body","获取集群节点的映射数组"],["body","\n"],["body","COMMAND"],["body","获取 Redis 命令详情数组"],["body","\n"],["body","COMMAND COUNT"],["body","获取 Redis 命令总数"],["body","\n"],["body","COMMAND GETKEYS"],["body","获取给定命令的所有键"],["body","\n"],["body","COMMAND INFO"],["body","获取指定 Redis 命令描述的数组"],["body","\n"],["body","DEBUG OBJECT"],["body","获取 key 的调试信息"],["body","\n"],["body","DEBUG SEGFAULT"],["body","让 Redis 服务崩溃"],["body","\n"],["body","FLUSHALL"],["body","删除所有数据库的所有key"],["body","\n"],["body","FLUSHDB"],["body","删除当前数据库的所有key"],["body","\n"],["body","SLOWLOG"],["body","管理 redis 的慢日志"],["body","\n"],["body","SYNC"],["body","用于复制功能(replication)的内部命令"],["body","\n"],["body","memory purge"],["body","重整内存碎片,主动释放已删除的内存,会阻塞主线程 4.0以上版本"],["body","\n\n\n"],["headingLink","redis-sever"],["heading","redis-sever"],["body","\n"],["body","**语法格式：**redis-server [参数]"],["body","\n"],["body","常用参数："],["body","\n"],["body","参数"],["body","说明"],["body","\n"],["body","--port"],["body","配置端口"],["body","\n"],["body","--slaveof"],["body","将当前服务器转变为指定服务器的从属服务器"],["body","\n"],["body","--loglevel"],["body","配置日志级别"],["body","\n"],["body","--sentinel"],["body","以哨兵模式运行"],["body","\n"],["body","--masterauth"],["body","如果主库设置了主从密码, 从库需要用该参数指定主从密码"],["body","\n"],["body","-a"],["body","指定密码"],["body","\n\n\n"],["headingLink","客户端选项redis-cli"],["heading","客户端选项redis-cli"],["body","\n"],["body","\n"],["body","选项"],["body","说明"],["body","案例"],["body","\n"],["body","-h"],["body","指定Redis server地址"],["body","\n"],["body","-p"],["body","指定Redis server端口号"],["body","\n"],["body","-s"],["body","指定服务器套接字(覆盖主机名和端口)。"],["body","\n"],["body","-a"],["body","指定密码"],["body","\n"],["body","-u"],["body","url格式的地址"],["body","\n"],["body","-r"],["body","将命令重复执行N次"],["body","\n"],["body","-i"],["body","每隔N秒执行一次命令，必须与-r一起使用。"],["body","\n"],["body","-n"],["body","选择库号"],["body","\n"],["body","-x"],["body","代表从标准输入读取数据作为该命令的最后一个参数。"],["body","\n"],["body","-d"],["body","原始格式中的多块分隔符(默认值:\\n)。"],["body","\n"],["body","-c"],["body","连接cluster集群结点时用的，此选项可防止moved和ask异常。"],["body","\n"],["body","--csv"],["body","将数据导出为CSV格式的文件"],["body","\n"],["body","--scan"],["body","获取服务器所有的键"],["body","\n"],["body","--pattern"],["body","指定scan获取的key的pattern,正则表达式用于scan命令后过滤."],["body","\n"],["body","--slave"],["body","当前客户端模拟成当前redis节点的从节点，可用来获取指定redis节点的更新操作"],["body","\n"],["body","--rdb"],["body","导出rdb文件，保存导到指定的位置"],["body","\n"],["body","--pipe"],["body","将命令封装成redis通信协议定义的数据格式，批量发送给redis执行。"],["body","\n"],["body","--pipe-timeout"],["body","设置管道超时时间"],["body","\n"],["body","--bigkeys"],["body","统计bigkey的分布，使用scan命令对redis的键进行采样，从中找到内存占用比较大的键"],["body","\n"],["body","--hotkeys"],["body","找出server中热点key"],["body","\n"],["body","--stat"],["body","实时获取redis的统计信息。istat和info相比可以看到一些增加的数据,如:每秒请求数"],["body","\n"],["body","--raw"],["body","显示格式化的效果"],["body","\n"],["body","--no-raw"],["body","要求返回原始格式"],["body","\n"],["body","--eval"],["body","用于执行lua脚本"],["body","\n"],["body","--latency"],["body","持续采样服务器延迟"],["body","\n"],["body","--latency-history"],["body","持续采样服务器延迟并每隔(15秒)输出一个记录; 可以使用-i 更改间隔时间"],["body","\n"],["body","--latency-dist"],["body","使用彩色终端显示一系列延时特征"],["body","\n"],["body","--intrinsic-latency"],["body","固有延迟,由于操作系统或虚拟机/容器带来的延迟,需要在redis-server的本器上进行测量."],["body","\n"],["body","--ldb"],["body","与--eval一起使用可以启用Redis Lua调试器"],["body","\n"],["body","--ldb-sync-mode"],["body","比如--ldb，但是使用了同步Lua调试器, 此模式将阻塞服务器并更改脚本"],["body","\n"],["body","--lru-test"],["body","\n\n\n"],["headingLink","redis-cli-stat"],["heading","redis-cli stat"],["body","\n"],["body","选项"],["body","说明"],["body","案例"],["body","\n"],["body","keys"],["body","server中key的数量"],["body","\n"],["body","mem"],["body","键值对的总内存量"],["body","\n"],["body","clients"],["body","当前连接的总clients数量"],["body","\n"],["body","blocked"],["body","正在等待执行阻塞命令（BLPOP、BRPOP、BRPOPLPUSH 等等）的客户端数量"],["body","\n"],["body","requests"],["body","服务器请求总次数 (+1) 截止上次请求增加次数"],["body","\n"],["body","connections"],["body","服务器连接次数"],["body","\n\n\n"],["headingLink","性能测试工具redis-benchmark"],["heading","性能测试工具redis-benchmark"],["body","\n"],["body","redis-benchmark命令不属于redis-cli而是在Redis的其他工具,默认在Redis目录下"],["body","\n"],["body","选项"],["body","说明"],["body","案例"],["body","\n"],["body","-h"],["body","指定服务器主机名"],["body","\n"],["body","-p"],["body","指定服务器端口"],["body","\n"],["body","-s"],["body","指定服务器 socket"],["body","\n"],["body","-c"],["body","指定并发连接数"],["body","\n"],["body","-n"],["body","指定请求数"],["body","\n"],["body","-d"],["body","以字节的形式指定 SET/GET 值的数据大小"],["body","\n"],["body","-k"],["body","1=keep alive 0=reconnect"],["body","\n"],["body","-r"],["body","SET/GET/INCR 使用随机 key, SADD 使用随机值"],["body","\n"],["body","-P"],["body","通过管道传输  请求"],["body","\n"],["body","-q"],["body","强制退出 redis。仅显示 query/sec 值"],["body","\n"],["body","--csv"],["body","以 CSV 格式输出"],["body","\n"],["body","-l"],["body","生成循环，永久执行测试"],["body","\n"],["body","-t"],["body","仅运行以逗号分隔的测试命令列表。"],["body","\n"],["body","-I"],["body","Idle 模式。仅打开 N 个 idle 连接并等待。"],["body","\n\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","8.缓存中间件_Redis/index.html"],["title","Redis - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","简介"],["heading","简介"],["body","\n"],["body","是一个高性能的key-value数据库"],["body","\n"],["body","Redis支持数据的持久化"],["body","\n"],["body","Redis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储"],["body","\n"],["body","Redis支持数据的备份，即master-slave模式的数据备份"],["body","\n"],["headingLink","安装"],["heading","安装"],["body","\n"],["body","下载地址"],["body","\n"],["body","运行命令:redis-server.exe redis.conf"],["body","\n"],["body","redis-cli.exe -h 127.0.0.1 -p 6379"],["body","\n"],["body","curl -O  http://download.redis.io/releases/redis-6.0.6.tar.gz\nyum  -y  install  centos-release-scl\nyum  -y  install  devtoolset-9-gcc  devtoolset-9-gcc-c++  devtoolset-9-binutils\nscl enable devtoolset-9 bash\necho \"source /opt/rh/devtoolset-9/enable\" >>/etc/profile\ntar -xf redis-6.0.6.tar.gz\ncd redis-6.0.6\nmake\n\nmake  install  PREFIX=/usr/local/redis-6.0.6\ncp redis.conf sentinel.conf   /usr/local/redis-6.0.6/\n"],["body","\n"],["headingLink","文件目录"],["heading","文件目录"],["body","\n"],["body","文件名"],["body","说明"],["body","\n"],["body","redis-server"],["body","服务"],["body","\n"],["body","redis-cli"],["body","命令行客户端"],["body","\n"],["body","redis-benchmark"],["body","性能测试工具"],["body","\n"],["body","redis-check-aof"],["body","AOF 文件修复工具"],["body","\n"],["body","redis-check-dump"],["body","RDB 文件检查工具"],["body","\n"],["body","redis-sentinel"],["body","Sentinel 服务器（v2.8 后）"],["body","\n\n\n"],["headingLink","多数据库支持"],["heading","多数据库支持"],["body","\n"],["body","Redis 默认支持 16 个数据库，不可以自定义数据库名，只能根据编号命名，默认从 0 开始。"],["body","\n"],["body","不支持为每一个数据库设置密码，一个客户端要么能访问所有库，要么没权限访问。"],["body","\n"],["headingLink","redis-安全"],["heading","Redis 安全"],["body","\n\n"],["body","CONFIG get requirepass"],["body","\n"],["body","设置密码:CONFIG set requirepass \"xjq\""],["body","\n"],["body","验证密码:AUTH <password>"],["body","\n\n"],["headingLink","redis-性能测试"],["heading","Redis 性能测试"],["body","\n"],["body","redis-benchmark [option] [option value]"],["body","\n"],["body","\n"],["body","1"],["body","-h"],["body","指定服务器主机名"],["body","127.0.0.1"],["body","\n"],["body","2"],["body","-p"],["body","指定服务器端口"],["body","6379"],["body","\n"],["body","3"],["body","-s"],["body","指定服务器 socket"],["body","\n"],["body","4"],["body","-c"],["body","指定并发连接数"],["body","50"],["body","\n"],["body","5"],["body","-n"],["body","指定请求数"],["body","10000"],["body","\n"],["body","6"],["body","-d"],["body","以字节的形式指定 SET/GET 值的数据大小"],["body","2"],["body","\n"],["body","7"],["body","-k"],["body","1=keep alive 0=reconnect"],["body","1"],["body","\n"],["body","8"],["body","-r"],["body","SET/GET/INCR 使用随机 key, SADD 使用随机值"],["body","\n"],["body","9"],["body","-P"],["body","通过管道传输  请求"],["body","1"],["body","\n"],["body","10"],["body","-q"],["body","强制退出 redis。仅显示 query/sec 值"],["body","\n"],["body","11"],["body","--csv"],["body","以 CSV 格式输出"],["body","\n"],["body","12"],["body","-l"],["body","生成循环，永久执行测试"],["body","\n"],["body","13"],["body","-t"],["body","仅运行以逗号分隔的测试命令列表。"],["body","\n"],["body","14"],["body","-I"],["body","Idle 模式。仅打开 N 个 idle 连接并等待。"],["body","\n\n\n"],["headingLink","客户端命令"],["heading","客户端命令"],["body","\n"],["body","S.N."],["body","命令"],["body","描述"],["body","\n"],["body","1"],["body","CLIENT LIST"],["body","返回连接到 redis 服务的客户端列表"],["body","\n"],["body","2"],["body","CLIENT SETNAME"],["body","设置当前连接的名称"],["body","\n"],["body","3"],["body","CLIENT GETNAME"],["body","获取通过 CLIENT SETNAME 命令设置的服务名称"],["body","\n"],["body","4"],["body","CLIENT PAUSE"],["body","挂起客户端连接，指定挂起的时间以毫秒计"],["body","\n"],["body","5"],["body","CLIENT KILL"],["body","关闭客户端连接"],["body","\n\n\n"],["headingLink","脚本"],["heading","脚本"],["body","\n"],["body","Lua 语言（Open Rest，Nginx 也可以使用 lua 语言，有机会学习了解一下）"],["body","\n"],["body","这里不详细记录 Lua 语法，不过有一点思考，既然 Nginx 也可以使用 Lua，那么可以就有一种场景，Nginx 通过 lua 访问 Redis 读取数据，并且用 lua 渲染模板，达到页面直出，这样应该效率很高。"],["body","\n"],["headingLink","事务"],["heading","事务"],["body","\n"],["body","Redis 事务可以保证一个事务内的命令依次执行而不被其他命令插入。"],["body","\n"],["body","Redis 事务的异常处理，首先需要先明确什么原因导致执行出错。"],["body","\n"],["body","1）语法错，一旦前面有错，后面不会执行；"],["body","\n"],["body","2）运行错，一旦有错，后续的命令会继续执行；"],["body","\n"],["body","Redis 的事务没有关系数据库事务提供的回滚（rollback）[1]功能。为此开发者必须在事务执行出错后自己收拾剩下的摊子（将数据库复原回事务执行前的状态等）。"],["body","\n"],["headingLink","持久化"],["heading","持久化"],["body","\n\n"],["body","RDB （通过快照完成，当达到某种约定条件后自动生成一份备份并存储在硬盘上），快照原理"],["body","\n"],["body","AOF （存储非临时数据，每执行一条都会追加存储在硬盘，有一些性能影响）默认关闭，通过 appendonly yes 开启"],["body","\n\n"],["body","允许同时开启 RDB 和 AOF 两种模式。"],["body","\n"],["headingLink","集群"],["heading","集群"],["body","\n"],["body","Redis 支持集群，可以通过主从数据库来来规避单点数据库故障导致的问题。主数据库负责读写（读写分离也可以），当写操作导致数据变化时自动将数据同步给从库，从库只读，并只接受主库同步数据。"],["body","\n"],["body","配置文件，通过 slaveof 主库地址 主库端口 来完成主从复制的配置。"],["body","\n"],["body","\n"],["body","通过复制可以实现读写分离，以提高服务器负载能力。"],["body","\n"],["body","\n"],["body","关键字记录，Redis 支持哨兵，一主多从，需要自动监控 Redis 运行情况，作用：1)监控主从数据库运行正常；2)主数据库故障自动将从数据库转换成主数据库；细节待补充一篇琢磨透彻一点的分析文。"],["body","\n"],["headingLink","java使用redis"],["heading","Java使用redis"],["body","\n"],["headingLink","单链接"],["heading","单链接"],["body","\n"],["body","    Jedis jedis = new Jedis(\"localhost\", 6379, 100000);\n    jedis.auth(\"xjq\");\n\n    int i = 0;\n    try {\n        long start = System.currentTimeMillis();// 开始毫秒数\n        while (true) {\n            long end = System.currentTimeMillis();\n            if (end - start >= 1000) {// 当大于等于1000毫秒（相当于1秒）时，结束操作\n                break;\n            }\n            i++;\n            jedis.set(\"test\" + i, i + \"\");\n        }\n    } finally {// 关闭连接\n        jedis.close();\n    }\n    // 打印1秒内对Redis的操作次数\n    System.out.println(\"redis每秒操作：\" + i + \"次\");\n}\n"],["body","\n"],["headingLink","连接池"],["heading","连接池"],["body","\n"],["body","        JedisPoolConfig poolConfig = new JedisPoolConfig();\n// 最大空闲数\n        poolConfig.setMaxIdle(50);\n// 最大连接数\n        poolConfig.setMaxTotal(100);\n// 最大等待毫秒数\n        poolConfig.setMaxWaitMillis(20000);\n// 使用配置创建连接池\n        JedisPool pool = new JedisPool(poolConfig, \"localhost\");\n// 从连接池中获取单个连接\n        Jedis jedis = pool.getResource();\n// 如果需要密码\n        jedis.auth(\"xjq\");\n\n        System.out.println(jedis.get(\"xjq\"));\n"],["body","\n"],["headingLink","在-spring-中使用-redis"],["heading","在 Spring 中使用 Redis"],["body","\n"],["body","    @Bean\n    public RedisTemplate redisTemplate(){\n        RedisStandaloneConfiguration configuration = new RedisStandaloneConfiguration();\n        configuration.setHostName(\"127.0.0.1\");\n        configuration.setPort(6379);\n        configuration.setPassword(\"xjq\");\n        JedisConnectionFactory jedisConnectionFactory = new JedisConnectionFactory(configuration);\n        RedisTemplate<Object, Object> redisTemplate = new RedisTemplate<>();\n        redisTemplate.setConnectionFactory(jedisConnectionFactory);\n        redisTemplate.setKeySerializer(new StringRedisSerializer());\n        redisTemplate.setValueSerializer(new StringRedisSerializer() );\n        return redisTemplate;\n    }\n"],["body","\n"],["body","private static void spring_test() {\n    AnnotationConfigApplicationContext applicationContext = new AnnotationConfigApplicationContext(MyConfig.class);\n    applicationContext.register(MyConfig.class);\n\n    RedisTemplate bean = applicationContext.getBean(RedisTemplate.class);\n    bean.opsForValue().set(\"xjq\",\"sxxx\");\n}\n"],["body","\n"],["headingLink","springbooter配置"],["heading","springBooter配置"],["body","\n"],["body","@Bean\npublic RedisTemplate redisTemplate(RedisConnectionFactory redisConnectionFactory){\n    RedisTemplate<String, String> redisTemplate = new RedisTemplate<>();\n    redisTemplate.setKeySerializer(new StringRedisSerializer());\n    redisTemplate.setConnectionFactory(redisConnectionFactory);\n    redisTemplate.setValueSerializer(new StringRedisSerializer());\n    return redisTemplate;\n}\n"],["body","\n"],["body","spring:\n  redis:\n    password: xjq\n    host: localhost\n    port: 6379\n\n\n  main:\n    web-application-type: none\n"],["body","\n"],["body","redis学习"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","8.缓存中间件_Redis/redis-keys与scan.html"],["title","redis-keys与scan - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","问题描述"],["heading","问题描述"],["body","\n"],["body","Shiro 并没有直接提供 Redis 存储 Session 组件，所以使用 Github 一个开源组件 shiro-redis"],["body","\n"],["body","由于 Shiro 框架需要定期验证 Session 是否有效，于是 Shiro 底层将会调用 SessionDAO#getActiveSessions 获取所有的 Session 信息。"],["body","\n"],["body","底层使用用keys 命令查找 Redis 所有存储的 Session key，效率较慢"],["body","\n"],["body","public Set<byte[]> keys(byte[] pattern){\n    checkAndInit();\n    Set<byte[]> keys = null;\n    Jedis jedis = jedisPool.getResource();\n    try{\n        keys = jedis.keys(pattern);\n    }finally{\n        jedis.close();\n    }\n    return keys;\n}\n"],["body","\n"],["body","在最新版本中，shiro-redis 采用 scan命令代替 keys,从而修复这个问题。"],["body","\n"],["body","public Set<byte[]> keys(byte[] pattern) {\n    Set<byte[]> keys = null;\n    Jedis jedis = jedisPool.getResource();\n\n    try{\n        keys = new HashSet<byte[]>();\n        ScanParams params = new ScanParams();\n        params.count(count);\n        params.match(pattern);\n        byte[] cursor = ScanParams.SCAN_POINTER_START_BINARY;\n        ScanResult<byte[]> scanResult;\n        do{\n            scanResult = jedis.scan(cursor,params);\n            keys.addAll(scanResult.getResult());\n            cursor = scanResult.getCursorAsBytes();\n        }while(scanResult.getStringCursor().compareTo(ScanParams.SCAN_POINTER_START) > 0);\n    }finally{\n        jedis.close();\n    }\n    return keys;\n\n}\n"],["body","\n"],["body","为什么keys 指令会导致其他命令执行变慢？"],["body","\n"],["body","为什么Keys 指令查询会这么慢？"],["body","\n"],["body","为什么Scan 指令就没有问题？"],["body","\n"],["headingLink","redis-执行命令的原理"],["heading","Redis 执行命令的原理"],["body","\n"],["body","\n"],["body","由于 Redis 单线程执行命令，只能顺序从队列取出任务开始执行。"],["body","\n"],["body","只要 3 这个过程执行命令速度过慢，队列其他任务不得不进行等待，这对外部客户端看来，Redis 好像就被阻塞一样，一直得不到响应。"],["body","\n"],["headingLink","keys-原理"],["heading","KEYS 原理"],["body","\n"],["body","Redis 底层使用字典这种结构，这个结构与 Java HashMap 底层比较类似。"],["body","\n"],["body","keys命令需要返回所有的符合给定模式 pattern 的 Redis 中键，为了实现这个目的，Redis 不得不遍历字典中 ht[0]哈希表底层数组，这个时间复杂度为 O(N)（N 为 Redis 中 key 所有的数量）。"],["body","\n"],["headingLink","scan-原理"],["heading","SCAN 原理"],["body","\n"],["body","最后我们来看下第三个问题，为什么scan 指令就没有问题？"],["body","\n"],["body","这是因为 scan命令采用一种黑科技-基于游标的迭代器。"],["body","\n"],["body","每次调用 scan 命令，Redis 都会向用户返回一个新的游标以及一定数量的 key。下次再想继续获取剩余的 key，需要将这个游标传入 scan 命令， 以此来延续之前的迭代过程。"],["body","\n"],["body","简单来讲，scan 命令使用分页查询 redis 。"],["body","\n"],["body","下面是一个 scan 命令的迭代过程示例："],["body","\n"],["body","scan 命令使用游标这种方式，巧妙将一次全量查询拆分成多次，降低查询复杂度。"],["body","\n"],["body","虽然 scan 命令时间复杂度与 keys一样，都是 O(N)，但是由于 scan 命令只需要返回少量的 key，所以执行速度会很快。"],["body","\n"],["body","最后，虽然scan 命令解决 keys不足，但是同时也引入其他一些缺陷："],["body","\n\n"],["body","同一个元素可能会被返回多次，这就需要我们应用程序增加处理重复元素功能。"],["body","\n"],["body","如果一个元素在迭代过程增加到 redis，或者说在迭代过程被删除，那个这个元素会被返回，也可能不会。"],["body","\n\n"],["body","除了 scan以外，redis 还有其他几个用于增量迭代命令："],["body","\n\n"],["body","sscan:用于迭代当前数据库中的数据库键，用于解决 smembers 可能产生阻塞问题"],["body","\n"],["body","hscan命令用于迭代哈希键中的键值对，用于解决 hgetall 可能产生阻塞问题。"],["body","\n"],["body","zscan:命令用于迭代有序集合中的元素（包括元素成员和元素分值），用于产生 zrange 可能产生阻塞问题。"],["body","\n\n"],["headingLink","总结"],["heading","总结"],["body","\n"],["body","Redis 使用单线程执行操作命令，所有客户端发送过来命令，Redis 都会现放入队列，然后从队列中顺序取出执行相应的命令。"],["body","\n"],["body","所以不要在生产执行 keys、smembers、hgetall、zrange这类可能造成阻塞的指令，如果真需要执行，可以使用相应的scan 命令渐进式遍历，可以有效防止阻塞问题。"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","8.缓存中间件_Redis/redis发布与订阅.html"],["title","redis发布与订阅 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","命令"],["heading","命令"],["body","\n"],["headingLink","发布"],["heading","发布"],["body","\n"],["body","publish channel message\n"],["body","\n"],["headingLink","订阅"],["heading","订阅"],["body","\n"],["body","subscribe channel [channel ...]\n"],["body","\n"],["body","使用订阅命令，需要主要几点："],["body","\n\n"],["body","\n"],["body","客户端执行订阅指令之后，就会进入订阅状态，之后就只能接收 subscribe、psubscribe、unsubscribe、punsubscribe 这四个命令。"],["body","\n"],["body","\n"],["body","\n"],["body","第二，新订阅的客户端，是无法收到这个频道之前的消息，这是因为 Redis 并不会对发布的消息持久化的。"],["body","\n"],["body","\n\n"],["headingLink","模式匹配的订阅方式"],["heading","模式匹配的订阅方式"],["body","\n"],["body","psubscribe pay.*\npunsubscribe pay.*\n"],["body","\n"],["headingLink","基于-jedis-开发发布订阅"],["heading","基于 Jedis 开发发布/订阅"],["body","\n"],["body","发布"],["body","\n"],["body","HostAndPort hostAndPort1 = new HostAndPort(\"192.168.3.16\",7000);\nHostAndPort hostAndPort2 = new HostAndPort(\"192.168.3.16\",7001);\nHostAndPort hostAndPort3 = new HostAndPort(\"192.168.3.16\",7002);\nHostAndPort hostAndPort4 = new HostAndPort(\"192.168.3.16\",7003);\nHostAndPort hostAndPort5 = new HostAndPort(\"192.168.3.16\",7004);\nHostAndPort hostAndPort6 = new HostAndPort(\"192.168.3.16\",7005);\nHashSet<HostAndPort> objects = new HashSet<>();\nobjects.add(hostAndPort1);\nobjects.add(hostAndPort2);\nobjects.add(hostAndPort3);\nobjects.add(hostAndPort4);\nobjects.add(hostAndPort5);\nobjects.add(hostAndPort6);\n\nJedisCluster jedisCluster = new JedisCluster(objects);\n\njedisCluster.publish(\"xjq\",\"yes\");\n"],["body","\n"],["body","订阅"],["body","\n"],["body","jedisCluster.subscribe(new JedisPubSub() {\n    @Override\n    public void onMessage(String channel, String message) {\n        System.out.printf(\"I have rececive a message from %s,message is %s%n\",channel,message);\n    }\n},\"xjq\");\n"],["body","\n"],["headingLink","redis-发布订阅实际应用"],["heading","Redis 发布订阅实际应用"],["body","\n"],["headingLink","redis-sentinel-节点发现"],["heading","Redis Sentinel 节点发现"],["body","\n"],["body","Redis Sentinel 节点主要使用发布订阅机制，实现新节点的发现，以及交换主节点的之间的状态"],["body","\n"],["body","如下所示，每一个 Sentinel 节点将会定时向 _sentinel_:hello 频道发送消息，并且每个 Sentinel 都会订阅这个节点。"],["body","\n"],["body","\n"],["body","这样一旦有节点往这个频道发送消息，其他节点就可以立刻收到消息。"],["body","\n"],["body","这样一旦有的新节点加入，它往这个频道发送消息，其他节点收到之后，判断本地列表并没有这个节点，于是就可以当做新的节点加入本地节点列表。"],["body","\n"],["body","除此之外，每次往这个频道发送消息内容可以包含节点的状态信息，这样可以作为后面 Sentinel 领导者选举的依据。"],["body","\n"],["body","以上都是对于 Redis 服务端来讲，对于客户端来讲，我们也可以用到发布订阅机制。"],["body","\n"],["body","当 Redis Sentinel 进行主节点故障转移，这个过程各个阶段会通过发布订阅对外提供。"],["body","\n"],["body","对于我们客户端来讲，比较关心切换之后的主节点，这样我们及时切换主节点的连接（旧节点此时已故障，不能再接受操作指令），"],["body","\n"],["body","客户端可以订阅 +switch-master频道，一旦 Redis Sentinel 结束了对主节点的故障转移就会发布主节点的的消息。"],["body","\n"],["headingLink","redission-分布式锁"],["heading","redission 分布式锁"],["body","\n"],["body","就是采用服务通知的机制实现分布式锁的唤醒"],["body","\n"],["body","当线程加锁失败之后，线程将会订阅 redisson_lock__channel_xxx（xx 代表锁的名称） 频道，使用异步线程监听消息，然后利用 Java 中 Semaphore 使当前线程进入阻塞"],["body","\n"],["body","一旦其他客户端进行解锁，redission 就会往这个redisson_lock__channel_xxx 发送解锁消息。"],["body","\n"],["body","等异步线程收到消息，将会调用 Semaphore 释放信号量，从而让当前被阻塞的线程唤醒去加锁"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","8.缓存中间件_Redis/redis数据结构/Redis有序集合(sortedSet).html"],["title","Redis有序集合(sortedSet) - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","redis-有序集合sorted-set"],["heading","Redis 有序集合(sorted set)"],["body","\n\n"],["body","\n"],["body","Redis 有序集合和集合一样也是 string 类型元素的集合,且不允许重复的成员。"],["body","\n"],["body","\n"],["body","\n"],["body","不同的是每个元素都会关联一个 double 类型的分数。redis 正是通过分数来为集合中的成员进行从小到大的排序。"],["body","\n"],["body","\n\n"],["body","有序集合的成员是唯一的,但分数(score)却可以重复。"],["body","\n"],["body","集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。 集合中最大的成员数为 232 - 1 (4294967295, 每个集合可存储40多亿个成员)。"],["body","\n"],["body","示例"],["body","\n"],["body","redis 127.0.0.1:6379> ZADD runoobkey 1 redis\n(integer) 1\nredis 127.0.0.1:6379> ZADD runoobkey 2 mongodb\n(integer) 1\nredis 127.0.0.1:6379> ZADD runoobkey 3 mysql\n(integer) 1\nredis 127.0.0.1:6379> ZADD runoobkey 3 mysql\n(integer) 0\nredis 127.0.0.1:6379> ZADD runoobkey 4 mysql\n(integer) 0\nredis 127.0.0.1:6379> ZRANGE runoobkey 0 10 WITHSCORES\n\n1) \"redis\"\n2) \"1\"\n3) \"mongodb\"\n4) \"2\"\n5) \"mysql\"\n6) \"4\n"],["body","\n"],["headingLink","redis-有序集合命令"],["heading","Redis 有序集合命令"],["body","\n"],["body","添加成员、或者更新已有成员分数\nZADD key score1 member1 [score2 member2]"],["body","\n"],["body","获取有序集合的成员数\nZCARD key"],["body","\n"],["body","统计指定区间分数的成员数\nZCOUNT key min max"],["body","\n"],["body","有序集合中对指定成员的分数加上增量\nZINCRBY key increment member"],["body","\n"],["body","计算给定的一个或多个有序集的交集并将结果集存储在新的有序集合 destination 中\nZINTERSTORE destination numkeys key [key ...]"],["body","\n"],["body","在有序集合中计算在指定字典区间内成员数量\nZLEXCOUNT key min max"],["body","\n"],["body","通过索引区间返回有序集合指定区间内的成员\nZRANGE key start stop [WITHSCORES]"],["body","\n"],["body","通过字典区间返回有序集合的成员\nZRANGEBYLEX key min max [LIMIT offset count]"],["body","\n"],["body","通过分数返回有序集合指定区间内的成员\nZRANGEBYSCORE key min max [WITHSCORES] [LIMIT]"],["body","\n"],["body","返回有序集合中指定成员的索引\nZRANK key member"],["body","\n"],["body","移除有序集合中的一个或多个成员\nZREM key member [member ...]"],["body","\n"],["body","移除有序集合中给定的字典区间的所有成员\nZREMRANGEBYLEX key min max"],["body","\n"],["body","移除有序集合中给定的排名区间的所有成员\nZREMRANGEBYRANK key start stop"],["body","\n"],["body","移除有序集合中给定的分数区间的所有成员\nZREMRANGEBYSCORE key min max"],["body","\n"],["body","返回有序集中指定区间内的成员，通过索引，分数从高到低\nZREVRANGE key start stop [WITHSCORES]"],["body","\n"],["body","返回有序集中指定分数区间内的成员，分数从高到低排序\nZREVRANGEBYSCORE key max min [WITHSCORES]"],["body","\n"],["body","返回有序集合中指定成员的排名，有序集成员按分数值递减(从大到小)排序\nZREVRANK key member"],["body","\n"],["body","返回有序集中，成员的分数值\nZSCORE key member"],["body","\n"],["body","计算给定的一个或多个有序集的并集，并存储在新的 key 中\nZUNIONSTORE destination numkeys key [key ...]"],["body","\n"],["body","迭代有序集合中的元素（包括元素成员和元素分值）\nZSCAN key cursor [MATCH pattern] [COUNT count]"],["body","\n"],["body"," zincrby z1 100 html\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","8.缓存中间件_Redis/redis数据结构/基本数据结构.html"],["title","基本数据结构 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","五种基础数据结构"],["heading","五种基础数据结构"],["body","\n"],["body","redis 的所有数据结构都以唯一的 key 字符串作为名称，通过唯一的 key 获取相对应的 value 数据。不同的数据类型的差异就在于 value 的结构不一样。"],["body","\n\n"],["body","string（字符串）"],["body","\n"],["body","list（列表）"],["body","\n"],["body","hash（字典）"],["body","\n"],["body","set（集合）"],["body","\n"],["body","zset（有序集合）"],["body","\n\n"],["headingLink","string"],["heading","string"],["body","\n"],["body","string 是最简单的数据结构，其内部就是一个字符数组。string 是动态字符串，相当于 ArrayList，当字符串长度小于 1M 时扩容都是加倍现有的空间。当字符串长度大于 1M 时每次扩容只会增加 1M 的空间。字符串长度最大为 512M。"],["body","\n"],["headingLink","键值对操作"],["heading","键值对操作"],["body","\n"],["body","127.0.0.1:6379> set name codehole\nOK\n127.0.0.1:6379> get name\n\"codehole\"\n127.0.0.1:6379> exists name\n(integer) 1\n127.0.0.1:6379> del name\n(integer) 1\n127.0.0.1:6379> get name\n(nil)\n127.0.0.1:6379>\n"],["body","\n"],["headingLink","批量键值对操作"],["heading","批量键值对操作"],["body","\n"],["body","mset key value [key value ...]\nmget key [key ...]\n"],["body","\n"],["body","127.0.0.1:6379> set name1 codehole\nOK\n127.0.0.1:6379> set name2 holycoder\nOK\n127.0.0.1:6379> mget name1 name2 name3\n1) \"codehole\"\n2) \"holycoder\"\n3) (nil)\n127.0.0.1:6379> mset name1 boy name2 girl name3 unknown\nOK\n127.0.0.1:6379> mget name1 name2 name3\n1) \"boy\"\n2) \"girl\"\n3) \"unknown\"\n127.0.0.1:6379>\n"],["body","\n"],["headingLink","过期和-set-命令拓展"],["heading","过期和 set 命令拓展"],["body","\n\n"],["body","expire：设置指定 key 的过期时间"],["body","\n"],["body","setex：添加 key 并设置过期时间"],["body","\n"],["body","setnx：如果key不存在添加，key存在则不添加。"],["body","\n\n"],["headingLink","计数"],["heading","计数"],["body","\n\n"],["body","incr key"],["body","\n"],["body","incrby key increment"],["body","\n\n"],["body","如果 value 是整数，可以对其进行自增操作。注意：自增是有范围的，范围在 signed long 的最大值和最小值之间，超过返回会报错。"],["body","\n"],["headingLink","list"],["heading","list"],["body","\n\n"],["body","list 相当于 java 中的 LinkedList，由于是链表结构故新增和删除特别快，但是索引定位特别慢。redis 中的 list 是双向链表结构，支持前后遍历，当列表最后一个元素被删除时，该数据结构会被自动删除。"],["body","\n"],["body","redis 的 list 结构常用来做异步队列使用。将需要延后处理的任务序列化成字符串，放入 redis 的 list，另一个线程从这个列表中轮训数据进行处理。"],["body","\n"],["body","使用 list 可以实现先进先出（队列）和后进先出（栈）的效果，添加命令有：lpush、rpush，区别在于取出命令的区别：lpop、rpop。"],["body","\n"],["body","lpop、rpop记忆：假设 list 元素都是从右边进来(rpush)，左边出是队列，对应命令是 lpop；右边出是栈，对应的命令是 rpop。"],["body","\n\n"],["headingLink","右进左出队列"],["heading","右进左出：队列"],["body","\n\n"],["body","rpush key value [value ...] 从右边添加数据，相反的还有 lpush。"],["body","\n"],["body","llen key"],["body","\n"],["body","lpop key"],["body","\n\n"],["headingLink","右进右出栈"],["heading","右进右出：栈"],["body","\n\n"],["body","rpush key value [value ...]"],["body","\n"],["body","rlen key"],["body","\n"],["body","rpop key"],["body","\n\n"],["headingLink","慢操作"],["heading","慢操作"],["body","\n\n"],["body","lindex key index：相当于 Java 链表的 get(int index)，由于需要对链表进行遍历，性能随着 index 增大而变差。"],["body","\n"],["body","ltrim key start stop：修剪 start 和 end 区间之间的值，外边的去掉。"],["body","\n"],["body","lrange key start stop：获取索引范围内的值。"],["body","\n"],["body","索引值可以为负数，-1 代表倒数第一个元素，-2 代表倒数第二个元素，这点和 Groovy 的 Range 对象很相似。"],["body","\n\n"],["body","example"],["body","\n"],["body","127.0.0.1:6379> rpush books python java golang\n(integer) 3\n127.0.0.1:6379> lindex books 1\n\"java\"\n127.0.0.1:6379> lrange books 0 -1\n1) \"python\"\n2) \"java\"\n3) \"golang\"\n127.0.0.1:6379> ltrim books 1 -1 # 去掉第一个元素\nOK\n127.0.0.1:6379> lrange books 0 -1 # 查看所有元素\n1) \"java\"\n2) \"golang\"\n127.0.0.1:6379> ltrim books 1 0 # 区间范围长度为负数，会清空 list。\nOK\n127.0.0.1:6379> llen books\n(integer) 0\n127.0.0.1:6379>\n"],["body","\n"],["headingLink","快速列表"],["heading","快速列表"],["body","\n"],["body","redis list 结构并不简单是一个 linkedList，而是一种称之为「快速链表quicklist」的结构。"],["body","\n"],["body","在列表元素较少的情况下，会使用一块连续的内存存储，这个结构是 ziplist，即「压缩列表」。"],["body","\n"],["body","它将所有的元素彼此紧挨着一起存储，分配的是一块连续的内存。"],["body","\n"],["body","当数据量比较多的时候才会改成 quicklist，因为普通链表需要附加的指针空间太大，会浪费空间，还会加重内存的碎片化。"],["body","\n"],["body","比如某普通链表存的只是 int 类型的数据，结构上还需要两个额外的指针 prev 和 next。"],["body","\n"],["body","所以 redis 将链表和 ziplist 结合起来组成了 quicklist。将多个 ziplist 使用双向指针串起来使用。"],["body","\n"],["headingLink","hash"],["heading","hash"],["body","\n"],["body","\n"],["body","hash 相当于 java 的 HashMap，value 是一个键值对。它是一个无序字典，最后一个元素移除后该结构会被删除。"],["body","\n"],["body","\n"],["body","hash 相当于 java 的 HashMap，value 是一个键值对。它是一个无序字典，最后一个元素移除后该结构会被删除。"],["body","\n"],["body","适用场景：使用 string 时保存一个用户信息需要将整个对象序列化存入，当我们只需要某个属性却需要把整个对象取出，这样会浪费网络流量和时间。使用 hash 可以将用户信息分段存储，可以只取出部分数据。"],["body","\n"],["headingLink","基本操作"],["heading","基本操作"],["body","\n\n"],["body","hset key field value"],["body","\n"],["body","hget key field"],["body","\n"],["body","hgetall key"],["body","\n"],["body","hlen key"],["body","\n"],["body","hmset key field value [field value ...]"],["body","\n"],["body","hmget key field [field ...]"],["body","\n\n"],["headingLink","example"],["heading","example"],["body","\n"],["body","127.0.0.1:6379> hset userList user1 zhangsan\n(integer) 1\n127.0.0.1:6379> hset userList user2 lisi\n(integer) 1\n127.0.0.1:6379> hset userList user3 wangwu\n(integer) 1\n127.0.0.1:6379> hgetall userList # 获取所有，key value 依次输出。\n1) \"user1\"\n2) \"zhangsan\"\n3) \"user2\"\n4) \"lisi\"\n5) \"user3\"\n6) \"wangwu\"\n127.0.0.1:6379> hlen userList\n(integer) 3\n127.0.0.1:6379> hget userList user2\n\"lisi\"\n127.0.0.1:6379> hset userList user2 lisa # 修改值返回0\n(integer) 0\n127.0.0.1:6379> hget userList user2\n\"lisa\"\n127.0.0.1:6379> hmset userList user4 Harry user5 Jessica # 多个设值\nOK\n127.0.0.1:6379> hgetall userList\n 1) \"user1\"\n 2) \"zhangsan\"\n 3) \"user2\"\n 4) \"lisa\"\n 5) \"user3\"\n 6) \"wangwu\"\n 7) \"user4\"\n 8) \"Harry\"\n 9) \"user5\"\n10) \"Jessica\"\n127.0.0.1:6379> hmget userList user4 user5 user6 # 多个取值\n1) \"Harry\"\n2) \"Jessica\"\n3) (nil)\n"],["body","\n"],["headingLink","计数-1"],["heading","计数"],["body","\n"],["body","hash 的 field 也可以进行计数，用法和 incrby 一样。"],["body","\n\n"],["body","hincrby key field increment"],["body","\n\n"],["body","127.0.0.1:6379> hset user-Harry age 21\n(integer) 1\n127.0.0.1:6379> hget user-Harry age\n\"21\"\n127.0.0.1:6379> hincrby user-Harry age 1 # 年龄 +1\n(integer) 22\n127.0.0.1:6379> hget user-Harry age\n\"22\"\n"],["body","\n"],["headingLink","set"],["heading","set"],["body","\n"],["body","set 相当于 java 的 HashSet，以键值对存储，无序，唯一。"],["body","\n\n"],["body","sadd key member [member ...]"],["body","\n"],["body","smembers key"],["body","\n"],["body","sismember key member"],["body","\n"],["body","scard key"],["body","\n"],["body","spop key [count]"],["body","\n\n"],["body","127.0.0.1:6379> sadd books python\n(integer) 1\n127.0.0.1:6379> sadd books python # 重复添加不会生效\n(integer) 0\n127.0.0.1:6379> sadd books java golang\n(integer) 2\n127.0.0.1:6379> smembers books # 查看所有成员\n1) \"python\"\n2) \"java\"\n3) \"golang\"\n127.0.0.1:6379> sismember books java # 是否是一个成员\n(integer) 1\n127.0.0.1:6379> sismember books rust\n(integer) 0\n127.0.0.1:6379> scard books # 相当于 count()\n(integer) 3\n127.0.0.1:6379> spop books # 弹出一个\n\"python\"\n127.0.0.1:6379> smembers books\n1) \"java\"\n2) \"golang\"\n"],["body","\n"],["headingLink","zset"],["heading","zset"],["body","\n"],["headingLink","介绍"],["heading","介绍"],["body","\n"],["body","有序列表，类似于 Sorted 和 HashMap 的结合体。"],["body","\n"],["body","一方面它是一个 set，保证元素的唯一性；另一方面它可以为每个 value 赋予一个 score 作为这个 value 的排序权重。"],["body","\n"],["body","其内部实现用的是一种叫做「跳跃链表」的数据结构。"],["body","\n"],["body","适用场景："],["body","\n\n"],["body","存储粉丝列表，value 是粉丝的用户Id，score 是关注时间。我们可以以此对粉丝列表按关注时间排序；"],["body","\n"],["body","存储学生成绩，value 是学生的ID，score 是他的成绩，以此对成绩分组进行排序。"],["body","\n\n"],["headingLink","常用命令"],["heading","常用命令"],["body","\n\n"],["body","zadd key score member [score member ...]：添加一个或多个成员"],["body","\n"],["body","zrange key start stop [WITHSCORES]：查看成员区间，WITHSCORES 表示显示成绩。"],["body","\n"],["body","zrevrange key start stop [WITHSCORES]：反向查看成员区间"],["body","\n"],["body","zcard key：成员个数"],["body","\n"],["body","zscore key member：查看成员排序权重"],["body","\n"],["body","zrank key member：查看指定成员的排名"],["body","\n"],["body","zrangebyscore key min max [WITHSCORES] [LIMIT offset count]：以成绩排名显示"],["body","\n"],["body","zrem key member [member ...]：移除指定成员"],["body","\n\n"],["headingLink","过期时间"],["heading","过期时间"],["body","\n"],["body","redis 的所有数据结构都可以设置过期时间（单位 s），时间到了，redis 会自动删除相应的对象。"],["body","\n\n"],["body","过期是以对象为单位的，即一个 key 是一个对象。比如一个 Hash 结构的过期是整个 key 的过期而不是某个子 key 的过期。"],["body","\n"],["body","如果一个字符串设置了过期时间，然后调用 set 方法设置了它，它的过期时间会消失。"],["body","\n\n"],["body","127.0.0.1:6379> set codehole yoyo\nOK\n127.0.0.1:6379> expire codehole 600\n(integer) 1\n127.0.0.1:6379> ttl codehole\n(integer) 596\n127.0.0.1:6379> set codehole yoyo\nOK\n127.0.0.1:6379> ttl codehole\n(integer) -1\n127.0.0.1:6379>\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","8.缓存中间件_Redis/redis集群.html"],["title","redis集群 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","概念"],["heading","概念"],["body","\n"],["headingLink","主从哨兵的局限性"],["heading","主从+哨兵的局限性"],["body","\n"],["body","之前介绍过Redis主从集群+哨兵的搭建，架构如下图所示"],["body","\n"],["body","\n"],["body","这种集群模式下水平扩容和垂直扩容都可以实现，并且可以实现高可用性和易用性"],["body","\n\n"],["body","水平扩容：比如增加一套主从集群，在predixy代理处配置hash寻址，让部分数据可以被新加入的主从集群存储，水平扩容的实现强烈依赖于predixy代理。"],["body","\n"],["body","垂直扩容：比如增加某个集群的内存，提升单机/单集群的处理能力"],["body","\n"],["body","高可用性：一套哨兵集群监控多套redis主从集群，高可用的实现依赖于哨兵"],["body","\n"],["body","易用性：指的是客户端的易用性，也是依赖于predixy代理来实现"],["body","\n\n"],["body","但是此架构下还是有缺陷的："],["body","\n\n"],["body","水平扩容问题：水平扩容依赖于predixy实现既是优点也是缺点，因为predixy的实现是根据crc16计算key的哈希值，然后通过modula也就是求模的办法将key分布到不同的集群中去，所以说当水平扩容的时候会涉及到大量缓存重建"],["body","\n"],["body","可用性问题：集群的高可用性是建立在哨兵集群之上的，假设哨兵集群全部宕机，那么整个集群的故障转移功能将会丧失，也不能动态发现新加入的集群，最终导致集群的可用性受到影响。"],["body","\n\n"],["body","那么redis cluster又是怎么解决上面两个问题的呢？"],["body","\n\n"],["body","针对水平扩容问题：redis使用hash槽算法，默认分配16384个hash槽位（2^14），然后将槽位均匀分配到不同的redis实例中去，找数据的时候通过crc16(key) % 16384找到对应的槽位，再看槽位在哪台实例上，最后去实例上取数据，使用槽位将具体的数据与redis实例解耦，当新增或者减少redis实例的时候自动将槽位均匀迁移到其他可用的redis实例上去。"],["body","\n"],["body","针对可用性问题：redis使用流行病协议，即Gossip/ˈɡɒsɪp/ Protocol ，每台redis主机即使客户端也是服务端，随时都在向整个集群扩散自己的可用性状态，实际上就是基于P2P的 去中心化网络拓扑架构，没有中心节点，所有节点通过Gossip协议通信，所有节点既是数据存储节点，也是控制节点。"],["body","\n\n"],["body","下面就针对水平扩容问题的hash寻址算法和针对可用性问题的流行病协议详细讨论"],["body","\n"],["headingLink","hash寻址算法"],["heading","hash寻址算法"],["body","\n"],["headingLink","普通hash"],["heading","普通hash"],["body","\n"],["body","普通hash也就是最简单的hash算法，即"],["body","\n"],["body","index = hash(key) % N\n"],["body","\n"],["body","新增加了一个节点，那么所有key取模的结果都变了，导致所有的数据都要重新迁移一遍，如果节点下线了呢？那么毫无疑问所有数据都要还原回去，就redis而言，这就叫大量缓存的重建，那么有没有新增/删除节点影响不那么大的hash算法呢？答案肯定是有，下面轮到一致性hash出场。"],["body","\n"],["headingLink","一致性hash"],["heading","一致性hash"],["body","\n"],["body","\n"],["body","一致哈希由MIT的Karger及其合作者提出，现在这一思想已经扩展到其它领域。在这篇1997年发表的学术论文中介绍了“一致哈希”如何应用于用户易变的分布式Web服务中。哈希表中的每一个代表分布式系统中一个节点，在系统添加或删除节点只需要移动K/n （方法K是总key的个数，n是节点个数）"],["body","\n"],["body","\n"],["body","一致性hash的特性"],["body","\n\n"],["body","平衡性：尽可能让数据尽可能分散到所有节点上，避免造成极其不均匀"],["body","\n"],["body","单调性：要求在新增或者减少节点的时候，原有的结果绝大部分不受影响，而新增的数据尽可能分配到新加的节点"],["body","\n"],["body","分散性：好的算法在不同终端，针对相同的数据的计算，得到的结果应该是一样的，一致性要很强"],["body","\n"],["body","负载：针对相同的节点，避免被不同终端映射不同的内容"],["body","\n"],["body","平滑性：对于增加节点或者减少节点，应该能够平滑过渡"],["body","\n\n"],["headingLink","hash环"],["heading","hash环"],["body","\n"],["body","普通hash算法导致大量数据迁移的根本原因是N的不确定性，有没有在N变化的时候影响范围更小的算法呢？有人提出了环的概念"],["body","\n"],["body","hash环通过构建环状的hash空间代替线性hash空间的方法解决了上面的问题，假设将0~2^32-1的hash空间分布到一个环上"],["body","\n\n"],["body","节点加入环：将节点通过hash(节点的信息如ip端口等) % 2^32-1取节点在环上位置"],["body","\n"],["body","数据读写：读写数据时同样取key的hash，即hash(key) % 2^32-1落到环上的某一位置，再顺时针找到离环最近的那个节点进行读写"],["body","\n\n"],["body","\n"],["body","新增与删除"],["body","\n"],["body","新增一个节点4，只会影响到节点2到节点4之间的数据，其他的数据不会被影响到，这也是一致性的体现"],["body","\n"],["body","删除一个节点也是同样的道理，假设删除节点4，也只是会影响到节点2到原节点4之间的数据"],["body","\n"],["body","缺点"],["body","\n"],["body","假设节点分布不均匀（hash算法并不能保证绝对的平衡性），那么大部分数据都会落在一个节点上，导致请求和数据倾斜，这样就不能很好的保证负载均衡。"],["body","\n"],["body","\n"],["body","那么解决办法就是增加虚拟节点（注意，此时环上全部都是虚拟节点），对每一个节点计算多个hash，尽量保证环上的节点是均匀的，如下图"],["body","\n"],["body","\n"],["headingLink","hash槽"],["heading","hash槽"],["body","\n"],["body","hash槽（hash slot）是redis中一致性hash的实现，很多文章将一致性hash环和hash槽分开来讲，其实hash槽也是一致性hash的一种实现。"],["body","\n"],["body","redis默认分配16384个hash槽位，然后将槽位均匀分配到不同的redis实例中去，找数据的时候通过CRC16算法计算后再取模找到对应的槽位（CRC16我们应该不陌生，这个winrar里面使用的CRC32是一样的，只是校验长度不一样而已），算法如下"],["body","\n"],["body","CRC16(key) % 16384\n"],["body","\n"],["body","\n"],["body","使用槽位将具体的数据与redis实例解耦，当新增或者减少redis实例的时候用redis cluster总线通过Ping/Pong报文进行广播，告知整个redis集群新节点上线/下线，并迁移槽位和更新集群中的槽位映射表，整个过程尽量保证hash槽的平均分"],["body","\n"],["body","那么是基于什么样的考虑，redis的作者没有用hash环呢？"],["body","\n"],["body","redis的作者认为他的CRC16(key) mod 16384的效果已经不错了，虽然没有一致性hash灵活，但实现很简单，节点增删时处理起来也很方便"],["body","\n"],["body","当然还有个原因是hash槽的分布更加均匀，如果有N个节点，那么每个节点都负载1/N，此处引用一句话总结"],["body","\n"],["body","那为什么hash槽是16384个呢？"],["body","\n"],["body","分布均匀"],["body","\n"],["body","实际上是因为CRC16会输出16bit的结果，可以看作是一个分布在0~2^16-1之间的数，redis的作者测试发现这个数对2^14求模的会将key在0-2^14-1之间分布得很均匀，2^14即16384"],["body","\n"],["body","节省空间"],["body","\n"],["body","还有个说法是为了节省存储空间，每个节点用一个Bitmap来存放其对应的槽，2k = 2*1024*8 = 16384，也就是说，每个节点用2k的内存空间，总共16384个比特位，就可以存储该结点对应了哪些槽。然后这2k的信息，通过Gossip协议，在节点之间传递"],["body","\n"],["headingLink","gossip协议流行病协议"],["heading","Gossip协议(流行病协议)"],["body","\n"],["body","redis cluster正是通过Gossip协议在节点之间同步数据的，所有节点都是对等的，既是数据存储节点，也是控制节点。redis cluster启动的时候会开两个端口，一个是常规的6379端口，另外一个端口一般是（6379+ 10000），这个就是所谓的Cluster总线，这个端口的作用就是就是利用Gossip协议进行节点之间的通信。"],["body","\n"],["body","这里顺便提一下反熵（Anti-Entropy），熵描述的是一个系统的混乱程度，大名鼎鼎的熵增定律指的是一个有序系统在无外力的作用下，会慢慢转化到无序的状态，所谓反熵就是需要借助外力来减少系统的混乱程度，redis通过Gossip协议传播节点之间的可用信息，使得整个系统有序可用，是反熵行为，假设redis集群奉行无为而治，那么整个集群会随着各种不确定性（比如内存满了、网络抖动等）变得越来越无序，可用性降低，符合熵增定律。"],["body","\n"],["headingLink","容错机制"],["heading","容错机制"],["body","\n"],["headingLink","主观下线"],["heading","主观下线"],["body","\n"],["body","集群中每个节点都会定期向其他节点发送ping消息，接受节点回复ping消息作为响应。如果在cluster-node-timeout时间内通信一直失败，则发送节点会认为接收节点存在故障，把接受节点标记为主观下线(pfail)状态。"],["body","\n"],["headingLink","客观下线"],["heading","客观下线"],["body","\n\n"],["body","当某个节点判断另一个节点主观下线后，相应的节点状态会跟随消息在集群内传播"],["body","\n"],["body","假设节点a标记节点b为主观下线，一段时间后节点a通过消息把节点b的状态发送到其他节点，当其他节点收到消息并解析出消息体中含有b的pfail状态，把节点b加入下线报告链表；"],["body","\n"],["body","当某一节点c收到节点b的pfail状态时，此时有超过一半的槽主节点都标记了节点b为pfail状态时，则标记故障节点b为客观下线；"],["body","\n"],["body","向集群广播一条pfail消息，通知集群内的所有节点标记故障节点b为客观下线状态并立刻生效，同时通知故障节点b的从节点触发故障转移流程"],["body","\n\n"],["headingLink","故障恢复"],["heading","故障恢复"],["body","\n"],["body","资格检查"],["body","\n"],["body","若从节点与主节点断线时间超过一定时间，则不具备资格"],["body","\n"],["body","准备选举时间"],["body","\n"],["body","当从节点符合故障转移资格后，要等待一段选举时间后才开始选举"],["body","\n"],["body","在故障节点的所有从节点中，复制偏移量最大的那个从节点最先开始（与主节点的数据最一致）进行选举，然后是次大的节点开始选举.....剩下其余的从节点等待到它们的选举时间到达后再进行选举"],["body","\n"],["body","发起选举"],["body","\n"],["body","选举投票"],["body","\n"],["body","只有持有槽的主节点才具有一张唯一的选票，从从节点收集到N/2 + 1个持有槽的主节点投票时，从节点可以执行替换主节点操作"],["body","\n"],["body","替换主节点"],["body","\n"],["body","当从节点收集到足够的选票之后，触发替换主节点操作"],["body","\n\n"],["body","当前从节点取消复制变为主节点"],["body","\n"],["body","撤销故障主节点负责的槽，并把这些槽委派给自己"],["body","\n"],["body","向集群广播自己的pong消息，通知集群内所有的节点当前从节点变为主节点并接管了故障主节点的槽信息"],["body","\n\n"],["headingLink","redis-cluster集群"],["heading","Redis Cluster集群"],["body","\n"],["headingLink","概念-1"],["heading","概念"],["body","\n\n"],["body","\n"],["body","Redis 集群是一个提供在多个Redis间节点间共享数据的程序集。"],["body","\n"],["body","\n"],["body","\n"],["body","Redis集群并不支持处理多个keys的命令,因为这需要在不同的节点间移动数据,从而达不到像Redis那样的性能,在高负载的情况下可能会导致不可预料的错误."],["body","\n"],["body","\n"],["body","\n"],["body","Redis 集群通过分区来提供一定程度的可用性,在实际环境中当某个节点宕机或者不可达的情况下继续处理命令. Redis 集群的优势:"],["body","\n\n"],["body","自动分割数据到不同的节点上。"],["body","\n"],["body","整个集群的部分节点失败或者不可达的情况下能够继续处理命令。"],["body","\n\n"],["body","\n\n"],["headingLink","redis-集群的数据分片"],["heading","Redis 集群的数据分片"],["body","\n"],["body","Redis 集群没有使用一致性hash, 而是引入了 哈希槽的概念."],["body","\n"],["body","Redis 集群有16384个哈希槽,每个key通过CRC16校验后对16384取模来决定放置哪个槽.集群的每个节点负责一部分hash槽,举个例子,比如当前集群有3个节点,那么:"],["body","\n\n"],["body","节点 A 包含 0 到 5500号哈希槽."],["body","\n"],["body","节点 B 包含5501 到 11000 号哈希槽."],["body","\n"],["body","节点 C 包含11001 到 16384号哈希槽."],["body","\n\n"],["body","这种结构很容易添加或者删除节点. 比如如果我想新添加个节点D, 我需要从节点 A, B, C中得部分槽到D上. 如果我想移除节点A,需要将A中的槽移到B和C节点上,然后将没有任何槽的A节点从集群中移除即可. 由于从一个节点将哈希槽移动到另一个节点并不会停止服务,所以无论添加删除或者改变某个节点的哈希槽的数量都不会造成集群不可用的状态."],["body","\n"],["headingLink","redis-集群的主从复制模型"],["heading","Redis 集群的主从复制模型"],["body","\n"],["body","为了使在部分节点失败或者大部分节点无法通信的情况下集群仍然可用，所以集群使用了主从复制模型,每个节点都会有N-1个复制品."],["body","\n"],["body","在我们例子中具有A，B，C三个节点的集群,在没有复制模型的情况下,如果节点B失败了，那么整个集群就会以为缺少5501-11000这个范围的槽而不可用."],["body","\n"],["body","然而如果在集群创建的时候（或者过一段时间）我们为每个节点添加一个从节点A1，B1，C1,那么整个集群便有三个master节点和三个slave节点组成，这样在节点B失败后，集群便会选举B1为新的主节点继续服务，整个集群便不会因为槽找不到而不可用了"],["body","\n"],["body","不过当B和B1 都失败后，集群是不可用的."],["body","\n"],["headingLink","redis-一致性保证"],["heading","Redis 一致性保证"],["body","\n"],["headingLink","不保证强一致性"],["heading","不保证强一致性"],["body","\n"],["body","异步复制"],["body","\n"],["body","Redis 并不能保证数据的强一致性. 这意味这在实际中集群在特定的条件下可能会丢失写操作."],["body","\n"],["body","第一个原因是因为集群是用了异步复制. 写操作过程:"],["body","\n\n"],["body","客户端向主节点B写入一条命令."],["body","\n"],["body","主节点B向客户端回复命令状态."],["body","\n"],["body","主节点将写操作复制给他得从节点 B1, B2 和 B3."],["body","\n\n"],["body","性能与一致性的权衡"],["body","\n"],["body","主节点对命令的复制工作发生在返回命令回复之后， 因为如果每次处理命令请求都需要等待复制操作完成的话， 那么主节点处理命令请求的速度将极大地降低 —— 我们必须在性能和一致性之间做出权衡。 注意：Redis 集群可能会在将来提供同步写的方法。 "],["body","\n"],["body","网络分裂导致非一致性"],["body","\n"],["body","Redis 集群另外一种可能会丢失命令的情况是集群出现了网络分区， 并且一个客户端与至少包括一个主节点在内的少数实例被孤立。"],["body","\n"],["body","举个例子 假设集群包含 A 、 B 、 C 、 A1 、 B1 、 C1 六个节点， 其中 A 、B 、C 为主节点， A1 、B1 、C1 为A，B，C的从节点， 还有一个客户端 Z1 假设集群中发生网络分区，那么集群可能会分为两方，大部分的一方包含节点 A 、C 、A1 、B1 和 C1 ，小部分的一方则包含节点 B 和客户端 Z1 ."],["body","\n"],["body","Z1仍然能够向主节点B中写入, 如果网络分区发生时间较短,那么集群将会继续正常运作,如果分区的时间足够让大部分的一方将B1选举为新的master，那么Z1写入B中得数据便丢失了."],["body","\n"],["body","注意， 在网络分裂出现期间， 客户端 Z1 可以向主节点 B 发送写命令的最大时间是有限制的， 这一时间限制称为节点超时时间（node timeout）， 是 Redis 集群的一个重要的配置选项："],["body","\n"],["headingLink","集群原理"],["heading","集群原理"],["body","\n"],["headingLink","键分布模型"],["heading","键分布模型"],["body","\n"],["body","键空间被分割为 16384 槽（slot），事实上集群的最大节点数量是 16384 个。（然而建议最大节点数量设置在1000这个数量级上）"],["body","\n"],["body","所有的主节点都负责 16384 个哈希槽中的一部分。当集群处于稳定状态时，集群中没有在执行重配置（reconfiguration）操作，每个哈希槽都只由一个节点进行处理（不过主节点可以有一个或多个从节点，可以在网络断线或节点失效时替换掉主节点）。"],["body","\n"],["body","以下是用来把键映射到哈希槽的算法（下一段哈希标签例外就是按照这个规则）："],["body","\n"],["headingLink","键哈希标签keys-hash-tags"],["heading","键哈希标签（Keys hash tags）"],["body","\n"],["body","设置hash标签确保同一个slot"],["body","\n"],["body","哈希标签是确保两个键都在同一个哈希槽里的一种方式"],["body","\n"],["body","为了实现哈希标签，哈希槽是用另一种不同的方式计算的。基本来说，如果一个键包含一个 “{…}” 这样的模式，只有 { 和 } 之间的字符串会被用来做哈希以获取哈希槽。但是由于可能出现多个 { 或 }，计算的算法如下："],["body","\n\n"],["body","如果键包含一个 { 字符。"],["body","\n"],["body","那么在 { 的右边就会有一个 }。"],["body","\n"],["body","在 { 和 } 之间会有一个或多个字符，第一个 } 一定是出现在第一个 { 之后。"],["body","\n\n"],["body","然后不是直接计算键的哈希，只有在第一个 { 和它右边第一个 } 之间的内容会被用来计算哈希值。"],["body","\n"],["body","例子："],["body","\n\n"],["body","比如这两个键 {user1000}.following 和 {user1000}.followers 会被哈希到同一个哈希槽里，因为只有 user1000 这个子串会被用来计算哈希值。"],["body","\n"],["body","对于 foo{}{bar} 这个键，整个键都会被用来计算哈希值，因为第一个出现的 { 和它右边第一个出现的 } 之间没有任何字符。"],["body","\n\n"],["body","def HASH_SLOT(key)\n    s = key.index \"{\"\n    if s\n        e = key.index \"}\",s+1\n        if e && e != s+1\n            key = key[s+1..e-1]\n        end\n    end\n    crc16(key) % 16384\nend\n"],["body","\n"],["headingLink","集群节点属性"],["heading","集群节点属性"],["body","\n"],["headingLink","固定唯一id"],["heading","固定唯一ID"],["body","\n"],["body","在集群中，每个节点都有一个唯一的名字。节点名字是一个十六进制表示的160 bit 随机数，这个随机数是节点第一次启动时获得的（通常是用 /dev/urandom）。 节点会把它的ID保存在配置文件里，以后永远使用这个ID，只要这个节点配置文件没有被系统管理员删除掉。"],["body","\n"],["body","节点ID是用于在整个集群中标识每个节点。一个给定的节点可以在不改变节点ID的情况下改变 IP 和地址。集群能检测到 IP 或端口的变化，然后使用在集群连接（cluster bus）上的 gossip 协议来发布广播消息，通知配置变更。"],["body","\n"],["body","每个节点都有其他相关信息是所有节点都知道的："],["body","\n\n"],["body","节点的 IP 地址和 TCP 端口号。"],["body","\n"],["body","各种标识。"],["body","\n"],["body","节点使用的哈希槽。"],["body","\n"],["body","最近一次用集群连接发送 ping 包的时间。"],["body","\n"],["body","最近一次在回复中收到一个 pong 包的时间。"],["body","\n"],["body","最近一次标识节点失效的时间。"],["body","\n"],["body","该节点的从节点个数。"],["body","\n"],["body","如果该节点是从节点，会有主节点ID信息。（如果它是个主节点则该信息置为0000000…）"],["body","\n\n"],["body","使用 CLUSTER NODES 命令可以获得以上的一些信息，这个命令可以发送到集群中的所有节点，无论主节点还是从节点"],["body","\n"],["body","redis-cli cluster nodes\n\nd1861060fe6a534d42d8a19aeb36600e18785e04 127.0.0.1:6379 myself - 0 1318428930 1 connected 0-1364\n3886e65cc906bfd9b1f7e7bde468726a052d1dae 127.0.0.1:6380 master - 1318428930 1318428931 2 connected 1365-2729\nd289c575dcbc4bdd2931585fd4339089e461a27d 127.0.0.1:6381 master - 1318428931 1318428931 3 connected 2730-4095\n各个域依次表示的是：节点ID，IP地址：端口号，标识，上一次发送 ping 包的时间，上一次收到 pong 包的时间，连接状态，节点使用的哈希槽。\n"],["body","\n"],["headingLink","集群拓扑结构"],["heading","集群拓扑结构"],["body","\n"],["body","Redis 集群是一个网状结构，每个节点都通过 TCP 连接跟其他每个节点连接。"],["body","\n"],["body","在一个有 N 个节点的集群中，每个节点都有 N-1 个流出的 TCP 连接，和 N-1 个流入的连接。 这些 TCP 连接会永久保持，并不是按需创建的。"],["body","\n"],["headingLink","节点握手"],["heading","节点握手"],["body","\n"],["body","节点总是在集群连接端口接受连接，甚至会回复接收到的 ping 包，即使发送 ping 包的节点是不可信的。 然而如果某个节点不被认为是在集群中，那么所有它发出的数据包都会被丢弃掉。"],["body","\n"],["body","只有在两种方式下，一个节点才会认为另一个节点是集群中的一部分："],["body","\n\n"],["body","\n"],["body","当一个节点使用 MEET 消息介绍自己。一个 meet 消息跟一个 PING 消息完全一样，但它会强制让接收者接受发送者为集群中的一部分。 只有在系统管理员使用以下命令要求的时候，节点才会发送 MEET 消息给其他节点："],["body","\n"],["body","CLUSTER MEET ip port\n"],["body","\n"],["body","\n\n\n"],["body","\n"],["body","一个已被信任的节点能通过传播gossip消息让另一个节点被注册为集群中的一部分"],["body","\n"],["body","如果 A 知道 B，B 知道 C，那么 B 会向 A 发送 C 的gossip消息。A 收到后就会把 C 当作是网络中的一部分，并且尝试连接 C"],["body","\n"],["body","从根本上来说，这表示集群能自动发现其他节点，但前提是有一个由系统管理员强制创建的信任关系"],["body","\n"],["body","\n\n"],["headingLink","moved-重定向"],["heading","MOVED 重定向"],["body","\n"],["body","一个 Redis 客户端可以自由地向集群中的任意节点（包括从节点）发送查询。接收的节点会分析查询，如果这个命令是集群可以执行的（就是查询中只涉及一个键），那么节点会找这个键所属的哈希槽对应的节点。"],["body","\n"],["body","如果刚好这个节点就是对应这个哈希槽，那么这个查询就直接被节点处理掉。否则这个节点会查看它内部的 哈希槽 -> 节点ID 映射，然后给客户端返回一个 MOVED 错误。"],["body","\n"],["body","GET x\n-MOVED 3999 127.0.0.1:6381\n"],["body","\n"],["body","当集群是稳定的时候，所有客户端最终都会得到一份哈希槽 -> 节点的映射表，这样能使得集群效率非常高：客户端直接定位目标节点，不用重定向、或代理或发生其他单点故障（single point of failure entities）。"],["body","\n"],["body","一个客户端也应该能处理本文后面将提到的 -ASK 重定向错误。"],["body","\n"],["headingLink","集群在线重配置live-reconfiguration"],["heading","集群在线重配置（live reconfiguration）"],["body","\n"],["body","Redis 集群支持在集群运行过程中添加或移除节点。实际上，添加或移除节点都被抽象为同一个操作，那就是把哈希槽从一个节点移到另一个节点。"],["body","\n\n"],["body","向集群添加一个新节点，就是把一个空节点加入到集群中并把某些哈希槽从已存在的节点移到新节点上。"],["body","\n"],["body","从集群中移除一个节点，就是把该节点上的哈希槽移到其他已存在的节点上。"],["body","\n"],["body","所以实现这个的核心是能把哈希槽移来移去。从实际角度看，哈希槽就只是一堆键，所以 Redis 集群在重组碎片（reshard）时做的就是把键从一个节点移到另一个节点。"],["body","\n\n"],["body","CLUSTER 的子命令，这些命令是用来操作 Redis 集群节点上的哈希槽转换表（slots translation table）。"],["body","\n\n"],["body","\n"],["body","CLUSTER ADDSLOTS slot1 [slot2] … [slotN]"],["body","\n"],["body","\n"],["body","\n"],["body","CLUSTER DELSLOTS slot1 [slot2] … [slotN]"],["body","\n"],["body","\n"],["body","\n"],["body","CLUSTER SETSLOT slot NODE node"],["body","\n"],["body","\n"],["body","\n"],["body","CLUSTER SETSLOT slot MIGRATING node"],["body","\n"],["body","\n"],["body","\n"],["body","CLUSTER SETSLOT slot IMPORTING node"],["body","\n"],["body","\n"],["body","\n"],["body","头两个命令，ADDSLOTS 和 DELSLOTS，就是简单地用来给一个 Redis 节点指派（assign）或移除哈希槽。 在哈希槽被指派后，节点会将这个消息通过 gossip 协议向整个集群传播。ADDSLOTS 命令通常是用于在一个集群刚建立的时候快速给所有节点指派哈希槽。"],["body","\n"],["body","\n"],["body","\n"],["body","当 SETSLOT 子命令使用 NODE 形式的时候，用来给指定 ID 的节点指派哈希槽。 除此之外哈希槽能通过两个特殊的状态来设定，MIGRATING 和 IMPORTING："],["body","\n\n"],["body","当一个槽被设置为 MIGRATING，原来持有该哈希槽的节点仍会接受所有跟这个哈希槽有关的请求，但只有当查询的键还存在原节点时，原节点会处理该请求，否则这个查询会通过一个 -ASK 重定向（-ASK redirection）转发到迁移的目标节点。"],["body","\n"],["body","当一个槽被设置为 IMPORTING，只有在接受到 ASKING 命令之后节点才会接受所有查询这个哈希槽的请求。如果客户端一直没有发送 ASKING 命令，那么查询都会通过 -MOVED 重定向错误转发到真正处理这个哈希槽的节点那里。"],["body","\n\n"],["body","\n\n"],["body","假设我们有两个 Redis 节点，称为 A 和 B。我们想要把哈希槽 8 从 节点A 移到 节点B，所以我们发送了这样的命令："],["body","\n\n"],["body","我们向 节点B 发送：CLUSTER SETSLOT 8 IMPORTING A"],["body","\n"],["body","我们向 节点A 发送：CLUSTER SETSLOT 8 MIGRATING B"],["body","\n\n"],["body","其他所有节点在每次被询问到的一个键是属于哈希槽 8 的时候，都会把客户端引向节点”A”。具体如下："],["body","\n\n"],["body","所有关于已存在的键的查询都由节点”A”处理。"],["body","\n"],["body","所有关于不存在于节点 A 的键都由节点”B”处理。"],["body","\n\n"],["headingLink","ask-重定向"],["heading","ASK 重定向"],["body","\n"],["body","为什么我们不能单纯地使用 MOVED 重定向呢？因为当我们使用 MOVED 的时候，意味着我们认为哈希槽永久地被另一个不同的节点处理，并且希望接下来的所有查询都尝试发到这个指定的节点上去。"],["body","\n"],["body","而 ASK 意味着我们只要下一个查询发送到指定节点上去。"],["body","\n"],["body","这个命令是必要的，因为下一个关于哈希槽 8 的查询需要的键或许还在节点 A 中，所以我们希望客户端尝试在节点 A 中查找，如果需要的话也在节点 B 中查找。 由于这是发生在 16384 个槽的其中一个槽，所以对于集群的性能影响是在可接受的范围。"],["body","\n"],["body","然而我们需要强制客户端的行为，以确保客户端会在尝试 A 中查找后去尝试在 B 中查找。如果客户端在发送查询前发送了 ASKING 命令，那么节点 B 只会接受被设为 IMPORTING 的槽的查询。 本质上来说，ASKING 命令在客户端设置了一个一次性标识（one-time flag），强制一个节点可以执行一次关于带有 IMPORTING 状态的槽的查询。"],["body","\n"],["body","所以从客户端看来，ASK 重定向的完整语义如下："],["body","\n\n"],["body","如果接受到 ASK 重定向，那么把查询的对象调整为指定的节点。"],["body","\n"],["body","先发送 ASKING 命令，再开始发送查询。"],["body","\n"],["body","现在不要更新本地客户端的映射表把哈希槽 8 映射到节点 B。"],["body","\n\n"],["body","一旦完成了哈希槽 8 的转移，节点 A 会发送一个 MOVED 消息，客户端也许会永久地把哈希槽 8 映射到新的 ip:端口号 上。"],["body","\n"],["headingLink","失效检测failure-detection"],["heading","失效检测（Failure detection）"],["body","\n"],["body","大部分失效"],["body","\n"],["body","Redis 集群失效检测是用来识别出大多数节点何时无法访问某一个主节点或从节点。当这个事件发生时，就提升一个从节点来做主节点；若如果无法提升从节点来做主节点的话，那么整个集群就置为错误状态并停止接收客户端的查询。"],["body","\n"],["body","每个节点都有一份跟其他已知节点相关的标识列表。其中有两个标识是用于失效检测，分别是 PFAIL 和 FAIL。*"],["body","\n\n"],["body","PFAIL 表示可能失效（Possible failure），这是一个非公认的（non acknowledged）失效类型。"],["body","\n"],["body","FAIL 表示一个节点已经失效，而且这个情况已经被大多数主节点在某段固定时间内确认过的了。"],["body","\n\n"],["body","PFAIL 标识:"],["body","\n"],["body","当一个节点在超过 NODE_TIMEOUT 时间后仍无法访问某个节点，那么它会用 PFAIL 来标识这个不可达的节点。无论节点类型是什么，主节点和从节点都能标识其他的节点为 PFAIL。"],["body","\n"],["body","Redis 集群节点的不可达性（non reachability）是指，发送给某个节点的一个活跃的 ping 包（active ping）(一个我们发送后要等待其回复的 ping 包)已经等待了超过 NODE_TIMEOUT 时间，那么我们认为这个节点具有不可达性。为了让这个机制能正常工作，NODE_TIMEOUT 必须比网络往返时间（network round trip time）大。"],["body","\n"],["body","节点为了在普通操作中增加可达性，当在经过一半 NODE_TIMEOUT 时间还没收到目标节点对于 ping 包的回复的时候，就会马上尝试重连接该节点。这个机制能保证连接都保持有效，所以节点间的失效连接通常都不会导致错误的失效报告。"],["body","\n"],["body","FAIL 标识:"],["body","\n"],["body","单独一个 PFAIL 标识只是每个节点的一些关于其他节点的本地信息，它不是为了起作用而使用的，也不足够触发从节点的提升。要让一个节点真正被认为失效了，那需要让 PFAIL 状态上升为 FAIL 状态。 "],["body","\n"],["body","每个节点向其他每个节点发送的 gossip 消息中有包含一些随机的已知节点的状态。最终每个节点都能收到一份其他每个节点的节点标识。使用这种方法，每个节点都有一套机制去标记他们检查到的关于其他节点的失效状态。"],["body","\n"],["body","当下面的条件满足的时候，会使用这个机制来让 PFAIL 状态升级为 FAIL 状态："],["body","\n\n"],["body","某个节点，我们称为节点 A，标记另一个节点 B 为 PFAIL。"],["body","\n"],["body","节点 A 通过 gossip 字段收集到集群中大部分主节点标识的节点 B 的状态信息。"],["body","\n"],["body","大部分主节点标记节点 B 为 PFAIL 状态，或者在 NODE_TIMEOUT * FAIL_REPORT_VALIDITY_MULT 这个时间内是处于 PFAIL 状态。"],["body","\n\n"],["body","如果以上所有条件都满足了，那么节点 A 会："],["body","\n\n"],["body","标记节点 B 为 FAIL。"],["body","\n"],["body","向所有可达节点发送一个 FAIL 消息。"],["body","\n\n"],["body","FAIL 消息会强制每个接收到这消息的节点把节点 B 标记为 FAIL 状态。"],["body","\n"],["body","注意，FAIL 标识基本都是单向的，也就是说，一个节点能从 PFAIL 状态升级到 FAIL 状态，但要清除 FAIL 标识只有以下两种可能方法："],["body","\n\n"],["body","节点已经恢复可达的，并且它是一个从节点。在这种情况下，FAIL 标识可以清除掉，因为从节点并没有被故障转移。"],["body","\n"],["body","节点已经恢复可达的，而且它是一个主节点，但经过了很长时间（N * NODE_TIMEOUT）后也没有检测到任何从节点被提升了。"],["body","\n\n"],["body","PFAIL -> FAIL 的转变使用一种弱协议（agreement）："],["body","\n\n"],["body","节点是在一段时间内收集其他节点的信息，所以即使大多数主节点要去”同意”标记某节点为 FAIL，实际上这只是表明说我们在不同时间里从不同节点收集了信息，得出当前的状态不一定是稳定的结论"],["body","\n"],["body","当每个节点检测到 FAIL 节点的时候会强迫集群里的其他节点把各自对该节点的记录更新为 FAIL，但没有一种方式能保证这个消息能到达所有节点。比如有个节点可能检测到了 FAIL 的节点，但是因为分区，这个节点无法到达其他任何一个节点。"],["body","\n\n"],["body","然而 Redis 集群的失效检测有一个要求：最终所有节点都应该同意给定节点的状态是 FAIL，哪怕它处于分区。有两种情况是来源于脑裂情况（？），或者是小部分节点相信该节点处于 FAIL 状态，或者是相信节点不处于 FAIL 状态。在这两种情况中，最后集群都会认为给定的节点只有一个状态："],["body","\n"],["body","**第 1 种情况: **如果大多数节点都标记了某个节点为 FAIL，由于链条反应，这个主节点最终会被标记为 FAIL。"],["body","\n"],["body","第 2 种情况: 当只有小部分的主节点标记某个节点为 FAIL 的时候，从节点的提升并不会发生（它是使用一个更正式的算法来保证每个节点最终都会知道节点的提升。），并且每个节点都会根据上面的清除规则（在经过了一段时间 > N * NODE_TIMEOUT 后仍没有从节点提升操作）来清除 FAIL 状态。"],["body","\n"],["body","本质上来说，FAIL 标识只是用来触发从节点提升（slave promotion）算法的安全部分"],["body","\n"],["body","理论上一个从节点会在它的主节点不可达的时候独立起作用并且启动从节点提升程序，然后等待主节点来拒绝认可该提升（如果主节点对大部分节点恢复连接）。PFAIL -> FAIL 的状态变化、弱协议、强制在集群的可达部分用最短的时间传播状态变更的 FAIL 消息，这些东西增加的复杂性有实际的好处。由于这种机制，如果集群处于错误状态的时候，所有节点都会在同一时间停止接收写入操作，这从使用 Redis 集群的应用的角度来看是个很好的特性。还有非必要的选举，是从节点在无法访问主节点的时候发起的，若该主节点能被其他大多数主节点访问的话，这个选举会被拒绝掉。"],["body","\n"],["headingLink","集群阶段cluster-epoch"],["heading","集群阶段（Cluster epoch）"],["body","\n"],["body","Redis 集群使用一个类似于木筏算法（Raft algorithm）”术语”的概念。在 Redis 集群中这个术语叫做 阶段（epoch），它是用来记录事件的版本号，所以当有多个节点提供了冲突的信息的时候，另外的节点就可以通过这个状态来了解哪个是最新的。 currentEpoch 是一个 64bit 的 unsigned 数。"],["body","\n"],["body","Redis 集群中的每个节点，包括主节点和从节点，都在创建的时候设置了 currentEpoch 为0。"],["body","\n"],["body","当节点接收到来自其他节点的 ping 包或 pong 包的时候，如果发送者的 epoch（集群连接消息头部的一部分）大于该节点的 epoch，那么更新发送者的 epoch 为 currentEpoch。"],["body","\n"],["body","由于这个语义，最终所有节点都会支持集群中较大的 epoch。"],["body","\n"],["body","这个信息在此处是用于，当一个节点的状态发生改变的时候为了执行一些动作寻求其他节点的同意（agreement）。"],["body","\n"],["body","目前这个只发生在从节点的提升过程，这个将在下一节中详述。本质上说，epoch 是一个集群里的逻辑时钟，并决定一个给定的消息赢了另一个带着更小 epoch 的消息。"],["body","\n"],["headingLink","配置阶段configuration-epoch"],["heading","配置阶段（Configuration epoch）"],["body","\n"],["body","每一个主节点总是通过发送 ping 包和 pong 包向别人宣传它的 configEpoch 和一份表示它负责的哈希槽的位图。"],["body","\n"],["body","当一个新节点被创建的时候，主节点中的 configEpoch 设为零。"],["body","\n"],["body","从节点由于故障转移事件被提升为主节点时，为了取代它那失效的主节点，会把 configEpoch 设置为它赢得选举的时候的 configEpoch 值。"],["body","\n"],["body","configEpoch 用于在不同节点提出不同的配置信息的时候（这种情况或许会在分区之后发生）解决冲突"],["body","\n"],["body","从节点也会在 ping 包和 pong 包中向别人宣传它的 configEpoch 域，不过从节点的这个域表示的是上一次跟它的主节点交换数据的时候主节点的 configEpoch 值。这能让其他个体检测出从节点的配置信息是不是需要更新了（主节点不会给一个配置信息过时的从节点投票）。"],["body","\n"],["body","每次由于一些已知节点的值比自己的值大而更新 configEpoch 值，它都会永久性地存储在 nodes.conf 文件中。"],["body","\n"],["body","当一个节点重启，它的 configEpoch 值被设为所有已知节点中最大的那个 configEpoch 值。"],["body","\n"],["headingLink","丛节点的选举和提升"],["heading","丛节点的选举和提升"],["body","\n"],["body","从节点的选举和提升都是由从节点处理的，主节点会投票要提升哪个从节点。一个从节点的选举是在主节点被至少一个具有成为主节点必备条件的从节点标记为 FAIL 的状态的时候发生的。"],["body","\n"],["body","当以下条件满足时，一个从节点可以发起选举："],["body","\n\n"],["body","该从节点的主节点处于 FAIL 状态。"],["body","\n"],["body","这个主节点负责的哈希槽数目不为零。"],["body","\n"],["body","从节点和主节点之间的重复连接（replication link）断线不超过一段给定的时间，这是为了确保从节点的数据是可靠的。"],["body","\n"],["body","一个从节点想要被推选出来，那么第一步应该是提高它的 currentEpoch 计数，并且向主节点们请求投票。"],["body","\n\n"],["body","从节点通过广播一个 FAILOVER_AUTH_REQUEST 数据包给集群里的每个主节点来请求选票。然后等待回复（最多等 NODE_TIMEOUT 这么长时间）。一旦一个主节点给这个从节点投票，会回复一个 FAILOVER_AUTH_ACK，并且在 NODE_TIMEOUT * 2 这段时间内不能再给同个主节点的其他从节点投票。在这段时间内它完全不能回复其他授权请求。"],["body","\n"],["body","从节点会忽视所有带有的时期（epoch）参数比 currentEpoch 小的回应（ACKs），这样能避免把之前的投票的算为当前的合理投票。"],["body","\n"],["body","一旦某个从节点收到了大多数主节点的回应，那么它就赢得了选举。否则，如果无法在 NODE_TIMEOUT 时间内访问到大多数主节点，那么当前选举会被中断并在 NODE_TIMEOUT * 4 这段时间后由另一个从节点尝试发起选举。"],["body","\n"],["body","从节点并不是在主节点一进入 FAIL 状态就马上尝试发起选举，而是有一点点延迟，这段延迟是这么计算的："],["body","\n"],["body","DELAY = 500 milliseconds + random delay between 0 and 500 milliseconds +\n        SLAVE_RANK * 1000 milliseconds.\n"],["body","\n"],["body","固定延时（fixed delay）确保我们会等到 FAIL 状态在集群内广播后，否则若从节点尝试发起选举，主节点们仍然不知道那个主节点已经 FAIL，就会拒绝投票。"],["body","\n"],["body","一旦有从节点赢得选举，它就会开始用 ping 和 pong 数据包向其他节点宣布自己已经是主节点，并提供它负责的哈希槽，设置 configEpoch 为 currentEpoch（选举开始时生成的）。"],["body","\n"],["body","为了加速其他节点的重新配置，该节点会广播一个 pong 包 给集群里的所有节点（那些现在访问不到的节点最终也会收到一个 ping 包或 pong 包，并且进行重新配置）。"],["body","\n"],["body","其他节点会检测到有一个新的主节点（带着更大的configEpoch）在负责处理之前一个旧的主节点负责的哈希槽，然后就升级自己的配置信息。 旧主节点的从节点，或者是经过故障转移后重新加入集群的该旧主节点，不仅会升级配置信息，还会配置新主节点的备份。"],["body","\n"],["headingLink","主节点回复从节点的投票请求"],["heading","主节点回复从节点的投票请求"],["body","\n"],["body","主节点接收到来自于从节点、要求以 FAILOVER_AUTH_REQUEST 请求的形式投票的请求。 要授予一个投票，必须要满足以下条件："],["body","\n\n"],["body","在一个给定的时段（epoch）里，一个主节点只能投一次票，并且拒绝给以前时段投票：每个主节点都有一个 lastVoteEpoch 域，一旦认证请求数据包（auth request packet）里的 currentEpoch 小于 lastVoteEpoch，那么主节点就会拒绝再次投票。当一个主节点积极响应一个投票请求，那么 lastVoteEpoch 会相应地进行更新。"],["body","\n"],["body","一个主节点投票给某个从节点当且仅当该从节点的主节点被标记为 FAIL。"],["body","\n"],["body","如果认证请求里的 currentEpoch 小于主节点里的 currentEpoch 的话，那么该请求会被忽视掉。因此，主节点的回应总是带着和认证请求一致的 currentEpoch。如果同一个从节点在增加 currentEpoch 后再次请求投票，那么保证一个来自于主节点的、旧的延迟回复不会被新一轮选举接受。"],["body","\n\n"],["body","主节点的 currentEpoch 是 5， lastVoteEpoch 是 1（在几次失败的选举后这也许会发生的）"],["body","\n\n"],["body","从节点的 currentEpoch 是 3。"],["body","\n"],["body","从节点尝试用 epoch 值为 4（3+1）来赢得选票，主节点回复 ok，里面的 currentEpoch 是 5，可是这个回复延迟了。"],["body","\n"],["body","从节点尝试用 epoch 值为 5（4+1）来再次赢得选票，收到的是带着 currentEpoch 值为 5 的延迟回复，这个回复会被当作有效的来接收。"],["body","\n"],["body","\n\n"],["body","主节点若已经为某个失效主节点的一个从节点投票后，在经过 NODE_TIMEOUT * 2 时间之前不会为同个失效主节点的另一个从节点投票。这并不是严格要求的，因为两个从节点用同个 epoch 来赢得选举的可能性很低，不过在实际中，系统确保正常情况当一个从节点被选举上，那么它有足够的时间来通知其他从节点，以避免另一个从节点发起另一个新的选举。"],["body","\n\n"],["body","\n"],["body","\n\n"],["body","主节点不会用任何方式来尝试选出最好的从节点，只要从节点的主节点处于 FAIL 状态并且投票主节点在这一轮中还没投票，主节点就能进行积极投票。"],["body","\n\n"],["body","\n"],["body","\n\n"],["body","若一个主节点拒绝为给定从节点投票，它不会给任何负面的回应，只是单纯忽略掉这个投票请求。"],["body","\n\n"],["body","\n"],["body","\n\n"],["body","主节点不会授予投票给那些 configEpoch 值比主节点哈希槽表里的 configEpoch 更小的从节点。记住，从节点发送了它的主节点的 configEpoch 值，还有它的主节点负责的哈希槽对应的位图。本质上来说，这意味着，请求投票的从节点必须拥有它想要进行故障转移的哈希槽的配置信息，而且信息应该比它请求投票的主节点的配置信息更新或者一致。"],["body","\n\n"],["body","\n\n"],["headingLink","从节点选举的竞争情况"],["heading","从节点选举的竞争情况"],["body","\n"],["body","这一节解释如何使用 epoch 概念来使得从节点提升过程对分区操作更有抵抗力。"],["body","\n\n"],["body","主节点不是无限期地可达。它拥有三个从节点 A，B，C。"],["body","\n"],["body","从节点 A 赢得了选举并且被推选为主节点。"],["body","\n"],["body","一个分区操作使得集群中的大多数节点无法访问节点 A。"],["body","\n"],["body","节点 B 赢得了选举并且被推选为主节点。"],["body","\n"],["body","一个分区操作使得集群中大多数节点无法访问节点 B。"],["body","\n"],["body","之前分区操作的问题被修复了，节点 A 又恢复可访问状态。"],["body","\n\n"],["body","此刻，节点 B 仍然失效，节点 A 恢复可访问，会与节点 C 竞选去获得选票对节点 B 进行故障转移。"],["body","\n"],["body","这两个有同样的哈希槽的从节点最终都会请求被提升，然而由于它们发布的 configEpoch 是不一样的，而且节点 C 的 epoch 比较大，所以所有的节点都会把它们的配置更新为节点 C 的。"],["body","\n"],["body","节点 A 会从来源于节点 C（负责同样哈希槽的节点）的 ping 包中检测出节点 C 的 epoch 是更大的，所以它会重新设置自己为节点 C 的一个从节点。"],["body","\n"],["headingLink","服务器哈希槽信息的传播规则"],["heading","服务器哈希槽信息的传播规则"],["body","\n"],["body","Redis 集群很重要的一个部分是用来传播关于集群节点负责哪些哈希槽的信息的机制。这对于新集群的启动和提升从节点来负责处理哈希槽（它那失效的主节点本该处理的槽）的能力来说是必不可少的。"],["body","\n"],["body","个体持续交流使用的 ping 包和 pong 包都包含着一个头部，这个头部是给发送者使用的，为了向别的节点宣传它负责的哈希槽。这是主要用来传播变更的机制，不过集群管理员手动进行重新配置是例外（比如为了在主节点间移动哈希槽，通过 redis-trib 来进行手动碎片整理）。"],["body","\n"],["body","当一个新的 Redis 集群节点创建的时候，它的本地哈希槽表（表示给定哈希槽和给定节点 ID 的映射关系表）被初始化，每个哈希槽被置为 nil，也就是，每个哈希槽都是没赋值的。"],["body","\n"],["body","一个节点要更新它的哈希槽表所要遵守的第一个规则如下："],["body","\n"],["body","规则 1：如果一个哈希槽是没有赋值的，然后有个已知节点认领它，那么我就会修改我的哈希槽表，把这个哈希槽和这个节点关联起来。"],["body","\n"],["body","由于这个规则，当一个新集群被创建的时候，只需要手动给哈希槽赋值上（通常是通过 redis-trib 命令行工具使用 CLUSTER 命令来实现）负责它的主节点，然后这些信息就会迅速在集群中传播开来。"],["body","\n"],["body","然而，当一个配置更新的发生是因为一个从节点在其主节点失效后被提升为主节点的时候，这个规则显然还不足够。新的主节点会宣传之前它做从节点的时候负责的哈希槽，但从其他节点看来这些哈希槽并没有被重新赋值，所以如果它们只遵守第一个规则的话就不会升级配置信息。"],["body","\n"],["body","由于这个原因就有第二个规则，是用来把一个已赋值给以前节点的哈希槽重新绑定到一个新的认领它的节点上"],["body","\n"],["body","规则 2：如果一个哈希槽已经被赋值了，有个节点它的 configEpoch 比哈希槽当前拥有者的值更大，并且该节点宣称正在负责该哈希槽，那么我们会把这个哈希槽重新绑定到这个新节点上。"],["body","\n"],["body","因为有这第二个规则，所以集群中的所有节点最终都会同意哈希槽的拥有者是所有声称拥有它的节点中 configEpoch 值最大的那个。"],["body","\n"],["headingLink","update-消息"],["heading","UPDATE 消息"],["body","\n"],["body","上面描述的传播哈希槽配置信息的系统只使用节点间交换信息的普通 ping 包和 pong 包。 这要求存在一个节点（可以是负责给定哈希槽的主节点或从节点）拥有更新后的配置信息，"],["body","\n"],["body","然而也存在例外。当有一个节点，它是唯一一个负责处理给定哈希槽的节点，有可能在分区操作后它恢复正常，但拥有的配置信息是过时的。"],["body","\n"],["body","例子：一个给定的哈希槽是由节点 A 和 B 负责的。节点 A 是一个主节点，然后它在某个时刻失效了，所以节点 B 被提升为主节点。过了一段时间节点 B 也失效了，集群没有其他备份节点可以来处理这个哈希槽，所以只能开始修复操作。"],["body","\n"],["body","在一段时间过后节点 A 恢复正常了，并且作为一个可写入的主节点重新加入集群，但它的配置信息是过时的。此时没有任何备份节点能更新它的配置信息。这就是 UPDATE 消息存在的目的：当一个节点检测到其他节点在宣传它的哈希槽的时候是用一份过时的配置信息，那么它就会向这个节点发送一个 UPDATE 消息，这个消息包含新节点的 ID 和它负责的哈希槽（以 bitmap 形式发送）。"],["body","\n"],["body","注意：目前更新配置信息可以用 ping 包/ pong 包，也可以用 UPDATE 消息，这两种方法是共享同一个代码路径（code path）。这两者在更新一个带有老旧信息的节点的配置信息时会有功能上的重复。然而这两种机制都是非常有用的，因为 ping / pong 包在一段时间后能填充（populate）新节点的哈希槽路由表，而 UPDATE 消息只是在一个过时配置信息被检测出来时才被发送出去，并且只覆盖那些需要修复的错误配置信息。"],["body","\n"],["headingLink","从结点迁移"],["heading","从结点迁移"],["body","\n"],["body","Redis 集群实现了一个叫做备份迁移（replica migration）的概念，以提高系统的可用性。"],["body","\n"],["body","在集群中有主节点-从节点的设定，如果主从节点间的映射关系是固定的，那么久而久之，当发生多个单一节点独立故障的时候，系统可用性会变得很有限。"],["body","\n"],["body","例如有一个每个主节点都只有一个从节点的集群，当主节点或者从节点故障失效的时候集群能让操作继续执行下去，但如果主从节点都失效的话就没法让操作继续执行下去。然而这样长期会积累很多由硬件或软件问题引起的单一节点独立故障。例如："],["body","\n\n"],["body","主节点 A 有且只有一个从节点 A1。"],["body","\n"],["body","主节点 A 失效了。A1 被提升为新的主节点。"],["body","\n"],["body","三个小时后，A1 因为一个独立事件（跟节点 A 的失效无关）失效了。由于没有其他从节点可以提升为主节点（因为节点 A 仍未恢复正常），集群没法继续进行正常操作。"],["body","\n\n"],["body","如果主从节点间的映射关系是固定的，那要让集群更有抵抗力地面对上面的情况的唯一方法就是为每个主节点添加从节点。然而这要付出的代价也更昂贵，因为要求 Redis 执行更多的实例、更多的内存等等。"],["body","\n"],["body","一个候选方案就是在集群中创建不对称性，然后让集群布局时不时地自动变化。例如，假设集群有三个主节点 A，B，C。节点 A 和 B 都各有一个从节点，A1 和 B1。节点 C 有两个从节点：C1 和 C2。"],["body","\n"],["body","备份迁移是从节点自动重构的过程，为了迁移到一个没有可工作从节点的主节点上。在上面提到的例子中，备份迁移过程如下："],["body","\n\n"],["body","主节点 A 失效。A1 被提升为主节点。"],["body","\n"],["body","节点 C2 迁移成为节点 A1 的从节点，要不然 A1 就没有任何从节点。"],["body","\n"],["body","三个小时后节点 A1 也失效了。"],["body","\n"],["body","节点 C2 被提升为取代 A1 的新主节点。"],["body","\n"],["body","集群仍然能继续正常工作。"],["body","\n\n"],["headingLink","迁移算法"],["heading","迁移算法"],["body","\n"],["body","迁移算法不用任何形式的协议，因为 Redis 集群中的从节点布局不是集群配置信息（配置信息要求前后一致并且/或者用 config epochs 来标记版本号）的一部分。 它使用的是一个避免在主节点没有备份时从节点大批迁移的算法。这个算法保证，一旦集群配置信息稳定下来，最终每个主节点都至少会有一个从节点作为备份。"],["body","\n\n"],["body","\n"],["body","每个从节点若检测出存在至少一个没有好的从节点的单一主节点，那么就会触发这个算法的执行"],["body","\n"],["body","\n"],["body","\n"],["body","采取行动的从节点是属于那些拥有最多从节点的主节点，并且不处于 FAIL 状态及拥有最小的节点 ID。"],["body","\n"],["body","\n"],["body","\n"],["body","如果有 10 个主节点，它们各有 1 个从节点，另外还有 2 个主节点，它们各有 5 个从节点。会尝试迁移的从节点是在那 2 个拥有 5 个从节点的主节点中的所有从节点里，节点 ID 最小的那个"],["body","\n"],["body","\n\n"],["body","竞争情况"],["body","\n"],["body","在集群配置信息不稳定的情况下，有可能发生一种竞争情况：多个从节点都认为自己是不处于 FAIL 状态并且拥有较小节点 ID（实际上这是一种比较难出现的状况）。如果这种情况发生的话，结果是多个从节点都会迁移到同个主节点下，不过这种结局是无害的。这种竞争发生的话，有时候会使得割让出从节点的主节点变成没有任何备份节点，当集群再次达到稳定状态的时候，本算法会再次执行，然后把从节点迁移回它原来的主节点。"],["body","\n"],["body","最终每个主节点都会至少有一个从节点作为备份节点。通常表现出来的行为是，一个从节点从一个拥有多个从节点的主节点迁移到一个孤立的主节点。"],["body","\n"],["body","这个算法能通过一个用户可配置的参数 cluster-migration-barrier 进行控制。这个参数表示的是，一个主节点在拥有多少个好的从节点的时候就要割让一个从节点出来。例如这个参数若被设为 2，那么只有当一个主节点拥有 2 个可工作的从节点时，它的一个从节点会尝试迁移。"],["body","\n"],["headingLink","发布订阅publishsubscribe"],["heading","发布/订阅（Publish/Subscribe）"],["body","\n"],["body","在一个 Redis 集群中，客户端能订阅任何一个节点，也能发布消息给任何一个节点。集群会确保发布的消息都会按需进行转发。 目前的实现方式是单纯地向所有节点广播所有的发布消息，在将来的实现中会用 bloom filters 或其他算法来优化。"],["body","\n"],["headingLink","集群配置"],["heading","集群配置"],["body","\n"],["body","#端口配置\nport 7000\n#集群启用\ncluster-enabled yes\n#保存节点配置文件的路径\ncluster-config-file nodes.conf\n#节点超时\ncluster-node-timeout 5000\n\nappendonly yes\n"],["body","\n"],["headingLink","实战"],["heading","实战"],["body","\n"],["headingLink","经典三主三从"],["heading","经典三主三从"],["body","\n"],["headingLink","规划"],["heading","规划"],["body","\n"],["body","7000 7001 7002 7003 7004 7005 自动配置主从\n"],["body","\n"],["headingLink","基础配置"],["heading","基础配置"],["body","\n"],["body","# 拷贝命令\nsed  's/bind 127.0.0.1/bind 0.0.0.0/g' ../redis.conf >redis.conf\nsed -i 's/^port 6379/port 7005/g' redis.conf\nsed -i 's/^protected-mode yes/protected-mode no/g' redis.conf\nsed -i 's/^daemonize no/daemonize yes/g' redis.conf\n"],["body","\n"],["headingLink","集群配置-1"],["heading","集群配置"],["body","\n"],["body","\n# 启动cluster\nsed -i 's/# cluster-enabled yes/cluster-enabled yes/g' redis.conf\n# 启用集群配置文件\nsed -i 's/# cluster-config-file nodes-6379.conf/cluster-config-file nodes-6379.conf/g'  redis.conf\n# 集群不需要所有槽都被映射也能提供服务\nsed -i 's/# cluster-require-full-coverage yes/cluster-require-full-coverage no/g' redis.conf\n"],["body","\n"],["headingLink","启停"],["heading","启停"],["body","\n"],["body","../../redis-server redis.conf\nkill -15 `ps aux | grep -v grep | grep redis | awk '{ print $2 }'`\n"],["body","\n"],["headingLink","完整配置"],["heading","完整配置"],["body","\n"],["body","kill -15 `ps aux | grep -v grep | grep redis | awk '{ print $2 }'`\nfor i in 7000 7001 7002 7003 7004 7005;\ndo\n    rm -rf $i;\n    mkdir $i;\n    cd $i;\n    sed  's/bind 127.0.0.1/bind 0.0.0.0/g' ../redis.conf >redis.conf\n    sed -i \"s/^port 6379/port $i/g\" redis.conf\n    sed -i 's/^protected-mode yes/protected-mode no/g' redis.conf\n    sed -i 's/^daemonize no/daemonize yes/g' redis.conf\n    sed -i 's/# cluster-enabled yes/cluster-enabled yes/g' redis.conf\n    sed -i 's/# cluster-config-file nodes-6379.conf/cluster-config-file nodes-6379.conf/g'  redis.conf \n    ../redis-server redis.conf\n    cd ../\ndone\n"],["body","\n"],["headingLink","启动集群"],["heading","启动集群"],["body","\n"],["body","随机指定主从"],["body","\n"],["body","./redis-cli --cluster create 192.168.3.16:7000 192.168.3.16:7001 192.168.3.16:7002 192.168.3.16:7003 192.168.3.16:7004 192.168.3.16:7005  --cluster-replicas 1\n"],["body","\n"],["body","手动指定"],["body","\n"],["body","./redis-cli --cluster create 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002  --cluster-replicas 0\n\n./redis-cli --cluster add-node --slave 127.0.0.1:7003 127.0.0.1:7000 --cluster-slave\n./redis-cli --cluster add-node --slave 127.0.0.1:7004 127.0.0.1:7001 --cluster-slave\n./redis-cli --cluster add-node --slave 127.0.0.1:7005 127.0.0.1:7002 --cluster-slave\n"],["body","\n"],["headingLink","查看集群"],["heading","查看集群"],["body","\n"],["body","查看集群信息"],["body","\n"],["body","cluster info\n"],["body","\n"],["body","查看集群节点"],["body","\n"],["body","cluster nodes\n"],["body","\n"],["headingLink","操作集群"],["heading","操作集群"],["body","\n"],["body","删除节点"],["body","\n"],["body","如果有slave，先删除slave或者将slave转移到其他master下\n"],["body","\n"],["body","删除slave"],["body","\n"],["body","redis-cli --cluster del-node 172.17.0.7:6379 'f9e78f563314a1d88796ec0ca2b13e4ac3cae75f'\n"],["body","\n"],["body","转移slot"],["body","\n"],["body","因为master上面有slot，所以首先reshard转移slot，假设将  127.0.0.1:7000 上面的slot转移到 127.0.0.1:7001 和127.0.0.1:7002  上"],["body","\n"],["body","#非交互式\nredis-cli --cluster reshard  127.0.0.1:7000\n\nredis-cli --cluster reshard 172.17.0.4:6379 --cluster-from '46623a0b2ec8abb8a0688769337e91268df3c73f' --cluster-to '14d129294d95867777a91d29b708413baa8a276c' --cluster-slots 2500 --cluster-yes\n\n\n redis-cli --cluster reshard host:port --cluster-from <arg> --cluster-to <arg> --cluster-slots <arg> --cluster-yes --cluster-timeout <arg> --cluster-pipeline <arg>\n\n# 参数说明：\n# host：port：必传参数，集群内任意节点地址，用来获取整个集群信息。\n# --cluster-from：制定源节点的id，如果有多个源节点，使用逗号分隔，如果是all源节点变为集群内所有主节点，在迁移过程中提示用户输入。\n# --cluster-to：需要迁移的目标节点的id，目标节点只能填写一个，在迁移过程中提示用户输入。\n# --cluster-slots：需要迁移槽的总数量，在迁移过程中提示用户输入。\n# --cluster-yes：当打印出reshard执行计划时，是否需要用户输入yes确认后再执行reshard。\n# --cluster-timeout：控制每次migrate操作的超时时间，默认为60000毫秒。\n# --cluster-pipeline：控制每次批量迁移键的数量，默认为10。\n"],["body","\n"],["body","#然后将剩余的slot转移到172.17.0.3上，执行下面一条命令即可，还剩余2962个slot\nredis-cli --cluster reshard 172.17.0.4:6379 --cluster-from '46623a0b2ec8abb8a0688769337e91268df3c73f' --cluster-to '8a7b1a4cf2980c031c0e5e912cf366981588e3c9' --cluster-slots 2962 --cluster-yes\n"],["body","\n"],["body","删除master"],["body","\n"],["body","redis-cli --cluster del-node 172.17.0.4:6379 '46623a0b2ec8abb8a0688769337e91268df3c73f'\n"],["body","\n"],["body","添加节点"],["body","\n"],["body","redis-cli --cluster add-node 172.17.0.4:6379 172.17.0.2:6379\n"],["body","\n"],["headingLink","故障转移"],["heading","故障转移"],["body","\n"],["body","上面的删除节点都是在已经情况下操作的，假设现在在未知情况下宕机了，那么会发生什么？"],["body","\n"],["headingLink","master宕机"],["heading","master宕机"],["body","\n"],["body","假设master宕机，让127.0.0.1:7000 宕机，过了一会儿，看cluster nodes信息发现 127.0.0.1:7000 有fail标志"],["body","\n"],["body","稍等片刻之后发现127.0.0.1:7004 变成了master"],["body","\n"],["body","在看看127.0.0.1:7004 的日志输出有下面这句话"],["body","\n"],["body","Failover election won: I'm the new master.\n"],["body","\n"],["body","也就是说当集群内的mater宕机后，slave被选举一个出来当做master，集群依然可用，假设原来的master恢复了，那么它将变成slave追随现在的master"],["body","\n"],["headingLink","master-slave宕机"],["heading","master-slave宕机"],["body","\n"],["body","假如master和slave双双宕机了呢？此时将都变成fail"],["body","\n"],["body","默认情况下此时集群将变得不可用，执行get指令时会报错"],["body","\n"],["body","(error) CLUSTERDOWN The cluster is down\n"],["body","\n"],["body","但是如果配置了cluster-require-full-coverage，那么集群依然部分可用，所谓部分可用即宕机的slot不可用，其他的slot还是可用的，参考：https://stackoverflow.com/quest"],["body","\n"],["body","cluster-require-full-coverage no\n"],["body","\n"],["body","很多文章都说此时slot会自动转移，但是我测试时并不会自动转移，仔细想一想，master-slave双双都在不可抗力下宕机了，那么里面的数据肯定是拿不出来的，怎么转移slot呢？"],["body","\n"],["headingLink","代理"],["heading","代理"],["body","\n"],["headingLink","安装"],["heading","安装"],["body","\n"],["body","# 安装编译环境，各种工具等，注意libstdc++-static可能安装不上\nyum install -y git wget gcc gcc-c++ libstdc++-static make telnet\n# 创建文件夹\ncd ~ && mkdir soft && cd soft\n# clone代码到本地\ngit clone https://github.com/joyieldInc/predixy.git\n# 编译安装\ncd predixy && make \n# 拷贝\nmkdir -p /opt/predixy && cp src/predixy /opt/predixy\n# 添加环境变量\ncat >> /etc/profile <<EOF\nexport PATH=$PATH:/opt/predixy\nEOF\necho  'source /etc/profile' >> ~/.bashrc && source ~/.bashrc\n# 帮助命令\npredixy -h\n"],["body","\n"],["headingLink","配置predixconf"],["heading","配置predix.conf"],["body","\n"],["body","# 开启日志\nsed -i 's/# Log .\\/predixy.log/Log .\\/predixy.log/g' predixy.conf\n#predixy 默认运行在7617端口\n# 引入cluster.conf\nsed -i 's/# Include cluster.conf/Include cluster.conf/g' predixy.conf\n# 注释测试 try.conf\nsed -i 's/Include try.conf/# Include try.conf/g' predixy.conf\n"],["body","\n"],["headingLink","配置clusterconf"],["heading","配置cluster.conf"],["body","\n"],["body","cat > cluster.conf <<EOF\nClusterServerPool {\n    #这个是主节点访问权重，如果是只把备节点用作备份不去做读写分离，直接将这个配置成100只去读主节点就好了。\n    MasterReadPriority 100\n    # redis实例的访问密码\n    # Password sjwkk123456\n    # 读写分离功能，从静态redis slave节点执行读请求的优先级，所谓静态节点，是指在本配置文件中显示列出的redis节点，不指定的话为0\n    StaticSlaveReadPriority 50 \n    # 功能见上，所谓动态节点是指在本配置文件中没有列出，但是通过redis sentinel动态发现的节点，不指定的话为0\n    DynamicSlaveReadPriority 50\n    # predixy会周期性的请求redis sentinel以获取最新的集群信息，该参数以秒为单位指定刷新周期，不指定的话为1秒\n    RefreshInterval 1\n    # 请求在predixy中最长的处理/等待时间，如果超过该时间redis还没有响应的话，那么predixy会关闭同redis的连接，并给客户端一个错误响应，对于blpop这种阻塞式命令，该选项不起作用，为0则禁止此功能，即如果redis不返回就一直等待，不指定的话为0\n    ServerTimeout 1\n    # 一个redis实例出现多少次才错误以后将其标记为失效，不指定的话为10\n    ServerFailureLimit 10\n    # 一个redis实例失效后多久后去检查其是否恢复正常，不指定的话为1秒\n    ServerRetryTimeout 1\n    #predixy与redis的连接tcp keepalive时间，为0则禁止此功能，不指定的话为0\n    KeepAlive 120\n    Servers {\n        # 配置所有节点地址\n        + 127.0.0.1:7000\n        + 127.0.0.1:7001\n        + 127.0.0.1:7002\n        + 127.0.0.1:7003\n        + 127.0.0.1:7004\n        + 127.0.0.1:7005\n    }\n}\nEOF\n"],["body","\n"],["headingLink","启动并测试"],["heading","启动并测试"],["body","\n"],["body","# 启动\npredixy /etc/predixy/conf/predixy.conf &\n\n# 查看日志\ntail -f /etc/predixy/conf/predixy.log \n\n# 连接代理\nredis-cli -h 172.17.0.8 -p 7617\n\n# 设置值\nset k1 aaa\n\n# 获取值\nget k1\n"],["body","\n"],["body","参考文章"],["body","\n"],["body","下载链接"],["body","\n"],["body","curl -O  http://download.redis.io/releases/redis-6.0.6.tar.gz\ntar -xf redis-6.0.6.tar.gz\ncd redis-6.0.6\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","8.缓存中间件_Redis/redis主从与哨兵.html"],["title","redis主从与哨兵 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","前言"],["heading","前言"],["body","\n"],["body","启用服务"],["body","\n"],["body","./redis-server /root/redis-5.0.7/etc/redis.conf \n"],["body","\n"],["body","关闭服务"],["body","\n"],["body","redis-cli -p 6379 shutdown \n"],["body","\n"],["headingLink","redis参数配置解析"],["heading","Redis参数配置解析"],["body","\n"],["headingLink","服务端配置"],["heading","服务端配置"],["body","\n"],["body","# redis进程是否以守护进程的方式运行，yes为是，no为否(不以守护进程的方式运行会占用一个终端)。 \ndaemonize no \n# 绑定的主机地址 \nbind 127.0.0.1 \n# redis进程的端口号 \nport 6379 \n#是否开启保护模式，默认开启。要是配置里没有指定bind和密码。开启该参数后，redis只会本地进行访问，拒绝外部访问。要是开启了密码和bind，可以开启。否则最好关闭设置为no。 \nprotected-mode yes\n# 指定redis进程的PID文件存放位置 \npidfile /var/run/redis.pid\n"],["body","\n"],["headingLink","客户端配置"],["heading","客户端配置"],["body","\n"],["body","# 客户端闲置多长时间后关闭连接，默认此参数为0即关闭此功能 \ntimeout 300 \n# 设置同一时间最大客户连接数，默认无限制。redis可以同时连接的客户端数为redis程序可以打开的最大文件描述符，如果设置 maxclients 0，表示不作限制。当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回 max number of clients reached 错误信息 \nmaxclients 128 \n# 设置redis连接密码，如果配置了连接密码，客户端在连接redis是需要通过AUTH<password>命令提供密码，默认关闭 \nrequirepass footbared \n# 指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key。当此方法处理后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。Redis新的vm机制，会把Key存放内存，Value会存放在swap区 \nmaxmemory<bytes> \n# 选择数据库，默认为0可以使用select <dbid>命令在连接上指定数据库id \ndatabases 16 \n"],["body","\n"],["headingLink","日志"],["heading","日志"],["body","\n"],["body","# redis日志级别，可用的级别有debug.verbose.notice.warning \nloglevel verbose \n# log文件输出位置，如果进程以守护进程的方式运行，此处又将输出文件设置为stdout的话，就会将日志信息输出到/dev/null里面去了 \nlogfile stdout \n"],["body","\n"],["headingLink","持久化"],["heading","持久化"],["body","\n"],["body","# 指定在多少时间内刷新次数达到多少的时候会将数据同步到数据文件 \nsave <seconds> <changes> \n# 指定存储至本地数据库时是否压缩文件，默认为yes即启用存储 \nrdbcompression yes \n# 指定本地数据库文件名 \ndbfilename dump.db \n# 指定本地数据问就按存放位置 \ndir ./ \n# 指定是否在每次更新操作后进行日志记录，Redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。因为redis本身同步数据文件是按上面save条件来同步的，所以有的数据会在一段时间内只存在于内存中。默认为no。 \nappendonly no \n# 指定跟新日志文件名默认为appendonly.aof \nappendfilename appendonly.aof \n# 指定更新日志的条件，有三个可选参数 - no：表示等操作系统进行数据缓存同步到磁盘(快)，always：表示每次更新操作后手动调用fsync()将数据写到磁盘(慢，安全)， everysec：表示每秒同步一次(折衷，默认值)； \nappendfsync everysec \n"],["body","\n"],["headingLink","集群配置"],["heading","集群配置"],["body","\n"],["body","# 指定当本机为slave服务时，设置master服务的IP地址及端口，在redis启动的时候他会自动跟master进行数据同步 \nreplicaof <masterip> <masterport> \n# 当master设置了密码保护时，slave服务连接master的密码 \nmasterauth <master-password> \n"],["body","\n"],["headingLink","主从同步原理"],["heading","主从同步原理"],["body","\n"],["headingLink","全量同步"],["heading","全量同步"],["body","\n"],["body","Redis全量复制一般发生在Slave初始化阶段，这时Slave需要将Master上的所有数据都复制一份。具体步骤如下： "],["body","\n\n"],["body","从服务器连接主服务器，发送SYNC命令； "],["body","\n"],["body","主服务器接收到SYNC命名后，开始执行BGSAVE命令生成RDB文件并使用缓冲区记录此后执行的所有写命令； "],["body","\n"],["body","主服务器BGSAVE执行完后，向所有从服务器发送快照文件，并在发送期间继续记录被执行的写命令； "],["body","\n"],["body","从服务器收到快照文件后丢弃所有旧数据，载入收到的快照； "],["body","\n"],["body","主服务器快照发送完毕后开始向从服务器发送缓冲区中的写命令； "],["body","\n"],["body","从服务器完成对快照的载入，开始接收命令请求，并执行来自主服务器缓冲区的写命令； "],["body","\n\n"],["body","sequenceDiagram\nparticipant a as 主服务器 \nparticipant b as 从服务器\nb ->>+ a:从服务器连接主服务器，发送sync指令\na ->>- a:执行 BGSAVE 生成快照，记录在此期间的写命令\na ->>+ b:发送快照\nb ->>- b:写入快照\na ->>+ b:发送缓存的写命令\nb ->>- b:执行写命令\n"],["body","\n"],["body","完成上面几个步骤后就完成了从服务器数据初始化的所有操作，从服务器此时可以接收来自用户的读请求。"],["body","\n"],["headingLink","增量同步"],["heading","增量同步"],["body","\n"],["body","Redis增量复制是指Slave初始化后开始正常工作时 主服务器发生的写操作同步到从服务器的过程。 "],["body","\n"],["body","增量复制的过程主要是主服务器每执行一个写命令就会向从服务器发送相同的写命令，从服务器接收并执行收到的写命令。"],["body","\n"],["body","Redis主从同步策略"],["body","\n"],["body","主从刚刚连接的时候，进行全量同步；"],["body","\n"],["body","全同步结束后，进行增量同步。"],["body","\n"],["body","当然，如果有需要，slave 在任何时候都可以发起全量同步。redis 策略是，无论如何，首先会尝试进行增量同步，如不成功，要求从机进行全量同步。"],["body","\n"],["body","注意点"],["body","\n"],["body","如果多个Slave断线了，需要重启的时候，因为只要Slave启动，就会发送sync请求和主机全量同步，当多个同时出现的时候，可能会导致Master IO剧增宕机。"],["body","\n"],["headingLink","redis-sentinel哨兵"],["heading","Redis Sentinel（哨兵）"],["body","\n"],["headingLink","简介"],["heading","简介"],["body","\n"],["body","Redis的主从复制下，一旦主节点由于故障不能提供服务，需要人工将从节点晋升为主节点，同时还要通知应用方更新主节点地址，对于很多应用场景这种故障处理的方法是无法接受的。但是Redis从2.8开始正式提供了Redis Sentinel（哨兵）架构来解决这个问题。"],["body","\n"],["body","​    Redis Sentinel是一个分布式架构，其中包含若干个Sentinel节点和Redis数据节点，每个Sentinel节点会对数据节点和其余Sentinel节点进行监控，当它发现节点不可达时，会对节点做下线标识。如果被标识的是主节点，它还会和其他Sentinel节点进行“协商”，当大多数Sentinel节点都认为主节点不可达时，它们会选举出一个Sentinel节点来完成自动故障转移的工作，同时会将这个变化通知给Redis应用方。整个过程完全是自动的，不需要人工来介入，所以这套方案很有效地解决了Redis的高可用问题。"],["body","\n"],["headingLink","实现原理"],["heading","实现原理"],["body","\n"],["body","三个定时监控任务"],["body","\n\n"],["body","\n"],["body","获取最新拓扑结构"],["body","\n"],["body","每隔10秒，每个Sentinel节点会向主节点和从节点发送info命令获取最新的拓扑结构。"],["body","\n"],["body","\n"],["body","\n"],["body","主节点判断"],["body","\n"],["body","每隔2秒，每个Sentinel节点会向Redis数据节点的__sentinel__:hello频道上发送该Sentinel节点对于主节点的判断以及当前Sentinel节点的信息，同时每个Sentinel节点也会订阅该频道，来了解其他Sentinel节点以及它们对主节点的判断。"],["body","\n"],["body","\n"],["body","\n"],["body","心跳检测"],["body","\n"],["body","每隔一秒，每个Sentinel节点会向主节点、从节点、其余Sentinel节点发送一条ping命令做一次心跳检测，来确认这些节点当前是否可达。"],["body","\n"],["body","\n\n"],["body","主观下线"],["body","\n"],["body","因为每隔一秒，每个Sentinel节点会向主节点、从节点、其余Sentinel节点发送一条ping命令做一次心跳检测，当这些节点超过down-after-milliseconds没有进行有效回复，Sentinel节点就会对该节点做失败判定，这个行为叫做主观下线。"],["body","\n"],["body","客观下线"],["body","\n"],["body","当Sentinel主观下线的节点是主节点时，该Sentinel节点会向其他Sentinel节点询问对主节点的判断，当超过<quorum>个数，那么意味着大部分的Sentinel节点都对这个主节点的下线做了同意的判定，于是该Sentinel节点认为主节点确实有问题，这时该Sentinel节点会做出客观下线的决定。"],["body","\n"],["body","领导者Sentinel节点选举"],["body","\n"],["body","Raft算法：假设s1(sentinel-1)最先完成客观下线，它会向其余Sentinel节点发送命令，请求成为领导者；收到命令的Sentinel节点如果没有同意过其他Sentinel节点的请求，那么就会同意s1的请求，否则拒绝；如果s1发现自己的票数已经大于等于某个值，那么它将成为领导者。"],["body","\n"],["body","故障转移"],["body","\n"],["body","1）领导者Sentinel节点在从节点列表中选出一个节点作为新的主节点"],["body","\n"],["body","2）上一步的选取规则是与主节点复制相似度最高的从节点"],["body","\n"],["body","3）领导者Sentinel节点让剩余的从节点成为新的主节点的从节点"],["body","\n"],["body","4）Sentinel节点集合会将原来的主节点更新为从节点，并保持着对其关注，当其恢复后命令它去复制新的主节点"],["body","\n"],["headingLink","主从哨兵"],["heading","主从+哨兵"],["body","\n"],["headingLink","主从结构哨兵sentinel"],["heading","主从结构+哨兵(sentinel)"],["body","\n"],["body","\n"],["body","一个主节点(master)可拥有多个从节点(slave)，从节点实现对主节点的复制，保证数据同步。"],["body","\n"],["body","而哨兵(sentinel)则对各节点进行监控，主要包括主节点存活检测、主从运行情况检测等，一旦主节点宕机，哨兵可自动进行故障转移 (failover)、主从切换。接下来就开始搭建这样一个集群，首先是主从结构，然后是哨兵模式，接着往下看。"],["body","\n"],["headingLink","redis-主从配置及数据同步"],["heading","Redis 主从配置及数据同步"],["body","\n"],["body","主\n192.168.1.210 6379\n从\n192.168.1.210 6380\n192.168.1.210 6381\n192.168.1.210 6382\n"],["body","\n"],["body","bind：0.0.0.0 \nport：6379 \nprotected-mode：no \ndaemonize：yes \nlogfile：./redis.log \nrequirepass：pwdtest@2019 \nmasterauth：pwdtest@2019 \nreplicaof 192.168.231.130 6379\n"],["body","\n"],["headingLink","redis-哨兵模式搭建"],["heading","Redis 哨兵模式搭建"],["body","\n"],["headingLink","哨兵模式详解"],["heading","哨兵模式详解"],["body","\n"],["body","Sentinel 使用的算法核心是 Raft 算法，主要用途就是用于分布式系统，系统容错，以及Leader选举，每个Sentinel都需要定期的执行以下任务："],["body","\n\n"],["body","每个 Sentinel 会自动发现其他 Sentinel 和从服务器，它以每秒钟一次的频率向它所知的主服务器、从服务器以及其他 Sentinel 实例发送一个 PING 命令。"],["body","\n"],["body","如果一个实例(instance)距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 那么这个实例会被 Sentinel 标记为主观下线。 有效回复可以是： +PONG 、 -LOADING 或者 -MASTERDOWN 。"],["body","\n"],["body","如果一个主服务器被标记为主观下线， 那么正在监视这个主服务器的所有Sentinel要以每秒一次的频率确认主服务器的确进入了主观下线状态。"],["body","\n"],["body","如果一个主服务器被标记为主观下线， 并且有足够数量的Sentinel(至少要达到配置文件指定的数量)在指定的时间范围内同意这一判断， 那么这个主服务器被标记为客观下线。"],["body","\n"],["body","在一般情况下， 每个Sentinel会以每 10 秒一次的频率向它已知的所有主服务器和从服务器发送 INFO 命令。 当一个主服务器被Sentinel标记为客观下线时，Sentinel向下线主服务器的所有从服务器发送 INFO 命令的频率会从 10 秒一次改为每秒一次。"],["body","\n"],["body","当没有足够数量的Sentinel同意主服务器已经下线， 主服务器的客观下线状态就会被移除。 当主服务器重新向Sentinel的 PING 命令返回有效回复时， 主服务器的主关下线状态就会被移除。"],["body","\n\n"],["body","\n"],["headingLink","哨兵配置"],["heading","哨兵配置"],["body","\n"],["body","基本信息配置"],["body","\n"],["body","# 哨兵sentinel实例运行的端口，默认26379   \nport 26379 \n# 哨兵sentinel的工作目录 \ndir ./\n# 是否开启保护模式，默认开启。 \nprotected-mode:no \n# 是否设置为后台启动。 \ndaemonize:yes \n \n# 哨兵sentinel的日志文件 \nlogfile:./sentinel.log \n"],["body","\n"],["body","哨兵配置"],["body","\n\n"],["body","监控 的master结点"],["body","\n"],["body","访问结点的密码"],["body","\n"],["body","下线间隔"],["body","\n"],["body","主备切换时 对新 master进行同步"],["body","\n"],["body","故障转移的超时时间"],["body","\n\n"],["body","# 哨兵sentinel监控的redis主节点的  \n## ip：主机ip地址 \n## port：哨兵端口号 \n## master-name：可以自己命名的主节点名字（只能由字母A-z、数字0-9 、这三个字符\".-_\"组成。） \n## quorum：当这些quorum个数sentinel哨兵认为master主节点失联 那么这时 客观上认为主节点失联了   \n# sentinel monitor <master-name> <ip> <redis-port> <quorum>   \nsentinel monitor mymaster 127.0.0.1 6379 2 \n \n# 当在Redis实例中开启了requirepass，所有连接Redis实例的客户端都要提供密码。 \n# sentinel auth-pass <master-name> <password>   \nsentinel auth-pass mymaster 123456   \n \n# 指定主节点应答哨兵sentinel的最大时间间隔，超过这个时间，哨兵主观上认为主节点下线，默认30秒   \n# sentinel down-after-milliseconds <master-name> <milliseconds> \nsentinel down-after-milliseconds mymaster 30000   \n \n# 指定了在发生failover主备切换时，最多可以有多少个slave同时对新的master进行同步。这个数字越小，完成failover所需的时间就越长；反之，但是如果这个数字越大，就意味着越多的slave因为replication而不可用。可以通过将这个值设为1，来保证每次只有一个slave，处于不能处理命令请求的状态。 \n# sentinel parallel-syncs <master-name> <numslaves> \nsentinel parallel-syncs mymaster 1   \n \n# 故障转移的超时时间failover-timeout，默认三分钟，可以用在以下这些方面： \n## 1. 同一个sentinel对同一个master两次failover之间的间隔时间。   \n## 2. 当一个slave从一个错误的master那里同步数据时开始，直到slave被纠正为从正确的master那里同步数据时结束。   \n## 3. 当想要取消一个正在进行的failover时所需要的时间。 \n## 4.当进行failover时，配置所有slaves指向新的master所需的最大时间。不过，即使过了这个超时，slaves依然会被正确配置为指向master，但是就不按parallel-syncs所配置的规则来同步数据了 \n# sentinel failover-timeout <master-name> <milliseconds>   \nsentinel failover-timeout mymaster 180000 \n \n# 当sentinel有任何警告级别的事件发生时（比如说redis实例的主观失效和客观失效等等），将会去调用这个脚本。一个脚本的最大执行时间为60s，如果超过这个时间，脚本将会被一个SIGKILL信号终止，之后重新执行。 \n# 对于脚本的运行结果有以下规则：   \n## 1. 若脚本执行后返回1，那么该脚本稍后将会被再次执行，重复次数目前默认为10。 \n## 2. 若脚本执行后返回2，或者比2更高的一个返回值，脚本将不会重复执行。   \n## 3. 如果脚本在执行过程中由于收到系统中断信号被终止了，则同返回值为1时的行为相同。 \n# sentinel notification-script <master-name> <script-path>   \nsentinel notification-script mymaster /var/redis/notify.sh \n \n# 这个脚本应该是通用的，能被多次调用，不是针对性的。 \n# sentinel client-reconfig-script <master-name> <script-path> \nsentinel client-reconfig-script mymaster /var/redis/reconfig.sh \n"],["body","\n"],["body","redis-cli -p 26379 \ninfo sentinel \n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","10.Redis/命令杂项.html"],["title","命令杂项 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["body","redis-cli -h IP地址 -p 端口 -n 数据库序号 -a 密码 keys \"Abc*\" | wc -l\n"],["body","\n"],["headingLink","scan"],["heading","SCAN"],["body","\n"],["body","scan命令：SCAN cursor [MATCH pattern] [COUNT count]\n"],["body","\n"],["body","1、复杂度虽然也是 O(n)，但是它是通过游标分步进行的，不会阻塞线程"],["body","\n"],["body","2、提供 count 参数，不是结果数量，是redis单次遍历字典槽位数量(约等于)"],["body","\n"],["body","3、同 keys 一样，它也提供模式匹配功能;"],["body","\n"],["body","4、服务器不需要为游标保存状态，游标的唯一状态就是 scan 返回给客户端的游标整数;"],["body","\n"],["body","5、返回的结果可能会有重复，需要客户端去重复，这点非常重要;"],["body","\n"],["body","6、单次返回的结果是空的并不意味着遍历结束，而要看返回的游标值是否为零"],["body","\n"],["body","7、SCAN命令是增量的循环，每次调用只会返回一小部分的元素。所以不会让redis假死"],["body","\n"],["body","8、SCAN命令返回的是一个游标，从0开始遍历，到0结束遍历"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/命令集合/TAG.html"],["title","TAG - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","增删改"],["heading","增删改"],["body","\n"],["headingLink","删除本地tag"],["heading","删除本地tag"],["body","\n"],["body","git tag -d tag_1.6.21\n"],["body","\n"],["headingLink","推送删除tag"],["heading","推送删除tag"],["body","\n"],["body","git push origin :refs/tags/tag_1.6.21\ngit push <remote> :refs/tags/<tagname>\n"],["body","\n"],["headingLink","新建本地tag"],["heading","新建本地tag"],["body","\n"],["body","git tag tag_1.6.23.1\n\n//带注释的标签\ngit tag -a <tagname> -m \"runoob.com标签\"\n"],["body","\n"],["headingLink","推送tag"],["heading","推送tag\t"],["body","\n"],["body","git push origin tag_1.6.22.2\n"],["body","\n"],["headingLink","一次性推送所有"],["heading","一次性推送所有"],["body","\n"],["body","git push origin --tags\n"],["body","\n"],["headingLink","查询"],["heading","查询"],["body","\n"],["headingLink","模糊查询"],["heading","模糊查询"],["body","\n"],["body"," git tag -l \"v1.8.5*\"\n"],["body","\n"],["headingLink","从tag签出"],["heading","从Tag签出"],["body","\n"],["body","如果你想查看一个标签指向的文件的版本，你可以对该标签执行 git checkout，尽管这会使你的存储库处于“detached HEAD”状态，这会产生一些不良的副作用："],["body","\n"],["body","git checkout tag_name\ngit switch -c <new-branch-name>\n"],["body","\n"],["body","git checkout -b <branch-name> <tag_name>\ngit checkout -b version2 v2.0.0\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/命令集合/REBASE.html"],["title","REBASE - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","合并时缩减成一个提交"],["heading","合并时缩减成一个提交"],["body","\n"],["body","git checkout master\ngit merge --squash dev\n"],["body","\n\n"],["body","本地提交多次。不push远程"],["body","\n"],["body","git status 查看 领先远程的 commit 个数"],["body","\n"],["body","查看在我的分支但是不在master分支的commit：git log mybracnh  ^master"],["body","\n"],["body","变基合并提交"],["body","\n\n"],["body","#合并前5个提交\ngit rebase -i HEAD~5\n\n#或者：合并到某个提交\ngit rebase -i 0b26a0f775\n"],["body","\n"],["headingLink","rebase-on-to"],["heading","Rebase on to"],["body","\n"],["body","把某个分支的某一段commit记录rebase到基点分支上"],["body","\n"],["body","git rebase  —onto a b c\n"],["body","\n\n"],["body","C分支：当前分支（必须是分支）"],["body","\n"],["body","A分支：基点（可以是分支，也可以是commit节点）"],["body","\n"],["body","B分支：起点  （可以是分支，也可以是commit节点， 如果是commit节点，就找到当前分支最新commit跟这个起点commit节点之间所有的节点，rebase到基点上， 这个起点必须在当前分支上，如果不在，那么这个起点就没有意义了，就相当于直接把C分支rebase到A分支上）"],["body","\n\n"],["headingLink","交互式变基底"],["heading","交互式变基底"],["body","\n"],["body","git rebase -i \n"],["body","\n\n"],["body","\n"],["body","合并一个分支中多次commit提交"],["body","\n"],["body","\n"],["body","\n"],["body","第一个commit的action不能是squash、fixup，最好是pick。"],["body","\n"],["body","\n\n"],["headingLink","六种action可以分为四类"],["heading","六种Action可以分为四类"],["body","\n\n"],["body","pick(p)，表明正在使用；"],["body","\n"],["body","reword(r)，表明仍然使用该提交对象，但是需修改提交信息；"],["body","\n"],["body","edit(e)，使用该提交对象，但是不合并提交对象；"],["body","\n"],["body","squash(s)，使用该提交对象，但是将此提交与上一次提交对象合并；"],["body","\n"],["body","fixup(f)，同squash值，但是丢失此次提交的日志信息；"],["body","\n"],["body","exec(x)，后接特定脚本，保存后将执行该脚本；"],["body","\n"],["body","drop(d)，移除该提交对象"],["body","\n\n"],["headingLink","第一类保留commit不合并"],["heading","第一类：保留commit，不合并"],["body","\n\n"],["body","\n"],["body","pick:  标记为pick的commit会在rebase操作后会直接保留下来，不做任何改动，也不会合并，最上面的commit最好标记为这一类"],["body","\n"],["body","\n"],["body","\n"],["body","reword： 这一类commit也会保存下来，不过在保存下来之前会有一次修改commit message的机会"],["body","\n"],["body","\n"],["body","\n"],["body","edit：这一类的commit也会直接保存下来，不过，当合并到这种类型的commit时，整个合并经常会暂停下来，你可以重新修改这次commit中的变动内容，比如给这个commit继续新增一些代码改动、或者修改commit message，然后git add(不要忘记 git add了)， 再继续使用git rebase —continue，来继续rebase操作"],["body","\n"],["body","\n\n"],["headingLink","第二类不保留commit与上一次commit合并"],["heading","第二类：不保留commit，与上一次commit合并"],["body","\n"],["body","squash： 标记为squash的commit在rebase操作完成后不会保留，它会与之相邻的上一次commit进行合并。同时它的commit message也会与上一次commit的message合并。"],["body","\n"],["body","fixup: 这类commit不会保留，会直接与相邻的上一次commit合并，与squash不同之处在于，它的commit message回直接丢弃，即这次commit会被视为对前一次commit的一次小的补充修改（fixup），commit message就以前一次为准"],["body","\n"],["headingLink","第三类不保留直接删除commit"],["heading","第三类：不保留，直接删除commit"],["body","\n"],["body","skip：标记为skip的commit会直接被删除，就相当于这次commit从来没有发生过。同时，这个commit中涉及的所有代码修改全部会被删除。"],["body","\n"],["headingLink","第四类执行命令"],["heading","第四类：执行命令"],["body","\n"],["headingLink","git-更安全的强制推送--force-with-lease"],["heading","Git 更安全的强制推送，--force-with-lease"],["body","\n\n"],["body","--force-with-lease 参数自 Git 的 1.8.5 版本开始提供，只在解决 git push --force 命令造成的安全问题。"],["body","\n"],["body","那么 git push --force 命令有什么安全问题"],["body","\n"],["body","--force 会使用本地分支的提交覆盖远端推送分支的提交。也就是说，如果其他人在相同的分支推送了新的提交，你的这一举动将“删除”他的那些提交！就算在强制推送之前先 fetch 并且 merge 或 rebase 了也是不安全的，因为这些操作到推送之间依然存在时间差，别人的提交可能发生在这个时间差之内。"],["body","\n"],["body","使用了 --force-with-lease 参数之后，上面那种安全问题就没有那么危险了。"],["body","\n"],["body","请特别注意——如果你 fetch 之后在本地的 origin 相关分支上已经看到了别人的提交，依然进行强制推送，你还是会覆盖别人的提交。也就是说，--force-with-lease 解决的是本地仓库不够新时，依然覆盖了远端新仓库的问题，如果你执意想要覆盖远端提交，只需要先 fetch 再推送，它也不会拒绝的。"],["body","\n\n"],["headingLink","一个原则"],["heading","一个原则"],["body","\n"],["body","说到最后，还有一个不得不提的原则："],["body","\n"],["body","\n"],["body","永远不要对已经推到主干分支服务器或者团队其他成员的提交进行变基，我们选择变基还是合并的范围应该在自己当前工作范围内。"],["body","\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/命令集合/LOG.html"],["title","LOG - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["body","1、git log -- filename(git log filename)\n可以看到该文件相关的commit记录"],["body","\n"],["body","2、git log -p filename -2\n可以显示该文件前2次提交的diff"],["body","\n"],["body","3、git show comit_id filename\n可以查看某次提交中的某个文件变化"],["body","\n"],["body","4、git show commit_id\n查看某次提交"],["body","\n"],["body","5、gitk --follow filename\n以图形化界面的方式显示修改列表"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/命令集合/RESET.html"],["title","RESET - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["body","git reset：重置暂存区（索引区） \n\ngit reset –soft：重置版本库 \n\ngit reset –mixed：重置版本库、重置暂存区 \n\ngit reset –hard:重置版本库、重置暂存区、重置工作区\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/命令集合/AMEND.html"],["title","AMEND - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","amend"],["heading","AMEND"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/命令集合/MERGE.html"],["title","MERGE - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","git-merge简介"],["heading","git-merge简介"],["body","\n"],["body","git-merge命令是用于从指定的commit(s)合并到当前分支的操作。"],["body","\n"],["body","这里的指定commit(s)是指从这些历史commit节点开始，一直到当前分开的时候。"],["body","\n"],["headingLink","合并方式"],["heading","合并方式"],["body","\n"],["body","Git merge的时候，有几种合并方式可以选择"],["body","\n"],["headingLink","--ff"],["heading","--ff"],["body","\n"],["body","如果能从一个分支的commit  直接 移动 到 被合并分支。则直接 更新 分支的 指针，而不会创建一个合并的提交"],["body","\n"],["body","这是默认行为，fast-forwar模式"],["body","\n"],["headingLink","--no-ff"],["heading","--no-ff"],["body","\n"],["body","即使能 fast-forward也 要 创建一个commit"],["body","\n"],["headingLink","--squash"],["heading","--squash"],["body","\n"],["body","将待合并的 分支与当前分支的 最近共同 的祖先结点 到  待合并的分支 的 head节点的所有 提交压缩成一个"],["body","\n"],["headingLink","--no-squash"],["heading","--no-squash"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/命令集合/PUSH.html"],["title","PUSH - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","简介"],["heading","简介"],["body","\n"],["body","推送提交到远程"],["body","\n"],["headingLink","命令格式"],["heading","命令格式"],["body","\n"],["body","git push <remotename> <commit SHA>:<remotebranchname>\n"],["body","\n\n"],["body","<remotename> 远程仓库名，默认为origin"],["body","\n"],["body","<commit SHA> 提交的唯一码"],["body","\n"],["body","<remotebranchname> 远程分支名"],["body","\n\n"],["body","HEAD: 是一个特别的指针，它是一个指向你正在工作的本地分支的指针，可以把它当做本地分支的别名"],["body","\n"],["body","refs/for/<branchName> :意义在于我们提交代码到服务器之后是需要经过code review 之后才能进行merge的"],["body","\n"],["headingLink","推送非裸仓库"],["heading","推送非裸仓库"],["body","\n"],["body","要保证工作区、索引区干净，且 因为远程仓库不能包含本地尚不存在的提交"],["body","\n"],["body","git push git+ssh://root@HostName:/disk2/xjq --force\n\n# 将当前变更 推送到 版本库、索引区\ngit config receive.denyCurrentBranch updateInstead\n# 将远程变更 推送到 版本库\ngit config receive.denyCurrentBranch ignore\n# 将远程变更 推送到 版本库\ngit config receive.denyCurrentBranch warn\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/杂项.html"],["title","杂项 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","工作区暂存区版本库"],["heading","工作区、暂存区、版本库"],["body","\n"],["headingLink","暂存区"],["heading","暂存区"],["body","\n"],["body","Git暂存区（stage，或称为index）的设计是Git最成功的设计之一，也是最难理解的一个设计。"],["body","\n"],["body","在版本库.git目录下，有一个index文件"],["body","\n"],["headingLink","记录时间戳"],["heading","记录时间戳"],["body","\n"],["body","当执行git status命令（或者git diff命令）扫描工作区改动的时候，先依据.git/index文件中记录的（工作区跟踪文件的）时间戳、长度等信息判断工作区文件是否改变"],["body","\n"],["body","如果工作区的文件时间戳改变，说明文件的内容可能被改变了，需要要打开文件，读取文件内容，和更改前的原始文件相比较，判断文件内容是否被更改"],["body","\n"],["body","如果文件内容没有改变，则将该文件新的时间戳记录到.git/index文件中"],["body","\n"],["body","因为判断文件是否更改，使用时间戳、文件长度等信息进行比较要比通过文件内容比较要快的多，所以Git这样的实现方式可以让工作区状态扫描更快速的执行，这也是Git高效的因素之一。"],["body","\n"],["headingLink","虚拟工作区"],["heading","虚拟工作区"],["body","\n"],["body","文件.git/index实际上就是一个包含文件索引的目录树，像是一个虚拟的工作区"],["body","\n"],["body","这个虚拟工作区的目录树中，记录了文件名、文件的状态信息（时间戳、文件长度等）"],["body","\n"],["body","文件的内容并不存储其中，而是保存在Git对象库.git/objects目录中，文件索引建立了文件和对象库中对象实体之间的对应"],["body","\n"],["body","下面这个图展示了工作区、版本库中的暂存区和版本库之间的关系。"],["body","\n"],["body","\n\n"],["body","图中左侧为工作区，右侧为版本库。在版本库中标记为index的区域是暂存区（stage，亦称index），标记为master的是master分支所代表的目录树。"],["body","\n"],["body","图中可以看出此时HEAD实际是指向master分支的一个“游标”。所以图示的命令中出现HEAD的地方可以用master来替换。"],["body","\n\n\n"],["body","\n"],["body","图中的objects标识的区域为Git的对象库，实际位于.git/objects目录下"],["body","\n"],["body","\n"],["body","\n"],["body","当对工作区修改（或新增）的文件执行git add命令时，暂存区的目录树被更新，同时工作区修改（或新增）的文件内容被写入到对象库中的一个新的对象中，而该对象的ID被记录在暂存区的文件索引中。"],["body","\n"],["body","\n"],["body","\n"],["body","当执行提交操作（git commit）时，暂存区的目录树写到版本库（对象库）中，master分支会做相应的更新。即master最新指向的目录树就是提交时原暂存区的目录树。"],["body","\n"],["body","\n"],["body","\n"],["body","当执行git reset HEAD命令时，暂存区的目录树会被重写，被master分支指向的目录树所替换，但是工作区不受影响。"],["body","\n"],["body","\n"],["body","\n"],["body","当执行**git rm –cached <file>**命令时，会直接从暂存区删除文件，工作区则不做出改变。"],["body","\n"],["body","\n"],["body","\n"],["body","当执行**git checkout .或者git checkout – <file>**命令时，会用暂存区全部或指定的文件替换工作区的文件。这个操作很危险，会清除工作区中未添加到暂存区的改动。"],["body","\n"],["body","\n"],["body","\n"],["body","当执行**git checkout HEAD .或者git checkout HEAD <file>**命令时，会用HEAD指向的master分支中的全部或者部分文件替换暂存区和以及工作区中的文件。这个命令也是极具危险性的，因为不但会清除工作区中未提交的改动，也会清除暂存区中未提交的改动。"],["body","\n"],["body","\n\n"],["body","使用暂存区或者版本库的文件 替换工作区的文件"],["body","\n"],["body","#全部替换\ngit checkout .\n#替换某个文件\ngit checkout – <file>\n#用HEAD指向的master分支中的全部或者部分文件替换暂存区和以及工作区中的文件。\n# v->w、s\ngit checkout HEAD .\n# 指定文件替换 v->w、s\ngit checkout HEAD <file>\n"],["body","\n"],["body","#清除当前工作区中没有加入版本库的文件和目录 f表示强制执行、d表示递归清理目录\ngit clean -fd\n"],["body","\n"],["body","# --pretty=raw参数以便显示每个提交对象的parent、tree属性\ngit log -2 --pretty=raw\n"],["body","\n"],["body","# -s 精简输出 -b输出分支名\ngit status -s -b\n"],["body","\n"],["headingLink","暂存区目录树的浏览"],["heading","暂存区目录树的浏览"],["body","\n"],["body","有什么办法能够像查看工作区一样的，直观的查看暂存区以及HEAD当中的目录树么？"],["body","\n"],["body","对于HEAD（版本库中当前提交）指向的目录树，可以使用Git底层命令git ls-tree来查看。"],["body","\n"],["body","# 使用-l参数，可以显示文件的大小。上面welcome.txt大小为25字节。\n$ git ls-tree -l HEAD\n100644 blob fd3c069c1de4f4bc9b15940f490aeb48852f3c42      25    welcome.txt\n"],["body","\n"],["body","要显示暂存区的目录树，可以使用git ls-files命令。"],["body","\n"],["body","$ git ls-files -s\n100644 18832d35117ef2f013c4009f5b2128dfaeff354f 0       a/b/c/hello.txt\n100644 51dbfd25a804c30e9d8dc441740452534de8264b 0       welcome.txt\n"],["body","\n"],["body","注意这个输出和之前使用git ls-tree命令输出不一样，如果想要使用git ls-tree命令"],["body","\n"],["body","需要先将暂存区的目录树写入Git对象库（用git write-tree命令），然后在针对git write-tree命令写入的 tree 执行git ls-tree命令。"],["body","\n"],["body","$ git write-tree\n9431f4a3f3e1504e03659406faa9529f83cd56f8\n$ git ls-tree -l 9431f4a\n040000 tree 53583ee687fbb2e913d18d508aefd512465b2092       -    a\n100644 blob 51dbfd25a804c30e9d8dc441740452534de8264b      34    welcome.txt\n"],["body","\n"],["headingLink","git-diff"],["heading","Git diff"],["body","\n"],["body","比较图"],["body","\n"],["body","\n"],["headingLink","git对象"],["heading","GIT对象"],["body","\n"],["headingLink","查看git对象结构"],["heading","查看GIT对象结构"],["body","\n"],["body","查看git对象类型"],["body","\n"],["body","$ git cat-file -t e695606\ncommit\n$ git cat-file -t f58d\ntree\n$ git cat-file -t a0c6\ncommit\n"],["body","\n"],["body","查看git对象结构"],["body","\n"],["body","$ git cat-file -p e695606\ntree f58da9a820e3fd9d84ab2ca2f1b467ac265038f9\nparent a0c641e92b10d8bcca1ed1bf84ca80340fdefee6\nauthor Jiang Xin <jiangxin@ossxp.com> 1291022581 +0800\ncommitter Jiang Xin <jiangxin@ossxp.com> 1291022581 +0800\n\n#查看tree对象的结构 \nweisanju@B-3BVVQ05P-2126 demo01 % git cat-file -p  df36838bf559b620ec5abdb30e6a593bb9af4ec7\n100644 blob c86a7936f2a61955b463708abdf29b7567124f27\ta.txt\n100644 blob 3da1ec26e9c8512eae062868a9ff9bae47e5625b\tb.txt\n100644 blob fa64294350b06d9617cbfa34e0a60aca01b6c8be\ttest.ini\n# tree对象中存在 blob对象、查看blob对象对应ID的 类别：就是blob类型\nweisanju@B-3BVVQ05P-2126 demo01 % git cat-file -t c86a7936f2a61955b463708abdf29b7567124f27\nblob\n\n#查询blob对象ID对应的 结构：就是文件内容\nweisanju@B-3BVVQ05P-2126 demo01 % git cat-file -p c86a7936f2a61955b463708abdf29b7567124f27\nHelloWorld2\n#这些对象保存在 Git库中的objects目录下（ID的前两位作为目录名，后38位作为文件名）\n\n\n# 用下面的命令可以看到所有这些对象（tree、blob、commit）在对象库中的实际位置。\nweisanju@B-3BVVQ05P-2126 demo01 % ls -l  .git/objects/c8/6a7936f2a61955b463708abdf29b7567124f27\n-r--r--r--  1 weisanju  staff  28 11 20 20:18 .git/objects/c8/6a7936f2a61955b463708abdf29b7567124f27\n"],["body","\n"],["body","对象之间的关系"],["body","\n"],["body","\n"],["headingLink","命名空间"],["heading","命名空间"],["body","\n"],["body","head与master的关系"],["body","\n"],["body","# 以下三个命令有同样的输出,在当前版本库中，HEAD、master和refs/heads/master具有相同的指向。\ngit log -1 HEAD\ngit log -1 master\ngit log -1 refs/heads/master\n"],["body","\n"],["body","# 找到了四个文件，其中在.git/logs目录下的文件稍后再予以关注，现在把目光锁定在.git/HEAD和.git/refs/heads/master上。\n$ find .git -name HEAD -o -name master\n.git/HEAD\n.git/logs/HEAD\n.git/logs/refs/heads/master\n.git/refs/heads/master\n# 输出文件内容\nweisanju@B-3BVVQ05P-2126 demo01 % cat .git/HEAD\nref: refs/heads/master\n# 输出文件内容：指向最新的提交。这是一个 commit对象\nweisanju@B-3BVVQ05P-2126 demo01 % cat .git/refs/heads/master\n0e1ac87c48749584745bbbcfa72798135a45c553\n"],["body","\n\n"],["body","\n"],["body","原来分支master指向的是一个提交ID（最新提交）。这样的分支实现是多么的巧妙啊："],["body","\n"],["body","\n"],["body","\n"],["body","可以从任何提交开始建立一条历史跟踪链，那么用一个文件指向这个链条的最新提交，那么这个文件就可以用于追踪整个提交历史了"],["body","\n"],["body","\n"],["body","\n"],["body","这个文件就是.git/refs/heads/master文件。"],["body","\n"],["body","\n"],["body","\n"],["body","目录.git/refs是保存引用的命名空间，其中.git/refs/heads目录下的引用又称为分支"],["body","\n"],["body","\n"],["body","\n"],["body","对于分支既可以使用正规的长格式的表示法，如refs/heads/master，也可以去掉前面的两级目录用master来表示"],["body","\n"],["body","\n"],["body","\n"],["body","Git 有一个底层命令git rev-parse可以用于显示引用对应的提交ID。"],["body","\n"],["body","\n\n"],["body","#可以看出它们都指向同一个对象\n$ git rev-parse master\ne695606fc5e31b2ff9038a48a3d363f4c21a3d86\n$ git rev-parse refs/heads/master\ne695606fc5e31b2ff9038a48a3d363f4c21a3d86\n$ git rev-parse HEAD\ne695606fc5e31b2ff9038a48a3d363f4c21a3d86\n"],["body","\n"],["body","\n"],["headingLink","sha1哈希值到底是什么如何生成的"],["heading","SHA1哈希值到底是什么，如何生成的？"],["body","\n"],["body","哈希(hash)是一种数据摘要算法（或称散列算法），是信息安全领域当中重要的理论基石。"],["body","\n"],["body","该算法将任意长度的输入经过散列运算转换为固定长度的输出"],["body","\n"],["body","固定长度的输出可以称为对应的输入的数字摘要或哈希值"],["body","\n"],["body","比较著名的摘要算法有：MD5和SHA1。Linux下sha1sum命令可以用于生成摘要。"],["body","\n"],["body","提交hash"],["body","\n"],["body","# 看看HEAD对应的提交的内容。使用**git cat-file**命令。\ngit cat-file commit HEAD\ngit cat-file commit HEAD | wc -c\n\n# 在提交信息的前面加上commit内容、commit 234<null>（<null>为空字符），然后执行SHA1哈希算法。\n$ ( printf \"commit 234\\000\"; git cat-file commit HEAD ) | sha1sum\ne695606fc5e31b2ff9038a48a3d363f4c21a3d86\n\n#查看提交的hash\n$ git rev-parse HEAD\ne695606fc5e31b2ff9038a48a3d363f4c21a3d86\n"],["body","\n"],["body","文件内容hash"],["body","\n"],["body","# 查看文件的内容\ngit cat-file blob HEAD:welcome.txt\n \n#在文件内容的前面加上blob 25<null>的内容，然后执行SHA1哈希算法。\n$ ( printf \"blob 25\\000\"; git cat-file blob HEAD:welcome.txt ) | sha1sum\nfd3c069c1de4f4bc9b15940f490aeb48852f3c42  -\n\n# 查看文件内容的hash\n$ git rev-parse HEAD:welcome.txt\nfd3c069c1de4f4bc9b15940f490aeb48852f3c42\n"],["body","\n"],["body","tree对象hash"],["body","\n"],["body","# HEAD对应的树的内容共包含39个字节。\ngit cat-file tree HEAD^{tree} | wc -c\n# 在树的内容的前面加上tree 39<null>的内容，然后执行SHA1哈希算法\n( printf \"tree 39\\000\"; git cat-file tree HEAD^{tree} ) | sha1sum\nf58da9a820e3fd9d84ab2ca2f1b467ac265038f9  -\n# 查看 head所执行的提交的 tree对象的hash\ngit rev-parse HEAD^{tree}\n"],["body","\n"],["body","在后面学习里程碑（Tag）的时候，会看到Tag对象（轻量级Tag除外）也是采用类似方法在对象库中存储的。"],["body","\n"],["headingLink","访问git库中的对象"],["heading","访问GIT库中的对象"],["body","\n"],["headingLink","规则"],["heading","规则"],["body","\n\n"],["body","\n"],["body","采用部分的SHA1哈希值。不必写全40位的哈希值，只采用开头的部分，不和现有其他的冲突即可。"],["body","\n"],["body","\n"],["body","\n"],["body","使用master代表分支master中最新的提交，使用全称refs/heads/master亦可"],["body","\n"],["body","\n"],["body","\n"],["body","使用HEAD代表版本库中最近的一次提交。"],["body","\n"],["body","\n"],["body","\n"],["body","符号 ^`可以用于指代父提交。例如："],["body","\n\n"],["body","HEAD^代表版本库中上一次提交，即最近一次提交的父提交。"],["body","\n"],["body","HEAD^^则代表HEAD^的父提交。"],["body","\n\n"],["body","\n"],["body","\n"],["body","对于一个提交有多个父提交，可以在符号^后面用数字表示是第几个父提交。例如："],["body","\n\n"],["body","a573106^2含义是提交a573106的多个父提交中的第二个父提交。"],["body","\n"],["body","HEAD^1相当于HEAD^含义是HEAD多个父提交中的第一个。"],["body","\n"],["body","HEAD^^2含义是HEAD^（HEAD父提交）的多个父提交中的第二个。"],["body","\n\n"],["body","\n"],["body","\n"],["body","符号~<n>也可以用于指代祖先提交。下面两个表达式效果等同："],["body","\n"],["body","a573106~5\na573106^^^^^\n"],["body","\n"],["body","\n"],["body","\n"],["body","提交所对应的树对象，可以用类似如下的语法访问。"],["body","\n"],["body","a573106^{tree}\n"],["body","\n"],["body","\n"],["body","\n"],["body","某一此提交对应的文件对象，可以用如下的语法访问。"],["body","\n"],["body","a573106:path/to/file\n"],["body","\n"],["body","\n"],["body","\n"],["body","访问暂存区中的文件对象，可以用如下的语法访问。"],["body","\n"],["body",":path/to/file\n"],["body","\n"],["body","\n\n"],["headingLink","重置"],["heading","重置"],["body","\n"],["headingLink","重置原理"],["heading","重置原理"],["body","\n"],["body","\n"],["body","移动 refs/heads/<branch-name> 中的提交"],["body","\n"],["body","\n"],["body","$ cat .git/refs/heads/master\n4902dc375672fbf52a226e0354100b75d4fe31e3\n$ git log --graph --oneline\n* 4902dc3 does master follow this new commit?\n* e695606 which version checked in?\n* a0c641e who does commit?\n* 9e8a761 initialized.\n"],["body","\n"],["body","引用refs/heads/master就好像是一个游标，在有新的提交发生的时候指向了新的提交。"],["body","\n"],["body","Git提供了git reset命令，可以将“游标”指向任意一个存在的提交ID。下面的示例就尝试人为的更改游标。（注意下面的命令中使用了--hard参数，会破坏工作区未提交的改动，慎用。"],["body","\n"],["headingLink","用reflog挽救错误的重置"],["heading","用reflog挽救错误的重置"],["body","\n"],["body","如果没有记下重置前master分支指向的提交ID，想要重置回原来的提交真的是一件麻烦的事情（去对象库中一个一个地找）"],["body","\n"],["body","幸好Git提供了一个挽救机制，通过.git/logs目录下日志文件记录了分支的变更。"],["body","\n"],["body","默认非裸版本库（带有工作区）都提供分支日志功能，这是因为带有工作区的版本库都有如下设置："],["body","\n"],["body","$ git config core.logallrefupdates\ntrue\n"],["body","\n"],["body","查看一下master分支的日志文件"],["body","\n"],["body",".git/logs/refs/heads/master中的内容。下面命令显示了该文件的最后几行。"],["body","\n"],["body","为了排版的需要，还将输出中的40位的SHA1提交ID缩短。"],["body","\n"],["body","tail -5 .git/logs/refs/heads/master\n"],["body","\n"],["body","显示日志"],["body","\n"],["body","git reflog show master | head -5\n"],["body","\n"],["body","使用git reflog的输出和直接查看日志文件最大的不同在于显示顺序的不同"],["body","\n"],["body","即最新改变放在了最前面显示，而且只显示每次改变的最终的SHA1哈希值。"],["body","\n"],["body","还有个重要的区别在于使用git reflog的输出中还提供一个方便易记的表达式"],["body","\n"],["body","<refname>@{<n>}\n这个表达式的含义是引用<refname>之前第<n>次改变时的SHA1哈希值。\n"],["body","\n"],["body","那么将引用master切换到两次变更之前的值，可以使用下面的命令。"],["body","\n\n"],["body","\n"],["body","重置master为两次改变之前的值。"],["body","\n"],["body","$ git reset --hard master@{2}\nHEAD is now at 4902dc3 does master follow this new commit?\n"],["body","\n"],["body","\n\n"],["body","此时如果再用git reflog查看，会看到恢复master的操作也记录在日志中了。"],["body","\n"],["headingLink","命令详细"],["heading","命令详细"],["body","\n"],["body","用法一： git reset [-q] [<commit>] [--] <paths>...\n用法二： git reset [--soft | --mixed | --hard | --merge | --keep] [-q] [<commit>]\n"],["body","\n"],["body","其中 <commit> 都是可选项，可以使用引用或者提交ID，如果省略 <commit> 则相当于使用了HEAD的指向作为提交ID。"],["body","\n"],["body","上面列出的两种用法的区别在于，第一种用法在命令中包含路径<paths>,为了避免路径和引用（或者提交ID）同名而冲突，可以在<paths>前用两个连续的短线（减号）作为分隔。"],["body","\n"],["body","替换暂存区"],["body","\n"],["body","第一种用法（包含了路径<paths>的用法）不会重置引用，更不会改变工作区，而是用指定提交状态（<commit>）下的文件（<paths>）替换掉暂存区中的文件"],["body","\n"],["body","例如命令**git reset HEAD <paths>相当于取消之前执行的git add <paths>**命令时改变的暂存区。"],["body","\n"],["body","第二种用法（不使用路径<paths>的用法）则会重置引用。根据不同的选项，可以对暂存区或者工作区进行重置。参照下面的版本库模型图，来看一看不同的参数对第二种重置语法的影响。"],["body","\n"],["body","重置选项"],["body","\n"],["body","\n"],["body","命令格式: git reset [–soft | –mixed | –hard ] []"],["body","\n\n"],["body","\n"],["body","使用参数--hard，如：git reset –hard 。"],["body","\n"],["body","会执行上图中的1、2、3全部的三个动作。即："],["body","\n\n"],["body","替换引用的指向。引用指向新的提交ID。"],["body","\n"],["body","替换暂存区。替换后，暂存区的内容和引用指向的目录树一致。"],["body","\n"],["body","替换工作区。替换后，工作区的内容变得和暂存区一致，也和HEAD所指向的目录树内容相同。"],["body","\n\n"],["body","\n"],["body","\n"],["body","使用参数--soft，如:git reset –soft 。"],["body","\n"],["body","会执行上图中的操作1。即只更改引用的指向，不改变暂存区和工作区。"],["body","\n"],["body","\n"],["body","\n"],["body","使用参数--mixed或者不使用参数（缺省即为--mixed），如:git reset 。"],["body","\n"],["body","会执行上图中的操作1和操作2。即更改引用的指向以及重置暂存区，但是不改变工作区。"],["body","\n"],["body","\n\n"],["body","重置命令的不同用法"],["body","\n\n"],["body","\n"],["body","git reset：重置暂存区"],["body","\n"],["body","\n"],["body","\n"],["body","git reset HEAD：同上"],["body","\n"],["body","\n"],["body","\n"],["body","git reset – filename ：重置暂存区"],["body","\n"],["body","\n"],["body","\n"],["body","git reset HEAD filename 同上"],["body","\n"],["body","\n"],["body","\n"],["body","git reset –soft HEAD^"],["body","\n\n"],["body","版本库回退一个版本、暂存区没有、所以暂存区在与版本库比对时就会多出来一个 提交"],["body","\n"],["body","现象：将上一次提交拉回到暂存区"],["body","\n\n"],["body","在之前曾经介绍过一个修补提交命令git commit –amend，用于对最新的提交进行重新提交以修补错误的提交说明或者错误的提交文件。修补提交命令实际上相当于执行了下面两条命令。（注：文件.git/COMMIT_EDITMSG保存了上次的提交日志）"],["body","\n"],["body","$ git reset --soft HEAD^\n$ git commit -e -F .git/COMMIT_EDITMSG\n"],["body","\n"],["body","\n"],["body","\n"],["body","git reset HEAD^ "],["body","\n"],["body","\n"],["body","\n"],["body","git reset –mixed HEAD^"],["body","\n\n"],["body","版本库回退一个版本、暂存库回退一个版本、工作区不回退，则工作区多出"],["body","\n\n"],["body","\n"],["body","\n"],["body","git reset –hard HEAD^"],["body","\n\n"],["body","三个库全部回退"],["body","\n\n"],["body","\n\n"],["headingLink","恢复进度"],["heading","恢复进度"],["body","\n"],["headingLink","使用git-stash"],["heading","使用git stash"],["body","\n"],["body","命令git stash可以用于保存和恢复工作进度"],["body","\n\n"],["body","\n"],["body","命令：git stash"],["body","\n"],["body","保存当前工作进度。会分别对暂存区和工作区的状态进行保存。"],["body","\n"],["body","\n\n\n"],["body","\n"],["body","命令：git stash list"],["body","\n"],["body","显示进度列表。此命令显然暗示了git stash可以多次保存工作进度，并且在恢复的时候进行选择。"],["body","\n"],["body","\n"],["body","\n"],["body","命令： git stash show"],["body","\n"],["body","显示变更"],["body","\n"],["body","\n"],["body","\n"],["body","命令：git stash pop [–index] [<stash>]"],["body","\n"],["body","会恢复最新保存的工作进度，并将恢复的工作进度从存储的工作进度列表中清除。"],["body","\n"],["body","\n"],["body","\n"],["body","命令：git stash apply [–index] [<stash>]"],["body","\n"],["body","除了不删除恢复的进度之外，其余和git stash pop命令一样。"],["body","\n"],["body","\n"],["body","\n"],["body","命令：git stash drop []"],["body","\n"],["body","删除一个存储的进度。缺省删除最新的进度。"],["body","\n"],["body","\n"],["body","\n"],["body","命令：git stash clear"],["body","\n"],["body","删除所有存储的进度。"],["body","\n"],["body","\n\n\n"],["body","\n"],["body","命令：git stash branch  "],["body","\n"],["body","基于进度创建分支。对了，还没有讲到分支呢。;)"],["body","\n"],["body","\n\n"],["headingLink","探秘git-stash"],["heading","探秘git stash"],["body","\n"],["body","在执行git stash命令时，Git实际调用了一个脚本文件实现相关的功能，这个脚本的文件名就是git-stash。看看git-stash安装在哪里了。"],["body","\n"],["body","实际上在1.5.4之前的版本，Git会安装这些一百多个以git-格式命名的程序到可执行路径中。这样做的唯一好处就是不用借助任何扩展机制就可以实现命令行补齐：即键入git-后，连续两次键入<Tab>键，就可以把这一百多个命令显示出来。这种方式随着Git子命令的增加越来越显得混乱，因此在1.5.4版本开始，不再提供git-<cmd>格式的命令，而是用唯一的git命令。而之前的名为**git-<cmd>**的子命令则保存在非可执行目录下，由Git负责加载。"],["body","\n"],["body","最早很多Git命令都是用Shell或者Perl脚本语言开发的，在Git的发展中一些对运行效率要求高的命令用C语言改写。而git-stash（至少在Git 1.7.3.2版本）还是使用Shell脚本开发的，研究它会比研究用C写的命令要简单的多。"],["body","\n"],["body","显示 stash 引用空间的提交"],["body","\n"],["body","# 可以看到在提交关系图可以看到进度保存的最新提交是一个合并提交。最新的提交说明中有WIP字样（是Work In Progess的简称），说明代表了工作区进度。而最新提交的第二个父提交（上图中显示为第二个提交）有index on master字样，说明这个提交代表着暂存区的进度。\ngit log --graph --pretty=raw  refs/stash -2\n"],["body","\n"],["body","通过 logs/refs/stash 记录 stash的改变记录"],["body","\n"],["body"," ls -l .git/refs/stash .git/logs/refs/stash\n # 记录了 stash引用的变更日志\n git reflog show refs/stash\n"],["body","\n"],["body","果然上面显示的三个提交对应的三棵树各不相同。查看一下差异。用“原基线”代表进度保存时版本库的状态；用“原暂存区”代表进度保存时暂存区的状态，；用“原工作区”代表进度保存时工作区的状态。"],["body","\n\n"],["body","\n"],["body","原基线和原暂存区的差异比较。"],["body","\n"],["body","$ git diff stash@{1}^2^ stash@{1}^2\n\n"],["body","\n"],["body","\n"],["body","\n"],["body","原暂存区和原工作区的差异比较。"],["body","\n"],["body","$ git diff stash@{1}^2 stash@{1}\n"],["body","\n"],["body","\n"],["body","\n"],["body","原基线和原工作区的差异比较。"],["body","\n"],["body","$ git diff stash@{1}^1 stash@{1}\n"],["body","\n"],["body","\n\n"],["headingLink","设置远程跟踪分支"],["heading","设置远程跟踪分支"],["body","\n"],["body","git branch --set-upstream-to=origin/branch branch\n"],["body","\n"],["headingLink","删除远程分支"],["heading","删除远程分支"],["body","\n"],["body","git push remote_name -d remote_branch_name\n"],["body","\n"],["headingLink","创建远程分支"],["heading","创建远程分支"],["body","\n"],["body","git push <远程主机名> <本地分支名>（省略远程分支名，把本地分支推送到它追踪的远程分支）\n"],["body","\n"],["headingLink","推送分支并且指定默认主机"],["heading","推送分支，并且指定默认主机"],["body","\n"],["body","如果当前分支与多个主机存在追踪关系，则可以使用 -u 选项指定一个默认主机，这样后面就可以不加任何参数使用git push"],["body","\n"],["body","git push -u origin master\n"],["body","\n"],["headingLink","提交本地test分支到远程的master分支"],["heading","提交本地test分支到远程的master分支："],["body","\n"],["body","git push <远程主机名> <本地分支名>:<远程分支名>\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/git分支.html"],["title","git分支 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","分支简介"],["heading","分支简介"],["body","\n\n"],["body","在git中,每一次提交都相当于从当前版本重做一次快照,而分支类似于指针, 指向不同的快照"],["body","\n"],["body","当前工作目录所处的分支 则是用 head指针 标识"],["body","\n"],["body","新建分支类似于新建一个指针 指向某一个快照"],["body","\n"],["body","切换分支 将头指针 移向某一分支指针"],["body","\n\n"],["headingLink","操作命令"],["heading","操作命令"],["body","\n"],["body","#新建分支\ngit branch testing\n#分支切换,移动头指针在该分支指针上\ngit checkout testing\n#新建分支自动切换\ngit checkout -b <newbranchname>\n#删除分支\ngit branch -d hotfix\n"],["body","\n"],["headingLink","分支合并"],["heading","分支合并"],["body","\n"],["headingLink","命令"],["heading","命令"],["body","\n"],["body","#检出\ngit checkout master\n#合并\ngit merge iss53\n"],["body","\n\n"],["body","快速合并"],["body","\n\n"],["body","在合并的时候，你应该注意到了“快进（fast-forward）”这个词"],["body","\n"],["body","如果master分支 跟 iss53 位于同一链条则 不用进行合并"],["body","\n"],["body","直接 将 master分支 移动到 iss53分支"],["body","\n\n"],["body","\n"],["body","三方合并"],["body","\n"],["body","Git 会使用两个分支的末端所指的快照（C4 和 C5）以"],["body","\n"],["body","这两个分支的公共祖先（C2），做一个简单的三方合并,"],["body","\n"],["body","如果没有冲突,则会产生一个合并提交"],["body","\n"],["body","\n\n"],["headingLink","冲突时的分支合并"],["heading","冲突时的分支合并"],["body","\n\n"],["body","\n"],["body","对同一个文件的同一个部分进行了不同的修改,就会产生冲突此时 Git 做了合并，但是没有自动地创建一个新的合并提交"],["body","\n"],["body","\n"],["body","\n"],["body","git status 命令来查看那些因包含合并冲突而处于未合并（unmerged）状态的文件"],["body","\n"],["body","\n"],["body","\n"],["body","冲突内容类似 下面"],["body","\n"],["body","<<<<<<< HEAD:index.html\n<div id=\"footer\">contact : email.support@github.com</div>\n=======\n<div id=\"footer\">\n please contact us at support@github.com\n</div>\n>>>>>>> iss53:index.html\n"],["body","\n"],["body","要选择 上面下面的一个然后提交"],["body","\n"],["body","\n\n"],["headingLink","分支管理"],["heading","分支管理"],["body","\n"],["body","#查看当前分支的最后一次提交\ngit branch -v\n#查看哪些分支已合并到当前分支\ngit branch --merged\n#查看合并的分支 未完成的合并工作\ngit branch --no-merged\n\n"],["body","\n"],["headingLink","分支开发工作流"],["heading","分支开发工作流"],["body","\n"],["body","gitflow"],["body","\n"],["headingLink","远程分支"],["heading","远程分支"],["body","\n"],["headingLink","远程跟踪分支"],["heading","远程跟踪分支"],["body","\n\n"],["body","\n"],["body","远程跟踪分支是远程分支状态的引用。"],["body","\n"],["body","\n"],["body","\n"],["body","它们是你无法移动的本地引用。"],["body","\n"],["body","\n"],["body","\n"],["body","一旦你进行了网络通信， Git 就会为你移动它们以精确反映远程仓库的状态。"],["body","\n"],["body","\n"],["body","\n"],["body","请将它们看做书签， 这样可以提醒你该分支在远程仓库中的位置就是你最后一次连接到它们的位置。"],["body","\n"],["body","\n"],["body","\n"],["body","它们以 <remote>/<branch> 的形式命名"],["body","\n"],["body","\n\n"],["headingLink","git-clone-的操作"],["heading","git clone 的操作"],["body","\n"],["headingLink","远程分支的开始"],["heading","远程分支的开始"],["body","\n\n"],["body","Git 的 clone 命令会为你自动将其命名为 origin"],["body","\n"],["body","拉取它的所有数据， 创建一个指向它的 master 分支的指针,并且在本地将其命名为 origin/master,这个即为远程跟踪分支"],["body","\n"],["body","也会创建一个本地分支 指向 origin/master"],["body","\n\n"],["headingLink","远程分支的分叉"],["heading","远程分支的分叉"],["body","\n\n"],["body","\n"],["body","你在本地的 master 分支做了一些工作，在同一段时间内有其他人推送提交到 git.ourcompany.com 并且更新了它的 master 分支"],["body","\n"],["body","\n"],["body","\n"],["body","只要你保持不与 origin 服务器连接（并拉取数据），你的 origin/master 指针就不会移动。"],["body","\n"],["body","\n\n"],["body","#更新远程跟踪分支   \ngit fetch  <remote>\n#将本地分支与远程分支合并\ngit merge origin/serverfix\n#推送 本地分支 到远程分支\ngit push origin serverfix\n#推送到远程 origin, 将本地serverfix推送到 远程servefix\ngit push origin serverfix:serverfix\n\n"],["body","\n"],["headingLink","分支跟踪"],["heading","分支跟踪"],["body","\n"],["body","clone通常会自动地创建一个跟踪 origin/master 的 master 分支"],["body","\n"],["headingLink","手动跟踪某个分支"],["heading","手动跟踪某个分支"],["body","\n"],["body","git checkout --track origin/serverfix"],["body","\n"],["headingLink","checkout捷径"],["heading","checkout捷径"],["body","\n"],["body","如果你尝试检出的分支 (a) 不存在且 (b) 刚好只有一个名字与之匹配的远程分支，那么 Git 就会为你创建一个跟踪分支："],["body","\n"],["body","git checkout serverfix\n"],["body","\n"],["headingLink","自定义分支名称"],["heading","自定义分支名称"],["body","\n"],["body","git checkout -b sf origin/serverfix\n"],["body","\n"],["headingLink","查看所有设置的跟踪分支"],["heading","查看所有设置的跟踪分支"],["body","\n"],["body"," git branch -vv\n"],["body","\n"],["headingLink","示例"],["heading","示例"],["body","\n"],["body","$ git branch -vv\n  iss53     7e424c3 [origin/iss53: ahead 2] forgot the brackets\n  master    1ae2a45 [origin/master] deploying index fix\n* serverfix f8674d9 [teamone/server-fix-good: ahead 3, behind 1] this should do it\n  testing   5ea463a trying something new\n"],["body","\n"],["body","iss53 "],["body","\n"],["body","正在跟踪 origin/iss53 并且 “ahead” 是 2，意味着本地有两个提交还没有推送到服务器上。 "],["body","\n"],["body","master "],["body","\n"],["body","正在跟踪 origin/master 分支并且是最新的。 "],["body","\n"],["body","serverfix"],["body","\n"],["body","正在跟踪 teamone 服务器上的 server-fix-good 分支并且领先 3 落后 1， 意味着服务器上有一次提交还没有合并入同时本地有三次提交还没有推送。 "],["body","\n"],["body","testing "],["body","\n"],["body","分支并没有跟踪任何远程分支。"],["body","\n"],["body","​\t需要重点注意的一点是这些数字的值来自于你从每个服务器上最后一次抓取的数据。 这个命令并没有连接服务器，它只会告诉你关于本地缓存的服务器数据。 如果想要统计最新的领先与落后数字，需要在运行此命令前抓取所有的远程仓库。 可以像这样做："],["body","\n"],["body","git pull 的魔法经常令人困惑所以通常单独显式地使用 fetch 与 merge 命令会更好一些"],["body","\n"],["body","$ git fetch --all; git branch -vv\n"],["body","\n"],["headingLink","删除远程分支"],["heading","删除远程分支"],["body","\n"],["body"," git push origin --delete serverfix\n"],["body","\n"],["headingLink","变基"],["heading","变基"],["body","\n"],["headingLink","merge与rebase原理"],["heading","merge与rebase原理"],["body","\n"],["body","个人理解:将该分支的基底基于 某个你想合并的分支"],["body","\n"],["body","​\t在 Git 中整合来自不同分支的修改主要有两种方法：merge 以及 rebase。"],["body","\n"],["body","​\t它的原理是首先找到这两个分支（即当前分支 experiment、变基操作的目标基底分支 master） 的最近共同祖先 C2，然后对比当前分支相对于该祖先的历次提交，提取相应的修改并存为临时文件， 然后将当前分支指向目标基底 C3, 最后以此将之前另存为临时文件的修改依序应用。 （译注：写明了 commit id，以便理解，下同）"],["body","\n"],["body","​\tMerge是基于三方快照 ,合并提交生成最新的一个快照"],["body","\n"],["body","​\t变基:可以使用 rebase 命令将提交到某一分支上的所有修改都移至另一分支上，就好像“重新播放”一样。"],["body","\n"],["body","rebase的过程"],["body","\n"],["body","git checkout experiment\ngit rebase master\ngit checkout master\ngit merge experiment\n"],["body","\n"],["body","变基文档"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/git基础.html"],["title","git基础 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["body","GIT文档官网"],["body","\n"],["headingLink","获取git仓库"],["heading","获取GIT仓库"],["body","\n"],["body","一般有两种方式获取git仓库"],["body","\n\n"],["body","将尚未进行版本控制的本地目录转换为 Git 仓库；"],["body","\n"],["body","从其它服务器 克隆 一个已存在的 Git 仓库。"],["body","\n\n"],["headingLink","已存在目录初始化仓库"],["heading","已存在目录初始化仓库"],["body","\n"],["body","#初始化\ngit init\n#添加文件\ngit add *.c\n#提交\ngit commit -m\n"],["body","\n"],["headingLink","远程库克隆"],["heading","远程库克隆"],["body","\n"],["body","git clone <url>\n#这会在当前目录下创建一个名为 “libgit2” 的目录\ngit clone https://github.com/libgit2/libgit2\n#自定义目录名\ngit clone https://github.com/libgit2/libgit2 mylibgit\n#使用git协议或者 ssh协议\nuser@server:path/to/repo.git\n"],["body","\n"],["headingLink","记录文件更新到仓库"],["heading","记录文件更新到仓库"],["body","\n"],["headingLink","git文件状态"],["heading","git文件状态"],["body","\n"],["body","工作目录下的每一个文件都不外乎这两种状态: 已跟踪,未跟踪"],["body","\n\n"],["body","untracked"],["body","\n"],["body","unmodified"],["body","\n"],["body","modified"],["body","\n"],["body","staged"],["body","\n\n"],["body","查看状态"],["body","\n"],["body","git status\n#查看紧凑的状态\ngit status -s\n"],["body","\n"],["headingLink","忽略文件gitignore"],["heading","忽略文件.gitignore"],["body","\n"],["headingLink","gitingore格式规范"],["heading","gitingore格式规范"],["body","\n\n"],["body","所有空行或者以 # 开头的行都会被 Git 忽略。"],["body","\n"],["body","可以使用标准的 glob 模式匹配，它会递归地应用在整个工作区中。"],["body","\n"],["body","匹配模式可以以（/）开头防止递归。"],["body","\n"],["body","匹配模式可以以（/）结尾指定目录。"],["body","\n"],["body","要忽略指定模式以外的文件或目录，可以在模式前加上叹号（!）取反。"],["body","\n\n"],["headingLink","gitignore-pattern"],["heading","gitignore pattern"],["body","\n"],["body","所谓的 glob 模式是指 shell 所使用的简化了的正则表达式。"],["body","\n\n"],["body","星号匹配任意多个字符"],["body","\n"],["body","[abc]匹配任何在方括号的字符"],["body","\n"],["body","? 号一个任意字符"],["body","\n"],["body","[0-9]表示范围"],["body","\n"],["body","** 表示匹配任意中间目录 a/**/z"],["body","\n\n"],["headingLink","对比差异"],["heading","对比差异"],["body","\n"],["body","#只显示尚未暂存的改动\ngit diff\n#查看已经暂存的变化\ngit diff --staged\n"],["body","\n"],["headingLink","提交"],["heading","提交"],["body","\n"],["body","git commit\n\n//跳过使用暂存区域,自动把所有已经跟踪过的文件暂存起来一并提交\ngit commit -A\n"],["body","\n"],["headingLink","移除文件"],["heading","移除文件"],["body","\n"],["body","#删除工作区的改动文件\nrm file #暂存区-> untracked\n#删除git记录\ngit rm file \n\n#保留文件不想让git继续跟踪\ngit rm --cached file # tracked -> untracked\n\n#强制删除文件\ngit rm -f filename\n"],["body","\n"],["headingLink","重命名"],["heading","重命名"],["body","\n"],["body","git mv README.md README\n命令等价于三个命令\n\n mv README.md README\n git rm README.md\n git add README\n"],["body","\n"],["headingLink","查看提交历史"],["heading","查看提交历史"],["body","\n"],["headingLink","git-log常用选项"],["heading","git log常用选项"],["body","\n"],["body","选项"],["body","说明"],["body","\n"],["body","-p"],["body","按补丁格式显示每个提交引入的差异。"],["body","\n"],["body","--stat"],["body","显示每次提交的文件修改统计信息。"],["body","\n"],["body","--shortstat"],["body","只显示 --stat 中最后的行数修改添加移除统计。"],["body","\n"],["body","--name-only"],["body","仅在提交信息后显示已修改的文件清单。"],["body","\n"],["body","--name-status"],["body","显示新增、修改、删除的文件清单。"],["body","\n"],["body","--abbrev-commit"],["body","仅显示 SHA-1 校验和所有 40 个字符中的前几个字符。"],["body","\n"],["body","--relative-date"],["body","使用较短的相对时间而不是完整格式显示日期（比如“2 weeks ago”）。"],["body","\n"],["body","--graph"],["body","在日志旁以 ASCII 图形显示分支与合并历史。"],["body","\n"],["body","--pretty"],["body","使用其他格式显示历史提交信息。可用的选项包括 oneline、short、full、fuller 和 format（用来定义自己的格式）。"],["body","\n"],["body","--oneline"],["body","--pretty=oneline --abbrev-commit 合用的简写。"],["body","\n\n\n"],["headingLink","pretty-format格式"],["heading","pretty format格式"],["body","\n"],["body"," git log --pretty=format:\"%h - %an, %ar : %s\"\n"],["body","\n"],["body","选项"],["body","说明"],["body","\n"],["body","%H"],["body","提交的完整哈希值"],["body","\n"],["body","%h"],["body","提交的简写哈希值"],["body","\n"],["body","%T"],["body","树的完整哈希值"],["body","\n"],["body","%t"],["body","树的简写哈希值"],["body","\n"],["body","%P"],["body","父提交的完整哈希值"],["body","\n"],["body","%p"],["body","父提交的简写哈希值"],["body","\n"],["body","%an"],["body","作者名字"],["body","\n"],["body","%ae"],["body","作者的电子邮件地址"],["body","\n"],["body","%ad"],["body","作者修订日期（可以用 --date=选项 来定制格式）"],["body","\n"],["body","%ar"],["body","作者修订日期，按多久以前的方式显示"],["body","\n"],["body","%cn"],["body","提交者的名字"],["body","\n"],["body","%ce"],["body","提交者的电子邮件地址"],["body","\n"],["body","%cd"],["body","提交日期"],["body","\n"],["body","%cr"],["body","提交日期（距今多长时间）"],["body","\n"],["body","%s"],["body","提交说明"],["body","\n\n\n"],["headingLink","形象展示分支合并历史"],["heading","形象展示分支合并历史"],["body","\n"],["body"," git log --pretty=format:\"%h %s\" --graph\n"],["body","\n"],["headingLink","限制输出内容与长度"],["heading","限制输出内容与长度"],["body","\n"],["headingLink","查看最近的2条"],["heading","查看最近的2条"],["body","\n"],["body","git log -2 "],["body","\n"],["headingLink","时间限制"],["heading","时间限制"],["body","\n\n"],["body","绝对日期"],["body","\n\n"],["body","可以使用 这个日期 2008-01-15"],["body","\n\n"],["body","相对日期"],["body","\n\n"],["body","也可以是类似 \"2 years 1 day 3 minutes ago\" 的相对日期"],["body","\n\n"],["body","案例"],["body","\n\n"],["body","git log --since=2.weeks 最近两周\n"],["body","\n"],["headingLink","作者过滤"],["heading","作者过滤"],["body","\n"],["body","--author"],["body","\n"],["headingLink","提交说明搜索"],["heading","提交说明搜索"],["body","\n"],["body","--grep"],["body","\n"],["headingLink","pickaxe搜索"],["heading","pickAXE搜索"],["body","\n"],["body","只会显示那些添加或删除了该字符串的提交。 "],["body","\n"],["body","假设你想找出添加或删除了对某一个特定函数的引用的提交，可以调用："],["body","\n"],["body","git log -S function_name\n"],["body","\n"],["headingLink","常用限制输出选项"],["heading","常用限制输出选项"],["body","\n"],["body","选项"],["body","说明"],["body","\n"],["body","-<n>"],["body","最近的n条提交"],["body","\n"],["body","--since, --after"],["body","仅显示指定时间之后的提交。"],["body","\n"],["body","--until, --before"],["body","仅显示指定时间之前的提交。"],["body","\n"],["body","--author"],["body","仅显示作者匹配指定字符串的提交。"],["body","\n"],["body","--committer"],["body","仅显示提交者匹配指定字符串的提交。"],["body","\n"],["body","--grep"],["body","仅显示提交说明中包含指定字符串的提交。"],["body","\n"],["body","-S"],["body","仅显示添加或删除内容匹配指定字符串的提交。"],["body","\n\n\n"],["headingLink","撤销操作"],["heading","撤销操作"],["body","\n"],["headingLink","补漏"],["heading","补漏"],["body","\n\n"],["body","\n"],["body","使用场景"],["body","\n\n"],["body","有时候我们提交完了才发现漏掉了几个文件没有添加"],["body","\n"],["body","或者提交信息写错了"],["body","\n\n"],["body","\n"],["body","\n"],["body","可以运行带有 --amend 选项的提交命令来重新提交："],["body","\n"],["body","\n"],["body","\n"],["body","这个命令会将暂存区中的文件提交。 如果自上次提交以来你还未做任何修改"],["body","\n"],["body","\n"],["body","\n"],["body","并替换之前的提交日志"],["body","\n"],["body","\n\n"],["body","git commit --amend -m \"\"\n"],["body","\n"],["headingLink","取消暂存"],["heading","取消暂存"],["body","\n"],["body","#这个命令后续在了解\ngit reset HEAD CONTRIBUTING.md\n#正规用法\ngit restore --staged <file>...\n"],["body","\n"],["headingLink","撤消对文件的修改"],["heading","撤消对文件的修改"],["body","\n"],["body","git restore <file>...\n\n#用版本库最近的版本去替换当前的文件\ngit checkout file\n"],["body","\n"],["headingLink","远程仓库的使用"],["heading","远程仓库的使用"],["body","\n"],["headingLink","查看已配置的远程仓库"],["heading","查看已配置的远程仓库"],["body","\n"],["body","git remote  #默认的名称为 origin\ngit remote -v #详细\n"],["body","\n"],["headingLink","添加远程仓库"],["heading","添加远程仓库"],["body","\n"],["body","#配置远程库\ngit remote add <shortname> <url>\ngit remote add pb https://github.com/paulboone/ticgit\n#拉取代码\ngit fetch pb\n\n现在 Paul 的 master 分支可以在本地通过 pb/master 访问到\n"],["body","\n"],["headingLink","clone命令的行为"],["heading","clone命令的行为"],["body","\n"],["body","如果你使用 clone 命令克隆了一个仓库，命令会自动将其添加为远程仓库并默认以 “origin” 为简写"],["body","\n"],["body","git clone 命令会自动设置本地 master 分支跟踪克隆的远程仓库的 master 分支"],["body","\n"],["headingLink","gitpull"],["heading","gitpull"],["body","\n"],["body","如果你的当前分支设置了跟踪远程分支"],["body","\n"],["body","那么可以用 git pull 命令来自动抓取后合并该远程分支到当前分支"],["body","\n"],["body","运行 git pull 通常会从最初克隆的服务器上抓取数据并自动尝试合并到当前所在的分支。"],["body","\n"],["headingLink","推送远程分支"],["heading","推送远程分支"],["body","\n"],["body","git push <remote> <branch>\n git push origin master\n"],["body","\n"],["body","限制"],["body","\n\n"],["body","只有当你有所克隆服务器的写入权限,并且之前没有人推送过时，这条命令才能生效"],["body","\n"],["body","当已经有推送时,你必须先抓取他们的工作并将其合并进你的工作后才能推送"],["body","\n\n"],["headingLink","查看某个远程仓库"],["heading","查看某个远程仓库"],["body","\n"],["body","git remote show origin\n"],["body","\n"],["headingLink","远程仓库的重命名与移除"],["heading","远程仓库的重命名与移除"],["body","\n"],["body"," git remote rename pb paul\n git remote remove paul\n"],["body","\n"],["headingLink","打标签"],["heading","打标签"],["body","\n"],["headingLink","查看标签"],["heading","查看标签"],["body","\n"],["body","Git 可以给仓库历史中的某一个提交打上标签,使用这个功能来标记发布结点（ v1.0 、 v2.0 等等）"],["body","\n"],["body","#这个命令以字母顺序列出标签，但是它们显示的顺序并不重要。\ngit tag\n#可以模糊匹配\ngit tag -l \"v1.8.5*\"\n"],["body","\n"],["headingLink","创建标签"],["heading","创建标签"],["body","\n"],["body","git有两种标签"],["body","\n"],["headingLink","轻量标签"],["heading","轻量标签"],["body","\n"],["body","轻量标签很像一个不会改变的分支——它只是某个特定提交的引用。"],["body","\n"],["body","#不要使用-a, -m -s 等\ngit tag v1.4-lw\ngit show v1.4-lw\n"],["body","\n"],["headingLink","附注标签"],["heading","附注标签"],["body","\n"],["body","​\t而附注标签是存储在 Git 数据库中的一个完整对象， 它们是可以被校验的，其中包含打标签者的名字、电子邮件地址、日期时间， 此外还有一个标签信息，并且可以使用 GNU Privacy Guard （GPG）签名并验证。 通常会建议创建附注标签，这样你可以拥有以上所有信息。但是如果你只是想用一个临时的标签， 或者因为某些原因不想要保存这些信息，那么也可以用轻量标签。"],["body","\n"],["body","git tag -a v1.4 -m \"my version 1.4\"\n\ngit show v1.4\n"],["body","\n"],["headingLink","提交历史打标签"],["heading","提交历史打标签"],["body","\n"],["body","git log --pretty=oneline\ngit tag -a v1.2 9fceb02 #部分校验和足够区分\n"],["body","\n"],["headingLink","共享标签"],["heading","共享标签"],["body","\n"],["body","git push 命令并不会传送标签到远程仓库服务器上,在创建完标签后你必须显式地推送标签到共享服务器上。"],["body","\n"],["body"," git push origin <tagname>\n #把所有不在远程仓库服务器上的标签全部传送到那里。\n git push origin --tags\n"],["body","\n"],["headingLink","删除标签"],["heading","删除标签"],["body","\n"],["body"," git tag -d v1.4-lw\n \n #推送远程仓库\n git push origin :refs/tags/v1.4-lw\n  git push origin --delete <tagname>\n"],["body","\n"],["headingLink","检出标签"],["heading","检出标签"],["body","\n"],["body","git checkout 2.0.0\n\n"],["body","\n"],["body","仓库处于“分离头指针（detached HEAD）”的状态,如果你做了某些更改然后提交它们，标签不会发生变化， 但你的新提交将不属于任何分支，并且将无法访问，除非通过确切的提交哈希才能访问。 因此，如果你需要进行更改，比如你要修复旧版本中的错误，那么通常需要创建一个新分支："],["body","\n"],["headingLink","git别名"],["heading","Git别名"],["body","\n"],["body","通过 git config 文件来轻松地为每一个命令设置一个别名"],["body","\n"],["body","$ git config --global alias.co checkout\n$ git config --global alias.br branch\n$ git config --global alias.ci commit\n$ git config --global alias.st status\n$ git config --global alias.unstage 'reset HEAD --'\n\n$ git unstage fileA\n$ git reset HEAD -- fileA\ngit config --global alias.last 'log -1 HEAD'\n\n#执行外部命令 加上!\ngit config --global alias.visual '!gitk'\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/Git库管理.html"],["title","Git库管理 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","git库管理"],["heading","Git库管理"],["body","\n"],["body","版本库管理？那不是管理员要干的事情么，怎么放在“Git独奏”这一部分了？"],["body","\n"],["body","没有错，这是因为对于Git，每个用户都是自己版本库的管理员，所以在“Git独奏”的最后一章，来谈一谈Git版本库管理的问题"],["body","\n"],["body","如果下面的问题您没有遇到或者不感兴趣，读者大可以放心的跳过这一章。"],["body","\n\n"],["body","从网上克隆来的版本库，为什么对象库中找不到对象文件？而且引用目录里也看不到所有的引用文件？"],["body","\n"],["body","不小心添加了一个大文件到Git库中，用重置命令丢弃了包含大文件的提交，可是版本库不见小，大文件仍在对象库中。"],["body","\n"],["body","本地版本库的对象库里文件越来越多，这可能导致Git性能的降低。"],["body","\n\n"],["headingLink","对象和引用哪里去了"],["heading","对象和引用哪里去了"],["body","\n"],["body","git clone git://github.com/ossxp-com/gitdemo-commit-tree.git i-am-admin\n"],["body","\n"],["body","进入克隆的版本库，使用git show-ref命令看看所含的引用。"],["body","\n"],["body","$ cd /path/to/my/workspace/i-am-admin\n$ git show-ref\n6652a0dce6a5067732c00ef0a220810a7230655e refs/heads/master\n6652a0dce6a5067732c00ef0a220810a7230655e refs/remotes/origin/HEAD\n6652a0dce6a5067732c00ef0a220810a7230655e refs/remotes/origin/master\nc9b03a208288aebdbfe8d84aeb984952a16da3f2 refs/tags/A\n1a87782f8853c6e11aacba463af04b4fa8565713 refs/tags/B\n9f8b51bc7dd98f7501ade526dd78c55ee4abb75f refs/tags/C\n887113dc095238a0f4661400d33ea570e5edc37c refs/tags/D\n6decd0ad3201ddb3f5b37c201387511059ac120c refs/tags/E\n70cab20f099e0af3f870956a3fbbbda50a17864f refs/tags/F\n96793e37c7f1c7b2ddf69b4c1e252763c11a711f refs/tags/G\n476e74549047e2c5fbd616287a499cc6f07ebde0 refs/tags/H\n76945a15543c49735634d58169b349301d65524d refs/tags/I\nf199c10c3f1a54fa3f9542902b25b49d58efb35b refs/tags/J\n"],["body","\n"],["body","其中以refs/heads/开头的是分支；以refs/remotes/开头的是远程版本库分支在本地的映射，会在后面章节介绍；以refs/tags/开头的是里程碑。按照之前的经验，在.git/refs目录下应该有这些引用所对应的文件才是。看看都在么？"],["body","\n"],["body","$ find .git/refs/ -type f\n.git/refs/remotes/origin/HEAD\n.git/refs/heads/master\n"],["body","\n"],["body","为什么才有两个文件？实际上当运行下面的命令后，引用目录下的文件会更少："],["body","\n"],["body","$ git pack-refs --all\n$ find .git/refs/ -type f\n.git/refs/remotes/origin/HEAD\n"],["body","\n"],["body","$ find .git/objects/ -type f\n.git/objects/pack/pack-969329578b95057b7ea1208379a22c250c3b992a.idx\n.git/objects/pack/pack-969329578b95057b7ea1208379a22c250c3b992a.pack\n"],["body","\n"],["body","对象库中只有两个文件，本应该一个一个独立保存的对象都不见了。读者应该能够猜到，所有的对象文件都被打包到这两个文件中了，其中以.pack结尾的文件是打包文件，以.idx结尾的是索引文件。打包文件和对应的索引文件只是扩展名不同，都保存于.git/objects/pack/目录下。Git对于以SHA1哈希值作为目录名和文件名保存的对象有一个术语，称为松散对象。"],["body","\n"],["body","松散对象打包后会提高访问效率，而且不同的对象可以通过增量存储节省磁盘空间。"],["body","\n"],["body","可以通过Git一个底层命令可以查看索引中包含的对象："],["body","\n"],["body","$ git show-index < .git/objects/pack/pack-*.idx | head -5\n661 0cd7f2ea245d90d414e502467ac749f36aa32cc4 (0793420b)\n63020 1026d9416d6fc8d34e1edfb2bc58adb8aa5a6763 (ed77ff72)\n3936 15328fc6961390b4b10895f39bb042021edd07ea (13fb79ef)\n3768 1a588ca36e25f58fbeae421c36d2c39e38e991ef (86e3b0bd)\n2022 1a87782f8853c6e11aacba463af04b4fa8565713 (e269ed74)\n"],["body","\n"],["body","为什么克隆远程版本库就可以产生对象库打包以及引用打包的效果呢？"],["body","\n"],["body","这是因为克隆远程版本库时，使用了“智能”的通讯协议，远程Git服务器将对象打包后传输给本地，形成本地版本库的对象库中的一个包含所有对象的包以及索引文件"],["body","\n"],["body","无疑这样的传输方式——按需传输、打包传输，效率最高。"],["body","\n"],["body","克隆之后的版本库在日常的提交中，产生的新的对象仍旧以松散对象存在，而不是以打包的形式，日积月累会在本地版本库的对象库中形成大量的松散文件"],["body","\n"],["body","松散对象只是进行了压缩，而没有（打包文件才有的）增量存储的功能，会浪费磁盘空间，也会降低访问效率"],["body","\n"],["body","更为严重的是一些非正式的临时对象（暂存区操作中产生的临时对象）也以松散对象的形式保存在对象库中，造成磁盘空间的浪费。下一节就着手处理临时对象的问题。"],["body","\n"],["headingLink","暂存区操作引入的临时对象"],["heading","暂存区操作引入的临时对象"],["body","\n"],["body","暂存区操作有可能在对象库中产生临时对象，例如文件反复的修改和反复的向暂存区添加，或者添加到暂存区后不提交甚至直接撤销，就会产生垃圾数据占用磁盘空间"],["body","\n"],["body","然后将工作区中两个内容完全一样的大文件加入暂存区。"],["body","\n"],["body","$ git add bigfile bigfile.dup\n"],["body","\n"],["body","查看一下磁盘空间占用："],["body","\n\n"],["body","\n"],["body","工作区连同版本库共占用33MB。"],["body","\n"],["body","$ du -sh .\n33M     .\n"],["body","\n"],["body","\n"],["body","\n"],["body","其中版本库只占用了11MB。版本库空间占用是工作区的一半。"],["body","\n"],["body","如果再有谁说版本库空间占用一定比工作区大，可以用这个例子回击他。"],["body","\n"],["body","$ du -sh .git/\n11M     .git/\n"],["body","\n"],["body","\n\n"],["body","看看版本库中对象库内的文件，会发现多出了一个松散对象。之所以添加两个文件而只有一个松散对象，是因为Git对于文件的保存是将内容保存为blob对象中，和文件名无关，相同内容的不同文件会共享同一个blob对象。"],["body","\n"],["body","$ find .git/objects/ -type f\n.git/objects/2e/bcd92d0dda2bad50c775dc662c6cb700477aff\n.git/objects/pack/pack-969329578b95057b7ea1208379a22c250c3b992a.idx\n.git/objects/pack/pack-969329578b95057b7ea1208379a22c250c3b992a.pack\n"],["body","\n"],["body","如果不想提交，想将文件撤出暂存区，则进行如下操作。"],["body","\n\n"],["body","\n"],["body","当前暂存区的状态。"],["body","\n"],["body","$ git status -s\nA  bigfile\nA  bigfile.dup\n"],["body","\n"],["body","\n"],["body","\n"],["body","将添加的文件撤出暂存区。"],["body","\n"],["body","$ git reset HEAD\n"],["body","\n"],["body","\n"],["body","\n"],["body","通过查看状态，看到文件被撤出暂存区了。"],["body","\n"],["body","$ git status -s\n?? bigfile\n?? bigfile.dup\n"],["body","\n"],["body","\n\n"],["body","文件撤出暂存区后，在对象库中产生的blob松散对象仍然存在，通过查看版本库的磁盘占用就可以看出来。"],["body","\n"],["body","$ du -sh .git/\n11M     .git/\n"],["body","\n"],["body","Git提供了git fsck命令，可以查看到版本库中包含的没有被任何引用关联松散对象"],["body","\n"],["body","$ git fsck\ndangling blob 2ebcd92d0dda2bad50c775dc662c6cb700477aff\n"],["body","\n"],["body","标识为dangling的对象就是没有被任何引用直接或者间接关联到的对象。这个对象就是前面通过暂存区操作引入的大文件的内容。如何将这个文件从版本库中彻底删除呢？Git提供了一个清理的命令："],["body","\n"],["body","$ git prune\n"],["body","\n"],["body","用git prune清理之后，会发现："],["body","\n\n"],["body","\n"],["body","用git fsck查看，没有未被关联到的松散对象。"],["body","\n"],["body","$ git fsck\n"],["body","\n"],["body","\n"],["body","\n"],["body","版本库的空间占用也小了10MB，证明大的临时对象文件已经从版本库中删除了。"],["body","\n"],["body","$ du -sh .git/\n236K    .git/\n"],["body","\n"],["body","\n\n"],["headingLink","重置提交引入的对象"],["heading","重置提交引入的对象"],["body","\n"],["body","上一节用git prune命令清除暂存区操作时引入的临时对象，但是如果是用重置命令抛弃的提交和文件就不会轻易的被清除。"],["body","\n"],["body","将这两个大文件提交到版本库中。"],["body","\n\n"],["body","\n"],["body","添加到暂存区。"],["body","\n"],["body","$ git add bigfile bigfile.dup\n"],["body","\n"],["body","\n"],["body","\n"],["body","提交到版本库。"],["body","\n"],["body","$ git commit -m \"add bigfiles.\"\n[master 51519c7] add bigfiles.\n 2 files changed, 0 insertions(+), 0 deletions(-)\n create mode 100644 bigfile\n create mode 100644 bigfile.dup\n"],["body","\n"],["body","\n"],["body","\n"],["body","查看版本库的空间占用。"],["body","\n"],["body","$ du -sh .git/\n11M     .git/\n"],["body","\n"],["body","\n\n"],["body","做一个重置操作，抛弃刚刚针对两个大文件做的提交。"],["body","\n"],["body","$ git reset --hard HEAD^\n"],["body","\n"],["body","重置之后，看看版本库的变化。"],["body","\n\n"],["body","\n"],["body","版本库的空间占用没有变化，还是那么“庞大”。"],["body","\n"],["body","$ du -sh .git/\n11M     .git/\n"],["body","\n"],["body","\n"],["body","\n"],["body","查看对象库，看到三个松散对象。"],["body","\n"],["body","$ find .git/objects/ -type f\n.git/objects/info/packs\n.git/objects/2e/bcd92d0dda2bad50c775dc662c6cb700477aff\n.git/objects/d9/38dee8fde4e5053b12406c66a19183a24238e1\n.git/objects/51/519c7d8d60e0f958e135e8b989a78e84122591\n.git/objects/pack/pack-969329578b95057b7ea1208379a22c250c3b992a.idx\n.git/objects/pack/pack-969329578b95057b7ea1208379a22c250c3b992a.pack\n"],["body","\n"],["body","\n"],["body","\n"],["body","这三个松散对象分别对应于撤销的提交，目录树，以及大文件对应的blob对象。"],["body","\n"],["body","$ git cat-file -t 51519c7\ncommit\n$ git cat-file -t d938dee\ntree\n$ git cat-file -t 2ebcd92\nblob\n"],["body","\n"],["body","\n\n"],["body","向上一节一样，执行git prune命令，期待版本库空间占用会变小。可是："],["body","\n\n"],["body","\n"],["body","版本库空间占用没有变化！"],["body","\n"],["body","$ git prune\n$ du -sh .git/\n11M     .git/\n"],["body","\n"],["body","\n"],["body","\n"],["body","执行git fsck也看不到未被关联到的对象。"],["body","\n"],["body","$ git fsck\n"],["body","\n"],["body","\n"],["body","\n"],["body","除非像下面这样执行。"],["body","\n"],["body","$ git fsck --no-reflogs\ndangling commit 51519c7d8d60e0f958e135e8b989a78e84122591\n"],["body","\n"],["body","\n\n"],["body","还记得前面章节中介绍的reflog么？reflog是防止误操作的最后一道闸门。"],["body","\n"],["body","$ git reflog\n6652a0d HEAD@{0}: HEAD^: updating HEAD\n51519c7 HEAD@{1}: commit: add bigfiles.\n"],["body","\n"],["body","可以看到撤销的操作仍然记录在reflog中，正因如此Git认为撤销的提交和大文件都还被可以被追踪到，还在使用着，所以无法用git prune命令删除。"],["body","\n"],["body","如果确认真的要丢弃不想要的对象，需要对版本库的reflog做过期操作，相当于将.git/logs/下的文件清空。"],["body","\n\n"],["body","\n"],["body","使用下面的reflog过期命令做不到让刚刚撤销的提交过期，因为reflog的过期操作缺省只会让90天前的数据过期。"],["body","\n"],["body","\n"],["body","\n"],["body","$ git reflog expire --all\n$ git reflog\n6652a0d HEAD@{0}: HEAD^: updating HEAD\n51519c7 HEAD@{1}: commit: add bigfiles.\n"],["body","\n"],["body","\n\n"],["body","需要要为git reflog命令提供--expire=<date>参数，强制<date>之前的记录全部过期。"],["body","\n"],["body","$ git reflog expire --expire=now --all\n$ git reflog\n"],["body","\n"],["body","使用now作为时间参数，让 reflog 的全部记录都过期"],["body","\n"],["body","没有了 reflog，即回滚的添加大文件的提交从 reflog 中看不到后"],["body","\n"],["body","该提交对应的 commit 对象、tree 对象和 blob 对象就会成为未被关联的 dangling 对象"],["body","\n"],["body","可以用git prune命令清理"],["body","\n"],["headingLink","git管家git-gc"],["heading","Git管家：git gc"],["body","\n"],["body","实际操作中会很少用到git prune命令来清理版本库，而是会使用一个更为常用的命令git gc"],["body","\n"],["body","命令git gc就好比Git版本库的管家，会对版本库进行一系列的优化动作。"],["body","\n\n"],["body","\n"],["body","对分散在.git/refs下的文件进行打包，打包到文件.git/packed-refs中"],["body","\n"],["body","如果没有将配置gc.packrefs关闭，就会执行命令：git pack-refs –all –prune实现对引用的打包。"],["body","\n"],["body","\n"],["body","\n"],["body","丢弃90天前的reflog记录。"],["body","\n"],["body","会运行使reflog过期命令：git reflog expire –all。因为采用了缺省参数调用，因此只会清空reflog中90天前的记录。"],["body","\n"],["body","\n"],["body","\n"],["body","对松散对象进行打包。"],["body","\n\n"],["body","运行git repack命令，凡是有引用关联的对象都被打在包里，未被关联的对象仍旧以松散对象形式保存。"],["body","\n\n"],["body","\n"],["body","\n"],["body","清除未被关联的对象。缺省只清除2周以前的未被关联的对象。"],["body","\n\n"],["body","可以向git gc提供--prune=<date>参数，其中的时间参数传递给git prune –expire ，实现对指定日期之前的未被关联的松散对象进行清理。"],["body","\n\n"],["body","\n"],["body","\n"],["body","其他清理"],["body","\n\n"],["body","如运行git rerere gc对合并冲突的历史记录进行过期操作。"],["body","\n\n"],["body","\n"],["body","\n"],["body","例如像暂存区操作引入的没有关联的临时对象会最少保留2个星期，而因为重置而丢弃的提交和文件则会保留最少3个月。"],["body","\n"],["body","\n\n"],["body","复制两个大文件到工作区。"],["body","\n"],["body","$ cp /tmp/bigfile bigfile\n$ cp /tmp/bigfile bigfile.dup\n"],["body","\n"],["body","在文件bigfile.dup后面追加些内容，造成bigfile和bigfile.dup内容不同。"],["body","\n"],["body","$ echo \"hello world\" >> bigfile.dup\n"],["body","\n"],["body","将这两个稍有不同的文件提交到版本库。"],["body","\n"],["body","$ git add bigfile bigfile.dup\n$ git commit -m \"add bigfiles.\"\n[master c62fa4d] add bigfiles.\n 2 files changed, 0 insertions(+), 0 deletions(-)\n create mode 100644 bigfile\n create mode 100644 bigfile.dup\n"],["body","\n"],["body","可以看到版本库中提交进来的两个不同的大文件是不同的对象。"],["body","\n"],["body","$ git ls-tree HEAD | grep bigfile\n100644 blob 2ebcd92d0dda2bad50c775dc662c6cb700477aff    bigfile\n100644 blob 9e35f946a30c11c47ba1df351ca22866bc351e7b    bigfile.dup\n"],["body","\n"],["body","做版本库重置，抛弃最新的提交，即抛弃添加两个大文件的提交。"],["body","\n"],["body","$ git reset --hard HEAD^\nHEAD is now at 6652a0d Add Images for git treeview.\n"],["body","\n"],["body","此时的版本库有多大呢，还是像之前添加两个相同的大文件时占用11MB空间么？"],["body","\n"],["body","$ du -sh .git/\n22M     .git/\n"],["body","\n"],["body","版本库空间占用居然扩大了一倍！这显然是因为两个大文件分开存储造成的。可以用下面的命令在对象库中查看对象的大小。"],["body","\n"],["body","$ find .git/objects -type f -printf \"%-20p\\t%s\\n\"\n.git/objects/0c/844d2a072fd69e71638558216b69ebc57ddb64  233\n.git/objects/2e/bcd92d0dda2bad50c775dc662c6cb700477aff  11184682\n.git/objects/9e/35f946a30c11c47ba1df351ca22866bc351e7b  11184694\n.git/objects/c6/2fa4d6cb4c082fadfa45920b5149a23fd7272e  162\n.git/objects/info/packs 54\n.git/objects/pack/pack-969329578b95057b7ea1208379a22c250c3b992a.idx   2892\n.git/objects/pack/pack-969329578b95057b7ea1208379a22c250c3b992a.pack  80015\n"],["body","\n"],["body","输出的每一行用空白分隔，前面是文件名，后面是以字节为单位的文件大小。从上面的输出可以看出来，打包文件很小，但是有两个大的文件各自占用了11MB左右的空间。"],["body","\n"],["body","执行git gc并不会删除任何对象，因为这些对象都还没有过期。但是会发现版本库的占用变小了。"],["body","\n\n"],["body","\n"],["body","执行git gc对版本库进行整理。"],["body","\n"],["body","$ git gc\nCounting objects: 69, done.\nDelta compression using up to 2 threads.\nCompressing objects: 100% (49/49), done.\nWriting objects: 100% (69/69), done.\nTotal 69 (delta 11), reused 63 (delta 8)\n"],["body","\n"],["body","\n"],["body","\n"],["body","版本库空间占用小了一半！"],["body","\n"],["body","$ du -sh .git/\n11M     .git/\n"],["body","\n"],["body","\n"],["body","\n"],["body","原来是因为对象库重新打包，两个大文件采用了增量存储使得版本库变小。"],["body","\n"],["body","$ find .git/objects -type f -printf \"%-20p\\t%s\\n\" | sort\n.git/objects/info/packs 54\n.git/objects/pack/pack-7cae010c1b064406cd6c16d5a6ab2f446de4076c.idx 3004\n.git/objects/pack/pack-7cae010c1b064406cd6c16d5a6ab2f446de4076c.pack 11263033\n"],["body","\n"],["body","\n\n"],["body","如果想将抛弃的历史数据彻底丢弃，如下操作。"],["body","\n\n"],["body","\n"],["body","不再保留90天的reflog，而是将所有reflog全部即时过期。"],["body","\n"],["body","$ git reflog expire --expire=now --all\n"],["body","\n"],["body","\n"],["body","\n"],["body","通过git fsck可以看到有提交成为了未被关联的提交。"],["body","\n"],["body","$ git fsck\ndangling commit c62fa4d6cb4c082fadfa45920b5149a23fd7272e\n"],["body","\n"],["body","\n"],["body","\n"],["body","这个未被关联的提交就是删除大文件的提交。"],["body","\n"],["body","$ git show c62fa4d6cb4c082fadfa45920b5149a23fd7272e\ncommit c62fa4d6cb4c082fadfa45920b5149a23fd7272e\nAuthor: Jiang Xin <jiangxin@ossxp.com>\nDate:   Thu Dec 16 20:18:38 2010 +0800\n\n    add bigfiles.\n\ndiff --git a/bigfile b/bigfile\nnew file mode 100644\nindex 0000000..2ebcd92\nBinary files /dev/null and b/bigfile differ\ndiff --git a/bigfile.dup b/bigfile.dup\nnew file mode 100644\nindex 0000000..9e35f94\nBinary files /dev/null and b/bigfile.dup differ\n"],["body","\n"],["body","\n"],["body","\n"],["body","不带参数调用git gc虽然不会清除尚未过期（未到2周）的大文件，但是会将未被关联的对象从打包文件中移出，成为松散文件。"],["body","\n"],["body","$ git gc\nCounting objects: 65, done.\nDelta compression using up to 2 threads.\nCompressing objects: 100% (45/45), done.\nWriting objects: 100% (65/65), done.\nTotal 65 (delta 8), reused 63 (delta 8)\n"],["body","\n"],["body","\n"],["body","\n"],["body","未被关联的对象重新成为松散文件，所以.git版本库的空间占用又反弹了。"],["body","\n"],["body","$ du -sh .git/\n22M     .git/\n$ find .git/objects -type f -printf \"%-20p\\t%s\\n\" | sort\n.git/objects/0c/844d2a072fd69e71638558216b69ebc57ddb64  233\n.git/objects/2e/bcd92d0dda2bad50c775dc662c6cb700477aff  11184682\n.git/objects/9e/35f946a30c11c47ba1df351ca22866bc351e7b  11184694\n.git/objects/c6/2fa4d6cb4c082fadfa45920b5149a23fd7272e  162\n.git/objects/info/packs 54\n.git/objects/pack/pack-969329578b95057b7ea1208379a22c250c3b992a.idx 2892\n.git/objects/pack/pack-969329578b95057b7ea1208379a22c250c3b992a.pack 80015\n"],["body","\n"],["body","\n"],["body","\n"],["body","实际上如果使用立即过期参数--prune=now调用git gc，就不用再等2周了，直接就可以完成对未关联的对象的清理。"],["body","\n"],["body","$ git gc --prune=now\nCounting objects: 65, done.\nDelta compression using up to 2 threads.\nCompressing objects: 100% (45/45), done.\nWriting objects: 100% (65/65), done.\nTotal 65 (delta 8), reused 65 (delta 8)\n"],["body","\n"],["body","\n"],["body","\n"],["body","清理过后，版本库的空间占用降了下来。"],["body","\n"],["body","$ du -sh .git/\n240K    .git/\n"],["body","\n"],["body","\n\n"],["headingLink","git管家的自动执行"],["heading","Git管家的自动执行"],["body","\n"],["body","实际上对于1.6.6及以后版本的Git已经基本上不需要手动执行git gc命令了，因为部分Git命令会自动调用git gc –auto命令，在版本库确实需要整理的情况下自动开始整理操作。"],["body","\n"],["body","目前有如下Git命令会自动执行git gc –auto命令，实现对版本库的按需整理。"],["body","\n\n"],["body","执行命令git merge进行合并操作后，对版本库进行按需整理。"],["body","\n"],["body","执行命令git receive-pack，即版本库接收其他版本库推送（push）的提交后，版本库会做按需整理操作。\n\n"],["body","当版本库接收到其他版本库的推送（push）请求时，会调用git receive-pack命令以接收请求。在接收到推送的提交后，对版本库进行按需整理。"],["body","\n\n"],["body","\n"],["body","执行命令git rebase -i进行交互式变基操作后，会对版本库进行按需整理。"],["body","\n"],["body","执行命令git am对mbox邮箱中通过邮件提交的补丁在版本库中进行应用的操作后，会对版本库做按需整理操作。"],["body","\n\n"],["body","对于提供共享式“写操作”的Git版本库，可以免维护"],["body","\n"],["body","所谓的共享式写操作，就是版本库作为一个裸版本库放在服务器上，团队成员可以通过推送（push）操作将提交推送到共享的裸版本中。"],["body","\n"],["body","每一次推送操作都会触发git gc –auto命令，对版本库进行按需整理"],["body","\n"],["body","对于非独立工作的本地工作区，也可以免维护"],["body","\n"],["body","因为和他人协同工作的本地工作区会经常执行git pull操作从他人版本库或者从共享的版本库拉回新提交，执行git pull操作会，会触发git merge操作，因此也会对本地版本库进行按需整理。"],["body","\n"],["body","Git管家命令使用--auto参数调用，会进行按需整理。"],["body","\n"],["body","因为版本库整理操作对于大的项目可能会非常费时，因此实际的整理并不会经常被触发，即有着非常苛刻的触发条件。想要观察到触发版本库整理操作是非常不容易的事情。"],["body","\n"],["body","主要的触发条件是："],["body","\n"],["body","松散对象只有超过一定的数量时才会执行"],["body","\n"],["body","而且在统计松散对象数量时，为了降低在.git/objects/目录下搜索松散对象对系统造成的负担，实际采取了取样搜索，即只会对对象库下一个子目录.git/objects/17进行文件搜索。在缺省的配置下，只有该目录中对象数目超过27个，才会触发版本库的整理。"],["body","\n"],["body","至于为什么只在对象库下选择了一个子目录进行松散对象的搜索，这是因为SHA1哈希值是完全随机的，"],["body","\n"],["body","文件在由前两位哈希值组成的目录中差不多是平均分布的。至于为什么选择17，不知道对于作者Junio C Hamano有什么特殊意义，也许是向Linus Torvalds被评选为二十世纪最有影响力的100人中排名第17位而进行致敬。"],["body","\n"],["body","可以通过配置gc.auto的值，调整Git管家自动运行时触发版本库整理操作的频率，但是注意不要将gc.auto设置为0，否则git gc –auto命令永远不会触发版本库的整理。"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/GIT协作/远程版本库.html"],["title","远程版本库 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","远程版本库"],["heading","远程版本库"],["body","\n"],["body","Git作为分布式版本库控制系统，每个人都是本地版本库的主人，可以在本地的版本库中随心所欲地创建分支和里程碑。当需要多人协作时，问题就出现了"],["body","\n\n"],["body","如何避免因为用户把所有的本地分支都推送到共享版本库，从而造成共享版本库上分支的混乱？"],["body","\n"],["body","如何避免不同用户针对不同特性开发创建了相同名字的分支而造成分支名称的冲突？"],["body","\n"],["body","如何避免用户随意在共享版本库中创建里程碑而导致里程碑名称上的混乱和冲突？"],["body","\n"],["body","当用户向共享版本库及其他版本库推送时，每次都需要输入长长的版本库URL，太不方便了。"],["body","\n"],["body","当用户需要经常从多个不同的他人版本库中获取提交时，有没有办法不要总是输入长长的版本库URL？"],["body","\n"],["body","如果不带任何其他参数执行git fetch、git pull和git push到底是和哪个远程版本库及哪个分支进行交互？"],["body","\n\n"],["headingLink","远程分支"],["heading","远程分支"],["body","\n"],["body","Git允许一个版本库和任意多的版本库进行交"],["body","\n"],["body","先执行下面的命令，基于hello-world.git版本库再创建几个新的版本库。"],["body","\n"],["body","$ cd /path/to/repos/\n$ git clone --bare hello-world.git hello-user1.git\nCloning into bare repository hello-user1.git...\ndone.\n$ git clone --bare hello-world.git hello-user2.git\nCloning into bare repository hello-user2.git...\ndone.\n"],["body","\n"],["body","现在有了三个共享版本库：hello-world.git、hello-user1.git和hello-user2.git。现在有一个疑问，如果一个本地版本库需要和上面三个版本库进行互操作，三个共享版本库都存在一个master分支，会不会互相干扰、冲突或覆盖呢？"],["body","\n"],["body","先来看看hello-world远程共享版本库中包含的分支有哪些："],["body","\n"],["body","$ git ls-remote --heads file:///path/to/repos/hello-world.git\n8cffe5f135821e716117ee59bdd53139473bd1d8        refs/heads/hello-1.x\nbb4fef88fee435bfac04b8389cf193d9c04105a6        refs/heads/helper/master\ncf71ae3515e36a59c7f98b9db825fd0f2a318350        refs/heads/helper/v1.x\nc4acab26ff1c1125f5e585ffa8284d27f8ceea55        refs/heads/master\n"],["body","\n"],["body","原来远程共享版本库中有四个分支，其中hello-1.x分支是开发者user1创建的。现在重新克隆该版本库，如下："],["body","\n"],["body","$ cd /path/to/my/workspace/\n$ git clone file:///path/to/repos/hello-world.git\n...\n$ cd /path/to/my/workspace/hello-world\n"],["body","\n"],["body","执行git branch命令检查分支，会吃惊地看到只有一个分支master。"],["body","\n"],["body","$ git branch\n* master\n"],["body","\n"],["body","那么远程版本库中的其他分支哪里去了？为什么本地只有一个分支呢？执行git show-ref命令可以看到全部的本地引用。"],["body","\n"],["body","$ git show-ref\n"],["body","\n"],["body","从git show-ref的输出中发现了几个不寻常的引用，这些引用以refs/remotes/origin/为前缀，并且名称和远程版本库的分支名一一对应。"],["body","\n"],["body","这些引用实际上就是从远程版本库的分支拷贝过来的，称为远程分支。"],["body","\n"],["body","Git 的git branch命令也能够查看这些远程分支，不过要加上-r参数："],["body","\n"],["body","$ git branch -r\n  origin/HEAD -> origin/master\n  origin/hello-1.x\n  origin/helper/master\n  origin/helper/v1.x\n  origin/master\n"],["body","\n"],["body","Git这样的设计是非常巧妙的，在向远程版本库执行获取操作时，不是把远程版本库的分支原封不动地复制到本地版本库的分支中，而是复制到另外的命名空间。如在克隆一个版本库时，会将远程分支都复制到目录.git/refs/remotes/origin/下。这样向不同的远程版本库执行获取操作，因为远程分支相互隔离，所以就避免了相互的覆盖。"],["body","\n"],["body","那么克隆操作产生的远程分支为什么都有一个名为“origin/”的前缀呢？奥秘就在配置文件.git/config中。下面的几行内容出自该配置文件，为了说明方便显示了行号。"],["body","\n"],["body","6 [remote \"origin\"]\n7   fetch = +refs/heads/*:refs/remotes/origin/*\n8   url = file:///path/to/repos/hello-world.git\n"],["body","\n"],["body","这个小节可以称为[remote]小节，该小节以origin为名注册了一个远程版本库。该版本库的URL地址由第8行给出，会发现这个URL地址就是执行git clone命令时所用的地址。最具魔法的配置是第7行，这一行设置了执行git fetch origin操作时使用的默认引用表达式。"],["body","\n\n"],["body","该引用表达式以加号（+）开头，含义是强制进行引用的替换，即使即将进行的替换是非快进式的。"],["body","\n"],["body","引用表达式中使用了通配符，冒号前面的含有通配符的引用指的是远程版本库的所有分支，冒号后面的引用含义是复制到本地的远程分支目录中。"],["body","\n\n"],["body","正因为有了上面的[remote]配置小节，当执行git fetch origin操作时,就相当于执行了下面的命令，将远程版本库的所有分支复制为本地的远程分支。"],["body","\n"],["body","git fetch origin +refs/heads/*:refs/remotes/origin/*\n"],["body","\n"],["body","远程分支不是真正意义上的分支，是类似于里程碑一样的引用。，如果检出就会使得头指针HEAD处于分离头指针状态"],["body","\n"],["headingLink","分支追踪"],["heading","分支追踪"],["body","\n"],["body","为了能够在远程分支refs/remotes/origin/hello-1.x上进行工作，需要基于该远程分支创建本地分支"],["body","\n"],["body","远程分支可以使用简写origin/hello-1.x。如果Git的版本是1.6.6或者更新的版本，可以使用下面的命令同时完成分支的创建和切换。"],["body","\n"],["body","$ git checkout hello-1.x\nBranch hello-1.x set up to track remote branch hello-1.x from origin.\nSwitched to a new branch 'hello-1.x'\n"],["body","\n"],["body","如果Git的版本比较老，或注册了多个远程版本库，因此存在多个名为hello-1.x的远程分支，就不能使用上面简洁的分支创建和切换命令，而需要使用在上一章中学习到的分支创建命令，显式地从远程分支中创建本地分支。"],["body","\n"],["body","$ git checkout -b hello-1.x origin/hello-1.x\nBranch hello-1.x set up to track remote branch hello-1.x from origin.\nSwitched to a new branch 'hello-1.x'\n"],["body","\n"],["body","在上面基于远程分支创建本地分支的过程中，命令输出的第一行说的是建立了本地分支和远程分支的跟踪。和远程分支建立跟踪后，本地分支就具有下列特征："],["body","\n\n"],["body","检查工作区状态时，会显示本地分支和被跟踪远程分支提交之间的关系。"],["body","\n"],["body","当执行git pull命令时，会和被跟踪的远程分支进行合并（或者变基），如果两者出现版本偏离的话。"],["body","\n"],["body","当执行git push命令时，会推送到远程版本库的同名分支中。"],["body","\n\n"],["body","下面就在基于远程分支创建的本地跟踪分支中进行操作，看看本地分支是如何与远程分支建立关联的。"],["body","\n\n"],["body","先将本地hello-1.x分支向后重置两个版本。"],["body","\n\n"],["body","$ git reset --hard HEAD^^\nHEAD is now at ebcf6d6 blank commit for GnuPG-signed tag test.\n"],["body","\n\n"],["body","然后查看状态，显示当前分支相比跟踪分支落后了3个版本"],["body","\n\n"],["body","之所以落后三个版本而非两个版本是因为hello-1.x的最新提交是一个合并提交，包含两个父提交，因此上面的重置命令丢弃掉三个提交。"],["body","\n\n"],["body","执行git pull命令，会自动与跟踪的远程分支进行合并，相当于找回最新的3个提交。"],["body","\n"],["body","但是如果基于本地分支创建另外一个本地分支则没有分支跟踪的功能"],["body","\n\n"],["body","从远程分支创建本地分支，自动建立了分支间的跟踪，而从一个本地分支创建另外一个本地分支则没有"],["body","\n"],["body"," 9 [branch \"master\"]\n10   remote = origin\n11   merge = refs/heads/master\n12 [branch \"hello-1.x\"]\n13   remote = origin\n14   merge = refs/heads/hello-1.x\n"],["body","\n"],["body","如果希望在基于一个本地分支创建另外一个本地分支时也能够使用分支间的跟踪功能，就要在创建分支时提供--track参数。"],["body","\n"],["body","$ git checkout --track -b hello-jx hello-1.x\nBranch hello-jx set up to track local branch hello-1.x.\nSwitched to a new branch 'hello-jx'\n"],["body","\n"],["body","从Git库的配置文件中会看到为hello-jx分支设置的跟踪。"],["body","\n"],["body","因为跟踪的是本版本库的本地分支，所以第16行设置的远程版本库的名字为一个点。"],["body","\n"],["body","15 [branch \"hello-jx\"]\n16   remote = .\n17   merge = refs/heads/hello-1.x\n"],["body","\n"],["headingLink","远程版本库-1"],["heading","远程版本库"],["body","\n"],["body","名为origin的远程版本库是在版本库克隆时注册的，那么如何注册新的远程版本库呢？"],["body","\n"],["body","下面将版本库file:///path/to/repos/hello-user1.git以new-remote为名进行注册。"],["body","\n"],["body","$ git remote add new-remote file:///path/to/repos/hello-user1.git\n"],["body","\n"],["body","如果再打开版本库的配置文件.git/config会看到新的配置。"],["body","\n"],["body","12 [remote \"new-remote\"]\n13   url = file:///path/to/repos/hello-user1.git\n14   fetch = +refs/heads/*:refs/remotes/new-remote/*\n"],["body","\n"],["body","执行git remote命令，可以更为方便地显示已经注册的远程版本库。"],["body","\n"],["body","$ git remote -v\nnew-remote      file:///path/to/repos/hello-user1.git (fetch)\nnew-remote      file:///path/to/repos/hello-user1.git (push)\norigin  file:///path/to/repos/hello-world.git (fetch)\norigin  file:///path/to/repos/hello-world.git (push)\n"],["body","\n"],["body","现在执行git fetch并不会从新注册的 new-remote 远程版本库获取，因为当前分支设置的默认远程版本库为 origin"],["body","\n"],["body","要想从 new-remote 远程版本库中获取，需要为git fetch命令增加一个参数new-remote。"],["body","\n"],["body","$ git fetch new-remote\nFrom file:///path/to/repos/hello-user1\n * [new branch]      hello-1.x  -> new-remote/hello-1.x\n * [new branch]      helper/master -> new-remote/helper/master\n * [new branch]      helper/v1.x -> new-remote/helper/v1.x\n * [new branch]      master     -> new-remote/master\n"],["body","\n"],["body","从上面的命令输出中可以看出，远程版本库的分支复制到本地版本库前缀为new-remote的远程分支中去了。用git branch -r命令可以看到新增了几个远程分支。"],["body","\n"],["body","$ git branch -r\n  new-remote/hello-1.x\n  new-remote/helper/master\n  new-remote/helper/v1.x\n  new-remote/master\n  origin/HEAD -> origin/master\n  origin/hello-1.x\n  origin/helper/master\n  origin/helper/v1.x\n  origin/master\n"],["body","\n"],["body","更改远程版本库的地址"],["body","\n"],["body","如果远程版本库的URL地址改变，需要更换，该如何处理呢？"],["body","\n"],["body","手工修改.git/config文件是一种方法，"],["body","\n"],["body","用git config命令进行更改是第二种方法，"],["body","\n"],["body","还有一种方法是用git remote命令，如下："],["body","\n"],["body","$ git remote set-url new-remote file:///path/to/repos/hello-user2.git\n"],["body","\n"],["body","可以看到注册的远程版本库的URL地址已经更改。"],["body","\n"],["body","$ git remote -v\nnew-remote      file:///path/to/repos/hello-user2.git (fetch)\nnew-remote      file:///path/to/repos/hello-user2.git (push)\norigin  file:///path/to/repos/hello-world.git (fetch)\norigin  file:///path/to/repos/hello-world.git (push)\n"],["body","\n"],["body","从上面的输出中可以发现每一个远程版本库都有两个URL地址，分别是执行git fetch和git push命令时用到的URL地址。既然有两个地址，就意味着这两个地址可以不同，用下面的命令可以为推送操作设置单独的URL地址。"],["body","\n"],["body","$ git remote set-url --push new-remote /path/to/repos/hello-user2.git\n$ git remote -v\nnew-remote      file:///path/to/repos/hello-user2.git (fetch)\nnew-remote      /path/to/repos/hello-user2.git (push)\norigin  file:///path/to/repos/hello-world.git (fetch)\norigin  file:///path/to/repos/hello-world.git (push)\n"],["body","\n"],["body","当单独为推送设置了URL后，配置文件.git/config的对应[remote]小节也会增加一条新的名为pushurl的配置。如下："],["body","\n"],["body","远程版本库更新"],["body","\n"],["body","当注册了多个远程版本库并希望获取所有远程版本库的更新时，Git提供了一个简单的命令。"],["body","\n"],["body","$ git remote update\nFetching origin\nFetching user2\n"],["body","\n"],["body","如果某个远程版本库不想在执行git remote update时获得更新，可以通过参数关闭自动更新。例如下面的命令关闭远程版本库user2的自动更新。"],["body","\n"],["body","$ git config remote.user2.skipDefaultUpdate true\n$ git remote update\nFetching origin\n"],["body","\n"],["body","删除远程版本库"],["body","\n"],["body","如果想要删除注册的远程版本库，用git remote的rm子命令可以实现。例如删除注册的user2版本库。"],["body","\n"],["body","$ git remote rm user2\n"],["body","\n"],["headingLink","push和pull操作与远程版本库"],["heading","PUSH和PULL操作与远程版本库"],["body","\n"],["body","在Git分支一章，已经介绍过对于新建立的本地分支（没有建立和远程分支的追踪），执行git push命令是不会被推送到远程版本库中，"],["body","\n"],["body","这样的设置是非常安全的，避免了因为误操作将本地分支创建到远程版本库中。当不带任何参数执行git push命令，实际的执行过程是："],["body","\n\n"],["body","\n"],["body","如果为当前分支设置了\\<remote>，即由配置branch.\\<branchname>.remote给出了远程版本库代号，则不带参数执行git push相当于执行了git push <remote>"],["body","\n"],["body","\n"],["body","\n"],["body","如果没有为当前分支设置\\<remote>，则不带参数执行git push相当于执行了git push origin。"],["body","\n"],["body","\n"],["body","\n"],["body","要推送的远程版本库的URL地址由remote.<remote>.pushurl给出。如果没有配置，则使用remote.<remote>.url配置的URL地址。"],["body","\n"],["body","\n"],["body","\n"],["body","如果为注册的远程版本库设置了push参数，即通过remote.<remote>.push配置了一个引用表达式，则使用该引用表达式执行推送。"],["body","\n"],["body","\n"],["body","\n"],["body","否则使用“:”作为引用表达式。该表达式的含义是同名分支推送，即对所有在远程版本库有同名分支的本地分支执行推送。"],["body","\n"],["body","这也就是为什么在一个本地新建分支中执行git push推送操作不会推送也不会报错的原因，因为远程不存在同名分支，所以根本就没有对该分支执行推送，而推送的是其他分支（如果远程版本库有同名分支的话）。"],["body","\n"],["body","\n\n"],["body","在Git分支一章中就已经知道，如果需要在远程版本库中创建分支，则执行命令：git push  <new_branch>。即通过将本地分支推送到远程版本库的方式在远程版本库中创建分支。"],["body","\n"],["body","但是在接下来的使用中会遇到麻烦：不能执行git pull操作（不带参数）将远程版本库中其他人推送的提交获取到本地。"],["body","\n"],["body","这是因为没有建立本地分支和远程分支的追踪，即没有设置branch.<branchname>.remote的值和branch.<branchname>.merge的值。"],["body","\n"],["body","关于不带参数执行git pull命令解释如下："],["body","\n\n"],["body","如果为当前分支设置了<remote>，即由配置branch.<branchname>.remote给出了远程版本库代号，则不带参数执行git pull相当于执行了git pull 。"],["body","\n"],["body","如果没有为当前分支设置<remote>，则不带参数执行git pull相当于执行了git pull origin。"],["body","\n"],["body","要获取的远程版本库的URL地址由remote.<remote>.url给出。"],["body","\n"],["body","如果为注册的远程版本库设置了fetch参数，即通过remote.<remote>.fetch配置了一个引用表达式，则使用该引用表达式执行获取操作。"],["body","\n"],["body","接下来要确定合并的分支。如果设定了branch.<branchname>.merge，则对其设定的分支执行合并，否则报错退出。"],["body","\n\n"],["body","在执行git pull操作的时候可以通过参数--rebase设置使用变基而非合并操作，将本地分支的改动变基到跟踪分支上。为了避免因为忘记使用--rebase参数导致分支的合并，可以执行如下命令进行设置。注意将<branchname>替换为对应的分支名称。"],["body","\n"],["body","$ git config branch.<branchname>.rebase true\n"],["body","\n"],["body","有了这个设置之后，如果是在<branchname>工作分支中执行git pull命令，在遇到冲突（本地和远程分支出现偏离）的情况下，会采用变基操作，而不是默认的合并操作。"],["body","\n"],["body","如果为本地版本库设置参数branch.autosetuprebase，值为true，则在基于远程分支建立本地追踪分支时，会自动配置branch.<branchname>.rebase参数，在执行git pull命令时使用变基操作取代默认的合并操作。"],["body","\n"],["headingLink","里程碑和远程版本库"],["heading","里程碑和远程版本库"],["body","\n"],["body","远程版本库中的里程碑同步到本地版本库，会使用同样的名称，而不会像分支那样移动到另外的命名空间（远程分支）中，这可能会给本地版本库中的里程碑带来混乱"],["body","\n"],["body","前面的Git里程碑一章已经介绍了当执行git push命令推送时，默认不会将本地创建的里程碑带入远程版本库"],["body","\n"],["body","这样可以避免远程版本库上里程碑的泛滥。"],["body","\n"],["body","但是执行git fetch命令从远程版本库获取分支的最新提交时，如果获取的提交上建有里程碑，这些里程碑会被获取到本地版本库"],["body","\n"],["body","当删除注册的远程版本库时，远程分支会被删除，但是该远程版本库引入的里程碑不会被删除，日积月累本地版本库中的里程碑可能会变得愈加混乱。"],["body","\n"],["body","可以在执行git fetch命令的时候，设置不获取里程碑只获取分支及提交。通过提供-n或--no-tags参数可以实现。示例如下："],["body","\n"],["body","$ git fetch --no-tags file:///path/to/repos/hello-world.git \\\n      refs/heads/*:refs/remotes/hello-world/*\n"],["body","\n"],["body","在注册远程版本库的时候，也可以使用--no-tags参数，避免将远程版本库的里程碑引入本地版本库。例如："],["body","\n"],["body","$ git remote add --no-tags hell-world \\\n      file:///path/to/repos/hello-world.git\n"],["body","\n"],["headingLink","分支和里程碑的安全性"],["heading","分支和里程碑的安全性"],["body","\n"],["body","实际上Git版本库本身也提供了一些安全机制避免对版本库的破坏。"],["body","\n\n"],["body","\n"],["body","用reflog记录对分支的操作历史。"],["body","\n"],["body","默认创建的带工作区的版本库都会包含core.logallrefupdates为true的配置，这样在版本库中建立的每个分支都会创建对应的 reflog。但是创建的裸版本库默认不包含这个设置，也就不会为每个分支设置 reflog。如果团队的规模较小，可能因为分支误操作导致数据丢失，可以考虑为裸版本库添加core.logallrefupdates的相关配置。"],["body","\n"],["body","\n\n\n"],["body","关闭非快进式提交。\n\n"],["body","如果将配置receive.denyNonFastForwards设置为true，则禁止一切非快进式推送。但这个配置有些矫枉过正，更好的方法是搭建基于SSH协议的Git服务器，通过钩子脚本更灵活的进行配置。例如：允许来自某些用户的强制提交，而其他用户不能执行非快进式推送。"],["body","\n\n"],["body","\n\n\n"],["body","关闭分支删除功能。"],["body","\n\n"],["body","如果将配置receive.denyDeletes设置为true，则禁止删除分支。同样更好的方法是通过架设基于SSH协议的Git服务器，配置分支删除的用户权限。"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/GIT协作/Git支持的协议.html"],["title","Git支持的协议 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","git支持的协议"],["heading","Git支持的协议"],["body","\n"],["body","协议名称"],["body","语法格式"],["body","说明"],["body","\n"],["body","SSH协议（1）"],["body","ssh://[user@]example.com[:port]/path/to/repo.git/"],["body","可在URL中设置用户名和端口。 默认端口22。"],["body","\n"],["body","SSH协议（2）"],["body","[user@]example.com:path/to/repo.git/"],["body","更为精简的SCP格式表示法，更简洁。 但是非默认端口需要通过其他方式（如地址别名方式）设定。"],["body","\n"],["body","GIT协议"],["body","git://example.com[:port]/path/to/repo.git/"],["body","最常用的只读协议。"],["body","\n"],["body","HTTP[S]协议"],["body","http[s]://example.com[:port]/path/to/repo.git/"],["body","兼有智能协议和哑协议。"],["body","\n"],["body","FTP[S]协议"],["body","ftp[s]://example.com[:port]/path/to/repo.git/"],["body","哑协议。"],["body","\n"],["body","RSYNC协议"],["body","rsync://example.com/path/to/repo.git/"],["body","哑协议。"],["body","\n"],["body","本地协议（1）"],["body","file:///path/to/repo.git"],["body","\n"],["body","本地协议（2）"],["body","/path/to/repo.git"],["body","和file://格式的本地协议类似。但有细微差别。 例如克隆时不支持浅克隆，且采用直接的硬连接实现克隆。"],["body","\n\n\n"],["headingLink","智能协议"],["heading","智能协议"],["body","\n"],["body","在通讯时使用智能协议，会在两个通讯的版本库的各自一端分别打开两个程序进行数据交换。"],["body","\n"],["body","使用智能协议最直观的印象就是在数据传输过程中会有清晰的进度显示，而且因为是按需传输所以传输量更小，速度更快"],["body","\n"],["body","上述协议中SSH、GIT及本地协议（file://）属于智能协议。HTTP协议需要特殊的配置（用git-http-backend配置CGI），并且客户端需要使用Git 1.6.6或更高的版本才能够使用智能协议。"],["body","\n"],["headingLink","哑协议"],["heading","哑协议"],["body","\n"],["body","和智能协议相对的是哑协议。使用哑协议在访问远程版本库的时候，远程版本库不会运行辅助程序，而是完全依靠客户端去主动“发现”"],["body","\n"],["body","客户端需要访问文件.git/info/refs获取当前版本库的引用列表，并根据引用对应的提交ID直接访问对象库目录下的文件。"],["body","\n"],["body","如果对象文件被打包而不以松散对象形式存在，则Git客户端还要去访问文件.git/objects/info/packs以获得打包文件列表，并据此读取完整的打包文件，从打包文件中获取对象"],["body","\n"],["body","由此可见哑协议的效率非常之低，甚至会因为要获取一个对象而去访问整个pack包。"],["body","\n"],["body","使用哑协议最直观的感受是：传输速度非常慢，而且传输进度不可见，不知道什么时候才能够完成数据传输。"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/GIT协作/补丁文件交互.html"],["title","补丁文件交互 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","补丁文件交互"],["heading","补丁文件交互"],["body","\n"],["body","之前各个章节版本库间的交互都是通过git push和/或git pull命令实现的，这是Git最主要的交互模式，但并不是全部。使用补丁文件是另外一种交互方式"],["body","\n"],["body","使用补丁文件是另外一种交互方式，适用于参与者众多的大型项目进行分布式开发"],["body","\n"],["body","例如Git项目本身的代码提交就主要由贡献者通过邮件传递补丁文件实现的。"],["body","\n"],["body","这种使用补丁文件进行提交的方式可以提高项目的参与度。因为任何人都可以参与项目的开发，只要会将提交转化为补丁，会发邮件即可。"],["body","\n"],["headingLink","创建补丁"],["heading","创建补丁"],["body","\n"],["body","Git提供了将提交批量转换为补丁文件的命令：git format-patch。该命令后面的参数是一个版本范围列表，会将包含在此列表中的提交一一转换为补丁文件，每个补丁文件包含一个序号并从提交说明中提取字符串作为文件名。"],["body","\n"],["body","下面演示一下在user1工作区中，如何将master分支的最近3个提交转换为补丁文件。"],["body","\n\n"],["body","\n"],["body","进入user1工作区，切换到master分支。"],["body","\n"],["body","$ cd /path/to/user1/workspace/hello-world/\n$ git checkout master\n$ git pull\n"],["body","\n"],["body","\n"],["body","\n"],["body","执行下面的命令将最近三个提交转换为补丁文件。"],["body","\n"],["body","$ git format-patch -s HEAD~3..HEAD\n0001-Fix-typo-help-to-help.patch\n0002-Add-I18N-support.patch\n0003-Translate-for-Chinese.patch\n"],["body","\n"],["body","\n\n"],["body","在上面的git format-patch命令中使用了-s参数，会在导出的补丁文件中添加当前用户的签名。"],["body","\n"],["body","这个签名并非GnuPG式的数字签名，不过是将作者姓名添加到提交说明中而已，和在本书第2篇开头介绍的git commit -s命令的效果相同"],["body","\n"],["body","虽然签名很不起眼，但是对于以补丁方式提交数据却非常重要，因为以补丁方式提交可能因为合并冲突或其他原因使得最终提交的作者显示为管理员（提交者）的ID，在提交说明中加入原始作者的署名信息大概是作者唯一露脸的机会"],["body","\n"],["body","如果在提交时忘了使用-s参数添加签名，可以在用git format-path命令创建补丁文件的时候补救。"],["body","\n"],["body","看一下补丁文件的文件头，在下面代码中的第7行可以看到新增的签名。"],["body","\n"],["body"," 1 From d81896e60673771ef1873b27a33f52df75f70515 Mon Sep 17 00:00:00 2001\n 2 From: user1 <user1@sun.ossxp.com>\n 3 Date: Mon, 3 Jan 2011 23:48:56 +0800\n 4 Subject: [PATCH 1/3] Fix typo: -help to --help.\n 5\n 6\n 7 Signed-off-by: user1 <user1@sun.ossxp.com>\n 8 ---\n 9  src/main.c |    2 +-\n10  1 files changed, 1 insertions(+), 1 deletions(-)\n"],["body","\n"],["body","补丁文件有一个类似邮件一样的文件头（第1-4行），提交日志的第一行作为邮件标题（Subject），其余提交说明作为邮件内容（如果有的话），文件补丁用三个横线和提交说明分开。"],["body","\n"],["body","实际上这些补丁文件可以直接拿来作为邮件发送给项目的负责人"],["body","\n"],["headingLink","检查patch的情况"],["heading","检查Patch的情况"],["body","\n"],["body","$ git apply --stat 0001-limit-log-function.patch  # 查看patch的情况\n$ git apply --check 0001-limit-log-function.patch # 检查patch是否能够打上，如果没有任何输出，则说明无冲突，可以打上\n"],["body","\n"],["headingLink","回滚补丁"],["heading","回滚补丁"],["body","\n"],["body","git am --abort\n"],["body","\n"],["headingLink","邮件发送补丁"],["heading","邮件发送补丁"],["body","\n"],["body","Git提供了一个辅助邮件发送的命令git send-email。下"],["body","\n"],["body","下面用该命令将这三个补丁文件以邮件形式发送出去。"],["body","\n"],["body","$ git send-email *.patch\n0001-Fix-typo-help-to-help.patch\n0002-Add-I18N-support.patch\n0003-Translate-for-Chinese.patch\nThe following files are 8bit, but do not declare a Content-Transfer-Encoding.\n    0002-Add-I18N-support.patch\n    0003-Translate-for-Chinese.patch\nWhich 8bit encoding should I declare [UTF-8]?\nWho should the emails appear to be from? [user1 <user1@sun.ossxp.com>]\n\nEmails will be sent from: user1 <user1@sun.ossxp.com>\nWho should the emails be sent to? jiangxin\nMessage-ID to be used as In-Reply-To for the first email?\n...\nSend this email? ([y]es|[n]o|[q]uit|[a]ll): a\n...\n"],["body","\n"],["body","命令git send-email提供交互式字符界面，输入正确的收件人地址，邮件就批量地发送出去了。"],["body","\n"],["body","$ git format-patch HEAD^       #生成最近的1次commit的patch\n$ git format-patch HEAD^^      #生成最近的2次commit的patch\n$ git format-patch HEAD^^^     #生成最近的3次commit的patch\n$ git format-patch HEAD^^^^    #生成最近的4次commit的patch\n$ git format-patch <r1>..<r2>  #生成两个commit间的修改的patch（生成的patch不包含r1. <r1>和<r2>都是具体的commit号)\n$ git format-patch -1 <r1>     #生成单个commit的patch\n$ git format-patch <r1>        #生成某commit以来的修改patch（不包含该commit）\n$ git format-patch --root <r1> #生成从根到r1提交的所有patch\n"],["body","\n"],["body","在前面通过git send-email命令发送邮件给jiangxin用户。现在使用 Linux 上的mail命令检查一下邮件。"],["body","\n"],["body","$ mail\nMail version 8.1.2 01/15/2001.  Type ? for help.\n\"/var/mail/jiangxin\": 3 messages 3 unread\n>N  1 user1@sun.ossxp.c  Thu Jan 13 18:02   38/1120  [PATCH 1/3] Fix typo: -help to --help.\n N  2 user1@sun.ossxp.c  Thu Jan 13 18:02  227/6207  =?UTF-8?q?=5BPATCH=202/3=5D=20Add=20I18N=20support=2E?=\n N  3 user1@sun.ossxp.c  Thu Jan 13 18:02   95/2893  =?UTF-8?q?=5BPATCH=203/3=5D=20Translate=20for=20Chinese=2E?=\n&\n"],["body","\n"],["body","如果邮件不止这三封，需要将三个包含补丁的邮件挑选出来保存到另外的文件中。 在 mail 命令的提示符(&)下输入命令。"],["body","\n"],["body","& s 1-3 user1-mail-archive\n\"user1-mail-archive\" [New file]\n& q\n"],["body","\n"],["body","上面的操作在本地创建了一个由开发者user1的补丁邮件组成的归档文件user1-mail-archive，这个文件是mbox格式的，可以用mail命令打开。"],["body","\n"],["body","$ mail -f user1-mail-archive\nMail version 8.1.2 01/15/2001.  Type ? for help.\n\"user1-mail-archive\": 3 messages\n>   1 user1@sun.ossxp.c  Thu Jan 13 18:02   38/1121  [PATCH 1/3] Fix typo: -help to --help.\n    2 user1@sun.ossxp.c  Thu Jan 13 18:02  227/6208  =?UTF-8?q?=5BPATCH=202/3=5D=20Add=20I18N=20support=2E?=\n    3 user1@sun.ossxp.c  Thu Jan 13 18:02   95/2894  =?UTF-8?q?=5BPATCH=203/3=5D=20Translate=20for=20Chinese=2E?=\n& q\n"],["body","\n"],["body","保存在mbox中的邮件可以批量的应用在版本库中，使用git am命令。am是apply email的缩写。下面就演示一下如何应用补丁。"],["body","\n\n"],["body","\n"],["body","基于HEAD~3版本创建一个本地分支，以便在该分支下应用补丁。"],["body","\n"],["body","$ git checkout -b user1 HEAD~3\nSwitched to a new branch 'user1'\n"],["body","\n"],["body","\n"],["body","\n"],["body","将mbox文件user1-mail-archive中的补丁全部应用在当前分支上。"],["body","\n"],["body","$ git am user1-mail-archive\nApplying: Fix typo: -help to --help.\nApplying: Add I18N support.\nApplying: Translate for Chinese.\n"],["body","\n"],["body","\n\n"],["body","$ ls *.patch\n0001-Fix-typo-help-to-help.patch  0002-Add-I18N-support.patch  0003-Translate-for-Chinese.patch\n$ cat *.patch | git am\nApplying: Fix typo: -help to --help.\nApplying: Add I18N support.\nApplying: Translate for Chinese.\n"],["body","\n"],["headingLink","打patch发生冲突"],["heading","打patch发生冲突"],["body","\n"],["headingLink","方案一个人推荐"],["heading","方案一（个人推荐）："],["body","\n\n"],["body","根据git am失败的信息，找到发生冲突的具体patch文件，然后用命令git apply --reject <patch_name>，强行打这个patch，发生冲突的部分会保存为.rej文件（例如发生冲突的文件是a.txt，那么运行完这个命令后，发生conflict的部分会保存为a.txt.rej），未发生冲突的部分会成功打上patch"],["body","\n"],["body","根据.rej文件，通过编辑该patch文件的方式解决冲突"],["body","\n"],["body","废弃上一条am命令已经打了的patch：git am --abort"],["body","\n"],["body","重新打patch：git am ~/patch-set/*.patchpatch"],["body","\n\n"],["headingLink","方案二"],["heading","方案二"],["body","\n\n"],["body","根据git am失败的信息，找到发生冲突的具体patch文件，然后用命令git apply --reject <patch_name>，强行打这个patch，发生冲突的部分会保存为.rej文件（例如发生冲突的文件是a.txt，那么运行完这个命令后，发生conflict的部分会保存为a.txt.rej），未发生冲突的部分会成功打上patch"],["body","\n"],["body","根据.rej文件，通过编辑发生冲突的code文件的方式解决冲突"],["body","\n"],["body","将该patch涉及到的所有文件（不仅仅是发生冲突的文件）通过命令git add <file_name>添加到工作区中"],["body","\n"],["body","告诉git冲突已经解决，继续打patch: git am --resolved (git am --resolved 和 git am --continue是一样的)"],["body","\n\n"],["headingLink","分析"],["heading","分析"],["body","\n"],["body","分析：方案一和方案二主要区别是解决冲突的方法不一样。"],["body","\n"],["body","方案一是通过编辑patch文件的方式解决冲突，方案二是通过编辑冲突code文件的方式解决冲突。"],["body","\n"],["body","这两种方案区别比较大：经过实验，核心区别在于，方案二无法验证冲突有没有切实的解决。"],["body","\n"],["body","即使你在方案二的第二步乱改一通，也能“打完”发生冲突的patch（并没有检测修改后的code文件跟patch期望的是否相同）。"],["body","\n"],["body","因此，如果采用方案二，那么再解决code文件冲突后，需要人工去确认修改的正确性。"],["body","\n"],["body","patch就是打补丁，通过git工具把代码的差分，生成patch文件，然后通过git工具可以直接把patch文件的内容，merge到代码里面。"],["body","\n"],["headingLink","生成patch的命令"],["heading","生成patch的命令"],["body","\n"],["body","git diff > patch //本地变更 git diff 的内容，生成patch文件\ngit diff branchname --cached > patch //branch 之间差分生成patch文件\ngit diff commit-id-new commit-id-old --name-only|xargs tar cjvf xxxxx-patch.tar.bz2　可以根据提交按照目录树快速生成补丁文件.\ngit format-patch HEAD^ //最近一次提交节点的patch\ngit format-patch 节点A 节点B //两个节点之间的patch\n"],["body","\n"],["headingLink","使用patch"],["heading","使用patch"],["body","\n"],["body","git apply patch //将patch文件内容差分到本地，在使用patch之前可以使用以下命令，来测试，是否可以将patch完美打入本地src\ngit apply --check patch\n最后补充一句，强制打patch。\n\ngit apply --reject xxx.patch\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/GIT协作/冲突解决.html"],["title","冲突解决 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","冲突解决"],["heading","冲突解决"],["body","\n"],["headingLink","拉回操作中的合并"],["heading","拉回操作中的合并"],["body","\n"],["body","真实的运行环境中，用户间协同并不总是会一帆风顺，只要有合并就可能会有冲突。本章就重点介绍冲突解决机制。"],["body","\n"],["body","git pull操作解决非快进式推送问题的步骤"],["body","\n\n"],["body","用户user1向共享版本库推送时，因为user2强制推送已经改变了共享版本库中的提交状态，导致user1推送失败，"],["body","\n"],["body","用户user1执行PULL操作的第一阶段，将共享版本库master分支的最新提交拉回到本地，并更新到本地版本库特定的引用refs/remotes/origin/master（简称为origin/master）"],["body","\n"],["body","用户user1执行PULL操作的第二阶段，将本地分支master和共享版本库本地跟踪分支origin/master进行合并操作，"],["body","\n"],["body","用户user1执行PUSH操作，将本地提交推送到共享版本库中"],["body","\n\n"],["body","实际上拉回（PULL）操作是由两个步骤组成的，一个是获取（FETCH）操作，一个是合并（MERGE）操作，即："],["body","\n"],["body","git pull = git fetch + git merge\n"],["body","\n"],["body","将获取操作理解为将远程的共享版本库的对象（提交、里程碑、分支等）复制到本地即可。"],["body","\n"],["body","合并操作的命令行格式如下："],["body","\n"],["body","git merge [选项...] <commit>...\n"],["body","\n"],["body","合并操作的大多数情况，只须提供一个<commit>（提交ID或对应的引用：分支、里程碑等）作为参数"],["body","\n"],["body","合并操作将<commit>对应的目录树和当前工作分支的目录树的内容进行合并，合并后的提交以当前分支的提交作为第一个父提交，以\\<commit>为第二个父提交"],["body","\n"],["body","合并操作还支持将多个<commit>代表的分支和当前分支进行合并，过程类似"],["body","\n"],["body","默认情况下，合并后的结果会自动提交，但是如果提供--no-commit选项，则合并后的结果会放入暂存区，用户可以对合并结果进行检查、更改，然后手动提交"],["body","\n"],["body","合并操作并非总会成功，因为合并的不同提交可能同时修改了同一文件相同区域的内容，导致冲突。冲突会造成合并操作的中断，冲突的文件被标识"],["body","\n"],["body","用户可以对标识为冲突的文件进行冲突解决操作，然后更新暂存区，再提交，最终完成合并操作"],["body","\n"],["body","根据合并操作是否遇到冲突，以及不同的冲突类型，可以分为以下几种情况："],["body","\n\n"],["body","成功的自动合并"],["body","\n"],["body","逻辑冲突"],["body","\n"],["body","真正的冲突"],["body","\n"],["body","树冲突"],["body","\n\n"],["headingLink","自动合并"],["heading","自动合并"],["body","\n"],["body","Git的合并操作非常智能，大多数情况下会自动完成合并。不管是修改不同的文件，还是修改相同的文件（文件的不同位置），或者文件名变更。"],["body","\n"],["headingLink","修改不同的文件"],["heading","修改不同的文件"],["body","\n"],["body","$ git fetch\n$ git merge origin/master\n$ git push\n# 追溯文件的每一行\n$ git blame README\n07e9d082 (user1 2010-12-25 23:12:17 +0800 1) User1 hacked.\n^5174bf3 (user1 2010-12-19 15:52:29 +0800 2) Hello.\nbb0c74fa (user2 2010-12-25 23:14:27 +0800 3) User2 hacked.\n"],["body","\n"],["headingLink","修改相同文件不同地方"],["heading","修改相同文件不同地方"],["body","\n"],["headingLink","重命名与修改"],["heading","重命名与修改"],["body","\n"],["headingLink","修改相同文件相同地方"],["heading","修改相同文件相同地方"],["body","\n"],["body","记录冲突"],["body","\n"],["body","那么Git是如何记录合并过程及冲突的呢？实际上合并过程是通过.git目录下的几个文件进行记录的："],["body","\n\n"],["body","文件.git/MERGE_HEAD记录所合并的提交ID。"],["body","\n"],["body","文件.git/MERGE_MSG记录合并失败的信息。"],["body","\n"],["body","文件.git/MERGE_MODE标识合并状态。"],["body","\n\n"],["body","版本库暂存区中则会记录冲突文件的多个不同版本。可以使用git ls-files命令查看。"],["body","\n"],["body","$ git ls-files -s\n100644 ea501534d70a13b47b3b4b85c39ab487fa6471c2 1       doc/README.txt\n100644 5611db505157d312e4f6fb1db2e2c5bac2a55432 2       doc/README.txt\n100644 036dbc5c11b0a0cefc8247cf0e9a3e678f8de060 3       doc/README.txt\n100644 430bd4314705257a53241bc1d2cb2cc30f06f5ea 0       team/user1.txt\n100644 a72ca0b4f2b9661d12d2a0c1456649fc074a38e3 0       team/user2.txt\n"],["body","\n\n"],["body","\n"],["body","编号为1的暂存区用于保存冲突文件修改之前的副本，即冲突双方共同的祖先版本。可以用:1:<filename>访问。"],["body","\n"],["body","$ git show :1:doc/README.txt\nUser1 hacked.\nHello.\nUser2 hacked.\nUser2 hacked again.\n"],["body","\n"],["body","\n\n\n"],["body","编号为2的暂存区用于保存当前冲突文件在当前分支中修改的副本。可以用:2:<filename>访问。"],["body","\n\n"],["body","$ git show :2:doc/README.txt\nUser1 hacked.\nHello, user2.\nUser2 hacked.\nUser2 hacked again.\n"],["body","\n\n"],["body","\n"],["body","编号为3的暂存区用于保存当前冲突文件在合并版本（分支）中修改的副本。可以用:3:<filename>访问。"],["body","\n"],["body","$ git show :3:doc/README.txt\nUser1 hacked.\nHello, user1.\nUser2 hacked.\nUser2 hacked again.\n"],["body","\n"],["body","\n\n"],["headingLink","树冲突"],["heading","树冲突"],["body","\n"],["body","如果一个用户将某个文件改名，另外一个用户将同样的文件改为另外的名字，当这两个用户的提交进行合并操作时，Git显然无法替用户做出裁决，于是就产生了冲突。这种因为文件名修改造成的冲突，称为树冲突。"],["body","\n"],["body","此时查看一下用户user2本地版本库的暂存区，可以看到因为冲突在编号为1、2、3的暂存区出现了相同SHA1哈希值的对象，但是文件名各不相同。"],["body","\n"],["body","$ git ls-files -s\n100644 463dd451d94832f196096bbc0c9cf9f2d0f82527 2       README\n100644 463dd451d94832f196096bbc0c9cf9f2d0f82527 1       doc/README.txt\n100644 463dd451d94832f196096bbc0c9cf9f2d0f82527 3       readme.txt\n100644 430bd4314705257a53241bc1d2cb2cc30f06f5ea 0       team/user1.txt\n100644 a72ca0b4f2b9661d12d2a0c1456649fc074a38e3 0       team/user2.txt\n"],["body","\n"],["body","手工操作解决树冲突"],["body","\n\n"],["body","\n"],["body","删除文件readme.txt。"],["body","\n"],["body","在执行git rm操作过程会弹出三条警告，说共有三个文件待合并。"],["body","\n"],["body","$ git rm readme.txt\nREADME: needs merge\ndoc/README.txt: needs merge\nreadme.txt: needs merge\nrm 'readme.txt'\n"],["body","\n"],["body","\n\n\n"],["body","删除文件doc/README.txt。"],["body","\n\n"],["body","​\t执行删除过程，弹出的警告少了一条，因为前面的删除操作已经将一个冲突文件撤出暂存区了。"],["body","\n\n"],["body","添加文件README"],["body","\n\n"],["body","$ git add README\n"],["body","\n\n"],["body","\n"],["body","提交完成冲突解决。"],["body","\n"],["body","$ git commit -m \"fixed tree conflict.\"\n[master e82187e] fixed tree conflict.\n"],["body","\n"],["body","\n\n"],["headingLink","合并策略"],["heading","合并策略"],["body","\n"],["body","Git合并操作支持很多合并策略，默认会选择最适合的合并策略。例如，和一个分支进行合并时会选择recursive合并策略，当和两个或两个以上的其他分支进行合并时采用octopus合并策略。可以通过传递参数使用指定的合并策略，命令行如下："],["body","\n"],["body","git merge [-s <strategy>] [-X <strategy-option>] <commit>...\n"],["body","\n"],["body","其中参数-s用于设定合并策略，参数-X用于为所选的合并策略提供附加的参数。"],["body","\n"],["body","下面分别介绍不同的合并策略："],["body","\n\n"],["body","\n"],["body","resolve"],["body","\n"],["body","该合并策略只能用于合并两个头（即当前分支和另外的一个分支），使用三向合并策略。这个合并策略被认为是最安全、最快的合并策略。"],["body","\n"],["body","\n"],["body","\n"],["body","recursive"],["body","\n"],["body","该合并策略只能用于合并两个头（即当前分支和另外的一个分支），使用三向合并策略。这个合并策略是合并两个头指针时的默认合并策略。"],["body","\n"],["body","当合并的头指针拥有一个以上的祖先的时候，会针对多个公共祖先创建一个合并的树，并以此作为三向合并的参照。这个合并策略被认为可以实现冲突的最小化，而且可以发现和处理由于重命名导致的合并冲突。"],["body","\n"],["body","这个合并策略可以使用下列选项。"],["body","\n\n"],["body","\n"],["body","ours"],["body","\n"],["body","在遇到冲突的时候，选择我们的版本（当前分支的版本），而忽略他人的版本。如果他人的改动和本地改动不冲突，会将他人改动合并进来。"],["body","\n"],["body","不要将此模式和后面介绍的单纯的ours合并策略相混淆。后面介绍的ours合并策略直接丢弃其他分支的变更，无论冲突与否。"],["body","\n"],["body","\n"],["body","\n"],["body","theirs"],["body","\n"],["body","和ours选项相反，遇到冲突时选择他人的版本，丢弃我们的版本。"],["body","\n"],["body","\n"],["body","\n"],["body","subtree[=path]"],["body","\n"],["body","这个选项使用子树合并策略，比下面介绍的subtree（子树合并）策略的定制能力更强。下面的subtree合并策略要对两个树的目录移动进行猜测，而recursive合并策略可以通过此参数直接对子树目录进行设置。"],["body","\n"],["body","\n\n"],["body","\n"],["body","\n"],["body","octopus"],["body","\n"],["body","可以合并两个以上的头指针，但是拒绝执行需要手动解决的复杂合并。主要的用途是将多个主题分支合并到一起。这个合并策略是对三个及三个以上头指针进行合并时的默认合并策略。"],["body","\n"],["body","\n"],["body","\n"],["body","ours"],["body","\n"],["body","可以合并任意数量的头指针，但是合并的结果总是使用当前分支的内容，丢弃其他分支的内容。"],["body","\n"],["body","\n"],["body","\n"],["body","subtree"],["body","\n"],["body","这是一个经过调整的recursive策略。当合并树A和B时，如果B和A的一个子树相同，B首先进行调整以匹配A的树的结构，以免两棵树在同一级别进行合并。同时也针对两棵树的共同祖先进行调整。"],["body","\n"],["body","\n\n"],["headingLink","合并相关的设置"],["heading","合并相关的设置"],["body","\n"],["body","可以通过git config命令设置与合并相关的环境变量，对合并进行配置。下面是一些常用的设置。"],["body","\n\n"],["body","\n"],["body","merge.conflictstyle"],["body","\n"],["body","该变量定义冲突文件的显示风格，有两个可用的风格，默认的“merge”或“diff3”。"],["body","\n"],["body","默认的“merge”风格使用标准的冲突分界符（<<<<<<<、=======、>>>>>>>）对冲突内容进行标识，其中的两个文字块分别是本地的修改和他人的修改。"],["body","\n"],["body","如果使用“diff3”风格，则会在冲突中出现三个文字块，分别是：<<<<<<<和|||||||之间的本地更改版本、|||||||和=======之间的原始（共同祖先）版本和=======和>>>>>>>之间的他人更改的版本。例如："],["body","\n"],["body","User1 hacked.\n<<<<<<< HEAD\nHello, user2.\n||||||| merged common ancestors\nHello.\n=======\nHello, user1.\n>>>>>>> a123390b8936882bd53033a582ab540850b6b5fb\nUser2 hacked.\nUser2 hacked again.\n"],["body","\n"],["body","\n"],["body","\n"],["body","merge.tool"],["body","\n"],["body","执行git mergetool进行冲突解决时调用的图形化工具。变量merge.tool可以设置为如下内置支持的工具：“kdiff3”、“tkdiff”、“meld”、“xxdiff”、“emerge”、“vimdiff”、“gvimdiff”、“diffuse”、“ecmerge”、“tortoisemerge”、“p4merge”、“araxis”和“opendiff”。"],["body","\n"],["body","$ git config --global merge.tool kdiff3\n"],["body","\n"],["body","如果将merge.tool设置为其他值，则使用自定义工具进行冲突解决。自定义工具需要通过mergetool.<tool>.cmd对自定义工具的命令行进行设置。"],["body","\n"],["body","\n"],["body","\n"],["body","mergetool..path"],["body","\n"],["body","如果git mergetool支持的冲突解决工具安装在特殊位置，可以使用mergetool.<tool>.path对工具<tool>的安装位置进行设置。例如："],["body","\n"],["body","$ git config --global mergetool.kdiff3.path /path/to/kdiff3\n"],["body","\n"],["body","\n"],["body","\n"],["body","mergetool..cmd"],["body","\n"],["body","如果所用的冲突解决工具不在内置的工具列表中，还可以使用mergetool.<tool>.cmd对自定义工具的命令行进行设置，同时要将merge.tool设置为<tool>。"],["body","\n"],["body","自定义工具的命令行可以使用Shell变量。例如："],["body","\n"],["body","$ git config --global merge.tool mykdiff3\n$ git config --global mergetool.mykdiff3.cmd '/usr/bin/kdiff3\n             -L1 \"$MERGED (Base)\" -L2 \"$MERGED (Local)\" -L3 \"$MERGED (Remote)\"\n             --auto -o \"$MERGED\" \"$BASE\" \"$LOCAL\" \"$REMOTE\"'\n"],["body","\n"],["body","\n"],["body","\n"],["body","merge.log"],["body","\n"],["body","是否在合并提交的提交说明中包含合并提交的概要信息。默认为false。"],["body","\n"],["body","\n\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/git指针与引用.html"],["title","git指针与引用 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","orig_head与reflog"],["heading","ORIG_HEAD与reflog"],["body","\n\n"],["body","\n"],["body","HEAD指针代表当前工作路径"],["body","\n"],["body","\n"],["body","\n"],["body","HEAD与master指向同一id说明当前处在master分支"],["body","\n"],["body","\n"],["body","\n"],["body","针对某些危险操作，Git通过记录HEAD指针的上次所在的位置ORIG_HEAD提供了回退的功能。当你发现某些操作失误了，比如错误的reset到了一个很早很早的版本，可以使用git reset --hard ORIG_HEAD回退到上一次reset之前。"],["body","\n"],["body","\n"],["body","\n"],["body","Git在1.8.5版本之后，加入了HEAD@{}功能，它通过一个链表记录HEAD的移动路径。"],["body","\n"],["body","\n"],["body","\n"],["body","git reflog"],["body","\n"],["body","\n"],["body","\n"],["body","每一次移动HEAD指针，Git都会将移动的路径通过链表串起来，链表头部的HEAD@{0}即HEAD指针。"],["body","\n"],["body","\n"],["body","\n"],["body","但是HEAD@{1}并不一定是ORIG_HEAD！注意到，ORIG_HEAD仅仅是当进行危险操作（比如merge）时才会变更为HEAD指针的原值，而HEAD@{}链表则记录了每次HEAD的移动（包括commit）。"],["body","\n"],["body","\n"],["body","\n"],["body","考虑以下情况："],["body","\n"],["body","\n\n"],["body",".commit -> 2.merge -> 3.commit\n"],["body","\n"],["body","此时，HEAD@{0}、HEAD@{1}、HEAD@{2}分别指向3、2、1，而ORIG_HEAD指向的是1而非2。\n"],["body","\n\n"],["body","显然，有了reflog命令后HEAD链表比不知道什么变过的ORIG_HEAD更好用，因此如果你使用的是1.8.5版本之后的Git，推荐使用HEAD{}链表来代替ORIG_HEAD指针。"],["body","\n\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/gitflow.html"],["title","gitflow - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","gitflow分支概念"],["heading","gitflow分支概念"],["body","\n"],["body","分支名称"],["body","作用"],["body","生命周期"],["body","提交or合并"],["body","起止点"],["body","\n"],["body","Production (master)"],["body","记录历史发布版本"],["body","贯穿整个项目"],["body","不能提交，由release分支合并"],["body","整个项目"],["body","\n"],["body","Develop分支"],["body","记录历史开发功能"],["body","贯穿整个项目"],["body","不能提交，由feature分支，Bugfix分支，Release分支合并代码"],["body","整个项目"],["body","\n"],["body","Hotfix分支"],["body","解决线上bug"],["body","临时分支，紧急修复"],["body","可提交"],["body","由生产分支产生，最终合并进生产分支，与开发分支"],["body","\n"],["body","Reslease分支"],["body","用于本次的Release如文档，bug修复，测试"],["body","临时分支，发版阶段"],["body","可提交"],["body","由开发分支产生，合并到开发分支与生产分支"],["body","\n"],["body","Feature分支"],["body","用于某个功能的开发"],["body","临时分支，开发阶段"],["body","可提交"],["body","由Develop分支产生，合并到Develop分支中"],["body","\n\n\n"],["headingLink","从不同角度理解分支"],["heading","从不同角度理解分支"],["body","\n"],["headingLink","生命周期"],["heading","生命周期"],["body","\n\n"],["body","production 分支和develop分支 贯穿项目\n\n"],["body","production 分支 记录发布版本，由release分支合并而来"],["body","\n"],["body","develop 分支记录各个功能点的开发进度"],["body","\n\n"],["body","\n"],["body","其他分支，均为承担特定功能的 临时分支\n\n"],["body","hotfix bug修复分支，由生产分支产生，合并到生产分支与开发分支"],["body","\n"],["body","Reslease：用于本次的发布，由开发分支产生，合并到开发分支与生产分支"],["body","\n"],["body","feature：用于某个功能的开发 "],["body","\n\n"],["body","\n\n"],["headingLink","项目阶段"],["heading","项目阶段"],["body","\n\n"],["body","开发阶段主要涉及  feature分支 ，develop分支 "],["body","\n"],["body","发布阶段主要涉及release分支，production 分支，develop 分支"],["body","\n"],["body","紧急修复阶段：主要涉及hotfix分支，production分支，develop分支 "],["body","\n\n"],["headingLink","成员关注点"],["heading","成员关注点"],["body","\n\n"],["body","开发人员 关注develop 分支，feature分支，hotfix分支"],["body","\n"],["body","测试人员关注 release分支，hotfix分支"],["body","\n"],["body","项目经理关注production分支，release分支"],["body","\n\n"],["headingLink","实践个完整的git-flow流程"],["heading","实践⼀个完整的Git-Flow流程"],["body","\n"],["body","git init\ngit add .\ngit commit -m \"project init\"\ngit checkout -b develop master //切换分支时，从master分支创建 develop分支\ngit checkout -b feature-login develop\ngit add .\ngit commit -m \"add loginUser.html\"\ngit checkout develop\ngit merge --no-ff feature-login\ngit branch -d feature-login\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/index.html"],["title","GIT - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","关于版本控制"],["heading","关于版本控制"],["body","\n"],["body","版本控制是一种记录一个或若干文件内容变化,以便将来查阅特定版本修订情况的系统"],["body","\n"],["headingLink","版本控制系统的变迁"],["heading","版本控制系统的变迁"],["body","\n"],["headingLink","本地版本控制"],["heading","本地版本控制"],["body","\n\n"],["body","Revision Control System (RCS)  是一种最流行的本地版本控制系统"],["body","\n"],["body","工作原理是在硬盘上保存补丁集（补丁是指文件修订前后的变化）,通过应用所有的补丁，可以重新计算出各个版本的文件内容。"],["body","\n\n"],["headingLink","集中化的版本控制系统"],["heading","集中化的版本控制系统"],["body","\n"],["body","​\t但是如何让在不同系统上的开发者协同工作？集中化的版本控制系统（Centralized Version Control Systems，简称 CVCS）应运而生,诸如 CVS、Subversion 以及 Perforce 等,都有一个单一的集中管理的服务器，保存所有文件的修订版本,而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新"],["body","\n"],["body","​\t如果中心数据库所在的磁盘发生损坏,项目的整个变更历史将会丢失"],["body","\n"],["headingLink","分布式版本控制系统"],["heading","分布式版本控制系统"],["body","\n"],["body","​\t分布式版本控制系统（Distributed Version Control System，简称 DVCS）很好的解决了上面的问题,像 Git、Mercurial、Bazaar 以及 Darcs 等"],["body","\n"],["body","​\t客户端并不只提取最新版本的文件快照， 而是把代码仓库完整地镜像下来，包括完整的历史记录"],["body","\n"],["body","​\t任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复"],["body","\n"],["headingLink","什么是git"],["heading","什么是git"],["body","\n"],["headingLink","git-和其它版本控制系统的差别"],["heading","Git 和其它版本控制系统的差别"],["body","\n"],["body","Git 和其它版本控制系统（包括 Subversion 和近似工具）的主要差别在于 Git 对待数据的方法"],["body","\n\n"],["body","\n"],["body","其他系统对待数据的方式"],["body","\n"],["body","一组基本文件和每个文件随时间逐步累积的差异,通常给称为 (基于差异（delta-based）)"],["body","\n"],["body","\n"],["body","\n"],["body","而Git 更像是把数据看作是对小型文件系统的一系列快照"],["body","\n\n"],["body","每当你提交更新或保存项目状态时，它基本上就会对当时的全部文件创建一个快照并保存这个快照的索引"],["body","\n"],["body","为了效率,如果文件没有修改，Git 不再重新存储该文件，而是只保留一个链接指向之前存储的文件"],["body","\n"],["body","Git 对待数据更像是一个 快照流。"],["body","\n"],["body","Git 更像是一个小型的文件系统"],["body","\n\n"],["body","\n\n"],["headingLink","git保证完整性"],["heading","GIT保证完整性"],["body","\n\n"],["body","Git 中所有的数据在存储前都计算校验和，然后以校验和来引用"],["body","\n"],["body","Git 用以计算校验和的机制叫做 SHA-1 散列"],["body","\n"],["body","这是一个由 40 个十六进制字符(0~F)组成的字符串"],["body","\n"],["body","Git 数据库中保存的信息都是以文件内容的哈希值来索引"],["body","\n\n"],["headingLink","git-一般只添加数据"],["heading","Git 一般只添加数据"],["body","\n\n"],["body","你执行的 Git 操作，几乎只往 Git 数据库中 添加 数据, 你很难让 Git 执行任何不可逆操作"],["body","\n\n"],["headingLink","三种状态"],["heading","三种状态"],["body","\n"],["body","已提交（committed）"],["body","\n"],["body","已修改（modified） "],["body","\n"],["body","已暂存（staged）"],["body","\n"],["headingLink","git的配置"],["heading","GIT的配置"],["body","\n"],["headingLink","配置文件"],["heading","配置文件"],["body","\n\n"],["body","git config 命令来帮助设置 控制 Git 外观和行为的 配置变量"],["body","\n"],["body","这些变量可能会存储在三个不同的位置\n\n"],["body","/etc/gitconfig :所有用户的通用配置, git config 时带上 --system 选项时会读写该文件的配置变量"],["body","\n"],["body","/.gitconfig或/.config/git/config 当前用户的变量,--global选项为读写此文件,为当前用户的所有仓库的通用配置"],["body","\n"],["body",".git/config 针对该仓库 --local选项读取该文件,默认情况下使用它(当然，你需要进入某个 Git 仓库中才能让该选项生效)"],["body","\n\n"],["body","\n\n"],["body","每一个级别会覆盖上一级别的配置"],["body","\n"],["headingLink","设置用户信息"],["heading","设置用户信息"],["body","\n"],["body","安装完 Git 之后，要做的第一件事就是设置你的用户名和邮件地址"],["body","\n"],["body","$ git config --global user.name \"John Doe\"\n$ git config --global user.email johndoe@example.com\n"],["body","\n"],["headingLink","文本编辑器"],["heading","文本编辑器"],["body","\n"],["body","git config --global core.editor emacs\n\n$ git config --global core.editor \"'C:/Program Files/Notepad++/notepad++.exe' -multiInst -notabbar -nosession -noPlugin\"\n"],["body","\n"],["headingLink","命令其他使用"],["heading","命令其他使用"],["body","\n"],["body","你可能会看到重复的变量名，因为 Git 会从不同的文件中读取同一个配置,"],["body","\n"],["body","//检查某一项配置\ngit config <key>\ngit config list\n"],["body","\n"],["headingLink","获取帮助"],["heading","获取帮助"],["body","\n"],["body","//全面手册\ngit help config\n\n//快速参考\ngit add -h\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/git命令杂项.html"],["title","git命令杂项 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","拉取远程分支合并到本地分支"],["heading","拉取远程分支合并到本地分支"],["body","\n"],["body","git pull origin feature/develop-4.4.0-public\ngit pull upstream feature/develop-4.4.0-public\n"],["body","\n"],["headingLink","删除未加入到版本管理的文件"],["heading","删除未加入到版本管理的文件"],["body","\n"],["body","git clean -fd\n"],["body","\n"],["headingLink","将版本库里的head替换工作区与暂存库"],["heading","将版本库里的head替换工作区与暂存库"],["body","\n"],["body","git checkout head . \n"],["body","\n"],["headingLink","不分页"],["heading","不分页"],["body","\n"],["body","git config --global core.pager cat \n"],["body","\n"],["headingLink","分页"],["heading","分页"],["body","\n"],["body","git config --global core.pager less\n"],["body","\n"],["headingLink","bare-repo"],["heading","bare repo"],["body","\n"],["body","# 只有版本库，没有工作区的仓库。专门用于中心化存储\ngit init --bare\n\n# 克隆仓库，并作为裸仓库\ngit clone --mirror https://xxxx\n"],["body","\n"],["headingLink","git显示两个分支的提交差异"],["heading","GIT显示两个分支的提交差异"],["body","\n"],["body","# 在 newBranch不在 oldBranch的提交\ngit log oldBranch..newBranch\ngit branch oldbranch..newbranch\n"],["body","\n"],["headingLink","git-diff"],["heading","GIT DIFF"],["body","\n"],["body","# diff工具\ngit difftool\n# 比较文件名\ngit difftool location\\filename\n\n## diff 当前版本的文件和某个commit的某个文件\ngit difftool 3693493981a35c07f2bee7cae71f8e8bd95be625 -- filename\n\n\n## \ngit difftool [start commit]..[end commit] filename\n\n##\ngit log filename  # 查看某个文件的提交记录\n\ngit difftool 6cde26245763dd43f9505c7578a1f7be44b7fad1..8d5336398  -- filename\n\n\ngit diff HEAD^ -- filePath\n\n\ngit diff：是查看 workspace 与 index 的差别的。\ngit diff --cached：是查看 index 与 local repositorty 的差别的。\ngit diff HEAD：是查看 workspace 和 local repository 的差别的。（HEAD 指向的是 local repository 中最新提交的版本）\n"],["body","\n"],["body","注：git diff 后跟两个参数，如果只写一个参数，表示默认跟 workspace中的代码作比较。git diff 显示的结果为 第二个参数所指的代码在第一个参数所指代码基础上的修改。如，git diff HEAD 表示 workspace 在 最新commit的基础上所做的修改。"],["body","\n"],["headingLink","git-difftool"],["heading","git difftool"],["body","\n"],["body","git difftool [<options>] [<commit> [<commit>]] [--] [<path>…​]\n"],["body","\n\n"],["body","\n"],["body","git difftool是一个 Git 命令，允许您使用常见差异工具在修订之间比较和编辑文件。git difftool是前端git diff并接受相同的选项和参数。参见 git-diff [1]。"],["body","\n"],["body","-d   --dir-diff   : 将修改后的文件复制到临时位置，然后对它们执行一个目录 diff。该模式在启动 diff 工具之前从不提示。"],["body","\n"],["body","-y   --no-prompt   :启动 diff 工具前不要提示。"],["body","\n"],["body","--prompt   :在每次调用 diff 工具前提示。这是默认行为; 该选项用于覆盖任何配置设置。"],["body","\n"],["body","\n\n"],["body","-t <tool>   --tool=<tool>   \n   \n使用<tool>指定的 diff 工具。有效值包括 emerge，kompare，meld 和 vimdiff。运行git difftool --tool-help有效的<工具>设置列表。\n   \n如果没有指定 diff 工具，git difftool将使用配置变量diff.tool。\n如果配置变量diff.tool没有设置，git difftool会选择一个合适的默认值。\n\n您可以通过设置配置变量明确提供工具的完整路径difftool.<tool>.path。例如，您可以通过设置配置 kdiff3 的绝对路径difftool.kdiff3.path。否则，git difftool假定该工具在 PATH 中可用。\n"],["body","\n"],["headingLink","不合并特定文件"],["heading","不合并特定文件"],["body","\n"],["body","echo 'index.php merge=ours' >> .gitattributes\ngit add .gitattributes\n"],["body","\n"],["headingLink","git-只合并某个目录文件"],["heading","git 只合并某个目录/文件"],["body","\n"],["body","git checkout 分支名 目录/** 目录2/**\n\n比如：git checkout pmc dist/**\n\n(目录下可能还有多个目录所以用/** 不用/*，单独只合并某个文件的话，路径准确就行)\n"],["body","\n"],["headingLink","git-设置-合并分支时-忽略某个文件"],["heading","git 设置 合并分支时 忽略某个文件"],["body","\n"],["headingLink","git解决冲突时使用outrs"],["heading","git解决冲突时使用outrs"],["body","\n"],["body"," merge\ngit checkout --ours word.txt    # => chocolate\ngit checkout --theirs word.txt  # => boycott\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/git错误提交后场景.html"],["title","git错误提交后场景 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","修改最后一次提交"],["heading","修改最后一次提交"],["body","\n"],["body","修补提交"],["body","\n"],["body","git commit --amend\n"],["body","\n"],["body","它可以把我们这一次的修改合并到上一条历史记录当中"],["body","\n"],["headingLink","修改多个信息"],["heading","修改多个信息"],["body","\n"],["body","--amend虽然好用，但是它只能修改最后一次的提交信息，如果我们想要修改的提交记录在那之前，我们应该怎么办呢？"],["body","\n"],["body","git当中并没有提供直接的工具来实现这一点，不过我们可以使用rebase来达成。我们可以加上-i进行交互式地变基，我们可以在任何想要的修改完成之后停止，也可以添加文件或者是做其他想要做的事情。但是我们变基的目标不是某一个分支而是当前分支的某一个历史节点，所以我们需要提供一个具体的commitid或者是指针位置。"],["body","\n"],["body","git rebase -i的功能非常强大，我们几乎可以使用它来完成所有一切我们想要完成的事情。"],["body","\n"],["body","比如我们想要修改倒数第二次提交，我们可以执行git rebase -i HEAD~3。也就是以倒数第三个节点作为基准节点执行变基，这时候git会进入一个vim窗口，在这个窗口当中我们可以看到最近的三次提交记录。"],["body","\n"],["body","首先我们可以看到上面的三行就是我们可以修改的三个commit，分别展示的是要执行的操作以及commitid以及commit message。这里的操作默认的是pick，也就是使用该commit。关于我们可以执行的操作git在下方也给了充分的提示，其中比较常用的有pick、edit以及squash。"],["body","\n"],["body","这一次我们想要做的是修改提交记录，所以我们应该执行edit，我们把想要修改的commit前的pick改成edit。比如这样："],["body","\n"],["body","退出之后，git会自动带我们回到我们选择edit的分支提交之后的版本。我们进行我们想要的修改，这里我在第15篇文章当中加上了一行：尝试rebase。之后再使用git add以及git commit --amend进行修改提交结果。"],["body","\n"],["body","再之后我们执行git rebase --continue，把剩下要应用的变更应用完成"],["body","\n"],["body","一切都结束之后，我们可以使用一下git show命令查看一下我们修改的bee9ce3这个commit的记录。可以看到已经多了这一行，说明我们的修改成功了。"],["body","\n"],["headingLink","撤回提交"],["heading","撤回提交"],["body","\n"],["headingLink","revert撤回"],["heading","revert撤回"],["body","\n"],["body","\n"],["body","revert 可以取消指定的某次提交内容，原理是生成相应的提交 抵消"],["body","\n"],["body","\n"],["headingLink","概述"],["heading","概述"],["body","\n"],["body","当讨论 revert 时，需要分两种情况，因为 commit 分为两种：一种是常规的 commit，也就是使用 git commit 提交的 commit；另一种是 merge commit，在使用 git merge 合并两个分支之后，你将会得到一个新的 merge commit。"],["body","\n"],["body","merge commit 和普通 commit 的不同之处在于 merge commit 包含两个 parent commit，代表该 merge commit 是从哪两个 commit 合并过来的。"],["body","\n"],["body","git show bd86846\n"],["body","\n"],["body","revert 常规 commit"],["body","\n"],["body","`git revert <commit id>` # 即可，git 会生成一个新的 commit，将指定的 commit 内容从当前分支上撤除。\n"],["body","\n"],["body","revert merge commit"],["body","\n"],["body","revert merge commit 有一些不同，这时需要添加 -m 选项以代表这次 revert 的是一个 merge commit"],["body","\n"],["body","但如果直接使用 git revert ，git 也不知道到底要撤除哪一条分支上的内容，这时需要指定一个 parent number 标识出\"主线\"，主线的内容将会保留，而另一条分支的内容将被 revert。"],["body","\n"],["body","如上面的例子中，从 git show 命令的结果中可以看到，merge commit 的 parent 分别为 ba25a9d 和 1c7036f，其中 ba25a9d 代表 master 分支（从图中可以看出），1c7036f 代表 will-be-revert 分支。需要注意的是 -m 选项接收的参数是一个数字，数字取值为 1 和 2，也就是 Merge 行里面列出来的第一个还是第二个。"],["body","\n"],["headingLink","reset"],["heading","reset"],["body","\n"],["body","记住 合并前的最后一个  commitId，"],["body","\n"],["headingLink","顺序变更合并拆分"],["heading","顺序变更、合并、拆分"],["body","\n"],["headingLink","顺序变更"],["heading","顺序变更"],["body","\n"],["body","我们不仅可以修改某一次commit当中的内容，还可以修改这些commit的相对顺序，以及可以让它们合并以及拆分。"],["body","\n"],["body","修改顺序其实很简单，我们只需要人为地修改rebase -i之后弹出的vim文件即可。比如说原本的记录是："],["body","\n"],["body","pick A change A\npick B change B\npick C change C\n"],["body","\n"],["body","如果我们想要更换顺序，我们只需要修改这个文件即可。比如变成："],["body","\n"],["body","pick B change B\npick A change A\npick C change C\n"],["body","\n"],["body","那么当我们在退出vim的时候，git会首先应用B commit的变更，再应用A最后应用C。"],["body","\n"],["headingLink","合并"],["heading","合并"],["body","\n"],["body","除此之外，我们还可以合并多个commit记录成一个。操作的方法也很简单，就是我们只需要把pick修改成squash。git会自动把所有squash的commit记录合并在一起。"],["body","\n"],["body","pick A change A\nsquash B change B\nsquash C change C\n"],["body","\n"],["headingLink","拆分"],["heading","拆分"],["body","\n"],["body","有的时候一个commit非常巨大，我们可能也会想要将它拆分，其实操作也很简单。比如我们想要把commit B拆分成两条，首先，我们在rebase的时候将commit B前面的pick修改成edit。"],["body","\n"],["body","pick A change A\nedit B change B\npick C change C\n"],["body","\n"],["body","当我们退出的时候，我们会进入到B commit刚刚提交完的状态。由于我们要做的是拆分B这个提交，所以我们需要执行git reset HEAD^，把上一次提交重置。然后再分别add我们想要拆分开来提交的文件。"],["body","\n"],["body","整个操作如下："],["body","\n"],["body","git reset HEAD^\ngit add test/*\ngit ci -m 'add test'\ngit add code/*\ngit ci -m 'update code'\ngit rebase --continue\n"],["body","\n"],["body","这样我们就把commit B拆分成了两个commit插入到了历史记录当中了。"],["body","\n"],["headingLink","注意"],["heading","注意"],["body","\n"],["body","最后的最后，大家需要注意，虽然这些手段在修改记录的时候非常好用。但是如果这些commit已经被提交到了远程，我们是不可以直接git push同步的。因为git会校验我们提交的hash值，发现对不上之后会禁止我们的提交。所以如果想要提交到远程的话，只能使用git push -f强制覆盖。但是这是一个非常非常危险的操作，如果你git push -f了，没有人会知道你到底修改了什么，只建议在自己独有的分支上如此操作，一定一定要谨慎使用。"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/基本操作.html"],["title","基本操作 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","tag"],["heading","Tag"],["body","\n"],["body","马上就要和之前实践遗留的数据告别了，告别之前是不是要留个影呢？在Git里，“留影”用的命令叫做tag，更加专业的术语叫做“里程碑”（打tag，或打标签）。"],["body","\n"],["body","$ cd /path/to/my/workspace/demo\n$ git tag -m \"Say bye-bye to all previous practice.\" old_practice\n"],["body","\n"],["body","只要知道里程碑无非也是一个引用，通过记录提交ID（或者创建Tag对象）来为当前版本库状态进行“留影”。"],["body","\n"],["body","可以执行git describe命令显示当前版本库的最新提交的版本号"],["body","\n\n"],["body","显示的时候会选取离该提交最近的里程碑作为“基础版本号”，"],["body","\n"],["body","后面附加标识距离“基础版本”的数字以及该提交的SHA1哈希值缩写"],["body","\n"],["body","因为最新的提交上恰好被打了一个“里程碑”，所以用“里程碑”的名字显示为版本号"],["body","\n\n"],["headingLink","删除文件"],["heading","删除文件"],["body","\n"],["headingLink","本地删除"],["heading","本地删除"],["body","\n"],["body","rm *.txt\n"],["body","\n"],["body","直接在工作区删除，对暂存区和版本库没有任何影响"],["body","\n"],["body","**git checkout – **可以让文件在工作区重现"],["body","\n"],["headingLink","执行git-rm命令删除文件"],["heading","执行git rm命令删除文件"],["body","\n"],["body","删除动作直接加入了暂存区"],["body","\n"],["body","$ git rm detached-commit.txt hack-1.txt new-commit.txt welcome.txt\n"],["body","\n"],["headingLink","命令git-add--u"],["heading","命令git add -u"],["body","\n"],["body","含义是将本地有改动（包括添加和删除）的文件写入到暂存区"],["body","\n"],["headingLink","恢复删除的文件"],["heading","恢复删除的文件"],["body","\n"],["body","执行下面的命令可以从历史（前一次提交）中恢复welcome.txt文件。"],["body","\n"],["body","$ git cat-file -p HEAD~1:welcome.txt > welcome.txt\n"],["body","\n"],["body","通过再次添加的方式恢复被删除的文件是最自然的恢复的方法。其他版本控制系统如CVS也采用同样的方法恢复删除的文件，但是有的版本控制系统如Subversion如果这样操作会有严重的副作用——文件变更历史被人为的割裂而且还会造成服务器存储空间的浪费。Git通过添加方式反删除文件没有副作用，这是因为在Git的版本库中相同内容的文件保存在一个blob对象中，而且即便是内容不同的blob对象在对象库打包整理过程中也会通过差异比较优化存储。"],["body","\n"],["headingLink","移动文件"],["heading","移动文件"],["body","\n"],["body","通过将welcome.txt改名为README文件来测试一下在Git中如何移动文件。Git提供了git mv命令完成改名操作。"],["body","\n"],["body","$ git mv welcome.txt README\n"],["body","\n"],["body","从提交日志中出现的文件相似度可以看出Git的改名实际上源自于Git对文件追踪的强大支持（文件内容作为blob对象保存在对象库中）"],["body","\n"],["body","改名操作实际上相当于对旧文件执行删除，对新文件执行添加，即完全可以不使用git mv操作，而是代之以git rm和一个git add操作。"],["body","\n"],["body","$ mv welcome.txt README\n$ git status -s\n D welcome.txt\n?? README\n"],["body","\n"],["headingLink","一个显示版本号的hello-world"],["heading","一个显示版本号的Hello World"],["body","\n"],["body","在本章的一开始为纪念前面的实践留了一个影，叫做old_practice。现在再次执行git describe看一下现在的版本号。"],["body","\n"],["body","$ git describe\nold_practice-3-gc024f34\n"],["body","\n"],["body","就是说：当前工作区的版本是“留影”后的第三个版本，提交ID是c024f34。"],["body","\n"],["body","下面的命令可以在提交日志中显示提交对应的里程碑（Tag）。其中参数--decorate可以在提交ID的旁边显示该提交关联的引用（里程碑或分支）。"],["body","\n"],["body","$ git log --oneline --decorate -4\nc024f34 (HEAD, master) README is from welcome.txt.\n63992f0 restore file: welcome.txt\n7161977 delete trash files. (using: git add -u)\n2b31c19 (tag: old_practice) Merge commit 'acc2f69'\n"],["body","\n"],["body","命令git describe的输出可以作为软件版本号，这个功能非常有用。因为这样可以很容易的实现将发布的软件包版本和版本库中的代码对应在一起，当发现软件包包含Bug时，可以最快、最准确的对应到代码上。"],["body","\n"],["body","下面的Hello World程序就实现了这个功能。创建目录src，并在src目录下创建下面的三个文件："],["body","\n\n"],["body","\n"],["body","文件：src/main.c"],["body","\n"],["body","没错，下面的几行就是这个程序的主代码，和输出相关代码的就两行，一行显示“Hello, world.”，另外一行显示软件版本。在显示软件版本时用到了宏_VERSION，这个宏的来源参考下一个文件。"],["body","\n"],["body","源代码："],["body","\n"],["body","\n"],["body","#include \"version.h\"\n#include <stdio.h>\n\nint\nmain()\n{\n    printf( \"Hello, world.\\n\" );\n    printf( \"version: %s.\\n\", _VERSION );\n    return 0;\n}\n"],["body","\n"],["body","\n"],["body","\n"],["body","\n"],["body","文件：src/version.h.in"],["body","\n"],["body","没错，这个文件名的后缀是.h.in。这个文件其实是用于生成文件version.h的模板文件。在由此模板文件生成的version.h的过程中，宏_VERSION的值 “” 会动态替换。"],["body","\n"],["body","源代码："],["body","\n"],["body","\n"],["body","#ifndef HELLO_WORLD_VERSION_H\n#define HELLO_WORLD_VERSION_H\n\n#define _VERSION \"<version>\"\n\n#endif\n"],["body","\n"],["body","\n"],["body","\n"],["body","\n"],["body","文件：src/Makefile"],["body","\n"],["body","这个文件看起来很复杂，而且要注意所有缩进都是使用一个<Tab>键完成的缩进，千万不要错误的写成空格，因为这是Makefile。这个文件除了定义如何由代码生成可执行文件hello之外，还定义了如何将模板文件version.h.in转换为version.h。在转换过程中用git describe命令的输出替换模板文件中的<version>字符串。"],["body","\n"],["body","源代码："],["body","\n"],["body","\n"],["body","OBJECTS = main.o\nTARGET = hello\n\nall: $(TARGET)\n\n$(TARGET): $(OBJECTS)\n        $(CC) -o $@ $^\n\nmain.o: | new_header\nmain.o: version.h\n\nnew_header:\n        @sed -e \"s/<version>/$$(git describe)/g\" \\\n                < version.h.in > version.h.tmp\n        @if diff -q version.h.tmp version.h >/dev/null 2>&1; \\\n        then \\\n                rm version.h.tmp; \\\n        else \\\n                echo \"version.h.in => version.h\" ; \\\n                mv version.h.tmp version.h; \\\n        fi\n\nclean:\n        rm -f $(TARGET) $(OBJECTS) version.h\n\n.PHONY: all clean\n"],["body","\n"],["body","\n"],["body","\n\n"],["body","上述三个文件创建完毕之后，进入到src目录，试着运行一下。先执行make编译，再运行编译后的程序hello。"],["body","\n"],["body","$ cd src\n$ make\nversion.h.in => version.h\ncc    -c -o main.o main.c\ncc -o hello main.o\n$ ./hello\nHello, world.\nversion: old_practice-3-gc024f34.\n"],["body","\n"],["headingLink","使用git-add--i选择性添加"],["heading","使用git add -i选择性添加"],["body","\n"],["body","执行git add -i命令，进入一个交互式界面，首先显示的是工作区状态。显然因为版本库进行了清理，所以显得很“干净”。"],["body","\n"],["headingLink","文件忽略"],["heading","文件忽略"],["body","\n"],["body","执行下面的命令可以在这个目下创建一个名为.gitignore的文件（注意文件的前面有个点），把这些要忽略的文件写在其中，文件名可以使用通配符。注意：第2行到第5行开头的右尖括号是cat命令的提示符，不是输入。"],["body","\n"],["body","$ cat > .gitignore << EOF\n> hello\n> *.o\n> *.h\n> EOF\n"],["body","\n"],["body","文件.gitignore的作用范围是其所处的目录及其子目录，因此如果把刚刚创建的.gitignore移动到上一层目录（仍位于工作区内）也应该有效。"],["body","\n"],["body","只有使用了--ignored参数，才会在状态显示中看到被忽略的文件。"],["body","\n"],["body","$ git status --ignored -s\n!! hello\n!! hello.h\n!! main.o\n!! version.h\n"],["body","\n"],["headingLink","忽略只对未跟踪文件有效对于已加入版本库的文件无效"],["heading","忽略只对未跟踪文件有效，对于已加入版本库的文件无效"],["body","\n"],["body","文件hello.h添加到版本库后，就不再受到.gitignore设置的文件忽略影响了，对hello.h的修改都会立刻被跟踪到。这是因为Git的文件忽略只是对未入库的文件起作用。"],["body","\n"],["body","偷懒式提交。（使用了-a参数提交，不用预先执行git add命令。）"],["body","\n"],["headingLink","本地独享式忽略文件"],["heading","本地独享式忽略文件"],["body","\n\n"],["body","一种是针对具体版本库的“独享式”忽略。即在版本库.git目录下的一个文件.git/info/exclude来设置文件忽略。"],["body","\n"],["body","另外一种是全局的“独享式”忽略。即通过Git的配置变量core.excludesfile指定的一个忽略文件，其设置的忽略对所有文件均有效。"],["body","\n\n"],["body","例如本地设置一个全局的独享的文件忽略列表（这个文件名可以随意设置）："],["body","\n"],["body","$ git config --global core.excludesfile /home/jiangxin/_gitignore\n$ git config core.excludesfile\n/home/jiangxin/_gitignore\n$ cat /home/jiangxin/_gitignore\n*~        # vim 临时文件\n*.pyc     # python 的编译文件\n.*.mmx    # 不是正则表达式哦，因为 FreeMind-MMX 的辅助文件以点开头\n"],["body","\n"],["headingLink","git忽略语法"],["heading","Git忽略语法"],["body","\n\n"],["body","忽略文件中的空行或者以井号（#）开始的行被忽略。"],["body","\n"],["body","可以使用通配符，参见Linux手册：glob(7)。例如：星号（*）代表任意多字符，问号（?）代表一个字符，方括号（[abc]）代表可选字符范围等。"],["body","\n"],["body","如果名称的最前面是一个路径分隔符（/），表明要忽略的文件在此目录下，而非子目录的文件。"],["body","\n"],["body","如果名称的最后面是一个路径分隔符（/），表明要忽略的是整个目录，同名文件不忽略，否则同名的文件和目录都忽略。"],["body","\n"],["body","通过在名称的最前面添加一个感叹号（!），代表不忽略。"],["body","\n\n"],["body","# 这是注释行 —— 被忽略\n*.a       # 忽略所有以 .a 为扩展名的文件。\n!lib.a    # 但是 lib.a 文件或者目录不要忽略，即使前面设置了对 *.a 的忽略。\n/TODO     # 只忽略根目录下的 TODO 文件，子目录的 TODO 文件不忽略。\nbuild/    # 忽略所有 build/ 目录下的文件。\ndoc/*.txt # 忽略文件如 doc/notes.txt，但是文件如 doc/server/arch.txt 不被忽略。\n"],["body","\n"],["headingLink","文件归档"],["heading","文件归档"],["body","\n"],["body","如果使用压缩工具（tar、7zip、winzip、rar等）将工作区文件归档，一不小心会把版本库（.git目录）包含其中，甚至将工作区中的忽略文件、临时文件也包含其中"],["body","\n"],["body","Git提供了一个归档命令：git archive，可以对任意提交对应的目录树建立归档。"],["body","\n\n"],["body","\n"],["body","基于最新提交建立归档文件latest.zip。"],["body","\n"],["body","$ git archive -o latest.zip HEAD\n"],["body","\n"],["body","\n"],["body","\n"],["body","只将目录src和doc建立到归档partial.tar中。"],["body","\n"],["body","$ git archive -o partial.tar  HEAD src doc\n"],["body","\n"],["body","\n"],["body","\n"],["body","基于里程碑v1.0建立归档，并且为归档中文件添加目录前缀1.0。"],["body","\n"],["body","$ git archive --format=tar --prefix=1.0/ v1.0 | gzip > foo-1.0.tar.gz\n"],["body","\n"],["body","\n\n"],["body","在建立归档时，如果使用树对象ID进行归档，则使用当前时间作为归档中文件的修改时间，"],["body","\n"],["body","而如果使用提交ID或里程碑等，则使用提交建立的时间作为归档中文件的修改时间。"],["body","\n"],["body","如果使用tar格式建立归档，并且使用提交ID或里程碑ID，还会把提交ID记录在归档文件的文件头中。记录在文件头中的提交ID可以通过git tar-commit-id命令获取。"],["body","\n"],["body","如果希望在建立归档时忽略某些文件或目录，可以通过为相应文件或目录建立export-ignore属性加以实现"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/克隆.html"],["title","克隆 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","鸡蛋不装在一个篮子里"],["heading","鸡蛋不装在一个篮子里"],["body","\n"],["body","Git的版本库目录和工作区在一起，因此存在一损俱损的问题，即如果删除一个项目的工作区，同时也会把这个项目的版本库删除掉。一个项目仅在一个工作区中维护太危险了，如果有两个工作区就会好很多。"],["body","\n"],["body","上图中一个项目使用了两个版本库进行维护，两个版本库之间通过拉回（PULL）和/或推送（PUSH）操作实现同步。"],["body","\n\n"],["body","版本库A通过克隆操作创建克隆版本库B。"],["body","\n"],["body","版本库A可以通过推送（PUSH）操作，将新提交传递给版本库B；"],["body","\n"],["body","版本库A可以通过拉回（PULL）操作，将版本库B中的新提交拉回到自身（A）。"],["body","\n\n"],["body","\n"],["body","Git使用git clone命令实现版本库克隆，主要有如下三种用法："],["body","\n"],["body","用法1: git clone <repository> <directory>\n用法2: git clone --bare   <repository> <directory.git>\n用法3: git clone --mirror <repository> <directory.git>\n"],["body","\n"],["body","这三种用法的区别如下："],["body","\n\n"],["body","用法1将<repository>指向的版本库创建一个克隆到<directory>目录。目录<directory>相当于克隆版本库的工作区，文件都会检出，版本库位于工作区下的.git目录中。"],["body","\n"],["body","用法2和用法3创建的克隆版本库都不含工作区，直接就是版本库的内容，这样的版本库称为裸版本库。一般约定俗成裸版本库的目录名以.git为后缀，所以上面示例中将克隆出来的裸版本库目录名写做<directory.git>。"],["body","\n"],["body","用法3区别于用法2之处在于用法3克隆出来的裸版本对上游版本库进行了注册，这样可以在裸版本库中使用git fetch命令和上游版本库进行持续同步。"],["body","\n"],["body","用法3只在 1.6.0 或更新版本的Git才提供。"],["body","\n\n"],["body","Git的PUSH和PULL命令的用法相似，使用下面的语法："],["body","\n"],["body","git push [<remote-repos> [<refspec>]]\ngit pull [<remote-repos> [<refspec>]]\n"],["body","\n"],["body","其中方括号的含义是参数可以省略，<remote-repos>是远程版本库的地址或名称，<refspec>是引用表达式，暂时理解为引用即可。在后面的章节再具体介绍PUSH和PULL命令的细节。"],["body","\n"],["headingLink","对等工作区"],["heading","对等工作区"],["body","\n"],["body","不使用--bare或者--mirror创建出来的克隆包含工作区，这样就会产生两个包含工作区的版本库。这两个版本库是对等的"],["body","\n"],["body","\n"],["body","但是往往提交是在一个版本（A）中进行的，另外一个（B）作为备份。对于这种对等工作区模式，版本库的同步只有一种可行的操作模式"],["body","\n"],["body","就是在备份库（B）执行 git pull 命令从源版本库（A）拉回新的提交实现版本库同步"],["body","\n"],["body","为什么不能从版本库A向版本库B执行 git push 的推送操作呢？看看下面的操作。"],["body","\n"],["body","执行克隆命令，将版本库/path/to/my/workspace/demo克隆到/path/to/my/workspace/demo-backup。"],["body","\n"],["body","$ git clone /path/to/my/workspace/demo /path/to/my/workspace/demo-backup\nCloning into /path/to/my/workspace/demo-backup...\ndone.\n"],["body","\n"],["body","进入 demo 版本库，生成一些测试提交（使用--allow-empty参数可以生成空提交）。"],["body","\n"],["body","$ cd /path/to/my/workspace/demo/\n$ git commit --allow-empty -m \"sync test 1\"\n[master 790e72a] sync test 1\n$ git commit --allow-empty -m \"sync test 2\"\n[master f86b7bf] sync test 2\n"],["body","\n"],["body","能够在 demo 版本库向 demo-backup 版本库执行PUSH操作么？"],["body","\n"],["body","不行：允许向工作区推送已经检出的分支"],["body","\n"],["body","为了实现同步，需要进入到备份版本库中，执行git pull命令。"],["body","\n"],["body","$ git pull\nFrom /path/to/my/workspace/demo\n   6e6753a..f86b7bf  master     -> origin/master\nUpdating 6e6753a..f86b7bf\nFast-forward\n"],["body","\n"],["body","为什么执行 git pull 拉回命令没有像执行 git push 命令那样提供那么多的参数呢？"],["body","\n"],["body","这是因为在执行git clone操作后，克隆出来的demo-backup版本库中对源版本库（上游版本库）进行了注册"],["body","\n"],["body","，所以当在 demo-backup 版本库执行拉回操作，无须设置上游版本库的地址。"],["body","\n"],["body","在 demo-backup 版本库中可以使用下面的命令查看对上游版本库的注册信息："],["body","\n"],["body","$ cd /path/to/my/workspace/demo-backup\n$ git remote -v\norigin  /path/to/my/workspace/demo (fetch)\norigin  /path/to/my/workspace/demo (push)\n"],["body","\n"],["body","实际注册上游远程版本库的奥秘都在Git的配置文件中（略去无关的行）："],["body","\n"],["body","$ cat /path/to/my/workspace/demo-backup/.git/config\n...\n[remote \"origin\"]\n        fetch = +refs/heads/*:refs/remotes/origin/*\n        url = /path/to/my/workspace/demo\n[branch \"master\"]\n        remote = origin\n        merge = refs/heads/master\n"],["body","\n"],["body","关于配置文件[remote]小节和[branch]小节的奥秘在后面的章节予以介绍。"],["body","\n"],["headingLink","克隆生成裸版本库"],["heading","克隆生成裸版本库"],["body","\n"],["body","上一节在对等工作区模式下，工作区之间执行推送，可能会引发大段的错误输出，如果采用裸版本库则没有相应的问题。这是因为裸版本库没有工作区。没有工作区还有一个好处就是空间占用会更小。"],["body","\n"],["body","\n"],["body","使用--bare参数克隆demo版本库到/path/to/repos/demo.git，然后就可以从 demo 版本库向克隆的裸版本库执行推送操作了"],["body","\n"],["body","（为了说明方便，使用了/path/to/repos/作为Git裸版本的根路径，在后面的章节中这个目录也作为Git服务器端版本库的根路径。可以在磁盘中以root账户创建该路径并设置正确的权限。）"],["body","\n"],["body","$ git clone --bare /path/to/my/workspace/demo /path/to/repos/demo.git\nCloning into bare repository /path/to/repos/demo.git...\ndone.\n"],["body","\n"],["body","克隆出来的/path/to/repos/demo.git目录就是版本库目录，不含工作区。"],["body","\n\n"],["body","\n"],["body","看看/path/to/repos/demo.git目录的内容。"],["body","\n"],["body","$ ls -F /path/to/repos/demo.git\nbranches/  config  description  HEAD  hooks/  info/  objects/  packed-refs  refs/\n"],["body","\n"],["body","\n"],["body","\n"],["body","还可以看到demo.git版本库core.bare的配置为true。"],["body","\n"],["body","$ git --git-dir=/path/to/repos/demo.git config core.bare\ntrue\n"],["body","\n"],["body","\n\n"],["body","这个方式实现版本库本地镜像显然是更好的方法，因为可以直接在工作区修改、提交，然后执行git push命令实现推送。稍有一点遗憾的是推送命令还需要加上裸版本库的路径。这个遗憾在后面介绍远程版本库的章节会给出解决方案。"],["body","\n"],["headingLink","创建生成裸版本库"],["heading","创建生成裸版本库"],["body","\n"],["body","裸版本库不但可以通过克隆的方式创建，还可以通过git init命令以初始化的方式创建。之后的同步方式和上一节大同小异。"],["body","\n"],["body","$ git init --bare /path/to/repos/demo-init.git\nInitialized empty Git repository in /path/to/repos/demo-init.git/\n"],["body","\n"],["body","可是空版本库没有内容啊，那就执行PUSH操作为其创建内容呗。"],["body","\n"],["body","$ cd /path/to/my/workspace/demo\n$ git push /path/to/repos/demo-init.git\nNo refs in common and none specified; doing nothing.\nPerhaps you should specify a branch such as 'master'.\nfatal: The remote end hung up unexpectedly\nerror: failed to push some refs to '/path/to/repos/demo-init.git'\n"],["body","\n"],["body","没有指定要推送的引用，而且两个版本库也没有共同的引用。\n所以什么也没有做。\n可能您需要提供要推送的分支名，如 'master'。\n严重错误：远程操作意外终止\n错误：部分引用推送失败，至 '/path/to/repos/demo-init.git'\n"],["body","\n"],["body","关于这个问题详细说明要在后面的章节介绍，这里先说一个省略版：因为/path/to/repos/demo-init.git 版本库刚刚初始化完成，还没有任何提交更不要说分支了"],["body","\n"],["body","当执行git push命令时，如果没有设定推送的分支，而且当前分支也没有注册到远程某个分支"],["body","\n"],["body","将检查远程分支是否有和本地相同的分支名（如master），如果有，则推送，否则报错。"],["body","\n"],["body","所以需要把git push命令写的再完整一些。像下面这样操作，就可以完成向空的裸版本库的推送。"],["body","\n"],["body","$ git push /path/to/repos/demo-init.git master:master\n"],["body","\n"],["body","上面的git push命令也可以简写为：git push /pat h/to/repos/demo-init.git master。"],["body","\n"],["body","推送成功了么？看看demo-init.git版本库中的提交。"],["body","\n"],["body","$ git --git-dir=/path/to/repos/demo-init.git log --oneline -2\n0285742 sync test 4\nd4b42b7 sync test 3\n"],["body","\n"],["body","好了继续在 demo 中执行几次提交。"],["body","\n"],["body","$ cd /path/to/my/workspace/demo/\n$ git commit --allow-empty -m \"sync test 5\"\n[master 424aa67] sync test 5\n$ git commit --allow-empty -m \"sync test 6\"\n[master 70a5aa7] sync test 6\n"],["body","\n"],["body","然后再向demo-init.git推送。注意这次使用的命令。"],["body","\n"],["body","$ git push /path/to/repos/demo-init.git\n"],["body","\n"],["body","为什么这次使用git push命令后面没有跟上分支名呢？这是因为远程版本库（demo-init.git）中已经不再是空版本库了，而且有名为master的分支。"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/GIT回滚.html"],["title","GIT回滚 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","回退合并提交"],["heading","回退合并提交"],["body","\n"],["body","git revert <Commit> -m 1\n"],["body","\n\n"],["body","其中 -m 指定回退到合并时两个提交里哪一个"],["body","\n\n"],["headingLink","假设某个合并提交详情如下"],["heading","假设某个合并提交详情如下："],["body","\n"],["body","commit 38eccd547e5dfeb4bc7d3f6988824177f9474214\nMerge: 92909 418db\nAuthor: Foo Bar <test@example.com>\nDate:   Tue Dec 7 10:23:33 2021 +0100\n\nMerge branch 'test'\n"],["body","\n\n"],["body","\n"],["body","-m 1 表示撤销 merge 并回退到92909这个提交上"],["body","\n"],["body","\n"],["body","\n"],["body","-m 2 表示撤销 merge 并回退到418db这个提交上"],["body","\n"],["body","\n\n"],["headingLink","使用-git-reset"],["heading","使用 git reset"],["body","\n\n"],["body","查看合并提交的 提交ID"],["body","\n\n"],["body","commit 38eccd547e5dfeb4bc7d3f6988824177f9474214\nMerge: 92909 418db\n"],["body","\n\n"],["body","\n"],["body","git reset --hard"],["body","\n"],["body","git reset --hard 92909\n"],["body","\n"],["body","\n"],["body","\n"],["body","一般左边的ID是 目标合并分支，右边的ID是 源合并分支"],["body","\n"],["body","\n"],["body","\n"],["body","git reset 是直接改变 head指针"],["body","\n"],["body","\n"],["body","\n"],["body","如果跟远程库不一致 则可能需要强制push"],["body","\n"],["body","\n\n"],["headingLink","git-修改-commit-message"],["heading","GIT 修改 COMMIT MESSAGE"],["body","\n"],["body"," git commit --amend\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/历史提交.html"],["title","历史提交 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","查看提交信息"],["heading","查看提交信息"],["body","\n"],["headingLink","git-rev-parse"],["heading","git rev-parse"],["body","\n"],["body","命令git rev-parse是Git的一个底层命令，其功能非常丰富（或者说杂乱），很多Git脚本或工具都会用到这条命令。"],["body","\n"],["body","例如可以显示Git版本库的位置（--git-dir），当前工作区目录的深度（--show-cdup），甚至可以用于被Git无关应用用于解析命令行参数（--parseopt）。"],["body","\n\n"],["body","\n"],["body","显示分支。"],["body","\n"],["body","$ git rev-parse --symbolic --branches\n"],["body","\n"],["body","\n\n\n"],["body","\n"],["body","显示里程碑"],["body","\n"],["body","$ git rev-parse --symbolic --tags\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\n"],["body","\n"],["body","\n"],["body","\n"],["body","显示定义的所有引用"],["body","\n"],["body","其中refs/remotes/目录下的引用成为远程分支（或远程引用），在后面的章节会予以介绍。"],["body","\n"],["body","$ git rev-parse --symbolic --glob=refs/*\nrefs/heads/master\nrefs/remotes/origin/HEAD\nrefs/remotes/origin/master\nrefs/tags/A\nrefs/tags/B\nrefs/tags/C\nrefs/tags/D\nrefs/tags/E\nrefs/tags/F\nrefs/tags/G\nrefs/tags/H\nrefs/tags/I\nrefs/tags/J\n"],["body","\n"],["body","\n"],["body","\n"],["body","命令git rev-parse另外一个重要的功能就是将一个Git对象表达式表示为对应的SHA1哈希值"],["body","\n\n"],["body","\n"],["body","显示HEAD对应的SHA1哈希值。"],["body","\n"],["body","$ git rev-parse  HEAD\n6652a0dce6a5067732c00ef0a220810a7230655e\n"],["body","\n"],["body","\n"],["body","\n"],["body","命令git describe的输出也可以显示为SHA1哈希值。"],["body","\n"],["body","$ git describe\nA-1-g6652a0d\n$ git rev-parse A-1-g6652a0d\n6652a0dce6a5067732c00ef0a220810a7230655e\n"],["body","\n"],["body","\n"],["body","\n"],["body","可以同时显示多个表达式的SHA1哈希值。"],["body","\n"],["body","$ git rev-parse  master  refs/heads/master\n6652a0dce6a5067732c00ef0a220810a7230655e\n6652a0dce6a5067732c00ef0a220810a7230655e\n"],["body","\n"],["body","\n"],["body","\n"],["body","可以用哈希值的前几位指代整个哈希值"],["body","\n"],["body","$ git rev-parse  6652  6652a0d\n6652a0dce6a5067732c00ef0a220810a7230655e\n6652a0dce6a5067732c00ef0a220810a7230655e\n"],["body","\n"],["body","\n"],["body","\n"],["body","里程碑的两种表示法均指向相同的对象。"],["body","\n\n"],["body","\n"],["body","里程碑对象不一定是提交，有可能是一个Tag对象。Tag对象包含说明或者签名，还包括到对应提交的指向。"],["body","\n"],["body","$ git rev-parse  A  refs/tags/A\nc9b03a208288aebdbfe8d84aeb984952a16da3f2\nc9b03a208288aebdbfe8d84aeb984952a16da3f2\n\n"],["body","\n"],["body","\n"],["body","\n"],["body","里程碑A指向了一个Tag对象而非提交的时候，用下面的三个表示法都可以指向里程碑对应的提交。"],["body","\n"],["body","\n\n"],["body","$ git rev-parse  A^{}  A^0  A^{commit}\n81993234fc12a325d303eccea20f6fd629412712\n81993234fc12a325d303eccea20f6fd629412712\n81993234fc12a325d303eccea20f6fd629412712\n"],["body","\n"],["body","\n"],["body","\n"],["body","连续的^符号依次沿着父提交进行定位至某一祖先提交。^后面的数字代表该提交的第几个父提交。"],["body","\n"],["body","$ git rev-parse  A^^3^2  F^2  J^{}\n3252fcce40949a4a622a1ac012cb120d6b340ac8\n3252fcce40949a4a622a1ac012cb120d6b340ac8\n3252fcce40949a4a622a1ac012cb120d6b340ac8\n"],["body","\n"],["body","\n"],["body","\n"],["body","记号~<n>就相当于连续<n>个符号^。"],["body","\n"],["body","$ git rev-parse  A~3  A^^^  G^0\ne80aa7481beda65ae00e35afc4bc4b171f9b0ebf\ne80aa7481beda65ae00e35afc4bc4b171f9b0ebf\ne80aa7481beda65ae00e35afc4bc4b171f9b0ebf\n"],["body","\n"],["body","\n"],["body","\n"],["body","显示里程碑A对应的目录树。下面两种写法都可以。"],["body","\n\n"],["body","\n"],["body","$ git rev-parse  A^{tree}  A:\n95ab9e7db14ca113d5548dc20a4872950e8e08c0\n95ab9e7db14ca113d5548dc20a4872950e8e08c0\n"],["body","\n"],["body","\n\n"],["body","\n"],["body","\n"],["body","显示树里面的文件，下面两种表示法均可"],["body","\n"],["body","$ git rev-parse  A^{tree}:src/Makefile  A:src/Makefile\n96554c5d4590dbde28183e9a6a3199d526eeb925\n96554c5d4590dbde28183e9a6a3199d526eeb925\n"],["body","\n"],["body","\n"],["body","\n"],["body","暂存区里的文件和HEAD中的文件相同"],["body","\n\n"],["body","\n"],["body","$ git rev-parse  :gitg.png  HEAD:gitg.png\nfc58966ccc1e5af24c2c9746196550241bc01c50\nfc58966ccc1e5af24c2c9746196550241bc01c50\n"],["body","\n"],["body","\n\n"],["body","\n"],["body","\n"],["body","还可以通过在提交日志中查找字串的方式显示提交。"],["body","\n"],["body","$ git rev-parse :/\"Commit A\"\n81993234fc12a325d303eccea20f6fd629412712\n"],["body","\n"],["body","\n"],["body","\n"],["body","再有就是reflog相关的语法，参见“Git重置”章节中关于reflog的介绍。"],["body","\n"],["body","$ git rev-parse HEAD@{0} master@{0}\n6652a0dce6a5067732c00ef0a220810a7230655e\n6652a0dce6a5067732c00ef0a220810a7230655e\n"],["body","\n"],["body","\n\n"],["body","\n\n"],["headingLink","版本范围表示法"],["heading","版本范围表示法"],["body","\n"],["body","有的Git命令可以使用一个版本范围作为参数，命令git rev-list可以帮助研究Git的各种版本范围语法。"],["body","\n"],["body","一个提交ID实际上就可以代表一个版本列表：含义是：该版本开始的所有历史提交。"],["body","\n"],["body","$ git rev-list --oneline  A\n8199323 Commit A: merge B with C.\n0cd7f2e commit C.\n776c5c9 Commit B: merge D with E and F\nbeb30ca Commit F: merge I with J\n212efce Commit D: merge G with H\n634836c commit I.\n3252fcc commit J.\n83be369 commit E.\n2ab52ad commit H.\ne80aa74 commit G.\n"],["body","\n"],["body","两个或多个版本，相当于每个版本单独使用时指代的列表的并集。"],["body","\n"],["body","$ git rev-list --oneline  D  F\nbeb30ca Commit F: merge I with J\n212efce Commit D: merge G with H\n634836c commit I.\n3252fcc commit J.\n2ab52ad commit H.\ne80aa74 commit G.\n"],["body","\n"],["body","在一个版本前面加上符号（^）含义是取反，即排除这个版本及其历史版本。"],["body","\n"],["body","$ git rev-list --oneline  ^G D\n212efce Commit D: merge G with H\n2ab52ad commit H.\n"],["body","\n"],["body","和上面等价的“点点”表示法。使用两个点连接两个版本，如G..D，就相当于^G D。"],["body","\n"],["body","$ git rev-list --oneline  G..D\n212efce Commit D: merge G with H\n2ab52ad commit H.\n"],["body","\n"],["body","版本取反，参数的顺序不重要，但是“点点”表示法前后的版本顺序很重要。"],["body","\n\n"],["body","\n"],["body","语法：^B C"],["body","\n"],["body","  $ git rev-list --oneline  ^B C\n"],["body","\n"],["body","\n\n"],["body","0cd7f2e commit C"],["body","\n\n"],["body","\n"],["body","语法：C ^B"],["body","\n"],["body","$ git rev-list --oneline  C ^B\n0cd7f2e commit C.\n"],["body","\n"],["body","\n"],["body","\n"],["body","语法：B..C相当于^B C"],["body","\n"],["body","$ git rev-list --oneline  B..C\n0cd7f2e commit C.\n"],["body","\n"],["body","\n"],["body","\n"],["body","语法：C..B相当于^C B"],["body","\n"],["body","$ git rev-list --oneline  C..B\n776c5c9 Commit B: merge D with E and F\n212efce Commit D: merge G with H\n83be369 commit E.\n2ab52ad commit H.\ne80aa74 commit G.\n"],["body","\n"],["body","\n\n"],["body","三点表示法的含义是两个版本共同能够访问到的除外。"],["body","\n"],["body","B和C共同能够访问到的F、I、J排除在外。"],["body","\n"],["body","$ git rev-list --oneline  B...C\n0cd7f2e commit C.\n776c5c9 Commit B: merge D with E and F\n212efce Commit D: merge G with H\n83be369 commit E.\n2ab52ad commit H.\ne80aa74 commit G.\n"],["body","\n"],["body","三点表示法，两个版本的前后顺序没有关系。"],["body","\n"],["body","实际上r1...r2相当于r1 r2 --not $(git merge-base --all r1 r2)，和顺序无关。"],["body","\n"],["body","$ git rev-list --oneline  C...B\n0cd7f2e commit C.\n776c5c9 Commit B: merge D with E and F\n212efce Commit D: merge G with H\n83be369 commit E.\n2ab52ad commit H.\ne80aa74 commit G.\n"],["body","\n"],["body","某提交的历史提交，自身除外，用语法r1^@表示。"],["body","\n"],["body","$ git rev-list --oneline  B^@\nbeb30ca Commit F: merge I with J\n212efce Commit D: merge G with H\n634836c commit I.\n3252fcc commit J.\n83be369 commit E.\n2ab52ad commit H.\ne80aa74 commit G.\n"],["body","\n\n"],["body","\n"],["body","提交本身不包括其历史提交，用语法r1^!表示。"],["body","\n"],["body","$ git rev-list --oneline  B^!\n776c5c9 Commit B: merge D with E and F\n\n$ git rev-list --oneline  F^! D\nbeb30ca Commit F: merge I with J\n212efce Commit D: merge G with H\n2ab52ad commit H.\n"],["body","\n"],["body","\n\n"],["headingLink","浏览日志git-log"],["heading","浏览日志：git log"],["body","\n"],["body","参数代表版本范围"],["body","\n"],["body","当不使用任何参数调用，相当于使用了缺省的参数HEAD，即显示当前HEAD能够访问到的所有历史提交"],["body","\n"],["body","$ git log --oneline F^! D\nbeb30ca Commit F: merge I with J\n212efce Commit D: merge G with H\n2ab52ad commit H.\ne80aa74 commit G.\n"],["body","\n"],["body","分支图显示"],["body","\n"],["body","通过--graph参数调用git log可以显示字符界面的提交关系图"],["body","\n"],["body","而且不同的分支还可以用不同的颜色来表示。如果希望每次查看日志的时候都看到提交关系图，可以设置一个别名，用别名来调用。"],["body","\n"],["body","显示最近的几条日志"],["body","\n"],["body","可以使用参数-<n>（为数字），显示最近的条日志。"],["body","\n"],["body","显示每次提交的具体改动"],["body","\n"],["body","使用参数-p可以在显示日志的时候同时显示改动。"],["body","\n"],["body","显示每次提交的变更概要"],["body","\n"],["body","可以使用--stat参数"],["body","\n"],["body","$ git log --stat --oneline  I..C\n0cd7f2e commit C.\n README    |    1 +\n doc/C.txt |    1 +\n 2 files changed, 2 insertions(+), 0 deletions(-)\nbeb30ca Commit F: merge I with J\n3252fcc commit J.\n README           |    7 +++++++\n doc/J.txt        |    1 +\n src/.gitignore   |    3 +++\n src/Makefile     |   27 +++++++++++++++++++++++++++\n src/main.c       |   10 ++++++++++\n src/version.h.in |    6 ++++++\n 6 files changed, 54 insertions(+), 0 deletions(-)\n"],["body","\n"],["body","定制输出"],["body","\n"],["body","Git的差异输出命令提供了很多输出模板提供选择，可以根据需要选择冗余显示或者精简显示。"],["body","\n\n"],["body","\n"],["body","参数--pretty=raw显示提交的原始数据。可以显示提交对应的树ID。"],["body","\n"],["body","\n"],["body","\n"],["body","参数--pretty=fuller会同时显示作者和提交者，两者可以不同。"],["body","\n"],["body","\n"],["body","\n"],["body","参数--pretty=oneline显然会提供最精简的日志输出。也可以使用--oneline参数，效果近似。"],["body","\n"],["body","\n\n"],["headingLink","差异比较git-diff"],["heading","差异比较：git diff"],["body","\n\n"],["body","比较里程碑B和里程碑A，用命令：git diff B A"],["body","\n"],["body","比较工作区和里程碑A，用命令：git diff A"],["body","\n"],["body","比较暂存区和里程碑A，用命令：git diff –cached A"],["body","\n"],["body","比较工作区和暂存区，用命令：git diff"],["body","\n"],["body","比较暂存区和HEAD，用命令：git diff –cached"],["body","\n"],["body","比较工作区和HEAD，用命令：git diff HEAD"],["body","\n\n"],["body","Git中文件在版本间的差异比较"],["body","\n"],["body","差异比较还可以使用路径参数，只显示不同版本间该路径下文件的差异。语法格式："],["body","\n"],["body","非Git目录/文件的差异比较"],["body","\n"],["body","命令git diff还可以在Git版本库之外执行，对非Git目录进行比较，就像GNU的diff命令一样。之所以提供这个功能是因为Git差异比较命令更为强大，提供了对GNU差异比较的扩展支持。"],["body","\n"],["body","$ git diff <path1> <path2>\n"],["body","\n"],["body","扩展的差异语法"],["body","\n"],["body","Git扩展了GNU的差异比较语法，提供了对重命名、二进制文件、文件权限变更的支持。在后面的“Git应用”辟专题介绍二进制文件的差异比较和补丁的应用。"],["body","\n"],["body","逐词比较，而非缺省的逐行比较"],["body","\n"],["body","Git的差异比较缺省是逐行比较，分别显示改动前的行和改动后的行，到底改动哪里还需要仔细辨别"],["body","\n"],["body","Git还提供一种逐词比较的输出，有的人会更喜欢。使用--word-diff参数可以显示逐词比较。"],["body","\n"],["headingLink","文件追溯git-blame"],["heading","文件追溯：git blame"],["body","\n"],["body","Git的文件追溯命令可以指出是谁在什么时候，什么版本引入的此Bug。"],["body","\n"],["body","当针对文件执行git blame命令，就会逐行显示文件，在每一行的行首显示此行最早是在什么版本引入的，由谁引入。"],["body","\n"],["body","只想查看某几行，使用-L n,m参数，如下："],["body","\n"],["body","$ git blame -L 6,+5 README\n81993234 (Jiang Xin 2010-12-09 14:30:15 +0800  6) * create node A.\n0cd7f2ea (Jiang Xin 2010-12-09 14:29:09 +0800  7) * create node C.\n"],["body","\n"],["headingLink","二分查找git-bisect"],["heading","二分查找：git bisect"],["body","\n"],["body","前面的文件追溯是建立在问题（Bug）已经定位（到代码上）的基础之上，然后才能通过错误的行（代码）找到人（提交者），打板子（教育或惩罚）。那么如何定位问题呢？Git的二分查找命令可以提供帮助。"],["body","\n"],["body","Git提供的git bisect命令是基于版本库的，自动化的问题查找和定位工作流程"],["body","\n"],["body","取代传统软件测试中粗放式的、针对软件发布版本的、无法定位到代码的测试。"],["body","\n"],["body","执行二分查找，在发现问题后，首先要找到一个正确的版本，如果所发现的问题从软件最早的版本就是错的，那么就没有必要执行二分查找了，还是老老实实的Debug吧"],["body","\n"],["body","但是如果能够找到一个正确的版本，即在这个正确的版本上问题没有发生，那么就可以开始使用git bisect命令在版本库中进行二分查找了："],["body","\n\n"],["body","工作区切换到已知的“好版本”和“坏版本”的中间的一个版本"],["body","\n"],["body","执行测试，问题重现，将版本库当前版本库为“坏版本”，如果问题没有重现，将当前版本标记为“好版本”。"],["body","\n"],["body","重复1-2，直至最终找到第一个导致问题出现的版本。"],["body","\n\n"],["body","example"],["body","\n"],["body","下面开始通过手动测试（查找doc/B.txt存在与否），借助Git二分查找定位“问题”版本。"],["body","\n\n"],["body","\n"],["body","首先确认工作在master分支。"],["body","\n"],["body","$ cd /path/to/my/workspace/gitdemo-commit-tree/\n$ git checkout master\nAlready on 'master'\n"],["body","\n"],["body","\n"],["body","\n"],["body","开始二分查找。"],["body","\n"],["body","$ git bisect start\n"],["body","\n"],["body","\n"],["body","\n"],["body","已经当前版本是“坏提交”，因为存在文件doc/B.txt。而G版本是“好提交”，因为不存在文件doc/B.txt。"],["body","\n"],["body","$ git cat-file -t master:doc/B.txt\nblob\n$ git cat-file -t G:doc/B.txt\nfatal: Not a valid object name G:doc/B.txt\n"],["body","\n"],["body","\n"],["body","\n"],["body","将当前版本（HEAD）标记为“坏提交”，将G版本标记为“好提交”。"],["body","\n"],["body","$ git bisect bad\n$ git bisect good G\nBisecting: 5 revisions left to test after this (roughly 2 steps)\n[0cd7f2ea245d90d414e502467ac749f36aa32cc4] commit C.\n"],["body","\n"],["body","\n"],["body","\n"],["body","自动定位到C提交。没有文件doc/B.txt，也是一个好提交。"],["body","\n"],["body","$ git describe\nC\n$ ls doc/B.txt\nls: 无法访问doc/B.txt: 没有那个文件或目录\n"],["body","\n"],["body","\n"],["body","\n"],["body","标记当前版本（C提交）为“好提交”。"],["body","\n"],["body","$ git bisect good\nBisecting: 3 revisions left to test after this (roughly 2 steps)\n[212efce1548795a1edb08e3708a50989fcd73cce] Commit D: merge G with H\n"],["body","\n"],["body","\n"],["body","\n"],["body","现在定位到D版本，这也是一个“好提交”。"],["body","\n"],["body","$ git describe\nD\n$ ls doc/B.txt\nls: 无法访问doc/B.txt: 没有那个文件或目录\n"],["body","\n"],["body","\n"],["body","\n"],["body","标记当前版本（D提交）为“好提交”。"],["body","\n"],["body","$ git bisect good\nBisecting: 1 revision left to test after this (roughly 1 step)\n[776c5c9da9dcbb7e463c061d965ea47e73853b6e] Commit B: merge D with E and F\n"],["body","\n"],["body","\n"],["body","\n"],["body","现在定位到B版本，这是一个“坏提交”。"],["body","\n"],["body","$ git bisect bad\nBisecting: 0 revisions left to test after this (roughly 0 steps)\n[83be36956c007d7bfffe13805dd2081839fd3603] commit E.\n"],["body","\n"],["body","\n"],["body","\n"],["body","现在定位到E版本，这是一个“好提交”。当标记E为好提交之后，输出显示已经成功定位到引入坏提交的最接近的版本。"],["body","\n"],["body","$ git bisect good\n776c5c9da9dcbb7e463c061d965ea47e73853b6e is the first bad commit\n"],["body","\n"],["body","\n"],["body","\n"],["body","最终定位的坏提交用引用refs/bisect/bad标识。可以如下方法切换到该版本。"],["body","\n"],["body","$ git checkout bisect/bad\nPrevious HEAD position was 83be369... commit E.\nHEAD is now at 776c5c9... Commit B: merge D with E and F\n"],["body","\n"],["body","\n"],["body","\n"],["body","当对“Bug”定位和修复后，撤销二分查找在版本库中遗留的临时文件和引用。"],["body","\n"],["body","撤销二分查找后，版本库切换回执行二分查找之前所在的分支。"],["body","\n"],["body","$ git bisect reset\nPrevious HEAD position was 776c5c9... Commit B: merge D with E and F\nSwitched to branch 'master'\n"],["body","\n"],["body","\n\n"],["body","把“好提交”标记成了“坏提交”该怎么办？"],["body","\n"],["body","在执行二分查找的过程中，一不小心就有可能犯错，将“好提交”标记为“坏提交”，或者相反。这将导致前面的查找过程也前功尽弃。Git的二分查找提供一个恢复查找进度的办法。"],["body","\n\n"],["body","\n"],["body","例如对E提交，本来是一个“好版本”却被错误的标记为“坏版本”。"],["body","\n"],["body","$ git bisect bad\n83be36956c007d7bfffe13805dd2081839fd3603 is the first bad commit\n"],["body","\n"],["body","\n"],["body","\n"],["body","用git bisect log命令查看二分查找的日志记录。"],["body","\n"],["body","把二分查找的日志保存在一个文件中。"],["body","\n"],["body","$ git bisect log > logfile\n"],["body","\n"],["body","\n"],["body","\n"],["body","编辑这个文件，删除记录了错误动作的行。"],["body","\n"],["body","以井号（#）开始的行是注释。"],["body","\n"],["body","$ cat logfile\n# bad: [6652a0dce6a5067732c00ef0a220810a7230655e] Add Images for git treeview.\n# good: [e80aa7481beda65ae00e35afc4bc4b171f9b0ebf] commit G.\ngit bisect start 'master' 'G'\n# good: [0cd7f2ea245d90d414e502467ac749f36aa32cc4] commit C.\ngit bisect good 0cd7f2ea245d90d414e502467ac749f36aa32cc4\n# good: [212efce1548795a1edb08e3708a50989fcd73cce] Commit D: merge G with H\ngit bisect good 212efce1548795a1edb08e3708a50989fcd73cce\n# bad: [776c5c9da9dcbb7e463c061d965ea47e73853b6e] Commit B: merge D with E and F\ngit bisect bad 776c5c9da9dcbb7e463c061d965ea47e73853b6e\n"],["body","\n"],["body","\n"],["body","\n"],["body","结束上一次出错的二分查找。"],["body","\n"],["body","$ git bisect reset\nPrevious HEAD position was 83be369... commit E.\nSwitched to branch 'master'\n"],["body","\n"],["body","\n"],["body","\n"],["body","通过日志文件恢复进度。"],["body","\n"],["body","$ git bisect replay logfile\nWe are not bisecting.\nBisecting: 5 revisions left to test after this (roughly 2 steps)\n[0cd7f2ea245d90d414e502467ac749f36aa32cc4] commit C.\nBisecting: 0 revisions left to test after this (roughly 0 steps)\n[83be36956c007d7bfffe13805dd2081839fd3603] commit E.\n"],["body","\n"],["body","\n"],["body","\n"],["body","再一次回到了提交E，这一次不要标记错了。"],["body","\n"],["body","$ git describe\nE\n$ git bisect good\n776c5c9da9dcbb7e463c061d965ea47e73853b6e is the first bad commit\n"],["body","\n"],["body","\n\n"],["body","二分查找使用自动化测试"],["body","\n"],["body","Git的二分查找命令支持run子命令，可以运行一个自动化测试脚本。"],["body","\n\n"],["body","如果脚本的退出码是0，正在测试的版本是一个“好版本”。"],["body","\n"],["body","如果脚本的退出码是125，正在测试的版本被跳过。"],["body","\n"],["body","如果脚本的退出码是1到127（125除外），正在测试的版本是一个“坏版本”。"],["body","\n\n"],["body","对于本例写一个自动化测试太简单了，无非就是判断文件是否存在，存在返回错误码1，不存在返回错误码0。"],["body","\n"],["body","测试脚本good-or-bad.sh如下："],["body","\n"],["body","#!/bin/sh\n\n[ -f doc/B.txt ] && exit 1\nexit 0\n"],["body","\n"],["body","用此自动化脚本执行二分查找就非常简单了。"],["body","\n\n"],["body","\n"],["body","从已知的坏版本master和好版本G，开始新一轮的二分查找。"],["body","\n"],["body","$ git bisect start master G\nBisecting: 5 revisions left to test after this (roughly 2 steps)\n[0cd7f2ea245d90d414e502467ac749f36aa32cc4] commit C.\n"],["body","\n"],["body","\n"],["body","\n"],["body","自动化测试，使用脚本good-or-bad.sh。"],["body","\n"],["body","$ git bisect run sh good-or-bad.sh\nrunning sh good-or-bad.sh\nBisecting: 3 revisions left to test after this (roughly 2 steps)\n[212efce1548795a1edb08e3708a50989fcd73cce] Commit D: merge G with H\nrunning sh good-or-bad.sh\nBisecting: 1 revision left to test after this (roughly 1 step)\n[776c5c9da9dcbb7e463c061d965ea47e73853b6e] Commit B: merge D with E and F\nrunning sh good-or-bad.sh\nBisecting: 0 revisions left to test after this (roughly 0 steps)\n[83be36956c007d7bfffe13805dd2081839fd3603] commit E.\nrunning sh good-or-bad.sh\n776c5c9da9dcbb7e463c061d965ea47e73853b6e is the first bad commit\nbisect run success\n"],["body","\n"],["body","\n"],["body","\n"],["body","定位到的“坏版本”是B。"],["body","\n"],["body","$ git describe refs/bisect/bad\nB\n"],["body","\n"],["body","\n\n"],["headingLink","获取历史版本"],["heading","获取历史版本"],["body","\n"],["body","提取历史提交中的文件无非就是下面表格中的操作，在之前的实践中多次用到，不再赘述。"],["body","\n"],["body","动作"],["body","命令格式"],["body","示例"],["body","\n"],["body","查看历史提交的目录树"],["body","git ls-tree <tree-ish> <paths>"],["body","git ls-tree 776c5c9 READMEgit ls-tree -r refs/tags/D doc"],["body","\n"],["body","整个工作区切换到历史版本"],["body","git checkout "],["body","git checkout HEAD^^"],["body","\n"],["body","检出某文件的历史版本"],["body","git checkout  – "],["body","git checkout refs/tags/D – READMEgit checkout 776c5c9 – doc"],["body","\n"],["body","检出某文件的历史版本到其他文件名"],["body","git show <commit>:<file> > new_name"],["body","git show 887113d:README > README.OLD"],["body","\n\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","9.源码管理_GIT/git使用案例.html"],["title","git使用案例 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","git设置"],["heading","GIT设置"],["body","\n"],["headingLink","设置客户端中文不显示数字"],["heading","设置客户端中文不显示数字"],["body","\n"],["body","git config --global core.quotepath false\n"],["body","\n"],["headingLink","储藏本地修改"],["heading","储藏本地修改"],["body","\n"],["body","1. *git fetch && git merge* \n2. *git stash*储藏本地修改\n3. *git stash pop*恢复储藏\n"],["body","\n"],["headingLink","commit操作"],["heading","COMMIT操作"],["body","\n"],["headingLink","执行回退"],["heading","执行回退"],["body","\n"],["body","#回退上一版本\ngit reset --hard HEAD^\ngit push -f origin master\n\n#回退上上个版本\ngit reset --hard HEAD^1\n"],["body","\n"],["headingLink","保持跟远程一致"],["heading","保持跟远程一致"],["body","\n"],["body","git fetch --all \n\ngit reset --hard origin/dev\n\ngit pull\n"],["body","\n"],["headingLink","项目过大时拉取不了git"],["heading","项目过大时拉取不了GIT"],["body","\n"],["body","当项目过大时，git clone时会出现error: RPC failed; HTTP 504 curl 22 The requested URL returned error: 504 Gateway Time-out的问题，此时我们可以只下载远程仓库中的最新的一个版本，而不下载其他老版本的内容，这样会大大减小存储与传输压力。\n\n\n"],["body","\n"],["body","我们可以在克隆时指定--depth 1，--depth后面的阿拉伯数字代表克隆仓库的最新几个版本，为1代表只克隆远程仓库的最新的一个版本。\n\n"],["body","\n"],["body","示例："],["body","\n"],["body","git clone --depth 1 https://github.com/dogescript/xxxxxxx.git\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","2.容器_k8s/k8s/index.html"],["title","k8s - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","安装"],["heading","安装"],["body","\n"],["body","安装工具"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","index.html"],["title","中间件 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","中间件"],["heading","中间件"],["body","\n\n\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","3.运维与部署_ansible/ansible/ansible中的变量.html"],["title","ansible中的变量 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","合法变量名"],["heading","合法变量名"],["body","\n"],["body","变量名可以为字母,数字以及下划线.变量始终应该以字母开头"],["body","\n"],["headingLink","在inventory中定义变量"],["heading","在Inventory中定义变量"],["body","\n"],["headingLink","在playbook中定义变量"],["heading","在playbook中定义变量"],["body","\n"],["body","- hosts: webservers\n  vars:\n    http_port: 80\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","3.运维与部署_ansible/ansible/inventor文件.html"],["title","inventor文件 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","前言"],["heading","前言"],["body","\n"],["body","inventor文件"],["body","\n"],["body","Ansible 可同时操作属于一个组的多台主机,组和主机之间的关系通过 inventory 文件配置. 默认的文件路径为 /etc/ansible/hosts"],["body","\n"],["body","组名分组"],["body","\n"],["body","方括号[]中是组名,用于对系统进行分类,便于对不同系统进行个别的管理."],["body","\n"],["body","一个系统可从属不同的组"],["body","\n"],["body","一个系统可以属于不同的组,比如一台服务器可以同时属于 webserver组 和 dbserver组"],["body","\n"],["body","mail.example.com\n\n[webservers]\nfoo.example.com\nbar.example.com\n\n[dbservers]\none.example.com\ntwo.example.com\nthree.example.com\n"],["body","\n"],["body","ssh连接收管主机"],["body","\n"],["body","如果有主机的SSH端口不是标准的22端口,可在主机名之后加上端口号,用冒号分隔"],["body","\n"],["body","badwolf.example.com:5309\n"],["body","\n"],["headingLink","构建清单"],["heading","构建清单"],["body","\n\n"],["body","\n"],["body","清单文件是 受管主机的IP或主机名 列表"],["body","\n"],["body","\n"],["body","\n"],["body","组名 用 [groupname]标识，组之间用 组名分隔"],["body","\n"],["body","\n"],["body","\n"],["body","可以使用YAML格式"],["body","\n"],["body","\n"],["body","\n"],["body","组"],["body","\n\n"],["body","有两个默认组，all, ungruoped"],["body","\n"],["body","all组包含每一个主机"],["body","\n"],["body","ungrouped 包含没有组的主机"],["body","\n"],["body","每一个组至少有两个组 all，ungrouped"],["body","\n"],["body","每一个主机 可以放在多个组"],["body","\n\n"],["body","\n"],["body","\n"],["body","主机名 符号"],["body","\n\n"],["body","数值区间：www[01:50].example.com"],["body","\n"],["body","字母区间：db-p[a:f].example.com"],["body","\n\n"],["body","\n"],["body","\n"],["body","添加变量"],["body","\n\n"],["body","\n"],["body","INI：host1 http_port=80 maxRequestsPerChild=808"],["body","\n"],["body","\n"],["body","\n"],["body","YAML："],["body","\n"],["body","atlanta:\n  host1:\n    http_port: 80\n    maxRequestsPerChild: 808\n\n"],["body","\n"],["body","\n\n"],["body","\n"],["body","\n"],["body","添加组变量，:vars"],["body","\n"],["body","[atlanta]\nhost1\nhost2\n\n[atlanta:vars]\nntp_server=ntp.atlanta.example.com\nproxy=proxy.atlanta.example.com\n"],["body","\n"],["body","atlanta:\n  hosts:\n    host1:\n    host2:\n  vars:\n    ntp_server: ntp.atlanta.example.com\n    proxy: proxy.atlanta.example.com\n"],["body","\n"],["body","\n"],["body","\n"],["body","使用children: 给组分组"],["body","\n"],["body","all:\n  children:\n    usa:\n      children:\n        southeast:\n          children:\n            atlanta:\n              hosts:\n                host1:\n                host2:\n            raleigh:\n              hosts:\n                host2:\n                host3:\n          vars:\n            some_server: foo.southeast.example.com\n            halon_system_timeout: 30\n            self_destruct_countdown: 60\n            escape_pods: 2\n        northeast:\n        northwest:\n        southwest:\n"],["body","\n"],["body","\n"],["body","\n"],["body","子组的变量 会覆盖父组的变量"],["body","\n"],["body","\n"],["body","\n"],["body","主机变量与 组变量 可以定义在如下路径"],["body","\n"],["body","/etc/ansible/group_vars/raleigh # can optionally end in '.yml', '.yaml', or '.json'\n/etc/ansible/group_vars/webservers\n/etc/ansible/host_vars/foosball\n"],["body","\n"],["body","\n\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","3.运维与部署_ansible/ansible/1.入门.html"],["title","入门 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","user-guide"],["heading","User Guide"],["body","\n"],["headingLink","getting-started"],["heading","Getting started"],["body","\n\n"],["body","·我想概述一下Ansible是如何工作的。我在哪里可以找到:\n\n"],["body","quick video overview"],["body","\n"],["body","text introduction"],["body","\n\n"],["body","\n"],["body","我准备好学习Ansible了：What Ansible concepts do I need to learn?"],["body","\n"],["body","即时命令行：How do I use ad hoc commands?"],["body","\n\n"],["headingLink","writing-tasks-plays-and-playbooks"],["heading","Writing tasks, plays, and playbooks"],["body","\n\n"],["body","我正在写我的第一本剧本。应该做什么： I know before I begin?"],["body","\n"],["body","我有一个任务或剧本的特定用例:\n\n"],["body","Executing tasks with elevated privileges or as a different user with become"],["body","\n"],["body","以提升的权限执行任务，或者以不同的用户的身份执行 任务   become"],["body","\n"],["body","使用不同参数重复执行任务  loops"],["body","\n"],["body","在不同的机器上执行任务， delegation"],["body","\n"],["body","当且仅当 一定条件满足  conditionals ，才运行任务。使用  tests 评估条件"],["body","\n"],["body","使用 blocks  任务分组"],["body","\n"],["body","仅当某些内容发生变化时才运行任务：handlers"],["body","\n"],["body","Changing the way Ansible handles failures"],["body","\n"],["body","Setting remote environment values"],["body","\n\n"],["body","\n"],["body","I want to take advantage of the power of re-usable Ansible artifacts. How do I create re-usable files and roles?"],["body","\n"],["body","I need to incorporate one file or playbook inside another. What is the difference between including and importing?"],["body","\n"],["body","I want to run selected parts of my playbook. How do I add and use tags?"],["body","\n\n"],["headingLink","working-with-inventory"],["heading","Working with inventory"],["body","\n\n"],["body","I have a list of servers and devices I want to automate. How do I create inventory to track them?"],["body","\n"],["body","I use cloud services and constantly have servers and devices starting and stopping. How do I track them using dynamic inventory?"],["body","\n"],["body","I want to automate specific sub-sets of my inventory. How do I use patterns?"],["body","\n\n"],["headingLink","interacting-with-data"],["heading","Interacting with data"],["body","\n"],["body","Once your playbook is ready to run, you may need to use these topics:"],["body","\n\n"],["body","Executing “dry run” playbooks with check mode and diff"],["body","\n"],["body","Running playbooks while troubleshooting with start and step"],["body","\n"],["body","Correcting tasks during execution with the Ansible debugger"],["body","\n"],["body","Controlling how my playbook executes with strategies and more"],["body","\n"],["body","Running tasks, plays, and playbooks asynchronously"],["body","\n\n"],["headingLink","advanced-features-and-reference"],["heading","Advanced features and reference"],["body","\n\n"],["body","Using advanced syntax"],["body","\n"],["body","Manipulating complex data"],["body","\n"],["body","Using plugins"],["body","\n"],["body","Using playbook keywords"],["body","\n"],["body","Using command-line tools"],["body","\n"],["body","Rejecting specific modules"],["body","\n"],["body","Module maintenance"],["body","\n\n"],["headingLink","traditional-table-of-contents"],["heading","Traditional Table of Contents"],["body","\n"],["body","If you prefer to read the entire User Guide, here’s a list of the pages in order:"],["body","\n\n"],["body","Ansible Quickstart Guide"],["body","\n"],["body","Ansible concepts\n\n"],["body","Control node"],["body","\n"],["body","Managed nodes"],["body","\n"],["body","Inventory"],["body","\n"],["body","Collections"],["body","\n"],["body","Modules"],["body","\n"],["body","Tasks"],["body","\n"],["body","Playbooks"],["body","\n\n"],["body","\n"],["body","Getting Started\n\n"],["body","Selecting machines from inventory"],["body","\n"],["body","Connecting to remote nodes"],["body","\n"],["body","Copying and executing modules"],["body","\n"],["body","Resources"],["body","\n"],["body","Next steps"],["body","\n\n"],["body","\n"],["body","Introduction to ad hoc commands\n\n"],["body","Why use ad hoc commands?"],["body","\n"],["body","Use cases for ad hoc tasks"],["body","\n\n"],["body","\n"],["body","Working with playbooks\n\n"],["body","Templating (Jinja2)"],["body","\n"],["body","Advanced playbooks features"],["body","\n"],["body","Playbook Example: Continuous Delivery and Rolling Upgrades"],["body","\n\n"],["body","\n"],["body","Intro to playbooks\n\n"],["body","Playbook syntax"],["body","\n"],["body","Playbook execution"],["body","\n"],["body","Ansible-Pull"],["body","\n"],["body","Verifying playbooks"],["body","\n\n"],["body","\n"],["body","Tips and tricks\n\n"],["body","General tips"],["body","\n"],["body","Playbook tips"],["body","\n"],["body","Inventory tips"],["body","\n"],["body","Execution tricks"],["body","\n\n"],["body","\n"],["body","Understanding privilege escalation: become\n\n"],["body","Using become"],["body","\n"],["body","Risks and limitations of become"],["body","\n"],["body","Become and network automation"],["body","\n"],["body","Become and Windows"],["body","\n\n"],["body","\n"],["body","Loops\n\n"],["body","Comparing loop and with_*"],["body","\n"],["body","Standard loops"],["body","\n"],["body","Registering variables with a loop"],["body","\n"],["body","Complex loops"],["body","\n"],["body","Ensuring list input for loop: using query rather than lookup"],["body","\n"],["body","Adding controls to loops"],["body","\n"],["body","Migrating from with_X to loop"],["body","\n\n"],["body","\n"],["body","Controlling where tasks run: delegation and local actions\n\n"],["body","Tasks that cannot be delegated"],["body","\n"],["body","Delegating tasks"],["body","\n"],["body","Delegation and parallel execution"],["body","\n"],["body","Delegating facts"],["body","\n"],["body","Local playbooks"],["body","\n\n"],["body","\n"],["body","Conditionals\n\n"],["body","Basic conditionals with when"],["body","\n"],["body","Commonly-used facts"],["body","\n\n"],["body","\n"],["body","Tests\n\n"],["body","Test syntax"],["body","\n"],["body","Testing strings"],["body","\n"],["body","Vault"],["body","\n"],["body","Testing truthiness"],["body","\n"],["body","Comparing versions"],["body","\n"],["body","Set theory tests"],["body","\n"],["body","Testing if a list contains a value"],["body","\n"],["body","Testing if a list value is True"],["body","\n"],["body","Testing paths"],["body","\n"],["body","Testing size formats"],["body","\n"],["body","Testing task results"],["body","\n\n"],["body","\n"],["body","Blocks\n\n"],["body","Grouping tasks with blocks"],["body","\n"],["body","Handling errors with blocks"],["body","\n\n"],["body","\n"],["body","Handlers: running operations on change\n\n"],["body","Handler example"],["body","\n"],["body","Controlling when handlers run"],["body","\n"],["body","Using variables with handlers"],["body","\n\n"],["body","\n"],["body","Error handling in playbooks\n\n"],["body","Ignoring failed commands"],["body","\n"],["body","Ignoring unreachable host errors"],["body","\n"],["body","Resetting unreachable hosts"],["body","\n"],["body","Handlers and failure"],["body","\n"],["body","Defining failure"],["body","\n"],["body","Defining “changed”"],["body","\n"],["body","Ensuring success for command and shell"],["body","\n"],["body","Aborting a play on all hosts"],["body","\n"],["body","Controlling errors in blocks"],["body","\n\n"],["body","\n"],["body","Setting the remote environment\n\n"],["body","Setting the remote environment in a task"],["body","\n\n"],["body","\n"],["body","Working with language-specific version managers"],["body","\n"],["body","Re-using Ansible artifacts\n\n"],["body","Creating re-usable files and roles"],["body","\n"],["body","Re-using playbooks"],["body","\n"],["body","Re-using files and roles"],["body","\n"],["body","Re-using tasks as handlers"],["body","\n\n"],["body","\n"],["body","Roles\n\n"],["body","Role directory structure"],["body","\n"],["body","Storing and finding roles"],["body","\n"],["body","Using roles"],["body","\n"],["body","Role argument validation"],["body","\n"],["body","Running a role multiple times in one playbook"],["body","\n"],["body","Using role dependencies"],["body","\n"],["body","Embedding modules and plugins in roles"],["body","\n"],["body","Sharing roles: Ansible Galaxy"],["body","\n\n"],["body","\n"],["body","Including and importing"],["body","\n"],["body","Tags\n\n"],["body","Adding tags with the tags keyword"],["body","\n"],["body","Special tags: always and never"],["body","\n"],["body","Selecting or skipping tags when you run a playbook"],["body","\n\n"],["body","\n"],["body","How to build your inventory\n\n"],["body","Inventory basics: formats, hosts, and groups"],["body","\n"],["body","Adding variables to inventory"],["body","\n"],["body","Assigning a variable to one machine: host variables"],["body","\n"],["body","Assigning a variable to many machines: group variables"],["body","\n"],["body","Organizing host and group variables"],["body","\n"],["body","How variables are merged"],["body","\n"],["body","Using multiple inventory sources"],["body","\n"],["body","Connecting to hosts: behavioral inventory parameters"],["body","\n"],["body","Inventory setup examples"],["body","\n\n"],["body","\n"],["body","Working with dynamic inventory\n\n"],["body","Inventory script example: Cobbler"],["body","\n"],["body","Inventory script example: OpenStack"],["body","\n"],["body","Other inventory scripts"],["body","\n"],["body","Using inventory directories and multiple inventory sources"],["body","\n"],["body","Static groups of dynamic groups"],["body","\n\n"],["body","\n"],["body","Patterns: targeting hosts and groups\n\n"],["body","Using patterns"],["body","\n"],["body","Common patterns"],["body","\n"],["body","Limitations of patterns"],["body","\n"],["body","Advanced pattern options"],["body","\n"],["body","Patterns and ad-hoc commands"],["body","\n"],["body","Patterns and ansible-playbook flags"],["body","\n\n"],["body","\n"],["body","Connection methods and details\n\n"],["body","ControlPersist and paramiko"],["body","\n"],["body","Setting a remote user"],["body","\n"],["body","Setting up SSH keys"],["body","\n"],["body","Running against localhost"],["body","\n"],["body","Managing host key checking"],["body","\n"],["body","Other connection methods"],["body","\n\n"],["body","\n"],["body","Working with command line tools\n\n"],["body","ansible"],["body","\n"],["body","ansible-config"],["body","\n"],["body","ansible-console"],["body","\n"],["body","ansible-doc"],["body","\n"],["body","ansible-galaxy"],["body","\n"],["body","ansible-inventory"],["body","\n"],["body","ansible-playbook"],["body","\n"],["body","ansible-pull"],["body","\n"],["body","ansible-vault"],["body","\n\n"],["body","\n"],["body","Using Variables\n\n"],["body","Creating valid variable names"],["body","\n"],["body","Simple variables"],["body","\n"],["body","When to quote variables (a YAML gotcha)"],["body","\n"],["body","List variables"],["body","\n"],["body","Dictionary variables"],["body","\n"],["body","Registering variables"],["body","\n"],["body","Referencing nested variables"],["body","\n"],["body","Transforming variables with Jinja2 filters"],["body","\n"],["body","Where to set variables"],["body","\n"],["body","Variable precedence: Where should I put a variable?"],["body","\n"],["body","Using advanced variable syntax"],["body","\n\n"],["body","\n"],["body","Discovering variables: facts and magic variables\n\n"],["body","Ansible facts"],["body","\n"],["body","Information about Ansible: magic variables"],["body","\n\n"],["body","\n"],["body","Encrypting content with Ansible Vault\n\n"],["body","Managing vault passwords"],["body","\n"],["body","Encrypting content with Ansible Vault"],["body","\n"],["body","Using encrypted variables and files"],["body","\n"],["body","Configuring defaults for using encrypted content"],["body","\n"],["body","When are encrypted files made visible?"],["body","\n"],["body","Format of files encrypted with Ansible Vault"],["body","\n\n"],["body","\n"],["body","Using filters to manipulate data\n\n"],["body","Handling undefined variables"],["body","\n"],["body","Defining different values for true/false/null (ternary)"],["body","\n"],["body","Managing data types"],["body","\n"],["body","Formatting data: YAML and JSON"],["body","\n"],["body","Combining and selecting data"],["body","\n"],["body","Randomizing data"],["body","\n"],["body","Managing list variables"],["body","\n"],["body","Selecting from sets or lists (set theory)"],["body","\n"],["body","Calculating numbers (math)"],["body","\n"],["body","Managing network interactions"],["body","\n"],["body","Hashing and encrypting strings and passwords"],["body","\n"],["body","Manipulating text"],["body","\n"],["body","Manipulating strings"],["body","\n"],["body","Managing UUIDs"],["body","\n"],["body","Handling dates and times"],["body","\n"],["body","Getting Kubernetes resource names"],["body","\n\n"],["body","\n"],["body","Lookups\n\n"],["body","Using lookups in variables"],["body","\n\n"],["body","\n"],["body","Interactive input: prompts\n\n"],["body","Encrypting values supplied by vars_prompt"],["body","\n"],["body","Allowing special characters in vars_prompt values"],["body","\n\n"],["body","\n"],["body","Module defaults\n\n"],["body","Module defaults groups"],["body","\n\n"],["body","\n"],["body","Validating tasks: check mode and diff mode\n\n"],["body","Using check mode"],["body","\n"],["body","Using diff mode"],["body","\n\n"],["body","\n"],["body","Executing playbooks for troubleshooting\n\n"],["body","start-at-task"],["body","\n"],["body","Step mode"],["body","\n\n"],["body","\n"],["body","Debugging tasks\n\n"],["body","Enabling the debugger"],["body","\n"],["body","Resolving errors in the debugger"],["body","\n"],["body","Available debug commands"],["body","\n"],["body","How the debugger interacts with the free strategy"],["body","\n\n"],["body","\n"],["body","Controlling playbook execution: strategies and more\n\n"],["body","Selecting a strategy"],["body","\n"],["body","Setting the number of forks"],["body","\n"],["body","Using keywords to control execution"],["body","\n\n"],["body","\n"],["body","Asynchronous actions and polling\n\n"],["body","Asynchronous ad hoc tasks"],["body","\n"],["body","Asynchronous playbook tasks"],["body","\n\n"],["body","\n"],["body","Advanced Syntax\n\n"],["body","Unsafe or raw strings"],["body","\n"],["body","YAML anchors and aliases: sharing variable values"],["body","\n\n"],["body","\n"],["body","Data manipulation\n\n"],["body","Loops and list comprehensions"],["body","\n"],["body","Complex Type transformations"],["body","\n\n"],["body","\n"],["body","Rejecting modules"],["body","\n"],["body","Sample Ansible setup\n\n"],["body","Sample directory layout"],["body","\n"],["body","Alternative directory layout"],["body","\n"],["body","Sample group and host variables"],["body","\n"],["body","Sample playbooks organized by function"],["body","\n"],["body","Sample task and handler files in a function-based role"],["body","\n"],["body","What the sample setup enables"],["body","\n"],["body","Organizing for deployment or configuration"],["body","\n"],["body","Using local Ansible modules"],["body","\n\n"],["body","\n"],["body","Working With Modules\n\n"],["body","Introduction to modules"],["body","\n"],["body","Module Maintenance & Support"],["body","\n"],["body","Return Values"],["body","\n\n"],["body","\n"],["body","Working with plugins\n\n"],["body","Action plugins"],["body","\n"],["body","Become plugins"],["body","\n"],["body","Cache plugins"],["body","\n"],["body","Callback plugins"],["body","\n"],["body","Cliconf plugins"],["body","\n"],["body","Connection plugins"],["body","\n"],["body","Docs fragments"],["body","\n"],["body","Filter plugins"],["body","\n"],["body","Httpapi plugins"],["body","\n"],["body","Inventory plugins"],["body","\n"],["body","Lookup plugins"],["body","\n"],["body","Modules"],["body","\n"],["body","Module utilities"],["body","\n"],["body","Netconf plugins"],["body","\n"],["body","Shell plugins"],["body","\n"],["body","Strategy plugins"],["body","\n"],["body","Terminal plugins"],["body","\n"],["body","Test plugins"],["body","\n"],["body","Vars plugins"],["body","\n\n"],["body","\n"],["body","Playbook Keywords\n\n"],["body","Play"],["body","\n"],["body","Role"],["body","\n"],["body","Block"],["body","\n"],["body","Task"],["body","\n\n"],["body","\n"],["body","Ansible and BSD\n\n"],["body","Connecting to BSD nodes"],["body","\n"],["body","Bootstrapping BSD"],["body","\n"],["body","Setting the Python interpreter"],["body","\n"],["body","Which modules are available?"],["body","\n"],["body","Using BSD as the control node"],["body","\n"],["body","BSD facts"],["body","\n"],["body","BSD efforts and contributions"],["body","\n\n"],["body","\n"],["body","Windows Guides\n\n"],["body","Setting up a Windows Host"],["body","\n"],["body","Windows Remote Management"],["body","\n"],["body","Using Ansible and Windows"],["body","\n"],["body","Desired State Configuration"],["body","\n"],["body","Windows performance"],["body","\n"],["body","Windows Frequently Asked Questions"],["body","\n\n"],["body","\n"],["body","Using collections\n\n"],["body","Installing collections"],["body","\n"],["body","Downloading collections"],["body","\n"],["body","Listing collections"],["body","\n"],["body","Verifying collections"],["body","\n"],["body","Using collections in a Playbook"],["body","\n"],["body","Simplifying module names with the collections keyword"],["body","\n"],["body","Using a playbook from a collection"],["body","\n\n"],["body","\n\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","3.运维与部署_ansible/ansible/index.html"],["title","ansible - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","installation"],["heading","Installation"],["body","\n"],["headingLink","从github获取ansible"],["heading","从Github获取Ansible"],["body","\n"],["body","如果你有一个github账户,可以跟进Ansible在Github的项目: Github project 我们在这里保持对bugs和feature ideas的跟踪."],["body","\n"],["headingLink","需要安装些什么"],["heading","需要安装些什么"],["body","\n\n"],["body","\n"],["body","Ansible默认通过 SSH 协议管理机器."],["body","\n"],["body","\n"],["body","\n"],["body","安装Ansible之后,不需要启动或运行一个后台进程,或是添加一个数据库.只要在一台电脑(可以是一台笔记本)上安装好,就可以通过这台电脑管理一组远程的机器.在远程被管理的机器上,不需要安装运行任何软件,"],["body","\n"],["body","\n"],["body","\n"],["body","因此升级Ansible版本不会有太多问题."],["body","\n"],["body","\n\n"],["headingLink","选择哪一个版本"],["heading","选择哪一个版本?"],["body","\n\n"],["body","\n"],["body","因为Ansible可以很简单的从源码运行,且不必在远程被管理机器上安装任何软件,很多Ansible用户会跟进使用开发版本."],["body","\n"],["body","\n"],["body","\n"],["body","Ansible一般每两个月出一个发行版本.小bugs一般在下一个发行版本中修复,并在稳定分支中做backports."],["body","\n"],["body","\n"],["body","\n"],["body","大bugs会在必要时出一个维护版本,不过这不是很频繁."],["body","\n"],["body","\n"],["body","\n"],["body","若你希望使用Ansible的最新版本,并且你使用的操作系统是 Red Hat Enterprise Linux (TM), CentOS, Fedora, Debian, Ubuntu,我们建议使用系统的软件包管理器."],["body","\n"],["body","\n"],["body","\n"],["body","另有一种选择是通过”pip”工具安装,”pip”是一个安装和管理Python包的工具."],["body","\n"],["body","\n"],["body","\n"],["body","若你希望跟进开发版本,想使用和测试最新的功能特性,我们会分享如何从源码运行Ansible的方法.从源码运行程序不需要进行软件安装."],["body","\n"],["body","\n\n"],["headingLink","对管理主机的要求"],["heading","对管理主机的要求"],["body","\n\n"],["body","目前,只要机器上安装了 Python 2.6 或 Python 2.7 (windows系统不可以做控制主机),都可以运行Ansible."],["body","\n"],["body","主机的系统可以是 Red Hat, Debian, CentOS, OS X, BSD的各种版本,等等."],["body","\n"],["body","自2.0版本开始,ansible使用了更多句柄来管理它的子进程,对于OS X系统,你需要增加ulimit值才能使用15个以上子进程,方法 sudo launchctl limit maxfiles 1024 2048,否则你可能会看见”Too many open file”的错误提示."],["body","\n\n"],["headingLink","对托管节点的要求"],["heading","对托管节点的要求"],["body","\n\n"],["body","通常我们使用 ssh 与托管节点通信，默认使用 sftp.如果 sftp 不可用，可在 ansible.cfg 配置文件中配置成 scp 的方式"],["body","\n"],["body","在托管节点上也需要安装 Python 2.4 或以上的版本.如果版本低于 Python 2.5 ,还需要额外安装一个模块:python-simplejson"],["body","\n"],["body","没安装python-simplejson,也可以使用Ansible的”raw”模块和script模块,因此从技术上讲,你可以通过Ansible的”raw”模块安装python-simplejson,之后就可以使用Ansible的所有功能了."],["body","\n"],["body","如果托管节点上开启了SElinux,你需要安装libselinux-python,这样才可使用Ansible中与copy/file/template相关的函数.你可以通过Ansible的yum模块在需要的托管节点上安装libselinux-python."],["body","\n"],["body","Python 3 与 Python 2 是稍有不同的语言,大多数Python程序还不能在 Python 3 中正确运行.一些Linux发行版(Gentoo, Arch)没有默认安装 Python 2.X 解释器.在这些系统上,你需要安装一个 Python 2.X 解释器,并在 inventory (详见 Inventory文件) 中设置 ‘ansible_python_interpreter’ 变量指向你的 2.X Python.你可以使用 ‘raw’ 模块在托管节点上远程安装Python 2.X."],["body","\n"],["body","例如：ansible myhost --sudo -m raw -a \"yum install -y python2 python-simplejson\" 这条命令可以通过远程方式在托管节点上安装 Python 2.X 和 simplejson 模块."],["body","\n"],["body","Red Hat Enterprise Linux, CentOS, Fedora, and Ubuntu 等发行版都默认安装了 2.X 的解释器,包括几乎所有的Unix系统也是如此."],["body","\n\n"],["headingLink","安装管理主机"],["heading","安装管理主机"],["body","\n\n"],["body","\n"],["body","从项目的checkout中可以很容易运行Ansible,Ansible的运行不要求root权限,也不依赖于其他软件,不要求运行后台进程,也不需要设置数据库."],["body","\n"],["body","\n"],["body","\n"],["body","因此我们社区的许多用户一直使用Ansible的开发版本,这样可以利用最新的功能特性,也方便对项目做贡献.因为不需要安装任何东西,跟进Ansible的开发版相对于其他开源项目要容易很多."],["body","\n"],["body","\n\n"],["headingLink","从源码安装的步骤"],["heading","从源码安装的步骤"],["body","\n"],["body","$ git clone git://github.com/ansible/ansible.git --recursive\n$ cd ./ansible\n"],["body","\n"],["headingLink","使用-bash"],["heading","使用 Bash:"],["body","\n"],["body","$ source ./hacking/env-setup\n"],["body","\n"],["headingLink","使用-fish"],["heading","使用 Fish:"],["body","\n"],["body","$ . ./hacking/env-setup.fish\n"],["body","\n"],["body","If you want to suppress spurious warnings/errors, use:"],["body","\n"],["body","$ source ./hacking/env-setup -q\n"],["body","\n"],["body","如果没有安装pip, 请先安装对应于你的Python版本的pip:"],["body","\n"],["body","$ sudo easy_install pip\n"],["body","\n"],["body","以下的Python模块也需要安装"],["body","\n"],["body","$ sudo pip install paramiko PyYAML Jinja2 httplib2 six\n"],["body","\n"],["body","注意,当更新ansible版本时,不只要更新git的源码树,也要更新git中指向Ansible自身模块的 “submodules” (不是同一种模块)"],["body","\n"],["body","$ git pull --rebase\n$ git submodule update --init --recursive\n"],["body","\n"],["body","一旦运行env-setup脚本,就意味着Ansible从源码中运行起来了.默认的inventory文件是 /etc/ansible/hosts.inventory文件也可以另行指定 (详见 Inventory文件) :"],["body","\n"],["body","$ echo \"127.0.0.1\" > ~/ansible_hosts\n$ export ANSIBLE_HOSTS=~/ansible_hosts\n"],["body","\n"],["body","你可以在手册的后续章节阅读更多关于 inventory 文件的使用,现在让我们测试一条ping命令:"],["body","\n"],["body","$ ansible all -m ping --ask-pass\n"],["body","\n"],["body","你也可以使用命令 “sudo make install”\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","3.运维与部署_ansible/ansible/3.内置模块/2.Shell.html"],["title","Shell - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","ansiblebuiltinshell-module--execute-shell-commands-on-targets"],["heading","ansible.builtin.shell module – Execute shell commands on targets"],["body","\n\n"],["body","位于 ansible-core "],["body","\n"],["body","可以直接指定 command 不用 collections:  关键字"],["body","\n"],["body","推荐使用 FQCN。避免命名冲突"],["body","\n\n"],["headingLink","synopsis"],["heading","Synopsis"],["body","\n\n"],["body","执行shell命令"],["body","\n\n\n"],["body","可以使用 free-form 格式指定"],["body","\n"],["body","类似 ansible.builtin.command ，但使用 (/bin/sh) 执行"],["body","\n"],["body","For Windows targets, use the ansible.windows.win_shell module instead."],["body","\n\n"],["headingLink","parameters"],["heading","Parameters"],["body","\n"],["body","Parameter"],["body","Comments"],["body","\n"],["body","chdir pathadded in 0.6 of ansible.builtin"],["body","Change into this directory before running the command."],["body","\n"],["body","cmd string"],["body","The command to run followed by optional arguments."],["body","\n"],["body","creates path"],["body","A filename, when it already exists, this step will not be run."],["body","\n"],["body","executable pathadded in 0.9 of ansible.builtin"],["body","Change the shell used to execute the command.This expects an absolute path to the executable."],["body","\n"],["body","free_form string"],["body","The shell module takes a free form command to run, as a string.There is no actual parameter named ‘free form’.See the examples on how to use this module."],["body","\n"],["body","removes pathadded in 0.8 of ansible.builtin"],["body","A filename, when it does not exist, this step will not be run."],["body","\n"],["body","stdin stringadded in 2.4 of ansible.builtin"],["body","Set the stdin of the command directly to the specified value."],["body","\n"],["body","stdin_add_newline booleanadded in 2.8 of ansible.builtin"],["body","Whether to append a newline to stdin data.Choices:noyes ← (default)"],["body","\n"],["body","warn booleanadded in 1.8 of ansible.builtin"],["body","Whether to enable task warnings.Choices:noyes ← (default)"],["body","\n\n\n"],["headingLink","attributes"],["heading","Attributes"],["body","\n"],["body","Attribute"],["body","Support"],["body","Description"],["body","\n"],["body","check_mode"],["body","partial"],["body","while the command itself is arbitrary and cannot be subject to the check mode semantics it adds creates/removes options as a workaround"],["body","Can run in check_mode and return changed status prediction withought modifying target"],["body","\n"],["body","diff_mode"],["body","none"],["body","Will return details on what has changed (or possibly needs changing in check_mode), when in diff mode"],["body","\n"],["body","platform"],["body","Platform: posix"],["body","Target OS/families that can be operated against"],["body","\n"],["body","raw"],["body","full"],["body","Indicates if an action takes a ‘raw’ or ‘free form’ string as an option and has it’s own special parsing of it"],["body","\n\n\n"],["headingLink","注意"],["heading","注意"],["body","\n\n"],["body","如果您想安全且可预测地执行命令, it may be better to use the ansible.builtin.command module instead. Best practices when writing playbooks will follow the trend of using ansible.builtin.command unless the ansible.builtin.shell module is explicitly required. When running ad-hoc commands, use your best judgement."],["body","\n"],["body","To sanitize any variables passed to the shell module, you should use {{ var | quote }} instead of just {{ var }} to make sure they do not include evil things like semicolons."],["body","\n"],["body","An alternative to using inline shell scripts with this module is to use the ansible.builtin.script module possibly together with the ansible.builtin.template module."],["body","\n"],["body","For rebooting systems, use the ansible.builtin.reboot or ansible.windows.win_reboot module."],["body","\n\n"],["headingLink","see-also"],["heading","See Also"],["body","\n\n"],["body","\n"],["body","ansible.builtin.command"],["body","\n"],["body","The official documentation on the ansible.builtin.command module."],["body","\n"],["body","\n"],["body","\n"],["body","ansible.builtin.raw"],["body","\n"],["body","The official documentation on the ansible.builtin.raw module."],["body","\n"],["body","\n"],["body","\n"],["body","ansible.builtin.script"],["body","\n"],["body","The official documentation on the ansible.builtin.script module."],["body","\n"],["body","\n"],["body","\n"],["body","ansible.windows.win_shell"],["body","\n"],["body","The official documentation on the ansible.windows.win_shell module."],["body","\n"],["body","\n\n"],["headingLink","examples"],["heading","Examples"],["body","\n"],["body","- name: Execute the command in remote shell; stdout goes to the specified file on the remote\n  ansible.builtin.shell: somescript.sh >> somelog.txt\n  \n\n- name: Change the working directory to somedir/ before executing the command\n  ansible.builtin.shell: somescript.sh >> somelog.txt\n  args:\n  chdir: somedir/\n  \n \n # You can also use the 'args' form to provide the options.\n- name: This command will change the working directory to somedir/ and will only run when somedir/somelog.txt doesn't exist\n  ansible.builtin.shell: somescript.sh >> somelog.txt\n  args:\n    chdir: somedir/\n    creates: somelog.txt\n    \n# You can also use the 'cmd' parameter instead of free form format.\n- name: This command will change the working directory to somedir/\n  ansible.builtin.shell:\n    cmd: ls -l | grep log\n    chdir: somedir/\n\n- name: Run a command that uses non-posix shell-isms (in this example /bin/sh doesn't handle redirection and wildcards together but bash does)\n  ansible.builtin.shell: cat < /tmp/*txt\n  args:\n    executable: /bin/bash\n\n- name: Run a command using a templated variable (always use quote filter to avoid injection)\n  ansible.builtin.shell: cat {{ myfile|quote }}\n  \n  \n  # You can use shell to run other executables to perform actions inline\n- name: Run expect to wait for a successful PXE boot via out-of-band CIMC\n  ansible.builtin.shell: |\n    set timeout 300\n    spawn ssh admin@{{ cimc_host }}\n\n    expect \"password:\"\n    send \"{{ cimc_password }}\\n\"\n\n    expect \"\\n{{ cimc_name }}\"\n    send \"connect host\\n\"\n\n    expect \"pxeboot.n12\"\n    send \"\\n\"\n\n    exit 0\n  args:\n    executable: /usr/bin/expect\n  delegate_to: localhost\n  \n# Disabling warnings\n- name: Using curl to connect to a host via SOCKS proxy (unsupported in uri). Ordinarily this would throw a warning\n  ansible.builtin.shell: curl --socks5 localhost:9000 http://www.ansible.com\n  args:\n    warn: no\n"],["body","\n"],["headingLink","return-values"],["heading","Return Values"],["body","\n"],["body","Key"],["body","Description"],["body","\n"],["body","cmd string"],["body","The command executed by the task.Returned: alwaysSample: “rabbitmqctl join_cluster rabbit@master”"],["body","\n"],["body","delta string"],["body","The command execution delta time.Returned: alwaysSample: “0:00:00.325771”"],["body","\n"],["body","end string"],["body","The command execution end time.Returned: alwaysSample: “2016-02-25 09:18:26.755339”"],["body","\n"],["body","msg boolean"],["body","changedReturned: alwaysSample: true"],["body","\n"],["body","rc integer"],["body","The command return code (0 means success).Returned: alwaysSample: 0"],["body","\n"],["body","start string"],["body","The command execution start time.Returned: alwaysSample: “2016-02-25 09:18:26.429568”"],["body","\n"],["body","stderr string"],["body","The command standard error.Returned: alwaysSample: “ls: cannot access foo: No such file or directory”"],["body","\n"],["body","stderr_lines list / elements=string"],["body","The command standard error split in lines.Returned: alwaysSample: [{“u\\u0027ls cannot access foo”: “No such file or directory\\u0027”}, “u\\u0027ls \\u2026\\u0027”]"],["body","\n"],["body","stdout string"],["body","The command standard output.Returned: alwaysSample: “Clustering node rabbit@slave1 with rabbit@master \\u2026”"],["body","\n"],["body","stdout_lines list / elements=string"],["body","The command standard output split in lines.Returned: alwaysSample: [“u\\u0027Clustering node rabbit@slave1 with rabbit@master \\u2026\\u0027”]"],["body","\n\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","3.运维与部署_ansible/ansible/3.内置模块/1.Command.html"],["title","Command - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","ansiblebuiltincommand-module--execute-commands-on-targets"],["heading","ansible.builtin.command module – Execute commands on targets"],["body","\n\n"],["body","位于 ansible-core "],["body","\n"],["body","可以直接指定 command 不用 collections:  关键字"],["body","\n"],["body","推荐使用 FQCN。避免命名冲突"],["body","\n\n"],["headingLink","synopsis"],["heading","Synopsis"],["body","\n\n"],["body","命令模块  采用 命令名称，后跟空格分隔的参数列表。"],["body","\n"],["body","给定的命令将在所有选定的节点上执行。"],["body","\n"],["body","命令不会通过shell处理，因此变量 (如 $HOSTNAME) 和 操作符 \"*\", \"<\", \">\", \"|\", \";\"and\"&\" 不会被处理。如果需要使用ansible.builtin.shell 模块"],["body","\n"],["body","创建 command tasks 比使用空格分隔参数的任务更容易阅读，  使用 args 或者cmd 传递参数"],["body","\n"],["body","自由格式命令 或 cmd参数，请参见示例。"],["body","\n"],["body","对于Windows目标， use the ansible.windows.win_command module instead."],["body","\n\n"],["body","This module has a corresponding action plugin."],["body","\n"],["headingLink","parameters"],["heading","Parameters"],["body","\n"],["body","Parameter"],["body","Comments"],["body","\n"],["body","argv list / elements=stringadded in 2.6 of ansible.builtin"],["body","1. 将命令作为列表而不是字符串传递"],["body","2. argv可以避免转义"],["body","3. 自由格式或者 argv 必须二者提供其一"],["body","\n"],["body","chdir pathadded in 0.6 of ansible.builtin"],["body","切换工作目录"],["body","\n"],["body","cmd string"],["body","执行的命令"],["body","\n"],["body","creates path"],["body","1. 文件名或 (自2.0起) glob模式"],["body","2. 如果已经存在匹配的文件，则不会运行此步骤."],["body","3.  在 removes 检查 前"],["body","\n"],["body","free_form string"],["body","1. 命令模块将一个自由形式的字符串作为命令运行."],["body","2. 类似于 modulename: cmdstr"],["body","\n"],["body","removes pathadded in 0.8 of ansible.builtin"],["body","1. 文件名或 (自2.0起) glob模式"],["body","2. 如果已经存在匹配的文件，则会运行此步骤."],["body","3. This is checked after creates is checked."],["body","\n"],["body","stdin stringadded in 2.4 of ansible.builtin"],["body","将命令的stdin 直接设置为指定值。"],["body","\n"],["body","stdin_add_newline booleanadded in 2.8 of ansible.builtin"],["body","If set to yes, append a newline to stdin data."],["body","Choices:"],["body","no"],["body","yes ← (default)"],["body","\n"],["body","strip_empty_ends booleanadded in 2.8 of ansible.builtin"],["body","Strip empty lines from the end of stdout/stderr in result"],["body","Choices:"],["body","no"],["body","yes ← (default)"],["body","\n"],["body","warn booleanadded in 1.8 of ansible.builtin"],["body","(deprecated) Enable or disable task warnings.This feature is deprecated and will be removed in 2.14.As of version 2.11, this option is now disabled by default.**Choices:**no ← (default)yes"],["body","\n\n\n"],["headingLink","see-also"],["heading","See Also"],["body","\n\n"],["body","\n"],["body","ansible.builtin.raw"],["body","\n"],["body","The official documentation on the ansible.builtin.raw module."],["body","\n"],["body","\n"],["body","\n"],["body","ansible.builtin.script"],["body","\n"],["body","The official documentation on the ansible.builtin.script module."],["body","\n"],["body","\n"],["body","\n"],["body","ansible.builtin.shell"],["body","\n"],["body","The official documentation on the ansible.builtin.shell module."],["body","\n"],["body","\n"],["body","\n"],["body","ansible.windows.win_command"],["body","\n"],["body","The official documentation on the ansible.windows.win_command module."],["body","\n"],["body","\n\n"],["headingLink","examples"],["heading","Examples"],["body","\n"],["body","- name: Return motd to registered var\n  ansible.builtin.command: cat /etc/motd\n  register: mymotd\n  \n  \n# free-form (string) arguments, all arguments on one line\n- name: Run command if /path/to/database does not exist (without 'args')\n  ansible.builtin.command: /usr/bin/make_database.sh db_user db_name creates=/path/to/database\n  \n# free-form (string) arguments, some arguments on separate lines with the 'args' keyword\n# 'args' is a task keyword, passed at the same level as the module\n- name: Run command if /path/to/database does not exist (with 'args' keyword)\n  ansible.builtin.command: /usr/bin/make_database.sh db_user db_name\n  args:\n    creates: /path/to/database\n    \n    \n# 'cmd' is module parameter\n- name: Run command if /path/to/database does not exist (with 'cmd' parameter)\n  ansible.builtin.command:\n    cmd: /usr/bin/make_database.sh db_user db_name\n    creates: /path/to/database\n    \n- name: Change the working directory to somedir/ and run the command as db_owner if /path/to/database does not exist\n  ansible.builtin.command: /usr/bin/make_database.sh db_user db_name\n  become: yes\n  become_user: db_owner\n  args:\n    chdir: somedir/\n    creates: /path/to/database\n    \n# argv (list) arguments, each argument on a separate line, 'args' keyword not necessary\n# 'argv' is a parameter, indented one level from the module\n- name: Use 'argv' to send a command as a list - leave 'command' empty\n  ansible.builtin.command:\n    argv:\n      - /usr/bin/make_database.sh\n      - Username with whitespace\n      - dbname with whitespace\n    creates: /path/to/database\n\n- name: Safely use templated variable to run command. Always use the quote filter to avoid injection issues\n  ansible.builtin.command: cat {{ myfile|quote }}\n  register: myoutput\n\n"],["body","\n"],["headingLink","return-values"],["heading","Return Values"],["body","\n"],["body","Key"],["body","Description"],["body","\n"],["body","cmd list / elements=string"],["body","The command executed by the task."],["body","Returned: always"],["body","Sample: [“echo”, “hello”]"],["body","\n"],["body","delta string"],["body","The command execution delta time."],["body","Returned: always"],["body","Sample: “0:00:00.001529”"],["body","\n"],["body","end string"],["body","The command execution end time."],["body","Returned: always"],["body","Sample: “2017-09-29 22:03:48.084657”"],["body","\n"],["body","msg boolean"],["body","changed"],["body","Returned: always"],["body","Sample: true"],["body","\n"],["body","rc integer"],["body","The command return code (0 means success)."],["body","Returned: always"],["body","Sample: 0"],["body","\n"],["body","start string"],["body","The command execution start time."],["body","Returned: always"],["body","Sample: “2017-09-29 22:03:48.083128”"],["body","\n"],["body","stderr string"],["body","The command standard error."],["body","Returned: always"],["body","Sample: “ls cannot access foo: No such file or directory”"],["body","\n"],["body","stderr_lines list / elements=string"],["body","The command standard error split in lines."],["body","Returned: always"],["body","Sample: [{“u\\u0027ls cannot access foo”: “No such file or directory\\u0027”}, “u\\u0027ls \\u2026\\u0027”]"],["body","\n"],["body","stdout string"],["body","The command standard output."],["body","Returned: always"],["body","Sample: “Clustering node rabbit@slave1 with rabbit@master \\u2026”"],["body","\n"],["body","stdout_lines list / elements=string"],["body","The command standard output split in lines."],["body","Returned: always"],["body","Sample: [“u\\u0027Clustering node rabbit@slave1 with rabbit@master \\u2026\\u0027”]"],["body","\n\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","3.运维与部署_ansible/ansible/3.内置模块/index.html"],["title","3.内置模块 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","ansiblebuiltin"],["heading","Ansible.Builtin"],["body","\n"],["body","Collection version 2.12.6.post0"],["body","\n\n"],["body","Description"],["body","\n"],["body","Communication"],["body","\n"],["body","Plugin Index"],["body","\n\n"],["headingLink","description"],["heading","Description"],["body","\n"],["body","这些都是ansible-core中包含的所有模块和插件。"],["body","\n"],["body","Author:"],["body","\n\n"],["body","Ansible, Inc."],["body","\n\n"],["body","Issue TrackerRepository (Sources)"],["body","\n"],["headingLink","communication"],["heading","Communication"],["body","\n\n"],["body","Matrix room #users:ansible.im: General usage and support questions."],["body","\n"],["body","IRC channel #ansible (Libera network): General usage and support questions."],["body","\n"],["body","Mailing list: Ansible Project List. (Subscribe)"],["body","\n\n"],["headingLink","plugin-index"],["heading","Plugin Index"],["body","\n"],["body","这些是ansible.builtin集合中的插件:"],["body","\n"],["headingLink","modules"],["heading","Modules"],["body","\n\n"],["body","add_host module – Add a host (and alternatively a group) to the ansible-playbook in-memory inventory"],["body","\n"],["body","apt module – Manages apt-packages"],["body","\n"],["body","apt_key module – Add or remove an apt key"],["body","\n"],["body","apt_repository module – Add and remove APT repositories"],["body","\n"],["body","assemble module – Assemble configuration files from fragments"],["body","\n"],["body","assert module – Asserts given expressions are true"],["body","\n"],["body","async_status module – Obtain status of asynchronous task"],["body","\n"],["body","blockinfile module – Insert/update/remove a text block surrounded by marker lines"],["body","\n"],["body","command module – Execute commands on targets"],["body","\n"],["body","copy module – Copy files to remote locations"],["body","\n"],["body","cron module – Manage cron.d and crontab entries"],["body","\n"],["body","debconf module – Configure a .deb package"],["body","\n"],["body","debug module – Print statements during execution"],["body","\n"],["body","dnf module – Manages packages with the dnf package manager"],["body","\n"],["body","dpkg_selections module – Dpkg package selection selections"],["body","\n"],["body","expect module – Executes a command and responds to prompts"],["body","\n"],["body","fail module – Fail with custom message"],["body","\n"],["body","fetch module – Fetch files from remote nodes"],["body","\n"],["body","file module – Manage files and file properties"],["body","\n"],["body","find module – Return a list of files based on specific criteria"],["body","\n"],["body","gather_facts module – Gathers facts about remote hosts"],["body","\n"],["body","get_url module – Downloads files from HTTP, HTTPS, or FTP to node"],["body","\n"],["body","getent module – A wrapper to the unix getent utility"],["body","\n"],["body","git module – Deploy software (or files) from git checkouts"],["body","\n"],["body","group module – Add or remove groups"],["body","\n"],["body","group_by module – Create Ansible groups based on facts"],["body","\n"],["body","hostname module – Manage hostname"],["body","\n"],["body","import_playbook module – Import a playbook"],["body","\n"],["body","import_role module – Import a role into a play"],["body","\n"],["body","import_tasks module – Import a task list"],["body","\n"],["body","include module – Include a play or task list"],["body","\n"],["body","include_role module – Load and execute a role"],["body","\n"],["body","include_tasks module – Dynamically include a task list"],["body","\n"],["body","include_vars module – Load variables from files, dynamically within a task"],["body","\n"],["body","iptables module – Modify iptables rules"],["body","\n"],["body","known_hosts module – Add or remove a host from the known_hosts file"],["body","\n"],["body","lineinfile module – Manage lines in text files"],["body","\n"],["body","meta module – Execute Ansible ‘actions’"],["body","\n"],["body","package module – Generic OS package manager"],["body","\n"],["body","package_facts module – Package information as facts"],["body","\n"],["body","pause module – Pause playbook execution"],["body","\n"],["body","ping module – Try to connect to host, verify a usable python and return pong on success"],["body","\n"],["body","pip module – Manages Python library dependencies"],["body","\n"],["body","raw module – Executes a low-down and dirty command"],["body","\n"],["body","reboot module – Reboot a machine"],["body","\n"],["body","replace module – Replace all instances of a particular string in a file using a back-referenced regular expression"],["body","\n"],["body","rpm_key module – Adds or removes a gpg key from the rpm db"],["body","\n"],["body","script module – Runs a local script on a remote node after transferring it"],["body","\n"],["body","service module – Manage services"],["body","\n"],["body","service_facts module – Return service state information as fact data"],["body","\n"],["body","set_fact module – Set host variable(s) and fact(s)."],["body","\n"],["body","set_stats module – Define and display stats for the current ansible run"],["body","\n"],["body","setup module – Gathers facts about remote hosts"],["body","\n"],["body","shell module – Execute shell commands on targets"],["body","\n"],["body","slurp module – Slurps a file from remote nodes"],["body","\n"],["body","stat module – Retrieve file or file system status"],["body","\n"],["body","subversion module – Deploys a subversion repository"],["body","\n"],["body","systemd module – Manage systemd units"],["body","\n"],["body","sysvinit module – Manage SysV services."],["body","\n"],["body","tempfile module – Creates temporary files and directories"],["body","\n"],["body","template module – Template a file out to a target host"],["body","\n"],["body","unarchive module – Unpacks an archive after (optionally) copying it from the local machine"],["body","\n"],["body","uri module – Interacts with webservices"],["body","\n"],["body","user module – Manage user accounts"],["body","\n"],["body","validate_argument_spec module – Validate role argument specs."],["body","\n"],["body","wait_for module – Waits for a condition before continuing"],["body","\n"],["body","wait_for_connection module – Waits until remote system is reachable/usable"],["body","\n"],["body","yum module – Manages packages with the yum package manager"],["body","\n"],["body","yum_repository module – Add or remove YUM repositories"],["body","\n\n"],["headingLink","become-plugins"],["heading","Become Plugins"],["body","\n\n"],["body","runas become – Run As user"],["body","\n"],["body","su become – Substitute User"],["body","\n"],["body","sudo become – Substitute User DO"],["body","\n\n"],["headingLink","cache-plugins"],["heading","Cache Plugins"],["body","\n\n"],["body","jsonfile cache – JSON formatted files."],["body","\n"],["body","memory cache – RAM backed, non persistent"],["body","\n\n"],["headingLink","callback-plugins"],["heading","Callback Plugins"],["body","\n\n"],["body","default callback – default Ansible screen output"],["body","\n"],["body","junit callback – write playbook output to a JUnit file."],["body","\n"],["body","minimal callback – minimal Ansible screen output"],["body","\n"],["body","oneline callback – oneline Ansible screen output"],["body","\n"],["body","tree callback – Save host events to files"],["body","\n\n"],["headingLink","connection-plugins"],["heading","Connection Plugins"],["body","\n\n"],["body","local connection – execute on controller"],["body","\n"],["body","paramiko_ssh connection – Run tasks via python ssh (paramiko)"],["body","\n"],["body","psrp connection – Run tasks over Microsoft PowerShell Remoting Protocol"],["body","\n"],["body","ssh connection – connect via SSH client binary"],["body","\n"],["body","winrm connection – Run tasks over Microsoft’s WinRM"],["body","\n\n"],["headingLink","inventory-plugins"],["heading","Inventory Plugins"],["body","\n\n"],["body","advanced_host_list inventory – Parses a ‘host list’ with ranges"],["body","\n"],["body","auto inventory – Loads and executes an inventory plugin specified in a YAML config"],["body","\n"],["body","constructed inventory – Uses Jinja2 to construct vars and groups based on existing inventory."],["body","\n"],["body","generator inventory – Uses Jinja2 to construct hosts and groups from patterns"],["body","\n"],["body","host_list inventory – Parses a ‘host list’ string"],["body","\n"],["body","ini inventory – Uses an Ansible INI file as inventory source."],["body","\n"],["body","script inventory – Executes an inventory script that returns JSON"],["body","\n"],["body","toml inventory – Uses a specific TOML file as an inventory source."],["body","\n"],["body","yaml inventory – Uses a specific YAML file as an inventory source."],["body","\n\n"],["headingLink","lookup-plugins"],["heading","Lookup Plugins"],["body","\n\n"],["body","config lookup – Lookup current Ansible configuration values"],["body","\n"],["body","csvfile lookup – read data from a TSV or CSV file"],["body","\n"],["body","dict lookup – returns key/value pair items from dictionaries"],["body","\n"],["body","env lookup – Read the value of environment variables"],["body","\n"],["body","file lookup – read file contents"],["body","\n"],["body","fileglob lookup – list files matching a pattern"],["body","\n"],["body","first_found lookup – return first file found from list"],["body","\n"],["body","indexed_items lookup – rewrites lists to return ‘indexed items’"],["body","\n"],["body","ini lookup – read data from a ini file"],["body","\n"],["body","inventory_hostnames lookup – list of inventory hosts matching a host pattern"],["body","\n"],["body","items lookup – list of items"],["body","\n"],["body","lines lookup – read lines from command"],["body","\n"],["body","list lookup – simply returns what it is given."],["body","\n"],["body","nested lookup – composes a list with nested elements of other lists"],["body","\n"],["body","password lookup – retrieve or generate a random password, stored in a file"],["body","\n"],["body","pipe lookup – read output from a command"],["body","\n"],["body","random_choice lookup – return random element from list"],["body","\n"],["body","sequence lookup – generate a list based on a number sequence"],["body","\n"],["body","subelements lookup – traverse nested key from a list of dictionaries"],["body","\n"],["body","template lookup – retrieve contents of file after templating with Jinja2"],["body","\n"],["body","together lookup – merges lists into synchronized list"],["body","\n"],["body","unvault lookup – read vaulted file(s) contents"],["body","\n"],["body","url lookup – return contents from URL"],["body","\n"],["body","varnames lookup – Lookup matching variable names"],["body","\n"],["body","vars lookup – Lookup templated value of variables"],["body","\n\n"],["headingLink","shell-plugins"],["heading","Shell Plugins"],["body","\n\n"],["body","cmd shell – Windows Command Prompt"],["body","\n"],["body","powershell shell – Windows PowerShell"],["body","\n"],["body","sh shell – POSIX shell (/bin/sh)"],["body","\n\n"],["headingLink","strategy-plugins"],["heading","Strategy Plugins"],["body","\n\n"],["body","debug strategy – Executes tasks in interactive debug session."],["body","\n"],["body","free strategy – Executes tasks without waiting for all hosts"],["body","\n"],["body","host_pinned strategy – Executes tasks on each host without interruption"],["body","\n"],["body","linear strategy – Executes tasks in a linear fashion"],["body","\n\n"],["headingLink","vars-plugins"],["heading","Vars Plugins"],["body","\n\n"],["body","host_group_vars vars – In charge of loading group_vars and host_vars"],["body","\n\n"],["body","List of collections with docs hosted here."],["body","\n"],["body"," PreviousNext "],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","3.运维与部署_ansible/ansible/2.AnsibleConcepts/index.html"],["title","2.AnsibleConcepts - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","ansible-concepts"],["heading","Ansible concepts"],["body","\n"],["body","这些概念对于Ansible的所有用途都是通用的。您需要了解它们才能将Ansible用于任何类型的自动化。本基本介绍提供了您需要遵循用户指南其余部分的背景。"],["body","\n\n"],["body","Control node"],["body","\n"],["body","Managed nodes"],["body","\n"],["body","Inventory"],["body","\n"],["body","Collections"],["body","\n"],["body","Modules"],["body","\n"],["body","Tasks"],["body","\n"],["body","Playbooks"],["body","\n\n"],["headingLink","control-node"],["heading","Control node"],["body","\n\n"],["body","任何安装了Ansible的机器"],["body","\n"],["body","您可以通过从任何控制节点调用Ansible或ansible-playbook命令来运行ansible命令和playbook。"],["body","\n"],["body","您可以使用任何具有Python安装的计算机作为控制节点-笔记本电脑，共享台式机和服务器都可以运行Ansible。"],["body","\n"],["body","但是，您不能将Windows机器用作控制节点。"],["body","\n"],["body","您可以有多个控制节点。"],["body","\n\n"],["headingLink","managed-nodes"],["heading","Managed nodes"],["body","\n"],["body","您使用Ansible管理的网络设备 (和/或服务器)。托管节点有时也称为 “主机”。Ansible未安装在托管节点上。"],["body","\n"],["headingLink","inventory"],["heading","Inventory"],["body","\n\n"],["body","托管节点列表。inventory file 有时也称为 “hostfile”"],["body","\n"],["body","您的清单可以为每个托管节点指定像ip地址这样的信息"],["body","\n"],["body","清单还可以组织托管节点，创建和嵌套组，以便于扩展"],["body","\n"],["body","详见： the Working with Inventory "],["body","\n\n"],["headingLink","collections"],["heading","Collections"],["body","\n\n"],["body","\n"],["body","Collections 是Ansible内容的发布格式，可以包括剧本、角色、模块和插件。"],["body","\n"],["body","\n"],["body","\n"],["body","You can install and use collections through Ansible Galaxy. "],["body","\n"],["body","\n"],["body","\n"],["body","To learn more about collections, see Using collections."],["body","\n"],["body","\n\n"],["headingLink","modules"],["heading","Modules"],["body","\n\n"],["body","Ansible执行的代码单位"],["body","\n"],["body","每个模块都有特定的用途，从在特定类型的数据库上管理用户到在特定类型的网络设备上管理VLAN接口"],["body","\n"],["body","您可以使用任务调用单个模块，或者在剧本中调用几个不同的模块"],["body","\n"],["body","从Ansible 2.10开始，模块在集合中分组"],["body","\n"],["body","关于Ansible包括多少个collections，详见 Collection Index."],["body","\n\n"],["headingLink","tasks"],["heading","Tasks"],["body","\n"],["body","Ansible中的行动单位。您可以使用临时命令执行一次单个任务。"],["body","\n"],["headingLink","playbooks"],["heading","Playbooks"],["body","\n\n"],["body","\n"],["body","已保存的任务顺序列表，以便您可以按该顺序重复运行这些任务"],["body","\n"],["body","\n"],["body","\n"],["body","剧本可以包括变量和任务。"],["body","\n"],["body","\n"],["body","\n"],["body","剧本用YAML编写，易于阅读，写作，分享和理解"],["body","\n"],["body","\n"],["body","\n"],["body","详见： Intro to playbooks."],["body","\n"],["body","\n\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","3.运维与部署_ansible/ansible/playbooks.html"],["title","playbooks - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","概述"],["heading","概述"],["body","\n"],["body","playBooks 练习 "],["body","\n"],["headingLink","playbook语言示例"],["heading","PlayBook语言示例"],["body","\n\n"],["body","playbook 由一个或多个 ‘plays’ 组成.它的内容是一个以 ‘plays’ 为元素的列表."],["body","\n"],["body","play是由一系列 中的 tasks组成"],["body","\n"],["body","一个任务是对一个 absinel模块的调用"],["body","\n\n"],["body","---\n- hosts: webservers\n  vars:\n    http_port: 80\n    max_clients: 200\n  remote_user: root\n  tasks:\n  - name: ensure apache is at the latest version\n    yum: pkg=httpd state=latest\n  - name: write the apache config file\n    template: src=/srv/httpd.j2 dest=/etc/httpd.conf\n    notify:\n    - restart apache\n  - name: ensure apache is running\n    service: name=httpd state=started\n  handlers:\n    - name: restart apache\n      service: name=httpd state=restarted\n"],["body","\n"],["headingLink","playbook基础"],["heading","playbook基础"],["body","\n"],["headingLink","主机与用户"],["heading","主机与用户"],["body","\n"],["body","要执行的主机，与用户"],["body","\n"],["body","---\n- hosts: webservers\n  remote_user: root\n"],["body","\n"],["body","再者,在每一个 task 中,可以定义自己的远程用户:"],["body","\n"],["body","---\n- hosts: webservers\n  remote_user: root\n  tasks:\n    - name: test connection\n      ping:\n      remote_user: yourname\n"],["body","\n"],["body","也支持从 sudo 执行命令:"],["body","\n"],["body","---\n- hosts: webservers\n  remote_user: yourname\n  sudo: yes\n"],["body","\n"],["body","同样的,你可以仅在一个 task 中,使用 sudo 执行命令,而不是在整个 play 中使用 sudo:"],["body","\n"],["body","---\n- hosts: webservers\n  remote_user: yourname\n  tasks:\n    - service: name=nginx state=started\n      sudo: yes\n"],["body","\n"],["body","你也可以登陆后,sudo 到不同的用户身份,而不是使用 root:"],["body","\n"],["body","---\n- hosts: webservers\n  remote_user: yourname\n  sudo: yes\n  sudo_user: postgres\n"],["body","\n"],["body","\n"],["body","如果你需要在使用 sudo 时指定密码,可在运行 ansible-playbook 命令时加上选项 --ask-sudo-pass (-K). 如果使用 sudo 时,playbook 疑似被挂起,可能是在 sudo prompt 处被卡住,这时可执行 Control-C 杀死卡住的任务,再重新运行一次."],["body","\n"],["body","\n"],["body","\n"],["body","当使用 sudo_user 切换到 非root 用户时,模块的参数会暂时写入 /tmp 目录下的一个随机临时文件. 当命令执行结束后,临时文件立即删除.这种情况发生在普通用户的切换时,比如从 ‘bob’ 切换到 ‘timmy’, 切换到 root 账户时,不会发生,如从 ‘bob’ 切换到 ‘root’,直接以普通用户或root身份登录也不会发生. 如果你不希望这些数据在短暂的时间内可以被读取（不可写）,请避免在 sudo_user 中传递未加密的密码. 其他情况下,’/tmp’ 目录不被使用,这种情况不会发生.Ansible 也有意识的在日志中不记录密码参数."],["body","\n"],["body","\n"],["headingLink","tasks-列表"],["heading","Tasks 列表"],["body","\n"],["body","每一个 play 包含了一个 task 列表（任务列表）.一个 task 在其所对应的所有主机上（通过 host pattern 匹配的所有主机）执行完毕之后,下一个 task 才会执行"],["body","\n"],["body","有一点需要明白的是（很重要）,在一个 play 之中,所有 hosts 会获取相同的任务指令"],["body","\n"],["body","每个 task 的目标在于执行一个 moudle, 通常是带有特定的参数来执行.在参数中可以使用变量（variables）."],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","3.运维与部署_ansible/ansible/临时命令行模式.html"],["title","临时命令行模式 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","简介"],["heading","简介"],["body","\n"],["body","在命令行即时输入的命令叫临时命令行 ad-hoc命令"],["body","\n"],["headingLink","语法格式"],["heading","语法格式"],["body","\n"],["body","ansible [pattern] -m [module] -a \"[module options]\""],["body","\n"],["body","pattern"],["body","\n"],["body","主机选择模式"],["body","\n"],["body","-m module"],["body","\n"],["body","-m 指定命令模块，默认是 commandModule"],["body","\n"],["body","-a "],["body","\n"],["body","模块选项"],["body","\n"],["headingLink","示例"],["heading","示例"],["body","\n"],["headingLink","重启"],["heading","重启"],["body","\n"],["body","# -fork10个进程来执行\nansible atlanta -a \"/sbin/reboot\" -f 10\n\n# 默认使用当前用户执行，可以指定用户\nansible atlanta -a \"/sbin/reboot\" -f 10 -u username //默认以当前用户运行\n\n# --ask-become-pass or -K ：提示密码输入\nansible atlanta -a \"/sbin/reboot\" -f 10 -u username --become [--ask-become-pass]\n"],["body","\n"],["headingLink","模式"],["heading","模式"],["body","\n"],["headingLink","commandmodule"],["heading","commandModule"],["body","\n"],["body","command 模块不支持 shell 变量,也不支持管道等 shell 相关的东西"],["body","\n"],["headingLink","shellmode"],["heading","shellmode"],["body","\n"],["body","ansible raleigh -m shell -a 'echo $TERM'\n"],["body","\n"],["headingLink","copy"],["heading","copy"],["body","\n"],["body","\n"],["body","从服务主机 copy到 受管理主机"],["body","\n"],["body","\n"],["body","ansible atlanta -m copy -a \"src=/etc/hosts dest=/tmp/hosts\"\n"],["body","\n"],["headingLink","文件管理"],["heading","文件管理"],["body","\n"],["body","\n"],["body","使用 file 模块可以做到修改文件的属主和权限,(在这里可替换为 copy 模块,是等效的):"],["body","\n"],["body","\n"],["body","# 改变文件 属组\nansible webservers -m file -a \"dest=/srv/foo/b.txt mode=600 owner=mdehaan group=mdehaan\"\n# 新建目录\nansible webservers -m file -a \"dest=/path/to/c mode=755 owner=mdehaan group=mdehaan state=directory\"\n#删除目录和文件\nansible webservers -m file -a \"dest=/path/to/c state=absent\n"],["body","\n"],["headingLink","管理包"],["heading","管理包"],["body","\n"],["body","#确认一个软件包已经安装,但不去升级它:\nansible webservers -m yum -a \"name=acme state=present\"\n# 确认一个软件包的安装版本:\nansible webservers -m yum -a \"name=acme-1.5 state=present\"\n#安装最新的包\nansible webservers -m yum -a \"name=acme state=latest\"\n#确认一个软件包还没有安装:\nansible webservers -m yum -a \"name=acme state=absent\"\n"],["body","\n"],["headingLink","管理用户和组"],["heading","管理用户和组"],["body","\n"],["body","\n"],["body","使用 ‘user’ 模块可以方便的创建账户,删除账户,或是管理现有的账户:"],["body","\n"],["body","\n"],["body","ansible all -m user -a \"name=foo password=<crypted password here>\"\nansible all -m user -a \"name=foo state=absent\"\n"],["body","\n"],["headingLink","从源代码管理中心部署服务"],["heading","从源代码管理中心部署服务"],["body","\n"],["body","# 直接使用 git 部署 webapp:\nansible webservers -m git -a \"repo=git://foo.example.org/repo.git dest=/srv/myapp version=HEAD\"\n"],["body","\n"],["headingLink","管理服务"],["heading","管理服务"],["body","\n"],["body","# 确认某个服务在所有的webservers上都已经启动:\nansible webservers -m service -a \"name=httpd state=started\"\n# 或是在所有的webservers上重启某个服务\nansible webservers -m service -a \"name=httpd state=restarted\"\n# 确认某个服务已经停止，如果没有停止则 停止服务：必须是被 systemctl管理的服务\nansible webservers -m service -a \"name=httpd state=stopped\"\n"],["body","\n"],["headingLink","有时限的后台操作"],["heading","有时限的后台操作"],["body","\n"],["body","#  -B 1800 表示最多运行30分钟, -P 60 表示每隔60秒获取一次状态信息.\nansible all -B 3600 -P 0 -a \"/usr/bin/long_running_operation --do-stuff\"\n"],["body","\n\n"],["body","Polling 获取状态信息的操作会在后台工作任务启动之后开始"],["body","\n"],["body","你希望所有的工作任务快速启动, --forks 这个选项的值 要设置得足够大,这是前面讲过的并发进程的个数.在运行指定的时间(由-B选项所指定)后,远程节点上的任务进程便会被终止."],["body","\n\n"],["headingLink","收集信息"],["heading","收集信息"],["body","\n"],["body","ansible all -m setup\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","3.运维与部署_ansible/ansible/ansible示例.html"],["title","ansible示例 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","公钥互信机制"],["heading","公钥互信机制"],["body","\n"],["body","ssh-key-gen -t rsa\nssh-copy-key 192.168.3.101  192.168.3.102 192.168.3.103\n"],["body","\n"],["headingLink","ping所有主机"],["heading","ping所有主机"],["body","\n"],["body","# ping所有主机\nansible all -m ping\n# 带用户名\nansible all -m ping -u bruce  \n# 以sudo运行\nansible all -m ping -u bruce --sudo\n# 以 sudo用户执行\nansible all -m ping -u bruce --sudo --sudo-user batman\n"],["body","\n"],["headingLink","往所有机器上写东西"],["heading","往所有机器上写东西"],["body","\n"],["body","ansible myserver  -m shell -a \"echo helloWorld>~/a.txt\"\n"],["body","\n"],["headingLink","包管理"],["heading","包管理"],["body","\n"],["body","# 安装 epel数据源\nansible myserver -m yum -a \"name=https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm state=present\"\n\n# 安装nginx\nansible myserver -m yum -a \"name=nginx.x86_64 state=present\"\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","3.运维与部署_ansible/ansible/Loops.html"],["title","Loops - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","loops"],["heading","Loops"],["body","\n\n"],["body","\n"],["body","Ansible提供 ， loop, with_<lookup>, and until 关键字多次执行任务。"],["body","\n"],["body","\n"],["body","\n"],["body","常用循环的示例 包括使用文件模块更改多个文件和/或目录的所有权， file module"],["body","\n"],["body","\n"],["body","\n"],["body","creating multiple users with the user module,"],["body","\n"],["body","\n"],["body","\n"],["body","并重复轮询步骤，直到达到某个结果。"],["body","\n"],["body","\n\n"],["headingLink","standard-loops"],["heading","Standard loops"],["body","\n"],["headingLink","iterating-over-a-simple-list"],["heading","Iterating over a simple list"],["body","\n"],["body","- name: Add several users\n  ansible.builtin.user:\n    name: \"{{ item }}\"\n    state: present\n    groups: \"wheel\"\n  loop:\n     - testuser1\n     - testuser2\n"],["body","\n"],["body","您可以在variables 文件中或  play 的 vars 部分中定义列表，然后参考任务中列表的名称。"],["body","\n"],["body","loop: \"{{ somelist }}\"\n\n"],["body","\n"],["body","- name: Add user testuser1\n  ansible.builtin.user:\n    name: \"testuser1\"\n    state: present\n    groups: \"wheel\"\n\n- name: Add user testuser2\n  ansible.builtin.user:\n    name: \"testuser2\"\n    state: present\n    groups: \"wheel\"\n"],["body","\n"],["body","注意"],["body","\n\n"],["body","\n"],["body","您可以将列表直接传递给某些插件的参数。"],["body","\n"],["body","\n"],["body","\n"],["body","大多数包装模块，例如yum和apt，都具有此功能。"],["body","\n"],["body","\n"],["body","\n"],["body","在可用时，将列表传递给参数比循环任务更好。例如"],["body","\n"],["body","\n\n"],["headingLink","iterating-over-a-list-of-hashes"],["heading","Iterating over a list of hashes"],["body","\n"],["body","- name: Add several users\n  ansible.builtin.user:\n    name: \"{{ item.name }}\"\n    state: present\n    groups: \"{{ item.groups }}\"\n  loop:\n    - { name: 'testuser1', groups: 'wheel' }\n    - { name: 'testuser2', groups: 'root' }\n"],["body","\n\n"],["body","When combining conditionals with a loop, "],["body","\n"],["body","the when: statement is processed separately for each item. See Basic conditionals with when for examples."],["body","\n\n"],["headingLink","iterating-over-a-dictionary"],["heading","Iterating over a dictionary"],["body","\n"],["body","To loop over a dict, use the dict2items:"],["body","\n"],["body","- name: Using dict2items\n  ansible.builtin.debug:\n    msg: \"{{ item.key }} - {{ item.value }}\"\n  loop: \"{{ tag_data | dict2items }}\"\n  vars:\n    tag_data:\n      Environment: dev\n      Application: payment\n"],["body","\n"],["headingLink","registering-variables-with-a-loop"],["heading","Registering variables with a loop"],["body","\n"],["body","output 注册为变量"],["body","\n"],["body","- name: Register loop output as a variable\n  ansible.builtin.shell: \"echo {{ item }}\"\n  loop:\n    - \"one\"\n    - \"two\"\n  register: echo\n"],["body","\n\n"],["body","使用 loop注册register  变量时： "],["body","\n"],["body","返回结果包含 results "],["body","\n"],["body","无loop的注册情况与此不同"],["body","\n\n"],["body","{\n    \"changed\": true,\n    \"msg\": \"All items completed\",\n    \"results\": [\n        {\n            \"changed\": true,\n            \"cmd\": \"echo \\\"one\\\" \",\n            \"delta\": \"0:00:00.003110\",\n            \"end\": \"2013-12-19 12:00:05.187153\",\n            \"invocation\": {\n                \"module_args\": \"echo \\\"one\\\"\",\n                \"module_name\": \"shell\"\n            },\n            \"item\": \"one\",\n            \"rc\": 0,\n            \"start\": \"2013-12-19 12:00:05.184043\",\n            \"stderr\": \"\",\n            \"stdout\": \"one\"\n        },\n        {\n            \"changed\": true,\n            \"cmd\": \"echo \\\"two\\\" \",\n            \"delta\": \"0:00:00.002920\",\n            \"end\": \"2013-12-19 12:00:05.245502\",\n            \"invocation\": {\n                \"module_args\": \"echo \\\"two\\\"\",\n                \"module_name\": \"shell\"\n            },\n            \"item\": \"two\",\n            \"rc\": 0,\n            \"start\": \"2013-12-19 12:00:05.242582\",\n            \"stderr\": \"\",\n            \"stdout\": \"two\"\n        }\n    ]\n}\n"],["body","\n"],["body","在注册变量上的后续循环以检查结果可能看起来像"],["body","\n"],["body","- name: Fail if return code is not 0\n  ansible.builtin.fail:\n    msg: \"The command ({{ item.cmd }}) did not have a 0 return code\"\n  when: item.rc != 0\n  loop: \"{{ echo.results }}\"\n"],["body","\n"],["body","在迭代过程中，当前项 的结果将被放置在变量中。"],["body","\n"],["body","- name: Place the result of the current item in the variable\n  ansible.builtin.shell: echo \"{{ item }}\"\n  loop:\n    - one\n    - two\n  register: echo\n  changed_when: echo.stdout != \"one\"\n"],["body","\n"],["headingLink","complex-loops"],["heading","Complex loops"],["body","\n"],["headingLink","iterating-over-nested-lists"],["heading","Iterating over nested lists"],["body","\n"],["body","您可以使用Jinja2表达式迭代复杂列表。例如，一个循环可以组合嵌套列表。"],["body","\n"],["body","- name: Give users access to multiple databases\n  community.mysql.mysql_user:\n    name: \"{{ item[0] }}\"\n    priv: \"{{ item[1] }}.*:ALL\"\n    append_privs: yes\n    password: \"foo\"\n  loop: \"{{ ['alice', 'bob'] | product(['clientdb', 'employeedb', 'providerdb']) | list }}\"\n"],["body","\n"],["headingLink","retrying-a-task-until-a-condition-is-met"],["heading","Retrying a task until a condition is met"],["body","\n\n"],["body","New in version 1.4."],["body","\n"],["body","您可以使用 \"until\" 关键字重试任务，直到满足特定条件。下面是一个例子:"],["body","\n\n"],["body","- name: Retry a task until a certain condition is met\n  ansible.builtin.shell: /usr/bin/foo\n  register: result\n  until: result.stdout.find(\"all systems go\") != -1\n  retries: 5\n  delay: 10\n"],["body","\n\n"],["body","\n"],["body","此任务最多运行5次，每次尝试之间延迟10秒。"],["body","\n"],["body","\n"],["body","\n"],["body","If the result of any attempt has “all systems go” in its stdout, the task succeeds"],["body","\n"],["body","\n"],["body","\n"],["body","“retries” 的默认值为3，“delay” 为5。"],["body","\n"],["body","\n"],["body","\n"],["body","要查看每次重试的结果，请使用-vv运行play。"],["body","\n"],["body","\n"],["body","\n"],["body","当使用 until 关键字，注册变量时 会多一个  attempts 记录 任务重试的次数"],["body","\n"],["body","\n\n"],["headingLink","looping-over-inventory"],["heading","Looping over inventory"],["body","\n\n"],["body","To loop over your inventory, or just a subset of it, you can use a regular loop with the ansible_play_batch or groups variables."],["body","\n\n"],["body","- name: Show all the hosts in the inventory\n  ansible.builtin.debug:\n    msg: \"{{ item }}\"\n  loop: \"{{ groups['all'] }}\"\n\n- name: Show all the hosts in the current play\n  ansible.builtin.debug:\n    msg: \"{{ item }}\"\n  loop: \"{{ ansible_play_batch }}\"\n"],["body","\n"],["body","There is also a specific lookup plugin inventory_hostnames that can be used like this"],["body","\n"],["body","- name: Show all the hosts in the inventory\n  ansible.builtin.debug:\n    msg: \"{{ item }}\"\n  loop: \"{{ query('inventory_hostnames', 'all') }}\"\n\n- name: Show all the hosts matching the pattern, ie all but the group www\n  ansible.builtin.debug:\n    msg: \"{{ item }}\"\n  loop: \"{{ query('inventory_hostnames', 'all:!www') }}\"\n"],["body","\n"],["body","More information on the patterns can be found in Patterns: targeting hosts and groups."],["body","\n"],["headingLink","ensuring-list-input-for-loop-using-query-rather-than-lookup"],["heading","Ensuring list input for loop: using query rather than lookup"],["body","\n\n"],["body","loop关键字需要一个列表作为输入，但是lookup关键字默认返回一个逗号分隔的值字符串"],["body","\n"],["body","Ansible 2.5引入了一个名为 query 的新Jinja2函数，该函数始终返回一个列表"],["body","\n"],["body","使用loop关键字时，提供更简单的接口和更可预测的查找插件输出。"],["body","\n"],["body","您可以指定 wantlist = True  强制  loop 返回列表以循环，也可以使用query代替。"],["body","\n\n"],["body","loop: \"{{ query('inventory_hostnames', 'all') }}\"\n\nloop: \"{{ lookup('inventory_hostnames', 'all', wantlist=True) }}\"\n"],["body","\n"],["headingLink","adding-controls-to-loops"],["heading","Adding controls to loops"],["body","\n"],["body","New in version 2.1."],["body","\n"],["body","The loop_control keyword lets you manage your loops in useful ways."],["body","\n"],["headingLink","limiting-loop-output-with-label"],["heading","Limiting loop output with label"],["body","\n"],["body","New in version 2.2."],["body","\n\n"],["body","当循环遍历复杂的数据结构时，任务的控制台输出可能是巨大的. "],["body","\n"],["body","限制显示的输出, use the label directive with loop_control."],["body","\n\n"],["body","- name: Create servers\n  digital_ocean:\n    name: \"{{ item.name }}\"\n    state: present\n  loop:\n    - name: server1\n      disks: 3gb\n      ram: 15Gb\n      network:\n        nic01: 100Gb\n        nic02: 10Gb\n        ...\n  loop_control:\n    label: \"{{ item.name }}\"\n"],["body","\n\n"],["body","\n"],["body","此任务的输出将仅显示每个 item 的 name field ，而不是多行 {{ item }} 变量的全部内容。"],["body","\n"],["body","\n"],["body","\n"],["body","这是为了使控制台输出更具可读性，而不是保护敏感数据。如果循环中有敏感数据，请在任务上设置no_log: yes以防止泄露。"],["body","\n"],["body","\n\n"],["headingLink","pausing-within-a-loop"],["heading","Pausing within a loop"],["body","\n"],["body","New in version 2.2."],["body","\n"],["body","要控制任务循环中每个itm执行之间的时间 (以秒为单位)，请使用带有loop_control的pause指令。"],["body","\n"],["body","# main.yml\n- name: Create servers, pause 3s before creating next\n  community.digitalocean.digital_ocean:\n    name: \"{{ item }}\"\n    state: present\n  loop:\n    - server1\n    - server2\n  loop_control:\n    pause: 3\n"],["body","\n"],["headingLink","tracking-progress-through-a-loop-with-index_var"],["heading","Tracking progress through a loop with index_var"],["body","\n"],["body","New in version 2.5."],["body","\n\n"],["body","跟踪你在循环中的位置"],["body","\n"],["body","使用 index_var directive with loop_control. "],["body","\n"],["body","此指令指定一个变量名，以包含当前循环索引。"],["body","\n\n"],["body","- name: Count our fruit\n  ansible.builtin.debug:\n    msg: \"{{ item }} with index {{ my_idx }}\"\n  loop:\n    - apple\n    - banana\n    - pear\n  loop_control:\n    index_var: my_idx\n"],["body","\n"],["body","index_var is 0 indexed."],["body","\n"],["headingLink","defining-inner-and-outer-variable-names-with-loop_var"],["heading","Defining inner and outer variable names with loop_var"],["body","\n"],["body","New in version 2.1."],["body","\n\n"],["body","通过使用  include_tasks  迭代 两个嵌套任务"],["body","\n"],["body","但是，默认情况下Ansible为每个循环设置循环变量 item。"],["body","\n"],["body","这意味着内部嵌套循环将覆盖外部循环中的item值"],["body","\n"],["body","您可以使用loop_var和loop_control为每个循环指定变量的名称。"],["body","\n\n"],["body","# main.yml\n- include_tasks: inner.yml\n  loop:\n    - 1\n    - 2\n    - 3\n  loop_control:\n    loop_var: outer_item\n\n# inner.yml\n- name: Print outer and inner items\n  ansible.builtin.debug:\n    msg: \"outer item={{ outer_item }} inner item={{ item }}\"\n  loop:\n    - a\n    - b\n    - c\n"],["body","\n"],["body","如果Ansible检测到当前循环使用的是已经定义的变量，则会引发错误以使任务失败。"],["body","\n"],["headingLink","extended-loop-variables"],["heading","Extended loop variables"],["body","\n"],["body","New in version 2.8."],["body","\n"],["body","从Ansible 2.8开始，您可以使用扩展选项来循环控制来获取扩展的循环信息。此选项将公开以下信息。"],["body","\n"],["body","Variable"],["body","Description"],["body","\n"],["body","ansible_loop.allitems"],["body","The list of all items in the loop"],["body","\n"],["body","ansible_loop.index"],["body","The current iteration of the loop. (1 indexed)"],["body","\n"],["body","ansible_loop.index0"],["body","The current iteration of the loop. (0 indexed)"],["body","\n"],["body","ansible_loop.revindex"],["body","The number of iterations from the end of the loop (1 indexed)"],["body","\n"],["body","ansible_loop.revindex0"],["body","The number of iterations from the end of the loop (0 indexed)"],["body","\n"],["body","ansible_loop.first"],["body","True if first iteration"],["body","\n"],["body","ansible_loop.last"],["body","True if last iteration"],["body","\n"],["body","ansible_loop.length"],["body","The number of items in the loop"],["body","\n"],["body","ansible_loop.previtem"],["body","The item from the previous iteration of the loop. Undefined during the first iteration."],["body","\n"],["body","ansible_loop.nextitem"],["body","The item from the following iteration of the loop. Undefined during the last iteration."],["body","\n\n\n"],["body","loop_control:\n  extended: yes\n"],["body","\n"],["body","When using loop_control.extended more memory will be utilized on the control node. "],["body","\n\n"],["body","当使用loop_control.extended时，控制节点上将利用更多的内存"],["body","\n"],["body","因为 ansible_loop.allitems  包含 所有数据的引用"],["body","\n"],["body","当序列化结果以显示在主ansible进程内的回调插件中时，这些引用可能会被取消引用，导致内存使用量增加。"],["body","\n\n"],["headingLink","accessing-the-name-of-your-loop_var"],["heading","Accessing the name of your loop_var"],["body","\n"],["body","New in version 2.8."],["body","\n\n"],["body","从 Ansible 2.8 可以获取 loop_control.loop_var 变量名称"],["body","\n"],["body","对于role authors, ，编写允许循环的角色， instead of dictating the required loop_var value, you can gather the value via the following"],["body","\n\n"],["body","\"{{ lookup('vars', ansible_loop_var) }}\"\n"],["body","\n"],["headingLink","see-also"],["heading","See also"],["body","\n\n"],["body","\n"],["body","Intro to playbooks"],["body","\n"],["body","An introduction to playbooks"],["body","\n"],["body","\n"],["body","\n"],["body","Roles"],["body","\n"],["body","Playbook organization by roles"],["body","\n"],["body","\n"],["body","\n"],["body","Tips and tricks"],["body","\n"],["body","Tips and tricks for playbooks"],["body","\n"],["body","\n"],["body","\n"],["body","Conditionals"],["body","\n"],["body","Conditional statements in playbooks"],["body","\n"],["body","\n"],["body","\n"],["body","Using Variables"],["body","\n"],["body","All about variables"],["body","\n"],["body","\n"],["body","\n"],["body","User Mailing List"],["body","\n"],["body","Have a question? Stop by the google group!"],["body","\n"],["body","\n"],["body","\n"],["body","Real-time chat"],["body","\n"],["body","How to join Ansible chat channels"],["body","\n"],["body","\n\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","3.运维与部署_ansible/ansible/Ansible.html"],["title","Ansible - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","关于ansible"],["heading","关于Ansible"],["body","\n"],["body","Ansible是一个IT自动化工具，能够配置系统，部署软件，编码高级的IT任务例如 持续部署或者 0宕机 滚动更新"],["body","\n"],["headingLink","安装ansible"],["heading","安装Ansible"],["body","\n"],["headingLink","安装ansible自身"],["heading","安装Ansible自身"],["body","\n"],["body","sudo yum install ansible"],["body","\n"],["headingLink","安装shell命令行自动补全"],["heading","安装shell命令行自动补全"],["body","\n"],["body","yum install epel-release"],["body","\n"],["body","yum install python-argcomplete"],["body","\n"],["headingLink","自动补全设置"],["heading","自动补全设置"],["body","\n"],["body","activate-global-python-argcomplete"],["body","\n"],["headingLink","概念"],["heading","概念"],["body","\n"],["headingLink","控制节点"],["heading","控制节点"],["body","\n"],["body","能够运行Ansible命令 和playbooks 通过"],["body","\n"],["body","/usr/bin/ansible or/usr/bin/ansible-playbook"],["body","\n"],["headingLink","受管节点"],["heading","受管节点"],["body","\n"],["body","Ansible管理的网络设备"],["body","\n"],["body","/etc/ansible/hosts 中记录着 受管结点的主机名"],["body","\n"],["headingLink","inventory"],["heading","Inventory"],["body","\n"],["body","一系列的受管节点，清单文件也叫做 hostfile,可以为每个受管节点指定IP，也可以用来创建或者嵌套组，方便扩容，"],["body","\n"],["headingLink","modules"],["heading","Modules"],["body","\n"],["body","ansible功能单位，每一个module都有专门的 功能，从管理特定数据库的用户到 管理特定类型的网络设备VLAN接口，可以执行一个模块的一个task，也可以执行多个模块的多个功能，也就是剧本"],["body","\n"],["headingLink","tasks"],["heading","tasks"],["body","\n"],["body","Ansible的 执行动作单位"],["body","\n"],["headingLink","playbooks"],["heading","playbooks"],["body","\n\n"],["body","有序的任务列表"],["body","\n"],["body","以YAML方式写的"],["body","\n\n"],["headingLink","动态清单"],["heading","动态清单"],["body","\n"],["body","description"],["body","\n\n"],["body","如果你的配置根据需求 时常变动，你可能需要从多个源头 载入hosts，例如云服务提供商，LDAP，Cobber,或者其他企业的CMDB"],["body","\n"],["body","Ansible提供两种方式，连接外部存储\n\n"],["body","inventory plugins：推荐使用plugins"],["body","\n"],["body","inventory scripts"],["body","\n\n"],["body","\n"],["body","红帽的 RedHatAnsibleTower 提供GUI界面编辑与同步，并提供web and Rest服务"],["body","\n\n"],["body","example with Cobber"],["body","\n\n"],["body","Ansible能与cobber无缝集成。cobber主要用于OS安装，DHCP,DNS 管理，从当轻量级的CMDB"],["body","\n\n"],["headingLink","模式定位主机和组"],["heading","模式：定位主机和组"],["body","\n\n"],["body","\n"],["body","ansible <pattern> -m <module_name> -a \"<module options>\"\""],["body","\n"],["body","\n"],["body","\n"],["body","pattern 是 playbook的 hosts 选项"],["body","\n"],["body","\n"],["body","\n"],["body","pattern模式"],["body","\n"],["body","Description"],["body","Pattern(s)"],["body","Targets"],["body","\n"],["body","All hosts"],["body","all (or *)"],["body","\n"],["body","One host"],["body","host1"],["body","\n"],["body","Multiple hosts"],["body","host1:host2 (or host1,host2)"],["body","\n"],["body","One group"],["body","webservers"],["body","\n"],["body","Multiple groups"],["body","webservers:dbservers"],["body","all hosts in webservers plus all hosts in dbservers"],["body","\n"],["body","Excluding groups"],["body","webservers:!atlanta"],["body","all hosts in webservers except those in atlanta"],["body","\n"],["body","Intersection of groups"],["body","webservers:&staging"],["body","any hosts in webservers that are also in staging"],["body","\n\n"],["body","\n"],["body","\n"],["body","pattern高级用法"],["body","\n\n"],["body","\n"],["body","使用变量 ， ansible-playbook， -e 传递的ansible-playbook"],["body","\n"],["body","\n"],["body","\n"],["body","使用组定位，"],["body","\n"],["body","webservers[0]       # == cobweb\nwebservers[-1]      # == weber\nwebservers[0:2]     # == webservers[0],webservers[1]\n                    # == cobweb,webbing\nwebservers[1:]      # == webbing,weber\nwebservers[:3]      # == cobweb,webbing,weber\n"],["body","\n"],["body","\n"],["body","\n"],["body","使用正则表达式 ，以 ~ 开头的"],["body","\n"],["body","\n"],["body","\n"],["body","在命令行选项指定 --limit "],["body","\n\n"],["body","\n"],["body","ansible-playbook site.yml --limit datacenter2 //指定主机\n"],["body","\n"],["body","\n"],["body","\n"],["body","ansible-playbook site.yml --limit @retry_hosts.txt //指定从文件读主机\n"],["body","\n"],["body","\n\n"],["body","\n\n"],["body","\n\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","5.日志收集/filebeat/1.HowFilebeatWorks.html"],["title","HowFilebeatWorks - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","how-filebeat-works"],["heading","How Filebeat works"],["body","\n"],["body","在本主题中，您将了解Filebeat的关键构建块以及它们如何协同工作。了解这些概念将有助于您就为特定用例配置Filebeat做出明智的决策。"],["body","\n"],["body","Filebeat consists of two main components: inputs and harvesters. These components work together to tail files and send event data to the output that you specify."],["body","\n"],["headingLink","what-is-a-harvester"],["heading","What is a harvester?"],["body","\n"],["body","收割机负责读取单个文件的内容。"],["body","\n"],["body","收割机一行一行地读取每个文件，并将内容发送到输出。每个文件启动一台收割机。"],["body","\n"],["body","收割机负责打开和关闭文件，这意味着在收割机运行时文件描述符保持打开状态"],["body","\n"],["body","If a file is removed or renamed while it’s being harvested, Filebeat continues to read the file."],["body","\n"],["body","如果文件在收割时被删除或重命名，Filebeat将继续读取该文件。这有一个副作用，即在收割机关闭之前保留磁盘上的空间。"],["body","\n"],["body","默认情况下，Filebeat将文件保持打开状态，直到达到  close_inactive "],["body","\n"],["body","Closing a harvester has the following consequences:"],["body","\n\n"],["body","The file handler is closed, freeing up the underlying resources if the file was deleted while the harvester was still reading the file."],["body","\n"],["body","The harvesting of the file will only be started again after scan_frequency has elapsed."],["body","\n"],["body","If the file is moved or removed while the harvester is closed, harvesting of the file will not continue."],["body","\n\n"],["body","To control when a harvester is closed, use the close_* configuration options."],["body","\n"],["headingLink","what-is-an-input"],["heading","What is an input?"],["body","\n"],["body","input负责管理收割机并查找所有要读取的来源。"],["body","\n"],["body","如果输入类型为log，则输入将查找驱动器上与定义的glob路径匹配的所有文件，并为每个文件启动一个收割机。每个input 都在其自己的Go例程中运行。"],["body","\n"],["body","以下示例将Filebeat配置为从与指定的glob模式匹配的所有日志文件中收获行:"],["body","\n"],["body","filebeat.inputs:\n- type: log\n  paths:\n    - /var/log/*.log\n    - /var/path2/*.log\n"],["body","\n"],["body","Filebeat currently supports several input types. "],["body","\n"],["body","每种输入类型可以多次定义。"],["body","\n"],["body","Filebeat目前支持几种输入类型。每种输入类型可以多次定义。日志输入会检查每个文件，以查看是否需要启动收割机，是否已经在运行，或者是否可以忽略该文件 (请参阅ignore_older)。仅当自收割机关闭以来文件的大小发生变化时，才会拾取新行。"],["body","\n"],["headingLink","how-does-filebeat-keep-the-state-of-files"],["heading","How does Filebeat keep the state of files?"],["body","\n"],["body","Filebeat保留每个文件的状态，并经常将状态刷新到注册表文件中的磁盘。状态用于记住收割机读取的最后一个偏移量，并确保发送所有日志行。如果无法访问输出 (例如Elasticsearch或Logstash)，则Filebeat会跟踪发送的最后一行，并在输出再次可用后继续读取文件。Filebeat运行时，每个输入的状态信息也保存在内存中。当Filebeat重新启动时，来自注册表文件的数据将用于重建状态，并且Filebeat在最后一个已知位置继续每个收割机。"],["body","\n"],["body","For each input, Filebeat keeps a state of each file it finds. Because files can be renamed or moved, the filename and path are not enough to identify a file. For each file, Filebeat stores unique identifiers to detect whether a file was harvested previously."],["body","\n"],["body","对于每个输入，Filebeat都会保留找到的每个文件的状态。因为文件可以重命名或移动，所以文件名和路径不足以识别文件。对于每个文件，Filebeat存储唯一的标识符，以检测先前是否已收获文件。"],["body","\n"],["body","如果您的用例涉及每天创建大量新文件，则可能会发现注册表文件变得太大。 See Registry file is too large for details about configuration options that you can set to resolve this issue."],["body","\n"],["headingLink","how-does-filebeat-ensure-at-least-once-delivery"],["heading","How does Filebeat ensure at-least-once delivery?"],["body","\n"],["body","Filebeat保证事件将至少一次传递到配置的输出，并且不会丢失数据。Filebeat能够实现此行为，因为它将每个事件的传递状态存储在注册表文件中。"],["body","\n"],["body","在定义的输出被阻止并且尚未确认所有事件的情况下，Filebeat将继续尝试发送事件，直到输出确认已收到事件为止。"],["body","\n"],["body","如果Filebeat在发送事件的过程中关闭，则它不会在关闭之前等待输出确认所有事件,"],["body","\n"],["body","重新启动Filebeat时，将再次发送发送到输出但在Filebeat关闭之前未确认的任何事件,这样可以确保每个事件至少发送一次，但是最终可能导致将重复的事件发送到输出。可以配置  shutdown_timeout option. 在关闭时等待延时"],["body","\n"],["body","Filebeat的至少一次传递保证有一个限制，涉及日志轮换和旧文件的删除。如果将日志文件写入磁盘并旋转速度快于Filebeat处理的速度，或者在输出不可用时删除了文件，则数据可能会丢失。在Linux上，由于inode重用，Filebeat也可以跳过行。， See Common problems for more details about the inode reuse issue."],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","5.日志收集/filebeat/index.html"],["title","filebeat - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","快速安装"],["heading","快速安装"],["body","\n"],["headingLink","step-1-install-filebeat"],["heading","Step 1: Install Filebeat"],["body","\n"],["body","curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-8.1.1-darwin-x86_64.tar.gz\ntar xzvf filebeat-8.1.1-darwin-x86_64.tar.gz\n"],["body","\n"],["body","curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-8.1.1-linux-x86_64.tar.gz\ntar xzvf filebeat-8.1.1-linux-x86_64.tar.gz\n"],["body","\n"],["headingLink","step-2-connect-to-the-elastic-stack"],["heading","Step 2: Connect to the Elastic Stack"],["body","\n"],["body","output.elasticsearch:\n  hosts: [\"https://myEShost:9200\"]\n  username: \"filebeat_internal\"\n  password: \"YOUR_PASSWORD\" \n  ssl:\n    enabled: true\n    ca_trusted_fingerprint: \"b9a10bbe64ee9826abeda6546fc988c8bf798b41957c33d05db736716513dc9c\" \n"],["body","\n"],["headingLink","step-3-collect-log-data"],["heading","Step 3: Collect log data"],["body","\n"],["body","filebeat modules list\n"],["body","\n"],["body","filebeat modules enable nginx\n"],["body","\n"],["body","在modules.d下的模块配置中，启用所需的数据集并更改模块设置以匹配您的环境。Datasets are disabled by default."],["body","\n"],["body","例如，日志位置是根据操作系统设置的。如果您的日志不在默认位置，请设置路径变量:"],["body","\n"],["body","- module: nginx\n  access:\n    enabled: true\n    var.paths: [\"/var/log/nginx/access.log*\"] \n"],["body","\n"],["body","To see the full list of variables for a module, see the documentation under Modules."],["body","\n"],["body","./filebeat test config -e\n"],["body","\n"],["body","Make sure your config files are in the path expected by Filebeat (see Directory layout), or use the -c flag to specify the path to the config file."],["body","\n"],["body","For more information about configuring Filebeat, also see:"],["body","\n\n"],["body","Configure Filebeat"],["body","\n"],["body","Config file format"],["body","\n"],["body","filebeat.reference.yml: This reference configuration file shows all non-deprecated options. You’ll find it in the same location as filebeat.yml."],["body","\n\n"],["headingLink","enable-and-configure-ecs-loggers-for-application-log-collection"],["heading","Enable and configure ECS loggers for application log collection"],["body","\n"],["body","虽然Filebeat可用于摄取原始的纯文本应用程序日志，但我们建议您在摄取时结构化你的日志。这使您可以提取字段，例如日志级别和异常堆栈跟踪。"],["body","\n"],["body","Elastic通过提供各种流行编程语言的应用程序日志格式化程序来简化此过程。这些插件将您的日志格式化为与ECS兼容的JSON，从而无需手动解析日志。"],["body","\n"],["body","See ECS loggers to get started."],["body","\n"],["headingLink","configure-filebeat-manually"],["heading","Configure Filebeat manually"],["body","\n"],["body","see configure the input manually."],["body","\n"],["headingLink","step-4-set-up-assets"],["heading","Step 4: Set up assets"],["body","\n"],["body","Filebeat comes with predefined assets for parsing, indexing, and visualizing your data. To load these assets:"],["body","\n"],["body","Filebeat带有预定义的assets，用于解析，索引和可视化数据。要加载这些assets:"],["body","\n"],["body","filebeat setup -e\n"],["body","\n"],["body","This step loads the recommended index template for writing to Elasticsearch and deploys the sample dashboards for visualizing the data in Kibana."],["body","\n"],["body","This step does not load the ingest pipelines used to parse log lines. By default, ingest pipelines are set up automatically the first time you run the module and connect to Elasticsearch."],["body","\n"],["body","此步骤不会加载用于解析日志行的摄取管道。默认情况下，第一次运行模块并连接到Elasticsearch时会自动设置ingest管道。"],["body","\n"],["body","需要连接到Elasticsearch (或Elasticsearch服务) 才能设置初始环境。如果您使用的是其他输出，例如Logstash，请参阅:"],["body","\n\n"],["body","Load the index template manually"],["body","\n"],["body","Load Kibana dashboards"],["body","\n"],["body","Load ingest pipelines"],["body","\n\n"],["headingLink","step-5-start-filebeat"],["heading","Step 5: Start Filebeat"],["body","\n"],["body","sudo chown root filebeat.yml \nsudo chown root modules.d/nginx.yml \nsudo ./filebeat -e\n"],["body","\n"],["headingLink","step-6-view-your-data-in-kibana"],["heading","Step 6: View your data in Kibana"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","5.日志收集/filebeat/ConfigureFilebeat/1.Input.html"],["title","Input - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","configure-inputs"],["heading","Configure inputs"],["body","\n"],["body","Filebeat modules provide the fastest getting started experience for common log formats"],["body","\n"],["body","To configure Filebeat manually (instead of using modules), you specify a list of inputs in the filebeat.inputs section of the filebeat.yml. Inputs specify how Filebeat locates and processes input data."],["body","\n"],["body","The list is a YAML array, so each input begins with a dash (-). You can specify multiple inputs, and you can specify the same input type more than once. For example:"],["body","\n"],["body","filebeat.inputs:\n- type: log\n  paths:\n    - /var/log/system.log\n    - /var/log/wifi.log\n- type: log\n  paths:\n    - \"/var/log/apache2/*\"\n  fields:\n    apache: true\n  fields_under_root: true\n"],["body","\n"],["body","对于最基本的配置，定义具有单个路径的单个输入。例如:"],["body","\n"],["body","filebeat.inputs:\n- type: log\n  enabled: true\n  paths:\n    - /var/log/*.log\n"],["body","\n"],["body","The input in this example harvests all files in the path /var/log/*.log, which means that Filebeat will harvest all files in the directory /var/log/ that end with .log. All patterns supported by Go Glob are also supported here."],["body","\n"],["body","To fetch all files from a predefined level of subdirectories, use this pattern: /var/log/*/*.log. This fetches all .log files from the subfolders of /var/log. It does not fetch log files from the /var/log folder itself. Currently it is not possible to recursively fetch all files in all subdirectories of a directory."],["body","\n"],["headingLink","input-types"],["heading","Input types"],["body","\n"],["body","You can configure Filebeat to use the following inputs:"],["body","\n\n"],["body","AWS CloudWatch"],["body","\n"],["body","AWS S3"],["body","\n"],["body","Azure Event Hub"],["body","\n"],["body","Cloud Foundry"],["body","\n"],["body","Container"],["body","\n"],["body","filestream"],["body","\n"],["body","GCP Pub/Sub"],["body","\n"],["body","HTTP Endpoint"],["body","\n"],["body","HTTP JSON"],["body","\n"],["body","journald"],["body","\n"],["body","Kafka"],["body","\n"],["body","Log"],["body","\n"],["body","MQTT"],["body","\n"],["body","NetFlow"],["body","\n"],["body","Office 365 Management Activity API"],["body","\n"],["body","Redis"],["body","\n"],["body","Stdin"],["body","\n"],["body","Syslog"],["body","\n"],["body","TCP"],["body","\n"],["body","UDP"],["body","\n\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","5.日志收集/filebeat/ConfigureFilebeat/index.html"],["title","ConfigureFilebeat - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","配置filebeat"],["heading","配置FileBeat"],["body","\n"],["body","要配置Filebeat，请编辑配置文件。默认配置文件称为filebeat.yml。文件的位置因平台而异。s"],["body","\n"],["body","To locate the file, see Directory layout."],["body","\n"],["body","还有一个名为filebeat.reference.yml的完整示例配置文件，它显示了所有非过时的的选项。"],["body","\n"],["body","See the Config File Format for more about the structure of the config file."],["body","\n"],["body","The following topics describe how to configure Filebeat:"],["body","\n\n"],["body","Inputs"],["body","\n"],["body","Modules"],["body","\n"],["body","General settings"],["body","\n"],["body","Project paths"],["body","\n"],["body","Config file loading"],["body","\n"],["body","Output"],["body","\n"],["body","SSL"],["body","\n"],["body","Index lifecycle management (ILM)"],["body","\n"],["body","Elasticsearch index template"],["body","\n"],["body","Kibana endpoint"],["body","\n"],["body","Kibana dashboards"],["body","\n"],["body","Processors"],["body","\n"],["body","Autodiscover"],["body","\n"],["body","Internal queue"],["body","\n"],["body","Load balancing"],["body","\n"],["body","Logging"],["body","\n"],["body","HTTP endpoint"],["body","\n"],["body","Regular expression support"],["body","\n"],["body","Instrumentation"],["body","\n"],["body","filebeat.reference.yml"],["body","\n\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","5.日志收集/filebeat/ConfigureFilebeat/2.InputFileStream.html"],["title","InputFileStream - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","filestream-input"],["heading","filestream input"],["body","\n"],["body","使用filestream输入从活动日志文件中读取行。它是新的，改进的替代the  log input.。它带有对现有输入的各种改进:"],["body","\n\n"],["body","Checking of close_* options happens out of band. Thus, if an output is blocked, Filebeat can close the reader and avoid keeping too many files open."],["body","\n"],["body","Detailed metrics are available for all files that match the paths configuration regardless of the harvester_limit. This way, you can keep track of all files, even ones that are not actively read."],["body","\n"],["body","The order of parsers is configurable. So it is possible to parse JSON lines and then aggregate the contents into a multiline event."],["body","\n"],["body","Some position updates and metadata changes no longer depend on the publishing pipeline. If the pipeline is blocked some changes are still applied to the registry."],["body","\n"],["body","Only the most recent updates are serialized to the registry. In contrast, the log input has to serialize the complete registry on each ACK from the outputs. This makes the registry updates much quicker with this input."],["body","\n"],["body","The input ensures that only offsets updates are written to the registry append only log. The log writes the complete file state."],["body","\n"],["body","Stale entries can be removed from the registry, even if there is no active input."],["body","\n\n"],["body","To configure this input, specify a list of glob-based paths that must be crawled to locate and fetch the log lines."],["body","\n"],["body","Example configuration:"],["body","\n"],["body","filebeat.inputs:\n- type: filestream\n  paths:\n    - /var/log/messages\n    - /var/log/*.log\n"],["body","\n"],["body","You can apply additional configuration settings (such as fields, include_lines, exclude_lines and so on) to the lines harvested from these files. "],["body","\n"],["body","The options that you specify are applied to all the files harvested by this input."],["body","\n"],["body","To apply different configuration settings to different files, you need to define multiple input sections:"],["body","\n"],["body","要将不同的配置设置应用于不同的文件，您需要定义多个 input sections:"],["body","\n"],["body","filebeat.inputs:\n- type: filestream \n  paths:\n    - /var/log/system.log\n    - /var/log/wifi.log\n- type: filestream \n  paths:\n    - \"/var/log/apache2/*\"\n  fields:\n    apache: true\n"],["body","\n"],["headingLink","reading-files-on-network-shares-and-cloud-providers"],["heading","Reading files on network shares and cloud providers"],["body","\n"],["body","但是，如果充分配置Filebeat，则可以减轻这些数据源的限制之一。"],["body","\n"],["body","默认情况下，Filebeat根据文件的inode和设备id识别文件"],["body","\n"],["body","但是，在网络共享和云提供商上，这些值可能会在文件的生命周期内更改。"],["body","\n"],["body","如果发生这种情况，Filebeat认为该文件是新文件，并重新发送该文件的全部内容。"],["body","\n"],["body","要解决此问题，您可以配置file_identity选项。除了默认的inode_deviceid之外，可能的值是path和inode_marker。"],["body","\n"],["body","在运行中间更改file_identity方法可能会导致输出中重复的事件。"],["body","\n\n"],["body","选择路径指示Filebeat根据其路径识别文件"],["body","\n"],["body","如果inode和设备id可能更改，这是避免重读文件的快速方法。"],["body","\n"],["body","但是，请记住，如果文件被旋转 (重命名)，它们将被重新读取并重新提交。"],["body","\n"],["body","如果inode保持不变，即使设备id已更改，也可以使用inode_marker选项，如果可能的话，您应该选择此方法，如果您的文件是轮转的，您必须配置一个可由Filebeat读取的标记文件，并在inode_marker的选项路径中设置路径。此文件的内容必须是设备唯一的，您可以将设备或挂载点的UUID放在存储输入的位置。下面的示例oneliner为选定的挂载点 '/logs' 生成一个隐藏的标记文件.请注意，您不应该在Windows上使用此选项，因为文件标识符可能会更加不稳定。"],["body","\n\n"],["body","$ lsblk -o MOUNTPOINT,UUID | grep /logs | awk '{print $2}' >> /logs/.filebeat-marker\n"],["body","\n"],["body","要将生成的文件设置为file_identity的标记，您应该通过以下方式配置输入:"],["body","\n"],["body","filebeat.inputs:\n- type: filestream\n  paths:\n    - /logs/*.log\n  file_identity.inode_marker.path: /logs/.filebeat-marker\n"],["body","\n"],["headingLink","reading-from-rotating-logs"],["heading","Reading from rotating logs"],["body","\n"],["body","When dealing with file rotation, avoid harvesting symlinks. Instead use the paths setting to point to the original file, and specify a pattern that matches the file you want to harvest and all of its rotated files. Also make sure your log rotation strategy prevents lost or duplicate messages. For more information, see Log rotation results in lost or duplicate events."],["body","\n"],["body","Furthermore, to avoid duplicate of rotated log messages, do not use the path method for file_identity. Or exclude the rotated files with exclude_files option."],["body","\n"],["body","此外，为避免重复旋转的日志消息，请勿将path方法用于file_identity。或者排除带有exclude_files选项的旋转文件。"],["body","\n"],["headingLink","prospector-options"],["heading","Prospector options"],["body","\n"],["body","\n"],["body","日志文件扫描程序"],["body","\n"],["body","\n"],["body","The prospector is running a file system watcher which looks for files specified in the paths option. At the moment only simple file system scanning is supported."],["body","\n"],["headingLink","paths"],["heading","paths"],["body","\n"],["body","A list of glob-based paths that will be crawled and fetched. All patterns supported by Go Glob are also supported here. For example, to fetch all files from a predefined level of subdirectories, the following pattern can be used: /var/log/*/*.log. This fetches all .log files from the subfolders of /var/log. It does not fetch log files from the /var/log folder itself. It is possible to recursively fetch all files in all subdirectories of a directory using the optional recursive_glob settings."],["body","\n"],["body","Filebeat starts a harvester for each file that it finds under the specified paths. You can specify one path per line. Each line begins with a dash (-)."],["body","\n"],["headingLink","scanner-options"],["heading","Scanner options"],["body","\n"],["body","The scanner watches the configured paths. It scans the file system periodically and returns the file system events to the Prospector."],["body","\n"],["headingLink","prospectorscannerrecursive_glob"],["heading","prospector.scanner.recursive_glob"],["body","\n"],["body","Enable expanding ** into recursive glob patterns. With this feature enabled, the rightmost ** in each path is expanded into a fixed number of glob patterns. For example: /foo/** expands to /foo, /foo/*, /foo/*/*, and so on. If enabled it expands a single ** into a 8-level deep * pattern."],["body","\n"],["body","This feature is enabled by default. Set prospector.scanner.recursive_glob to false to disable it."],["body","\n"],["headingLink","prospectorscannerexclude_files"],["heading","prospector.scanner.exclude_files"],["body","\n"],["body","A list of regular expressions to match the files that you want Filebeat to ignore. By default no files are excluded."],["body","\n"],["body","The following example configures Filebeat to ignore all the files that have a gz extension:"],["body","\n"],["body","filebeat.inputs:\n- type: filestream\n  ...\n  prospector.scanner.exclude_files: ['\\.gz$']\n"],["body","\n"],["headingLink","prospectorscannerinclude_files"],["heading","prospector.scanner.include_files"],["body","\n"],["body","By default no files are excluded. This option is the counterpart of prospector.scanner.exclude_files."],["body","\n"],["body","The following example configures Filebeat to exclude files that are not under /var/log:"],["body","\n"],["body","filebeat.inputs:\n- type: filestream\n  ...\n  prospector.scanner.include_files: ['^/var/log/.*']\n"],["body","\n"],["body","See Regular expression support for a list of supported regexp patterns."],["body","\n"],["headingLink","prospectorscannersymlinks"],["heading","prospector.scanner.symlinks"],["body","\n"],["body","The symlinks option allows Filebeat to harvest symlinks in addition to regular files. When harvesting symlinks, Filebeat opens and reads the original file even though it reports the path of the symlink."],["body","\n"],["body","符号链接选项允许Filebeat除了常规文件之外还收获符号链接。收获符号链接时，Filebeat会打开并读取原始文件，即使它报告符号链接的路径。"],["body","\n"],["body","配置收割符号链接时，请确保排除原始路径。如果同时配置 符号链接跟 原始链接，Filebeat将检测到问题并仅处理找到的第一个文件。"],["body","\n"],["body","但是，如果配置了两个不同的input (一个读取符号链接，另一个读取原始路径)，则将收集两个路径，从而导致Filebeat发送重复数据，并且输入会覆盖彼此的状态。"],["body","\n"],["body","如果指向日志文件的符号链接在文件名中具有其他元数据，并且您希望在Logstash中处理元数据，则符号链接选项可能会很有用。例如，Kubernetes日志文件就是这种情况。"],["body","\n"],["body","由于此选项可能会导致数据丢失，因此默认情况下将其禁用。"],["body","\n"],["headingLink","prospectorscannerresend_on_touch"],["heading","prospector.scanner.resend_on_touch"],["body","\n"],["body","如果启用了此选项，则如果文件的大小未更改，但其修改时间已更改为比以前晚的时间，则将重新发送文件。默认情况下它是禁用的，以避免意外重新发送文件。"],["body","\n"],["headingLink","prospectorscannercheck_interval"],["heading","prospector.scanner.check_interval"],["body","\n"],["body","Filebeat多久检查一次指定用于收割的路径中的新文件"],["body","\n"],["body","例如，当指定 了  /var/log/*,  目录使用check_interval指定的频率扫描目录中的文件。"],["body","\n"],["body","指定1s尽可能频繁地扫描目录，而不会导致Filebeat扫描太频繁,我们不建议将此值设置为 <1s。"],["body","\n"],["body","如果您需要近乎实时地发送日志行，请不要使用非常低的check_interval，而是调整close.on_state_change.inactive，以便文件处理程序保持打开状态并不断轮询文件。默认设置为10s"],["body","\n"],["headingLink","ignore_older"],["heading","ignore_older"],["body","\n"],["body","Filebeat忽略任何在指定时间跨度之前修改的文件"],["body","\n"],["body","如果您长时间保存日志文件，配置ignore_older可能特别有用"],["body","\n"],["body","例如，如果您想启动Filebeat，但只想发送上周的最新文件和文件，您可以配置此选项"],["body","\n"],["body","您可以使用时间字符串，如2h (2小时) 和5m (5分钟),默认为0，这将禁用设置,注释掉配置与将其设置为0具有相同的效果。"],["body","\n"],["body","您必须将ignore_older设置为大于close.on_state_change.inactive。"],["body","\n"],["body","受此设置影响的文件分为两类:"],["body","\n\n"],["body","从未收获的文件"],["body","\n"],["body","已收获但更新时间未超过 ignore_older 的文件"],["body","\n\n"],["body","对于以前从未见过的文件，偏移状态设置为文件末尾。如果状态已经存在，则偏移量不会更改。如果以后再次更新文件，则在设置的偏移位置继续读取。"],["body","\n"],["body","ignore_older设置依赖于文件的修改时间来确定文件是否被忽略。如果在将行写入文件时未更新文件的修改时间 (这可能发生在Windows上)，则ignore_older设置可能会导致Filebeat忽略文件，即使稍后添加了内容。"],["body","\n"],["body","要从注册表文件中删除以前收集的文件的状态，请使用clean_inactive配置选项。"],["body","\n"],["body","在文件被Filebeat忽略之前，必须关闭该文件。要确保在忽略文件时不再收集文件，必须将ignore_older设置为比close.on_state_change.inactive更长的持续时间。"],["body","\n"],["body","如果当前正在收获的文件属于ignore_older，则收割机将首先完成读取该文件，并在close.on_state_change.inactive到达之后，该文件将被忽略。"],["body","\n"],["headingLink","ignore_inactive"],["heading","ignore_inactive"],["body","\n"],["body","If this option is enabled, Filebeat ignores every file that has not been updated since the selected time."],["body","\n"],["body","如果启用了此选项，则Filebeat会忽略自选定时间以来尚未更新的每个文件"],["body","\n"],["body","Possible options are since_first_start and since_last_start."],["body","\n"],["body","可用的选项有since_first_start和since_last_start。"],["body","\n"],["body","第一个选项忽略自Filebeat第一次启动以来尚未更新的每个文件。当Filebeat可能由于配置更改或故障而重新启动时，这很有用。"],["body","\n"],["body","第二个选项告诉Filebeat从启动后更新的文件中读取。"],["body","\n"],["body","受此设置影响的文件分为两类:"],["body","\n\n"],["body","从未收割的文件"],["body","\n"],["body","自ignore_inactive以来已收割但未更新的文件。"],["body","\n\n"],["body","对于以前从未见过的文件，偏移状态设置为文件末尾。如果状态已经存在，则偏移量不会更改。如果以后再次更新文件，则在设置的偏移位置继续读取。"],["body","\n"],["body","The setting relies on the modification time of the file to determine if a file is ignored. If the modification time of the file is not updated when lines are written to a file (which can happen on Windows), the setting may cause Filebeat to ignore files even though content was added at a later time."],["body","\n"],["body","To remove the state of previously harvested files from the registry file, use the clean_inactive configuration option."],["body","\n"],["headingLink","close"],["heading","close.*"],["body","\n"],["body","close选项用于在指定时间之后 关闭 harvester ，关闭收割机意味着关闭文件处理程序。"],["body","\n"],["body","如果在收割机关闭后，文件更新了，则在 prospector.scanner.check_interval 时间经过后，该文件将再次被拾取。"],["body","\n"],["body","但是，如果在收割机关闭时移动或删除了文件，Filebeat将无法再次拾取文件，收割机未读取的任何数据都将丢失。"],["body","\n"],["body","*close.on_state_change.*设置异步的应用于从文件中读取，这意味着如果Filebeat由于输出被阻止，全队列或其他问题而处于阻止状态，则无论如何都将关闭文件。"],["body","\n"],["headingLink","closeon_state_changeinactive"],["heading","close.on_state_change.inactive"],["body","\n"],["body","When this option is enabled, 如果在指定的持续时间内未采集文件，Filebeat会关闭文件句柄. "],["body","\n"],["body","定义周期的计数器从收割机读取最后一个日志行开始。它不是基于文件的修改时间，如果关闭的文件再次更改，则将启动新的收割机，并且在 prospector.scanner.check_interval 时间间隔之后将再次被拾取"],["body","\n"],["body","我们建议您将close.on_state_change.inactive设置为大于  日志文件最不频繁更新的值"],["body","\n"],["body","例如，如果您的日志文件每隔几秒钟更新一次，则可以安全地将close.on_state_change.inactive设置为1m。"],["body","\n"],["body","如果存在具有非常不同的更新速率的日志文件，则可以使用具有不同值的多个配置。"],["body","\n"],["body","将close.on_state_change.inactive设置为较低的值意味着文件句柄更快关闭。"],["body","\n"],["body","但是，这有一个副作用，即如果收割机关闭，则不会近乎实时地发送新的日志行。"],["body","\n"],["body","关闭文件的时间戳不取决于文件的修改时间。相反，Filebeat使用内部时间戳，该时间戳反映了上次收获文件的时间。"],["body","\n"],["body","例如，如果将close.on_state_change.inactive设置为5分钟，则5分钟的倒计时将在收割机读取文件的最后一行后开始"],["body","\n"],["body","You can use time strings like 2h (2 hours) and 5m (5 minutes). The default is 5m."],["body","\n"],["headingLink","closeon_state_changerenamed"],["heading","close.on_state_change.renamed"],["body","\n"],["body","Only use this option if you understand that data loss is a potential side effect."],["body","\n"],["body","仅当您了解数据丢失是潜在的副作用时，才使用此选项。"],["body","\n"],["body","启用此选项后，文件重命名后，Filebeat会关闭文件处理程序。"],["body","\n"],["body","例如，在轮转文件时会发生这种情况。默认情况下，收割机保持打开状态并继续读取文件，因为文件处理程序不依赖于文件名。"],["body","\n"],["body","如果启用了 close.on_state_change.renamed 选项，重命名或者移动之后，不再与指定的文件模式匹配，，则不会再次拾取该文件。Filebeat不会读取完文件。"],["body","\n"],["body","配置基于路径的file_identity时，请勿使用此选项。启用该选项没有意义，因为Filebeat无法使用路径名作为唯一标识符来检测重命名。"],["body","\n"],["body","WINDOWS: 如果您的Windows日志轮换系统由于无法轮换文件而显示错误，则应启用此选项。"],["body","\n"],["headingLink","closeon_state_changeremoved"],["heading","close.on_state_change.removed"],["body","\n"],["body","通常，只有在 close.on_state_change.inactive 指定的持续时间内 inactive，文件才应被删除。"],["body","\n"],["body","但是，如果文件被提前删除，并且您没有启用close.on_state_change.removed，Filebeat将文件保持打开状态，以确保收割机已完成。"],["body","\n"],["body","如果此设置导致文件由于过早从磁盘中删除而无法完全读取，请禁用此选项。"],["body","\n"],["body","If this setting results in files that are not completely read because they are removed from disk too early, disable this option."],["body","\n"],["body","默认情况下启用此选项。如果禁用此选项，则还必须禁用clean.on_state_change.removed。"],["body","\n"],["body","WINDOWS: 如果您的Windows日志轮换系统由于无法轮换文件而显示错误，请确保启用此选项。"],["body","\n"],["headingLink","closereaderon_eof"],["heading","close.reader.on_eof"],["body","\n"],["body","仅当您了解数据丢失是潜在的副作用时，才使用此选项。"],["body","\n"],["body","启用此选项后，Filebeat会在到达文件末尾后立即关闭文件。当您的文件仅写入一次而不不时更新时，这很有用。例如，当您将每个日志事件写入新文件时，就会发生这种情况。默认情况下，此选项被禁用。"],["body","\n"],["headingLink","closereaderafter_interval"],["heading","close.reader.after_interval"],["body","\n"],["body","仅当您了解数据丢失是潜在的副作用时，才使用此选项。另一个副作用是，在超时到期之前，可能不会完全发送多行事件。"],["body","\n"],["body","启用此选项后，Filebeat会为每个收割机提供预定义的寿命。"],["body","\n"],["body","无论reader在文件中的位置如何，在close.reader.after_interval时间段过去后，读取都会停止"],["body","\n"],["body","当您只想在文件上花费预定义的时间时，此选项对于较旧的日志文件很有用。"],["body","\n"],["body","当close.reader.after_interval将在预定义的超时后关闭文件，如果文件仍在更新，Filebeat将根据定义的探矿者 prospector.scanner.check_interval 再次启动新的收割机。此收割机的close.reader.after_interval倒计时再次开始。"],["body","\n"],["body","此选项在输出被阻止的情况下特别有用，这使得Filebeat即使对于从磁盘中删除的文件也保持打开的文件处理程序。将close.reader.after_interval设置为5m可确保定期关闭文件，以便操作系统可以将其释放。"],["body","\n"],["body","If you set close.reader.after_interval to equal ignore_older, the file will not be picked up if it’s modified while the harvester is closed. "],["body","\n"],["body","这种设置的组合通常会导致数据丢失，并且不发送完整的文件。"],["body","\n"],["body","当您对包含多行事件的日志使用close.reader.after_interval时，收割机可能会在多行事件的中间停止，这意味着只发送部分事件。如果收割机再次启动并且文件仍然存在，则仅发送事件的第二部分。默认情况下，此选项设置为0，这意味着它被禁用。"],["body","\n"],["headingLink","clean_"],["heading","clean_*"],["body","\n"],["body","clean 选项用于清理注册表文件中的状态条目。这些设置有助于减小注册表文件的大小，并可以防止潜在的inode重用问题。 inode reuse issue."],["body","\n"],["headingLink","clean_inactive"],["heading","clean_inactive"],["body","\n"],["body","仅当您了解数据丢失是潜在的副作用时，才使用此选项。"],["body","\n"],["body","启用此选项后，Filebeat会在指定的不活动时间过去后删除文件的状态。"],["body","\n"],["body","只有当文件已经被Filebeat忽略 (文件比ignore_older更旧) 时，才能删除状态。clean_inactive设置必须大于ignore_older prospector.scanner.check_interval，以确保在仍在收集文件时未删除任何状态。否则，该设置可能会导致Filebeat不断重新发送完整内容，因为clean_inactive会删除仍由Filebeat检测到的文件的状态。如果文件更新或再次出现，则从头开始读取文件。"],["body","\n"],["body","clean_inactive配置选项对于减小注册表文件的大小很有用，尤其是在每天生成大量新文件的情况下。"],["body","\n"],["body","此配置选项对于防止Linux上inode重用导致的Filebeat问题也很有用,For more information, see Inode reuse causes Filebeat to skip lines."],["body","\n"],["body","每次重命名文件时，都会更新文件状态，并且clean_inactive的计数器再次从0开始。"],["body","\n"],["body","在测试期间，您可能会注意到注册表包含应基于clean_inactive设置删除的状态条目。发生这种情况是因为Filebeat在再次打开注册表以读取其他文件之前不会删除条目。如果您正在测试clean_inactive设置，请确保将Filebeat配置为从多个文件中读取，否则文件状态将永远不会从注册表中删除。"],["body","\n"],["headingLink","clean_removed"],["heading","clean_removed"],["body","\n"],["body","启用此选项后，如果无法在磁盘上，以最后一个已知名称，找到文件，则Filebeat会从注册表中清除文件。这也意味着在收割机完成后重命名的文件将被删除。默认情况下启用此选项。"],["body","\n"],["body","如果共享驱动器在短时间内消失并再次出现，则所有文件将从头开始再次读取，因为状态已从注册表文件中删除。在这种情况下，我们建议您禁用clean_removed选项。"],["body","\n"],["body","如果共享驱动器在短时间内消失并再次出现，则所有文件将从头开始再次读取，因为状态已从注册表文件中删除。在这种情况下，我们建议您禁用clean_removed选项。"],["body","\n"],["body","如果您还禁用了close_removed，则必须禁用此选项。"],["body","\n"],["headingLink","backoff"],["heading","backoff.*"],["body","\n"],["body","backoff指定Filebeat如何积极地抓取打开文件以进行更新。在大多数情况下，您可以使用默认值。"],["body","\n"],["headingLink","backoffinit"],["heading","backoff.init"],["body","\n"],["body","The backoff.init option defines how long Filebeat waits for the first time before checking a file again after EOF is reached. The backoff intervals increase exponentially. The default is 2s. Thus, the file is checked after 2 seconds, then 4 seconds, then 8 seconds and so on until it reaches the limit defined in backoff.max. Every time a new line appears in the file, the backoff.init value is reset to the initial value."],["body","\n"],["body","backoff.init选项定义Filebeat在到达EOF后再次检查文件之前第一次等待多长时间。退避间隔呈指数增长。默认值为2s。因此，在2秒，然后4秒，然后8秒等之后检查文件，直到达到在backoff.max中定义的限制。每次文件中出现新行时，backoff.init值都会重置为初始值。"],["body","\n"],["headingLink","backoffmax"],["heading","backoff.max"],["body","\n"],["body","达到EOF后，Filebeat在再次检查文件之前等待的最长时间。多次从检查文件中退出后，等待时间将永远不会超过backoff.max。因为读取新行最多需要10s，所以为backoff.max指定10s意味着，在最坏的情况下，如果Filebeat多次退避，则可以将新行添加到日志文件中。默认为10s。"],["body","\n"],["body","要求: (backoff.init <= backoff.max <= prospector.scanner.check_interval). If backoff.max needs to be higher, it is recommended to close the file handler instead and let Filebeat pick up the file again."],["body","\n"],["headingLink","file_identity"],["heading","file_identity"],["body","\n"],["body","Different file_identity methods can be configured to suit the environment where you are collecting log messages."],["body","\n"],["body","可以配置不同的file_identity方法以适合您收集日志消息的环境。"],["body","\n"],["body","Changing file_identity methods between runs may result in duplicated events in the output."],["body","\n"],["body","native"],["body","\n"],["body","The default behaviour of Filebeat is to differentiate between files using their inodes and device ids."],["body","\n"],["body","file_identity.native: ~\n"],["body","\n"],["body","path"],["body","\n"],["body","要根据文件的路径识别文件，请使用此策略。"],["body","\n"],["body","仅当您的日志文件轮转到超出输入范围或根本不轮转的文件夹时，才使用此策略。否则你最终会出现重复的事件。\n此策略不支持重命名文件。如果输入文件被重命名，如果新路径与输入的设置匹配，Filebeat将再次读取它。"],["body","\n"],["body","file_identity.path: ~\n"],["body","\n"],["body","inode_marker"],["body","\n"],["body","如果设备id不时更改，则必须使用此方法区分文件。Windows不支持此选项。"],["body","\n"],["body","Set the location of the marker file the following way:"],["body","\n"],["body","file_identity.inode_marker.path: /logs/.filebeat-marker\n"],["body","\n"],["headingLink","log-rotation"],["heading","Log rotation"],["body","\n"],["body","由于日志文件不断写入，因此必须rotate并清除它们，以防止记录器应用程序填满磁盘。旋转是由外部应用程序完成的，因此，Filebeat需要如何与之合作的信息。"],["body","\n"],["body","从rotate文件中读取时，请确保路径配置包括活动文件和所有rotated文件。"],["body","\n"],["body","默认情况下，Filebeat能够通过以下策略正确跟踪文件:"],["body","\n\n"],["body","创建: rotation时，创建新的活动文件名称 "],["body","\n"],["body","重命名: 旋转文件已重命名"],["body","\n\n"],["body","但是，在copytruncate策略的情况下，您应该为Filebeat提供其他配置。"],["body","\n"],["body","rotation.external.strategy.copytruncate"],["body","\n"],["body","This functionality is in technical preview and may be changed or removed in a future release. Elastic will apply best effort to fix any issues, but features in technical preview are not subject to the support SLA of official GA features."],["body","\n"],["body","If the log rotating application copies the contents of the active file and then truncates the original file, use these options to help Filebeat to read files correctly."],["body","\n"],["body","Set the option suffix_regex so Filebeat can tell active and rotated files apart. There are two supported suffix types in the input: numberic and date."],["body","\n"],["headingLink","numeric-suffix"],["heading","Numeric suffix"],["body","\n"],["body","If your rotated files have an incrementing index appended to the end of the filename, e.g. active file apache.log and the rotated files are named apache.log.1, apache.log.2, etc, use the following configuration."],["body","\n"],["body","---\nrotation.external.strategy.copytruncate:\n  suffix_regex: \\.\\d$\n---\n"],["body","\n"],["headingLink","date-suffix"],["heading","Date suffix"],["body","\n"],["body","If the rotation date is appended to the end of the filename, e.g. active file apache.log and the rotated files are named apache.log-20210526, apache.log-20210527, etc. use the following configuration:"],["body","\n"],["body","---\nrotation.external.strategy.copytruncate:\n  suffix_regex: \\-\\d{6}$\n  dateformat: -20060102\n---\n"],["body","\n"],["headingLink","encoding"],["heading","encoding"],["body","\n"],["body","The file encoding to use for reading data that contains international characters. See the encoding names recommended by the W3C for use in HTML5."],["body","\n"],["headingLink","exclude_lines"],["heading","exclude_lines"],["body","\n"],["body","正则表达式列表，以匹配您希望Filebeat排除的行。Filebeat删除列表中与正则表达式匹配的任何行。默认情况下，不会丢弃任何行。空行被忽略。"],["body","\n"],["body","The following example configures Filebeat to drop any lines that start with DBG."],["body","\n"],["body","filebeat.inputs:\n- type: filestream\n  ...\n  exclude_lines: ['^DBG']\n"],["body","\n"],["body","See Regular expression support for a list of supported regexp patterns."],["body","\n"],["headingLink","include_lines"],["heading","include_lines"],["body","\n"],["body","A list of regular expressions to match the lines that you want Filebeat to include. Filebeat exports only the lines that match a regular expression in the list. By default, all lines are exported. Empty lines are ignored."],["body","\n"],["body","正则表达式列表，以匹配您希望Filebeat包含的行。Filebeat仅导出与列表中的正则表达式匹配的行。默认情况下，所有行都被导出。空行被忽略。"],["body","\n"],["body","The following example configures Filebeat to export any lines that start with ERR or WARN:"],["body","\n"],["body","filebeat.inputs:\n- type: filestream\n  ...\n  include_lines: ['^ERR', '^WARN']\n"],["body","\n"],["body","如果同时定义了include_lines和exclude_lines，则Filebeat首先执行include_lines，然后执行exclude_lines。两个选项的定义顺序无关紧要。include_lines选项将始终在exclude_lines选项之前执行，即使exclude_lines出现在config文件中的include_lines之前。"],["body","\n"],["body","The following example exports all log lines that contain sometext, except for lines that begin with DBG (debug messages):"],["body","\n"],["body","filebeat.inputs:\n- type: filestream\n  ...\n  include_lines: ['sometext']\n  exclude_lines: ['^DBG']\n"],["body","\n"],["headingLink","buffer_size"],["heading","buffer_size"],["body","\n"],["body","The size in bytes of the buffer that each harvester uses when fetching a file. The default is 16384."],["body","\n"],["body","每个收割机在获取文件时使用的缓冲区的大小 (以字节为单位)。默认值为16384。"],["body","\n"],["headingLink","message_max_bytes"],["heading","message_max_bytes"],["body","\n"],["body","单个日志消息可以具有的最大字节数。mesage_max_bytes之后的所有字节都将被丢弃并且不发送。默认值为10mb (10485760)。"],["body","\n"],["headingLink","parsers"],["heading","parsers"],["body","\n"],["body","此选项期望日志行必须经过的解析器列表。"],["body","\n"],["body","Available parsers:"],["body","\n\n"],["body","multiline"],["body","\n"],["body","ndjson"],["body","\n"],["body","container"],["body","\n\n"],["body","在此示例中，Filebeat正在读取由3行组成并封装在单行JSON对象中的多行消息。多行消息存储在密钥msg下。"],["body","\n"],["body","filebeat.inputs:\n- type: filestream\n  ...\n  parsers:\n    - ndjson:\n        keys_under_root: true\n        message_key: msg\n    - multiline:\n        type: counter\n        lines_count: 3\n"],["body","\n"],["body","See the available parser settings in detail below."],["body","\n"],["headingLink","multiline"],["heading","multiline"],["body","\n"],["body","控制Filebeat如何处理跨越多行的日志消息的选项。有关配置多行选项的更多信息， See Multiline messages for more information"],["body","\n"],["headingLink","ndjson"],["heading","ndjson"],["body","\n"],["body","这些选项使Filebeat可以解码结构化为JSON消息的日志。Filebeat逐条处理日志，因此仅当每个消息有一个JSON对象时，JSON解码才起作用。"],["body","\n"],["body","解码发生在行过滤之前。如果您设置了message_key选项，则可以将JSON解码与过滤相结合。这在应用程序日志包装在JSON对象中的情况下会很有帮助，比如使用Docker时。"],["body","\n"],["body","Example configuration:"],["body","\n"],["body","- ndjson:\n    keys_under_root: true\n    add_error_key: true\n    message_key: log\n"],["body","\n"],["body","keys_under_root"],["body","\n"],["body","默认情况下，解码后的JSON放在输出文档中的 “json” 键下。如果启用此设置，则将在输出文档中复制键。默认值为false。"],["body","\n"],["body","overwrite_keys"],["body","\n"],["body","If keys_under_root and this setting are enabled, then the values from the decoded JSON object overwrite the fields that Filebeat normally adds (type, source, offset, etc.) in case of conflicts."],["body","\n"],["body","expand_keys"],["body","\n"],["body","If this setting is enabled, Filebeat will recursively de-dot keys in the decoded JSON, and expand them into a hierarchical object structure. For example, {\"a.b.c\": 123} would be expanded into {\"a\":{\"b\":{\"c\":123}}}. This setting should be enabled when the input is produced by an ECS logger."],["body","\n"],["body","add_error_key"],["body","\n"],["body","If this setting is enabled, Filebeat adds an \"error.message\" and \"error.type: json\" key in case of JSON unmarshalling errors or when a message_key is defined in the configuration but cannot be used."],["body","\n"],["body","message_key"],["body","\n"],["body","An optional configuration setting that specifies a JSON key on which to apply the line filtering and multiline settings. If specified the key must be at the top level in the JSON object and the value associated with the key must be a string, otherwise no filtering or multiline aggregation will occur."],["body","\n"],["body","一个可选的配置设置，它指定要在其上应用行过滤和多行设置的JSON键。如果指定了键，则键必须位于JSON对象的顶层，并且与键关联的值必须是字符串，否则将不会发生过滤或多行聚合。"],["body","\n"],["body","document_id"],["body","\n"],["body","Option configuration setting that specifies the JSON key to set the document id. If configured, the field will be removed from the original JSON document and stored in @metadata._id"],["body","\n"],["body","ignore_decoding_error"],["body","\n"],["body","An optional configuration setting that specifies if JSON decoding errors should be logged or not. If set to true, errors will not be logged. The default is false."],["body","\n"],["headingLink","container"],["heading","container"],["body","\n"],["body","Use the container parser to extract information from containers log files. It parses lines into common message lines, extracting timestamps too."],["body","\n\n"],["body","\n"],["body","stream"],["body","\n"],["body","Reads from the specified streams only: all, stdout or stderr. The default is all."],["body","\n"],["body","\n"],["body","\n"],["body","format"],["body","\n"],["body","Use the given format when parsing logs: auto, docker or cri. The default is auto, it will automatically detect the format. To disable autodetection set any of the other options."],["body","\n"],["body","\n\n"],["body","The following snippet configures Filebeat to read the stdout stream from all containers under the default Kubernetes logs path:"],["body","\n"],["body","  paths:\n    - \"/var/log/containers/*.log\"\n  parsers:\n    - container:\n        stream: stdout\n"],["body","\n"],["headingLink","common-options"],["heading","Common options"],["body","\n"],["body","所有输入都支持以下配置选项。"],["body","\n"],["headingLink","enabled"],["heading","enabled"],["body","\n"],["body","使用启用选项启用和禁用输入。默认情况下，enabled设置为true。"],["body","\n"],["headingLink","tags"],["heading","tags"],["body","\n"],["body","Filebeat包含在每个已发布事件的 “标签” 字段中的标签列表。标签使在Kibana中选择特定事件或在Logstash中应用条件过滤变得容易。这些标签将附加到常规配置中指定的标签列表中。"],["body","\n"],["body","Example:"],["body","\n"],["body","filebeat.inputs:\n- type: filestream\n  . . .\n  tags: [\"json\"]\n"],["body","\n"],["headingLink","fields"],["heading","fields"],["body","\n"],["body","您可以指定用于向输出添加其他信息的可选字段。例如，您可以添加可用于过滤日志数据的字段"],["body","\n"],["body","字段可以是标量值、数组、字典或这些的任何嵌套组合。"],["body","\n"],["body","默认情况下，您在此处指定的字段将在输出文档中的 fields子字典下分组。"],["body","\n"],["body","将自定义字段存储为顶级字段，将fields_under_root选项设置为true。如果在常规配置中声明了重复字段，则其值将被此处声明的值覆盖。"],["body","\n"],["body","filebeat.inputs:\n- type: filestream\n  . . .\n  fields:\n    app_id: query_engine_12\n"],["body","\n"],["headingLink","fields_under_root"],["heading","fields_under_root"],["body","\n"],["body","如果将此选项设置为true，则自定义字段将存储为输出文档中的顶级字段，而不是在fileds 子字典下分组。如果自定义字段名称与Filebeat添加的其他字段名称冲突，则自定义字段会覆盖其他字段。"],["body","\n"],["headingLink","processors"],["heading","processors"],["body","\n"],["body","A list of processors to apply to the input data."],["body","\n"],["body","See Processors for information about specifying processors in your config."],["body","\n"],["headingLink","pipeline"],["heading","pipeline"],["body","\n"],["body","The ingest pipeline ID to set for the events generated by this input."],["body","\n"],["body","The pipeline ID can also be configured in the Elasticsearch output, but this option usually results in simpler configuration files. If the pipeline is configured both in the input and output, the option from the input is used."],["body","\n"],["headingLink","keep_null"],["heading","keep_null"],["body","\n"],["body","如果将此选项设置为true，则将在输出文档中发布具有null值的字段。默认情况下，keep_null设置为false。"],["body","\n"],["headingLink","index"],["heading","index"],["body","\n"],["body","If present, this formatted string overrides the index for events from this input (for elasticsearch outputs), or sets the raw_index field of the event’s metadata (for other outputs). This string can only refer to the agent name and version and the event timestamp; for access to dynamic fields, use output.elasticsearch.index or a processor."],["body","\n"],["body","如果存在，则此格式化字符串将覆盖此输入 (对于elasticsearch输出) 中的事件的索引，或设置事件元数据的raw_index字段 (对于其他输出)。此字符串只能引用代理名称和版本以及事件时间戳; 要访问动态字段，请使用output.elasticsearch.index或处理器。"],["body","\n"],["body","Example value: \"%{[agent.name]}-myindex-%{+yyyy.MM.dd}\" might expand to \"filebeat-myindex-2019.11.01\"."],["body","\n"],["headingLink","publisher_pipelinedisable_host"],["heading","publisher_pipeline.disable_host"],["body","\n"],["body","默认情况下，所有事件都包含host.name。可以将此选项设置为true，以禁用将此字段添加到所有事件。默认值为false。"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","6.消息队列_RocketMQ/MQ消息的投递机制.html"],["title","MQ消息的投递机制 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","前言"],["heading","前言"],["body","\n"],["body","RocketMQ的消息投递分分为两种：一种是生产者往MQ Broker中投递"],["body","\n"],["body","另外一种则是MQ broker 往消费者 投递(这种投递的说法是从消息传递的角度阐述的，实际上底层是消费者从MQ broker 中Pull拉取的)"],["body","\n"],["headingLink","rocketmq的消息模型"],["heading","RocketMQ的消息模型"],["body","\n"],["body","\n"],["body","为了提高MQ的可用性和灵活性，一个Topic在实际存储的过程中，采用了多队列的方式"],["body","\n"],["body","每个消息队列在使用中应当保证先入先出（FIFO,First In First Out）的方式进行消费"],["body","\n"],["body","那么，基于这种模型，就会引申出两个问题："],["body","\n"],["body","生产者 在发送相同Topic的消息时，消息体应当被放置到哪一个消息队列(MessageQueue)中?"],["body","\n"],["body","消费者 在消费消息时，应当从哪些消息队列中拉取消息?"],["body","\n"],["headingLink","生产者producer投递消息的策略"],["heading","生产者(Producer)投递消息的策略"],["body","\n"],["headingLink","默认投递方式基于queue队列轮询算法投递"],["heading","默认投递方式：基于Queue队列轮询算法投递"],["body","\n"],["body","默认情况下，采用了最简单的轮询算法，这种算法有个很好的特性就是，保证每一个Queue队列的消息投递数量尽可能均匀，算法如下所示："],["body","\n"],["body","/**\n*  根据 TopicPublishInfo Topic发布信息对象中维护的index，每次选择队列时，都会递增\n*  然后根据 index % queueSize 进行取余，达到轮询的效果\n*\n*/\npublic MessageQueue selectOneMessageQueue(final TopicPublishInfo tpInfo, final String lastBrokerName) {\n        return tpInfo.selectOneMessageQueue(lastBrokerName);\n}\n\n/**\n*  TopicPublishInfo Topic发布信息对象中\n*/\npublic class TopicPublishInfo {\n    //基于线程上下文的计数递增，用于轮询目的\n    private volatile ThreadLocalIndex sendWhichQueue = new ThreadLocalIndex();\n   \n\n    public MessageQueue selectOneMessageQueue(final String lastBrokerName) {\n        if (lastBrokerName == null) {\n            return selectOneMessageQueue();\n        } else {\n            int index = this.sendWhichQueue.getAndIncrement();\n            for (int i = 0; i < this.messageQueueList.size(); i++) {\n                //轮询计算\n                int pos = Math.abs(index++) % this.messageQueueList.size();\n                if (pos < 0)\n                    pos = 0;\n                MessageQueue mq = this.messageQueueList.get(pos);\n                if (!mq.getBrokerName().equals(lastBrokerName)) {\n                    return mq;\n                }\n            }\n            return selectOneMessageQueue();\n        }\n    }\n\n    public MessageQueue selectOneMessageQueue() {\n        int index = this.sendWhichQueue.getAndIncrement();\n        int pos = Math.abs(index) % this.messageQueueList.size();\n        if (pos < 0)\n            pos = 0;\n        return this.messageQueueList.get(pos);\n    }\n}\n"],["body","\n"],["headingLink","消息投递延迟最小"],["heading","消息投递延迟最小"],["body","\n"],["body","默认的投递方式比较简单，但是也暴露了一个问题，就是有些Queue队列可能由于自身数量积压等原因，可能在投递的过程比较长，对于这样的Queue队列会影响后续投递的效果。"],["body","\n"],["body","基于这种现象，RocketMQ在每发送一个MQ消息后，都会统计一下消息投递的时间延迟，根据这个时间延迟，可以知道往哪些Queue队列投递的速度快"],["body","\n"],["body","在这种场景下，会优先使用消息投递延迟最小的策略，如果没有生效，再使用Queue队列轮询的方式。"],["body","\n"],["body","public class MQFaultStrategy {\n    /**\n     * 根据 TopicPublishInfo 内部维护的index,在每次操作时，都会递增，\n     * 然后根据 index % queueList.size(),使用了轮询的基础算法\n     *\n     */\n    public MessageQueue selectOneMessageQueue(final TopicPublishInfo tpInfo, final String lastBrokerName) {\n        if (this.sendLatencyFaultEnable) {\n            try {\n                // 从queueid 为 0 开始，依次验证broker 是否有效，如果有效\n                int index = tpInfo.getSendWhichQueue().getAndIncrement();\n                for (int i = 0; i < tpInfo.getMessageQueueList().size(); i++) {\n                    //基于index和队列数量取余，确定位置\n                    int pos = Math.abs(index++) % tpInfo.getMessageQueueList().size();\n                    if (pos < 0)\n                        pos = 0;\n                    MessageQueue mq = tpInfo.getMessageQueueList().get(pos);\n                    if (latencyFaultTolerance.isAvailable(mq.getBrokerName())) {\n                        if (null == lastBrokerName || mq.getBrokerName().equals(lastBrokerName))\n                            return mq;\n                    }\n                }\n                \n                // 从延迟容错broker列表中挑选一个容错性最好的一个 broker\n                final String notBestBroker = latencyFaultTolerance.pickOneAtLeast();\n                int writeQueueNums = tpInfo.getQueueIdByBroker(notBestBroker);\n                if (writeQueueNums > 0) {\n                     // 取余挑选其中一个队列\n                    final MessageQueue mq = tpInfo.selectOneMessageQueue();\n                    if (notBestBroker != null) {\n                        mq.setBrokerName(notBestBroker);\n                        mq.setQueueId(tpInfo.getSendWhichQueue().getAndIncrement() % writeQueueNums);\n                    }\n                    return mq;\n                } else {\n                    latencyFaultTolerance.remove(notBestBroker);\n                }\n            } catch (Exception e) {\n                log.error(\"Error occurred when selecting message queue\", e);\n            }\n          // 取余挑选其中一个队列\n            return tpInfo.selectOneMessageQueue();\n        }\n\n        return tpInfo.selectOneMessageQueue(lastBrokerName);\n    }\n}\n"],["body","\n"],["headingLink","顺序消息的投递方式"],["heading","顺序消息的投递方式"],["body","\n"],["body","上述两种投递方式属于对消息投递的时序性没有要求的场景，这种投递的速度和效率比较高。而在有些场景下，需要保证同类型消息投递和消费的顺序性。"],["body","\n"],["body","这种情况下，我们希望消费者消费消息的顺序和我们发送是一致的，然而，有上述MQ的投递和消费机制，我们无法保证顺序是正确的，对于顺序异常的消息，消费者 即使有一定的状态容错，也不能完全处理好这么多种随机出现组合情况。\n"],["body","\n"],["body","基于上述的情况，RockeMQ采用了这种实现方案：对于相同订单号的消息，通过一定的策略，将其放置在一个 queue队列中，然后消费者再采用一定的策略(一个线程独立处理一个queue,保证处理消息的顺序性)，能够保证消费的顺序性"],["body","\n"],["headingLink","消息投递队列选择"],["heading","消息投递队列选择"],["body","\n"],["body","生产者是如何能将相同订单号的消息发送到同一个queue队列的："],["body","\n"],["body","默认实现："],["body","\n"],["body","投递策略"],["body","策略实现类"],["body","说明"],["body","\n"],["body","随机分配策略"],["body","SelectMessageQueueByRandom"],["body","使用了简单的随机数选择算法"],["body","\n"],["body","基于Hash分配策略"],["body","SelectMessageQueueByHash"],["body","根据附加参数的Hash值，按照消息队列列表的大小取余数，得到消息队列的index"],["body","\n"],["body","基于机器机房位置分配策略"],["body","SelectMessageQueueByMachineRoom"],["body","开源的版本没有具体的实现，基本的目的应该是机器的就近原则分配"],["body","\n\n\n"],["headingLink","如何为消费者分配queue队列"],["heading","如何为消费者分配queue队列"],["body","\n"],["body","RocketMQ对于消费者消费消息有两种形式："],["body","\n\n"],["body","BROADCASTING:广播式消费，这种模式下，一个消息会被通知到每一个消费者"],["body","\n"],["body","CLUSTERING: 集群式消费，这种模式下，一个消息最多只会被投递到一个消费者上进行消费 模式如下："],["body","\n\n"],["headingLink","基于queue队列的分配"],["heading","基于queue队列的分配"],["body","\n"],["body","在RoketMQ底层，消息指定分配给消费者的实现，是通过queue队列分配给消费者的方式完成的：也就是说，消息分配的单位是消息所在的queue队列"],["body","\n"],["body","将queue队列指定给特定的消费者后，queue队列内的所有消息将会被指定到消费者进行消费。"],["body","\n"],["body","RocketMQ定义了策略接口AllocateMessageQueueStrategy，对于给定的消费者分组,和消息队列列表、消费者列表，当前消费者应当被分配到哪些queue队列"],["body","\n"],["body","/**\n * 为消费者分配queue的策略算法接口\n */\npublic interface AllocateMessageQueueStrategy {\n\n    /**\n     * Allocating by consumer id\n     *\n     * @param consumerGroup 当前 consumer群组\n     * @param currentCID 当前consumer id\n     * @param mqAll 当前topic的所有queue实例引用\n     * @param cidAll 当前 consumer群组下所有的consumer id set集合\n     * @return 根据策略给当前consumer分配的queue列表\n     */\n    List<MessageQueue> allocate(\n        final String consumerGroup,\n        final String currentCID,\n        final List<MessageQueue> mqAll,\n        final List<String> cidAll\n    );\n\n    /**\n     * 算法名称\n     *\n     * @return The strategy name\n     */\n    String getName();\n}\n"],["body","\n"],["headingLink","消费队列分配算法"],["heading","消费队列分配算法"],["body","\n"],["body","算法名称"],["body","含义"],["body","\n"],["body","AllocateMessageQueueAveragely"],["body","平均分配算法"],["body","\n"],["body","AllocateMessageQueueAveragelyByCircle"],["body","基于环形平均分配算法"],["body","\n"],["body","AllocateMachineRoomNearby"],["body","基于机房临近原则算法"],["body","\n"],["body","AllocateMessageQueueByMachineRoom"],["body","基于机房分配算法"],["body","\n"],["body","AllocateMessageQueueConsistentHash"],["body","基于一致性hash算法"],["body","\n"],["body","AllocateMessageQueueByConfig"],["body","基于配置分配算法"],["body","\n\n\n"],["headingLink","顺序-平均分配算法"],["heading","顺序-平均分配算法"],["body","\n"],["body","\n"],["body","源码"],["body","\n"],["body","package org.apache.rocketmq.client.consumer.rebalance;\nimport java.util.ArrayList;\nimport java.util.List;\nimport org.apache.rocketmq.client.consumer.AllocateMessageQueueStrategy;\nimport org.apache.rocketmq.client.log.ClientLogger;\nimport org.apache.rocketmq.logging.InternalLogger;\nimport org.apache.rocketmq.common.message.MessageQueue;\n/**\n * Average Hashing queue algorithm\n */\npublic class AllocateMessageQueueAveragely implements AllocateMessageQueueStrategy {\n    private final InternalLogger log = ClientLogger.getLog();\n\n    @Override\n    public List<MessageQueue> allocate(String consumerGroup, String currentCID, List<MessageQueue> mqAll,\n        List<String> cidAll) {\n        if (currentCID == null || currentCID.length() < 1) {\n            throw new IllegalArgumentException(\"currentCID is empty\");\n        }\n        if (mqAll == null || mqAll.isEmpty()) {\n            throw new IllegalArgumentException(\"mqAll is null or mqAll empty\");\n        }\n        if (cidAll == null || cidAll.isEmpty()) {\n            throw new IllegalArgumentException(\"cidAll is null or cidAll empty\");\n        }\n\n        List<MessageQueue> result = new ArrayList<MessageQueue>();\n        if (!cidAll.contains(currentCID)) {\n            log.info(\"[BUG] ConsumerGroup: {} The consumerId: {} not in cidAll: {}\",\n                consumerGroup,\n                currentCID,\n                cidAll);\n            return result;\n        }\n\t\t\t\t//当前消费者\n        int index = cidAll.indexOf(currentCID);\n        //获取多余的消费队列\n        int mod = mqAll.size() % cidAll.size();\n        //计算 当前消费者 应该所得的 消费队列的 个数\n        int averageSize =\n            mqAll.size() <= cidAll.size() ? 1 : (mod > 0 && index < mod ? mqAll.size() / cidAll.size()\n                + 1 : mqAll.size() / cidAll.size());\n        //计算 当前消费者所位于 消费队列 列表中的起点\n        int startIndex = (mod > 0 && index < mod) ? index * averageSize : index * averageSize + mod;\n        //如果 消费者过多，mqAll.size() - startIndex可能会 为负数，这就可能导致 后续的消费者无法获取到队列\n        int range = Math.min(averageSize, mqAll.size() - startIndex);\n        for (int i = 0; i < range; i++) {\n            result.add(mqAll.get((startIndex + i) % mqAll.size()));\n        }\n        return result;\n    }\n\n    @Override\n    public String getName() {\n        return \"AVG\";\n    }\n}\n"],["body","\n"],["headingLink","环形平均算法"],["heading","环形平均算法"],["body","\n"],["body","\n"],["body","环形顺序分配"],["body","\n"],["body","这种算法最终分配的结果是： consumer-1: #0，#4，#8 consumer-2: #1,  #5, # 9 consumer-3: #2，#6 consumer-4: #3，#7"],["body","\n"],["body","package org.apache.rocketmq.client.consumer.rebalance;\nimport java.util.ArrayList;\nimport java.util.List;\nimport org.apache.rocketmq.client.consumer.AllocateMessageQueueStrategy;\nimport org.apache.rocketmq.client.log.ClientLogger;\nimport org.apache.rocketmq.logging.InternalLogger;\nimport org.apache.rocketmq.common.message.MessageQueue;\n\n/**\n * Cycle average Hashing queue algorithm\n */\npublic class AllocateMessageQueueAveragelyByCircle implements AllocateMessageQueueStrategy {\n    private final InternalLogger log = ClientLogger.getLog();\n\n    @Override\n    public List<MessageQueue> allocate(String consumerGroup, String currentCID, List<MessageQueue> mqAll,\n        List<String> cidAll) {\n     \t\t......\t\n        int index = cidAll.indexOf(currentCID);\n        for (int i = index; i < mqAll.size(); i++) {\n            if (i % cidAll.size() == index) {\n                result.add(mqAll.get(i));\n            }\n        }\n        return result;\n    }\n\n    @Override\n    public String getName() {\n        return \"AVG_BY_CIRCLE\";\n    }\n}\n"],["body","\n"],["headingLink","基于机房临近原则算法"],["heading","基于机房临近原则算法"],["body","\n"],["body","\n"],["body","对于跨机房的场景，会存在网络、稳定性和隔离心的原因，该算法会根据queue的部署机房位置和消费者consumer的位置，过滤出当前消费者consumer相同机房的queue队列，然后再结合上述的算法，如基于平均分配算法在queue队列子集的基础上再挑选。相关代码实现如下："],["body","\n"],["body","package org.apache.rocketmq.client.consumer.rebalance;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.TreeMap;\nimport org.apache.commons.lang3.StringUtils;\nimport org.apache.rocketmq.client.consumer.AllocateMessageQueueStrategy;\nimport org.apache.rocketmq.client.log.ClientLogger;\nimport org.apache.rocketmq.common.message.MessageQueue;\nimport org.apache.rocketmq.logging.InternalLogger;\n\n/**\n * An allocate strategy proxy for based on machine room nearside priority. An actual allocate strategy can be\n * specified.\n *\n * If any consumer is alive in a machine room, the message queue of the broker which is deployed in the same machine\n * should only be allocated to those. Otherwise, those message queues can be shared along all consumers since there are\n * no alive consumer to monopolize them.\n */\npublic class AllocateMachineRoomNearby implements AllocateMessageQueueStrategy {\n    private final InternalLogger log = ClientLogger.getLog();\n\n    private final AllocateMessageQueueStrategy allocateMessageQueueStrategy;//actual allocate strategy\n    private final MachineRoomResolver machineRoomResolver;\n\n    public AllocateMachineRoomNearby(AllocateMessageQueueStrategy allocateMessageQueueStrategy,\n        MachineRoomResolver machineRoomResolver) throws NullPointerException {\n        if (allocateMessageQueueStrategy == null) {\n            throw new NullPointerException(\"allocateMessageQueueStrategy is null\");\n        }\n\n        if (machineRoomResolver == null) {\n            throw new NullPointerException(\"machineRoomResolver is null\");\n        }\n\n        this.allocateMessageQueueStrategy = allocateMessageQueueStrategy;\n        this.machineRoomResolver = machineRoomResolver;\n    }\n\n    @Override\n    public List<MessageQueue> allocate(String consumerGroup, String currentCID, List<MessageQueue> mqAll,\n        List<String> cidAll) {\n        ......\n        //group mq by machine room: broker部署机房\n        Map<String/*machine room */, List<MessageQueue>> mr2Mq = new TreeMap<String, List<MessageQueue>>();\n        for (MessageQueue mq : mqAll) {\n            String brokerMachineRoom = machineRoomResolver.brokerDeployIn(mq);\n            if (StringUtils.isNoneEmpty(brokerMachineRoom)) {\n                if (mr2Mq.get(brokerMachineRoom) == null) {\n                    mr2Mq.put(brokerMachineRoom, new ArrayList<MessageQueue>());\n                }\n                mr2Mq.get(brokerMachineRoom).add(mq);\n            } else {\n                throw new IllegalArgumentException(\"Machine room is null for mq \" + mq);\n            }\n        }\n\n        //group consumer by machine room，consumer部署机房\n        Map<String/*machine room */, List<String/*clientId*/>> mr2c = new TreeMap<String, List<String>>();\n        for (String cid : cidAll) {\n            String consumerMachineRoom = machineRoomResolver.consumerDeployIn(cid);\n            if (StringUtils.isNoneEmpty(consumerMachineRoom)) {\n                if (mr2c.get(consumerMachineRoom) == null) {\n                    mr2c.put(consumerMachineRoom, new ArrayList<String>());\n                }\n                mr2c.get(consumerMachineRoom).add(cid);\n            } else {\n                throw new IllegalArgumentException(\"Machine room is null for consumer id \" + cid);\n            }\n        }\n\n        List<MessageQueue> allocateResults = new ArrayList<MessageQueue>();\n\n        //1.allocate the mq that deploy in the same machine room with the current consumer\n        String currentMachineRoom = machineRoomResolver.consumerDeployIn(currentCID);\n        List<MessageQueue> mqInThisMachineRoom = mr2Mq.remove(currentMachineRoom);\n        List<String> consumerInThisMachineRoom = mr2c.get(currentMachineRoom);\n      \t//当前存在 borker 与 consumer部署在同一机房，则使用其余分配算法继续分配\n        if (mqInThisMachineRoom != null && !mqInThisMachineRoom.isEmpty()) {\n            allocateResults.addAll(allocateMessageQueueStrategy.allocate(consumerGroup, currentCID, mqInThisMachineRoom, consumerInThisMachineRoom));\n        }\n\n      \t//broker没有对应的 consumer部署的 集群 共享所有的 message queue\n        //2.allocate the rest mq to each machine room if there are no consumer alive in that machine room\n        for (String machineRoom : mr2Mq.keySet()) {\n            if (!mr2c.containsKey(machineRoom)) { // no alive consumer in the corresponding machine room, so all consumers share these queues\n                allocateResults.addAll(allocateMessageQueueStrategy.allocate(consumerGroup, currentCID, mr2Mq.get(machineRoom), cidAll));\n            }\n        }\n\n        return allocateResults;\n    }\n\n    @Override\n    public String getName() {\n        return \"MACHINE_ROOM_NEARBY\" + \"-\" + allocateMessageQueueStrategy.getName();\n    }\n\n    /**\n     * A resolver object to determine which machine room do the message queues or clients are deployed in.\n     *\n     * AllocateMachineRoomNearby will use the results to group the message queues and clients by machine room.\n     *\n     * The result returned from the implemented method CANNOT be null.\n     */\n    public interface MachineRoomResolver {\n        String brokerDeployIn(MessageQueue messageQueue);\n\n        String consumerDeployIn(String clientID);\n    }\n}\n\n"],["body","\n"],["headingLink","基于机房分配算法"],["heading","基于机房分配算法"],["body","\n"],["body","该算法适用于属于同一个机房内部的消息，去分配queue。这种方式非常明确，基于上面的机房临近分配算法的场景，这种更彻底，直接指定基于机房消费的策略。这种方式具有强约定性，比如broker名称按照机房的名称进行拼接，在算法中通过约定解析进行分配。\n"],["body","\n"],["body","/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.rocketmq.client.consumer.rebalance;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Set;\nimport org.apache.rocketmq.client.consumer.AllocateMessageQueueStrategy;\nimport org.apache.rocketmq.common.message.MessageQueue;\n\n/**\n * Computer room Hashing queue algorithm, such as Alipay logic room\n */\npublic class AllocateMessageQueueByMachineRoom implements AllocateMessageQueueStrategy {\n    private Set<String> consumeridcs;\n\n    @Override\n    public List<MessageQueue> allocate(String consumerGroup, String currentCID, List<MessageQueue> mqAll,\n        List<String> cidAll) {\n        List<MessageQueue> result = new ArrayList<MessageQueue>();\n        int currentIndex = cidAll.indexOf(currentCID);\n        if (currentIndex < 0) {\n            return result;\n        }\n        List<MessageQueue> premqAll = new ArrayList<MessageQueue>();\n      \t//挑选配置的 IDC\n        for (MessageQueue mq : mqAll) {\n            String[] temp = mq.getBrokerName().split(\"@\");\n            if (temp.length == 2 && consumeridcs.contains(temp[0])) {\n                premqAll.add(mq);\n            }\n        }\n\n        int mod = premqAll.size() / cidAll.size();\n        int rem = premqAll.size() % cidAll.size();\n        int startIndex = mod * currentIndex;\n        int endIndex = startIndex + mod;\n      \t//根据 mod rem 挑选队列\n        for (int i = startIndex; i < endIndex; i++) {\n            result.add(premqAll.get(i));\n        }\n      \t//如果 currentIndex在当前 rem的覆盖范围中\n        if (rem > currentIndex) {\n            result.add(premqAll.get(currentIndex + mod * cidAll.size()));\n        }\n        return result;\n    }\n\n    @Override\n    public String getName() {\n        return \"MACHINE_ROOM\";\n    }\n\n    public Set<String> getConsumeridcs() {\n        return consumeridcs;\n    }\n\n    public void setConsumeridcs(Set<String> consumeridcs) {\n        this.consumeridcs = consumeridcs;\n    }\n}\n"],["body","\n"],["headingLink","基于一致性hash算法"],["heading","基于一致性hash算法"],["body","\n"],["body","使用这种算法，会将consumer消费者作为Node节点构造成一个hash环，然后queue队列通过这个hash环来决定被分配给哪个consumer消费者。"],["body","\n"],["body","什么是一致性hash 算法 ? 一致性hash算法用于在分布式系统中，保证数据的一致性而提出的一种基于hash环实现的算法，：一致性哈希算法原理"],["body","\n"],["body","代码实现原理：MQ的一致性算法实现原理"],["body","\n"],["body","package org.apache.rocketmq.client.consumer.rebalance;\n\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.List;\nimport org.apache.rocketmq.client.consumer.AllocateMessageQueueStrategy;\nimport org.apache.rocketmq.client.log.ClientLogger;\nimport org.apache.rocketmq.common.consistenthash.ConsistentHashRouter;\nimport org.apache.rocketmq.common.consistenthash.HashFunction;\nimport org.apache.rocketmq.common.consistenthash.Node;\nimport org.apache.rocketmq.logging.InternalLogger;\nimport org.apache.rocketmq.common.message.MessageQueue;\n\n/**\n * Consistent Hashing queue algorithm\n */\npublic class AllocateMessageQueueConsistentHash implements AllocateMessageQueueStrategy {\n    private final InternalLogger log = ClientLogger.getLog();\n\n    private final int virtualNodeCnt;\n    private final HashFunction customHashFunction;\n\n    public AllocateMessageQueueConsistentHash() {\n        this(10);\n    }\n\n    public AllocateMessageQueueConsistentHash(int virtualNodeCnt) {\n        this(virtualNodeCnt, null);\n    }\n\n    public AllocateMessageQueueConsistentHash(int virtualNodeCnt, HashFunction customHashFunction) {\n        if (virtualNodeCnt < 0) {\n            throw new IllegalArgumentException(\"illegal virtualNodeCnt :\" + virtualNodeCnt);\n        }\n        this.virtualNodeCnt = virtualNodeCnt;\n        this.customHashFunction = customHashFunction;\n    }\n\n    @Override\n    public List<MessageQueue> allocate(String consumerGroup, String currentCID, List<MessageQueue> mqAll,\n        List<String> cidAll) {\n\n        Collection<ClientNode> cidNodes = new ArrayList<ClientNode>();\n        for (String cid : cidAll) {\n            cidNodes.add(new ClientNode(cid));\n        }\n\n        final ConsistentHashRouter<ClientNode> router; //for building hash ring\n        if (customHashFunction != null) {\n            router = new ConsistentHashRouter<ClientNode>(cidNodes, virtualNodeCnt, customHashFunction);\n        } else {\n            router = new ConsistentHashRouter<ClientNode>(cidNodes, virtualNodeCnt);\n        }\n\n        List<MessageQueue> results = new ArrayList<MessageQueue>();\n        for (MessageQueue mq : mqAll) {\n            ClientNode clientNode = router.routeNode(mq.toString());\n            if (clientNode != null && currentCID.equals(clientNode.getKey())) {\n                results.add(mq);\n            }\n        }\n        return results;\n    }\n\n    @Override\n    public String getName() {\n        return \"CONSISTENT_HASH\";\n    }\n\n    private static class ClientNode implements Node {\n        private final String clientID;\n\n        public ClientNode(String clientID) {\n            this.clientID = clientID;\n        }\n\n        @Override\n        public String getKey() {\n            return clientID;\n        }\n    }\n}\n"],["body","\n"],["headingLink","基于配置分配算法"],["heading","基于配置分配算法"],["body","\n"],["body","AllocateMessageQueueByConfig\n"],["body","\n"],["headingLink","多个broker模式下同一个topic的分区是如何分配的"],["heading","多个broker模式下同一个Topic的分区是如何分配的"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","6.消息队列_RocketMQ/概念和特性.html"],["title","概念和特性 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","概念和特性"],["heading","概念和特性"],["body","\n"],["headingLink","消息模型message-model"],["heading","消息模型（Message Model）"],["body","\n"],["body","RocketMQ主要由 Producer、Broker、Consumer 三部分组成，"],["body","\n\n"],["body","其中Producer 负责生产消息，"],["body","\n"],["body","Consumer 负责消费消息，"],["body","\n"],["body","Broker 负责存储消息。\n\n"],["body","Broker 在实际部署过程中对应一台服务器"],["body","\n"],["body","每个 Broker 可以存储多个Topic的消息"],["body","\n"],["body","每个Topic的消息也可以分片存储于不同的 Broker。"],["body","\n\n"],["body","\n\n"],["body","Message Queue 用于存储消息的物理地址，每个Topic中的消息地址存储于多个 Message Queue 中。"],["body","\n"],["body","ConsumerGroup 由多个Consumer 实例构成。"],["body","\n"],["headingLink","消息生产者producer"],["heading","消息生产者（Producer）"],["body","\n"],["body","负责生产消息，一般由业务系统负责生产消息。一个消息生产者会把业务应用系统里产生的消息发送到broker服务器。"],["body","\n"],["body","RocketMQ提供多种发送方式，同步发送、异步发送、顺序发送、单向发送。同步和异步方式均需要Broker返回确认信息，单向发送不需要。"],["body","\n"],["headingLink","消息消费者consumer"],["heading","消息消费者（Consumer）"],["body","\n"],["body","负责消费消息，一般是后台系统负责异步消费。一个消息消费者会从Broker服务器拉取消息、并将其提供给应用程序。从用户应用的角度而言提供了两种消费形式：拉取式消费、推动式消费。"],["body","\n"],["headingLink","主题topic"],["heading","主题（Topic）"],["body","\n"],["body","表示一类消息的集合，每个主题包含若干条消息，每条消息只能属于一个主题，是RocketMQ进行消息订阅的基本单位。"],["body","\n"],["headingLink","代理服务器broker-server"],["heading","代理服务器（Broker Server）"],["body","\n"],["body","消息中转角色，负责存储消息、转发消息。代理服务器在RocketMQ系统中负责接收从生产者发送来的消息并存储、同时为消费者的拉取请求作准备。代理服务器也存储消息相关的元数据，包括消费者组、消费进度偏移和主题和队列消息等。"],["body","\n"],["headingLink","名字服务name-server"],["heading","名字服务（Name Server）"],["body","\n"],["body","名称服务充当路由消息的提供者。生产者或消费者能够通过名字服务查找各主题相应的Broker IP列表。多个Namesrv实例组成集群，但相互独立，没有信息交换。"],["body","\n"],["headingLink","拉取式消费pull-consumer"],["heading","拉取式消费（Pull Consumer）"],["body","\n"],["body","Consumer消费的一种类型，应用通常主动调用Consumer的拉消息方法从Broker服务器拉消息、主动权由应用控制。一旦获取了批量消息，应用就会启动消费过程。"],["body","\n"],["headingLink","推动式消费push-consumer"],["heading","推动式消费（Push Consumer）"],["body","\n"],["body","Consumer消费的一种类型，该模式下Broker收到数据后会主动推送给消费端，该消费模式一般实时性较高。"],["body","\n"],["headingLink","生产者组producer-group"],["heading","生产者组（Producer Group）"],["body","\n"],["body","同一类Producer的集合，这类Producer发送同一类消息且发送逻辑一致。如果发送的是事务消息且原始生产者在发送之后崩溃，则Broker服务器会联系同一生产者组的其他生产者实例以提交或回溯消费。"],["body","\n"],["headingLink","消费者组consumer-group"],["heading","消费者组（Consumer Group）"],["body","\n"],["body","同一类Consumer的集合，这类Consumer通常消费同一类消息且消费逻辑一致。消费者组使得在消息消费方面，实现负载均衡和容错的目标变得非常容易。要注意的是，消费者组的消费者实例必须订阅完全相同的Topic。"],["body","\n"],["body","RocketMQ 支持两种消息模式：集群消费（Clustering）和广播消费（Broadcasting）。"],["body","\n"],["headingLink","集群消费clustering"],["heading","集群消费（Clustering）"],["body","\n"],["body","集群消费模式下,相同Consumer Group的每个Consumer实例平均分摊消息。"],["body","\n"],["headingLink","广播消费broadcasting"],["heading","广播消费（Broadcasting）"],["body","\n"],["body","广播消费模式下，相同Consumer Group的每个Consumer实例都接收全量的消息。"],["body","\n"],["headingLink","普通顺序消息normal-ordered-message"],["heading","普通顺序消息（Normal Ordered Message）"],["body","\n"],["body","普通顺序消费模式下，消费者通过同一个消息队列（ Topic 分区，称作 Message Queue） 收到的消息是有顺序的，不同消息队列收到的消息则可能是无顺序的。"],["body","\n"],["headingLink","严格顺序消息strictly-ordered-message"],["heading","严格顺序消息（Strictly Ordered Message）"],["body","\n"],["body","严格顺序消息模式下，消费者收到的所有消息均是有顺序的。"],["body","\n"],["headingLink","消息message"],["heading","消息（Message）"],["body","\n"],["body","消息系统所传输信息的物理载体，生产和消费数据的最小单位，每条消息必须属于一个主题。RocketMQ中每个消息拥有唯一的Message ID，且可以携带具有业务标识的Key。系统提供了通过Message ID和Key查询消息的功能。"],["body","\n"],["headingLink","标签tag"],["heading","标签（Tag）"],["body","\n"],["body","为消息设置的标志，用于同一主题下区分不同类型的消息。来自同一业务单元的消息，可以根据不同业务目的在同一主题下设置不同标签。标签能够有效地保持代码的清晰度和连贯性，并优化RocketMQ提供的查询系统。消费者可以根据Tag实现对不同子主题的不同消费逻辑，实现更好的扩展性。"],["body","\n"],["headingLink","特性features"],["heading","特性(features)"],["body","\n"],["headingLink","订阅与发布"],["heading","订阅与发布"],["body","\n"],["body","消息的发布是指某个生产者向某个topic发送消息；消息的订阅是指某个消费者关注了某个topic中带有某些tag的消息，进而从该topic消费数据。"],["body","\n"],["headingLink","消息顺序"],["heading","消息顺序"],["body","\n"],["body","消息有序 指的是：能按照发送的顺序来消费，例如：一个订单产生了三条消息分别是订单创建、订单付款、订单完成"],["body","\n"],["body","消费时要按照这个顺序消费才能有意义，但是同时订单之间是可以并行消费的。RocketMQ可以严格的保证消息有序"],["body","\n"],["body","顺序消息分为全局顺序消息与分区顺序消息"],["body","\n\n"],["body","\n"],["body","全局顺序是指某个Topic下的所有消息都要保证顺序"],["body","\n\n"],["body","全局顺序 对于指定的一个 Topic，所有消息按照严格的先入先出（FIFO）的顺序进行发布和消费。 适用场景：性能要求不高，所有的消息严格按照 FIFO 原则进行消息发布和消费的场景"],["body","\n\n"],["body","\n"],["body","\n"],["body","部分顺序消息只要保证每一组消息被顺序消费即可。"],["body","\n\n"],["body","分区顺序 对于指定的一个 Topic，所有消息根据 sharding key 进行区块分区。 同一个分区内的消息按照严格的 FIFO 顺序进行发布和消费"],["body","\n"],["body","Sharding key 是顺序消息中用来区分不同分区的关键字段，和普通消息的 Key 是完全不同的概念"],["body","\n"],["body","适用场景：性能要求高，以 sharding key 作为分区字段，在同一个区块中严格的按照 FIFO 原则进行消息发布和消费的场景。"],["body","\n\n"],["body","\n\n"],["headingLink","消息过滤"],["heading","消息过滤"],["body","\n"],["body","RocketMQ的消费者可以根据Tag进行消息过滤，也支持自定义属性过滤。"],["body","\n"],["body","消息过滤目前是在Broker端实现的，"],["body","\n\n"],["body","优点是减少了对于Consumer无用消息的网络传输，"],["body","\n"],["body","缺点是增加了Broker的负担、而且实现相对复杂。"],["body","\n\n"],["headingLink","消息可靠性"],["heading","消息可靠性"],["body","\n"],["body","RocketMQ支持消息的高可靠，影响消息可靠性的几种情况："],["body","\n\n"],["body","Broker非正常关闭"],["body","\n"],["body","Broker异常Crash"],["body","\n"],["body","OS Crash"],["body","\n"],["body","机器掉电，但是能立即恢复供电情况"],["body","\n"],["body","机器无法开机（可能是cpu、主板、内存等关键设备损坏）"],["body","\n"],["body","磁盘设备损坏"],["body","\n\n"],["body","1)、2)、3)、4) 四种情况都属于硬件资源可立即恢复情况，RocketMQ在这四种情况下能保证消息不丢，或者丢失少量数据（依赖刷盘方式是同步还是异步）。"],["body","\n"],["body","5)、6)属于单点故障，且无法恢复，一旦发生，在此单点上的消息全部丢失"],["body","\n"],["body","RocketMQ在这两种情况下，通过异步复制，可保证99%的消息不丢，但是仍然会有极少量的消息可能丢失"],["body","\n"],["body","通过同步双写技术可以完全避免单点，同步双写势必会影响性能，适合对消息可靠性要求极高的场合，例如与Money相关的应用。注：RocketMQ从3.0版本开始支持同步双写。"],["body","\n"],["headingLink","至少一次"],["heading","至少一次"],["body","\n"],["body","至少一次(At least Once)指每个消息必须投递一次。Consumer先Pull消息到本地，消费完成后，才向服务器返回ack，如果没有消费一定不会ack消息，所以RocketMQ可以很好的支持此特性。"],["body","\n"],["headingLink","回溯消费"],["heading","回溯消费"],["body","\n"],["body","回溯消费是指Consumer已经消费成功的消息，由于业务上需求需要重新消费，要支持此功能，Broker在向Consumer投递成功消息后，消息仍然需要保留。并且重新消费一般是按照时间维度，例如由于Consumer系统故障，恢复后需要重新消费1小时前的数据，那么Broker要提供一种机制，可以按照时间维度来回退消费进度。RocketMQ支持按照时间回溯消费，时间维度精确到毫秒。"],["body","\n"],["headingLink","事务消息"],["heading","事务消息"],["body","\n"],["body","RocketMQ事务消息（Transactional Message）是指应用本地事务和发送消息操作可以被定义到全局事务中，要么同时成功，要么同时失败。RocketMQ的事务消息提供类似 X/Open XA 的分布事务功能，通过事务消息能达到分布式事务的最终一致。"],["body","\n"],["headingLink","定时消息"],["heading","定时消息"],["body","\n"],["body","定时消息（延迟队列）是指消息发送到broker后，不会立即被消费，等待特定时间投递给真正的topic"],["body","\n"],["body","broker有配置项messageDelayLevel，默认值为“1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h”，18个level。"],["body","\n"],["body","可以配置自定义messageDelayLevel"],["body","\n"],["body","注意，messageDelayLevel是broker的属性，不属于某个topic,发消息时，设置delayLevel等级即可：msg.setDelayLevel(level)"],["body","\n"],["body","level有以下三种情况："],["body","\n\n"],["body","level == 0，消息为非延迟消息"],["body","\n"],["body","1<=level<=maxLevel，消息延迟特定时间，例如level==1，延迟1s"],["body","\n"],["body","level > maxLevel，则level== maxLevel，例如level==20，延迟2h"],["body","\n\n"],["body","原理"],["body","\n\n"],["body","定时消息会暂存在名为SCHEDULE_TOPIC_XXXX的topic中"],["body","\n"],["body","并根据delayTimeLevel存入特定的queue，queueId = delayTimeLevel – 1"],["body","\n"],["body","即一个queue只存相同延迟的消息，保证具有相同发送延迟的消息能够顺序消费。"],["body","\n"],["body","broker会调度地消费SCHEDULE_TOPIC_XXXX，将消息写入真实的topic。"],["body","\n\n"],["body","需要注意的是，定时消息会在第一次写入和调度写入真实topic时都会计数，因此发送数量、tps都会变高。"],["body","\n"],["headingLink","消息重试"],["heading","消息重试"],["body","\n"],["body","Consumer消费消息失败后，要提供一种重试机制，令消息再消费一次。Consumer消费消息失败通常可以认为有以下几种情况："],["body","\n\n"],["body","由于消息本身的原因，例如反序列化失败，消息数据本身无法处理（例如话费充值，当前消息的手机号被注销，无法充值）等。这种错误通常需要跳过这条消息，再消费其它消息，而这条失败的消息即使立刻重试消费，99%也不成功，所以最好提供一种定时重试机制，即过10秒后再重试。"],["body","\n"],["body","由于依赖的下游应用服务不可用，例如db连接不可用，外系统网络不可达等。遇到这种错误，即使跳过当前失败的消息，消费其他消息同样也会报错。这种情况建议应用sleep 30s，再消费下一条消息，这样可以减轻Broker重试消息的压力。"],["body","\n\n"],["body","原理"],["body","\n\n"],["body","RocketMQ会为每个消费组都设置一个Topic名称为“%RETRY%+consumerGroup”的重试队列（这里需要注意的是，这个Topic的重试队列是针对消费组，而不是针对每个Topic设置的）"],["body","\n"],["body","用于暂时保存因为各种异常而导致Consumer端无法消费的消息"],["body","\n"],["body","考虑到异常恢复起来需要一些时间，会为重试队列设置多个重试级别，每个重试级别都有与之对应的重新投递延时，重试次数越多投递延时就越大"],["body","\n"],["body","RocketMQ对于重试消息的处理是先保存至Topic名称为“SCHEDULE_TOPIC_XXXX”的延迟队列中，后台定时任务按照对应的时间进行Delay后重新保存至“%RETRY%+consumerGroup”的重试队列中。"],["body","\n\n"],["headingLink","消息重投"],["heading","消息重投"],["body","\n"],["body","生产者在发送消息时"],["body","\n\n"],["body","同步消息失败会重投"],["body","\n"],["body","异步消息有重试"],["body","\n"],["body","oneway没有任何保证"],["body","\n\n"],["body","消息重复"],["body","\n"],["body","消息重投保证消息尽可能发送成功、不丢失，但可能会造成消息重复，消息重复在RocketMQ中是无法避免的问题"],["body","\n"],["body","消息重复在一般情况下不会发生，当出现消息量大、网络抖动，消息重复就会是大概率事件。"],["body","\n"],["body","另外，生产者主动重发、consumer负载变化也会导致重复消息。"],["body","\n"],["body","如下方法可以设置消息重试策略："],["body","\n\n"],["body","retryTimesWhenSendFailed:同步发送失败重投次数，默认为2，因此生产者会最多尝试发送retryTimesWhenSendFailed + 1次。不会选择上次失败的broker，尝试向其他broker发送，最大程度保证消息不丢。超过重投次数，抛出异常，由客户端保证消息不丢。当出现RemotingException、MQClientException和部分MQBrokerException时会重投。"],["body","\n"],["body","retryTimesWhenSendAsyncFailed:异步发送失败重试次数，异步重试不会选择其他broker，仅在同一个broker上做重试，不保证消息不丢。"],["body","\n"],["body","retryAnotherBrokerWhenNotStoreOK:消息刷盘（主或备）超时或slave不可用（返回状态非SEND_OK），是否尝试发送到其他broker，默认false。十分重要消息可以开启。"],["body","\n\n"],["headingLink","流量控制"],["heading","流量控制"],["body","\n"],["headingLink","生产者流控"],["heading","生产者流控"],["body","\n"],["body","\n"],["body","因为broker处理能力达到瓶颈"],["body","\n"],["body","\n\n"],["body","commitLog文件被锁时间超过osPageCacheBusyTimeOutMills时，参数默认为1000ms，返回流控。"],["body","\n"],["body","如果开启transientStorePoolEnable == true，且broker为异步刷盘的主机，且transientStorePool中资源不足，拒绝当前send请求，返回流控。"],["body","\n"],["body","broker每隔10ms检查send请求队列头部请求的等待时间，如果超过waitTimeMillsInSendQueue，默认200ms，拒绝当前send请求，返回流控。"],["body","\n"],["body","broker通过拒绝send 请求方式实现流量控制。"],["body","\n\n"],["body","生产者流控，不会尝试消息重投。"],["body","\n"],["headingLink","消费者流控"],["heading","消费者流控"],["body","\n\n"],["body","消费者本地缓存消息数超过pullThresholdForQueue时，默认1000。"],["body","\n"],["body","消费者本地缓存消息大小超过pullThresholdSizeForQueue时，默认100MB。"],["body","\n"],["body","消费者本地缓存消息跨度超过consumeConcurrentlyMaxSpan时，默认2000。"],["body","\n\n"],["body","消费者流控的结果是降低拉取频率。"],["body","\n"],["headingLink","死信队列"],["heading","死信队列"],["body","\n"],["body","死信队列用于处理无法被正常消费的消息。"],["body","\n"],["body","场景"],["body","\n\n"],["body","当一条消息初次消费失败，消息队列会自动进行消息重试"],["body","\n"],["body","达到最大重试次数后，若消费依然失败，则表明消费者在正常情况下无法正确地消费该消息"],["body","\n"],["body","此时，消息队列 不会立刻将消息丢弃，而是将其发送到该消费者对应的特殊队列中。"],["body","\n"],["body","RocketMQ将这种正常情况下无法被消费的消息称为死信消息（Dead-Letter Message），将存储死信消息的特殊队列称为死信队列（Dead-Letter Queue）"],["body","\n"],["body","在RocketMQ中，可以通过使用console控制台对死信队列中的消息进行重发来使得消费者实例再次进行消费。"],["body","\n\n"],["headingLink","启动与安装"],["heading","启动与安装"],["body","\n"],["headingLink","start-name-server"],["heading","Start Name Server"],["body","\n"],["body"," > nohup sh bin/mqnamesrv &\n  > tail -f ~/logs/rocketmqlogs/namesrv.log\n  The Name Server boot success...\n"],["body","\n"],["headingLink","start-broker"],["heading","Start Broker"],["body","\n"],["body","  > nohup sh bin/mqbroker -n localhost:9876 &\n  > tail -f ~/logs/rocketmqlogs/broker.log \n  The broker[%s, 172.30.30.233:10911] boot success...\n"],["body","\n"],["headingLink","send--receive-messages"],["heading","Send & Receive Messages"],["body","\n"],["body"," > export NAMESRV_ADDR=localhost:9876\n > sh bin/tools.sh org.apache.rocketmq.example.quickstart.Producer\n SendResult [sendStatus=SEND_OK, msgId= ...\n\n > sh bin/tools.sh org.apache.rocketmq.example.quickstart.Consumer\n ConsumeMessageThread_%d Receive New Messages: [MessageExt...\n"],["body","\n"],["headingLink","shutdown-servers"],["heading","Shutdown Servers"],["body","\n"],["body","> sh bin/mqshutdown broker\nThe mqbroker(36695) is running...\nSend shutdown request to mqbroker(36695) OK\n\n> sh bin/mqshutdown namesrv\nThe mqnamesrv(36664) is running...\nSend shutdown request to mqnamesrv(36664) OK\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","6.消息队列_RocketMQ/案例.html"],["title","案例 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","基本样例"],["heading","基本样例"],["body","\n"],["headingLink","依赖"],["heading","依赖"],["body","\n"],["body","<dependency>\n    <groupId>org.apache.rocketmq</groupId>\n    <artifactId>rocketmq-client</artifactId>\n    <version>4.9.1</version>\n</dependency>\n"],["body","\n"],["body","compile 'org.apache.rocketmq:rocketmq-client:4.3.0'\n"],["body","\n"],["body","1 基本样例"],["body","\n\n"],["body","\n"],["body","1.1 加入依赖："],["body","\n"],["body","\n"],["body","\n"],["body","1.2 消息发送"],["body","\n\n"],["body","1、Producer端发送同步消息"],["body","\n"],["body","2、发送异步消息"],["body","\n"],["body","3、单向发送消息"],["body","\n\n"],["body","\n"],["body","\n"],["body","1.3 消费消息"],["body","\n"],["body","\n"],["body","\n"],["body","2 顺序消息样例"],["body","\n\n"],["body","2.1 顺序消息生产"],["body","\n"],["body","2.2 顺序消费消息"],["body","\n\n"],["body","\n"],["body","\n"],["body","3 延时消息样例"],["body","\n\n"],["body","3.1 启动消费者等待传入订阅消息"],["body","\n"],["body","3.2 发送延时消息"],["body","\n"],["body","3.3 验证"],["body","\n"],["body","3.4 延时消息的使用场景"],["body","\n"],["body","3.5 延时消息的使用限制"],["body","\n\n"],["body","\n"],["body","\n"],["body","4 批量消息样例"],["body","\n\n"],["body","4.1 发送批量消息"],["body","\n"],["body","4.2 消息列表分割"],["body","\n\n"],["body","\n"],["body","\n"],["body","5 过滤消息样例"],["body","\n\n"],["body","5.1 基本语法"],["body","\n"],["body","5.2 使用样例\n\n"],["body","1、生产者样例"],["body","\n"],["body","2、消费者样例"],["body","\n\n"],["body","\n\n"],["body","\n"],["body","\n"],["body","6 消息事务样例"],["body","\n\n"],["body","6.1 发送事务消息样例\n\n"],["body","1、创建事务性生产者"],["body","\n"],["body","2、实现事务的监听接口"],["body","\n\n"],["body","\n"],["body","6.2 事务消息使用上的限制"],["body","\n\n"],["body","\n\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","6.消息队列_RocketMQ/index.html"],["title","RocketMQ - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","消息队列功能介绍"],["heading","消息队列功能介绍"],["body","\n"],["headingLink","应用解耦"],["heading","应用解耦"],["body","\n"],["headingLink","流量削峰"],["heading","流量削峰"],["body","\n"],["headingLink","动态扩容"],["heading","动态扩容"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","6.消息队列_RocketMQ/架构和设计.html"],["title","架构和设计 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","技术架构"],["heading","技术架构"],["body","\n"],["body",""],["body","\n"],["body","RocketMQ架构上主要分为四部分，如上图所示:"],["body","\n\n"],["body","Producer: 消息发布的角色，支持分布式集群方式部署,Producer通过MQ的负载均衡模块选择相应的Broker集群队列进行消息投递，投递的过程支持快速失败并且低延迟。"],["body","\n"],["body","Consumer : 消息消费的角色，支持分布式集群方式部署，支持以push推，pull拉两种模式对消息进行消费，同时也支持集群方式和广播方式的消费，它提供实时消息订阅机制，可以满足大多数用户的需求。"],["body","\n"],["body","NameServer:NameServer是一个非常简单的Topic路由注册中心，其角色类似Dubbo中的zookeeper，支持Broker的动态注册与发现。主要包括两个功能：\n\n"],["body","Broker管理，NameServer接受Broker集群的注册信息并且保存下来作为路由信息的基本数据，供心跳检测机制，检查Broker是否还存活"],["body","\n"],["body","路由信息管理，每个NameServer将保存关于Broker集群的整个路由信息和用于客户端查询的队列信息。然后Producer和Conumser通过NameServer就可以知道整个Broker集群的路由信息，从而进行消息的投递和消费"],["body","\n\n"],["body","\n\n"],["body","​\t\tNameServer通常也是集群的方式部署，各实例间相互不进行信息通讯，Broker是向每一台NameServer注册\t\t自己的路由信息，所以每一个NameServer实例上面都保存一份完整的路由信息"],["body","\n"],["body","​\t\t当某个NameServer因某种原因下线了，Broker仍然可以向其它NameServer同步其路由信息，Producer,Consumer仍然可以动态感知Broker的路由的信息"],["body","\n\n"],["body","BrokerServer：Broker主要负责消息的存储、投递和查询以及服务高可用保证，为了实现这些功能，Broker包含了以下几个重要子模块。\n\n"],["body","Remoting Module：整个Broker的实体，负责处理来自clients端的请求。"],["body","\n"],["body","Client Manager：负责管理客户端(Producer/Consumer)和维护Consumer的Topic订阅信息"],["body","\n"],["body","Store Service：提供方便简单的API接口处理消息存储到物理硬盘和查询功能。"],["body","\n"],["body","HA Service：高可用服务，提供Master Broker 和 Slave Broker之间的数据同步功能。"],["body","\n"],["body","Index Service：根据特定的Message key对投递到Broker的消息进行索引服务，以提供消息的快速查询。"],["body","\n\n"],["body","\n\n"],["body","\n"],["headingLink","部署架构"],["heading","部署架构"],["body","\n"],["headingLink","部署结构图"],["heading","部署结构图"],["body","\n"],["body","\n"],["headingLink","rocketmq-网络部署特点"],["heading","RocketMQ 网络部署特点"],["body","\n\n"],["body","\n"],["body","NameServer是一个几乎无状态节点，可集群部署，节点之间无任何信息同步。"],["body","\n"],["body","\n"],["body","\n"],["body","Broker部署相对复杂，Broker分为Master与Slave，一个Master可以对应多个Slave，但是一个Slave只能对应一个Master"],["body","\n\n"],["body","Master与Slave 的对应关系通过指定相同的BrokerName，不同的BrokerId 来定义，BrokerId为0表示Master，非0表示Slave"],["body","\n"],["body","Master也可以部署多个。每个Broker与NameServer集群中的所有节点建立长连接，定时注册Topic信息到所有NameServer"],["body","\n"],["body","注意：当前RocketMQ版本在部署架构上支持一Master多Slave，但只有BrokerId=1的从服务器才会参与消息的读负载。"],["body","\n\n"],["body","\n"],["body","\n"],["body","Producer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer获取Topic路由信息，并向提供Topic 服务的Master建立长连接，且定时向Master发送心跳。Producer完全无状态，可集群部署。"],["body","\n"],["body","\n"],["body","\n"],["body","Consumer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer获取Topic路由信息，并向提供Topic服务的Master、Slave建立长连接，且定时向Master、Slave发送心跳。Consumer既可以从Master订阅消息，也可以从Slave订阅消息，消费者在向Master拉取消息时，Master服务器会根据拉取偏移量与最大偏移量的距离（判断是否读老消息，产生读I/O），以及从服务器是否可读等因素建议下一次是从Master还是Slave拉取。"],["body","\n"],["body","\n\n"],["body","结合部署架构图，描述集群工作流程："],["body","\n\n"],["body","启动NameServer，NameServer起来后监听端口，等待Broker、Producer、Consumer连上来，相当于一个路由控制中心。"],["body","\n"],["body","Broker启动，跟所有的NameServer保持长连接，定时发送心跳包。心跳包中包含当前Broker信息(IP+端口等)以及存储所有Topic信息。注册成功后，NameServer集群中就有Topic跟Broker的映射关系。"],["body","\n"],["body","收发消息前，先创建Topic，创建Topic时需要指定该Topic要存储在哪些Broker上，也可以在发送消息时自动创建Topic。"],["body","\n"],["body","Producer发送消息，启动时先跟NameServer集群中的其中一台建立长连接，并从NameServer中获取当前发送的Topic存在哪些Broker上，轮询从队列列表中选择一个队列，然后与队列所在的Broker建立长连接从而向Broker发消息。"],["body","\n"],["body","Consumer跟Producer类似，跟其中一台NameServer建立长连接，获取当前订阅Topic存在哪些Broker上，然后直接跟Broker建立连接通道，开始消费消息。"],["body","\n\n"],["headingLink","设计design"],["heading","设计(design)"],["body","\n"],["headingLink","消息存储"],["heading","消息存储"],["body","\n"],["body","消息存储是RocketMQ中最为复杂和最为重要的一部分，本节将分别从"],["body","\n\n"],["body","RocketMQ的消息存储整体架构"],["body","\n"],["body","PageCache与Mmap内存映射"],["body","\n"],["body","RocketMQ中两种不同的刷盘方式"],["body","\n\n"],["body","三方面来分别展开叙述。"],["body","\n"],["headingLink","消息存储整体架构"],["heading","消息存储整体架构"],["body","\n"],["body","消息存储架构图中主要有下面三个跟消息存储相关的文件构成。"],["body","\n"],["headingLink","commitlog"],["heading","CommitLog"],["body","\n"],["body","\n"],["body","消息主体以及元数据的存储主体"],["body","\n"],["body","\n"],["body","存储Producer端写入的消息主体内容,消息内容不是定长的"],["body","\n\n"],["body","单个文件大小默认1G"],["body","\n"],["body","文件名长度为20位，左边补零，剩余为起始偏移量"],["body","\n\n"],["body","示例"],["body","\n"],["body","比如00000000000000000000代表了第一个文件，起始偏移量为0，文件大小为1G=1073741824；当第一个文件写满了，第二个文件为00000000001073741824，起始偏移量为1073741824，以此类推。消息主要是顺序写入日志文件，当文件满了，写入下一个文件；"],["body","\n"],["headingLink","consumequeue"],["heading","ConsumeQueue"],["body","\n"],["body","\n"],["body","消息消费队列，引入的目的主要是提高消息消费的性能"],["body","\n"],["body","\n"],["body","设计目的"],["body","\n"],["body","由于RocketMQ是基于主题topic的订阅模式，消息消费是针对主题进行的，如果要遍历commitlog文件中根据topic检索消息是非常低效的"],["body","\n"],["body","ConsumeQueue（逻辑消费队列）作为消费消息的索引"],["body","\n\n"],["body","保存了指定Topic下的队列消息在CommitLog中的起始物理偏移量offset，消息大小size和消息Tag的HashCode值"],["body","\n"],["body","consumequeue文件可以看成是基于topic的commitlog索引文件"],["body","\n"],["body","故consumequeue文件夹的组织方式如下：\n\n"],["body","topic/queue/file三层组织结构，具体存储路径为：$HOME/store/consumequeue/{topic}/{queueId}/{fileName}"],["body","\n"],["body","同样consumequeue文件采取定长设计，每一个条目共20个字节，分别为8字节的commitlog物理偏移量、4字节的消息长度、8字节tag hashcode，单个文件由30W个条目组成，可以像数组一样随机访问每一个条目，每个ConsumeQueue文件大小约5.72M；"],["body","\n\n"],["body","\n\n"],["headingLink","indexfile"],["heading","IndexFile"],["body","\n"],["body","\n"],["body","IndexFile（索引文件）提供了一种可以通过key或时间区间来查询消息的方法"],["body","\n"],["body","\n\n"],["body","Index文件的存储位置是：$HOME \\store\\index${fileName}"],["body","\n"],["body","文件名fileName是以创建时的时间戳命名的"],["body","\n"],["body","固定的单个IndexFile文件大小约为400M"],["body","\n"],["body","一个IndexFile可以保存 2000W个索引，"],["body","\n"],["body","IndexFile的底层存储设计为在文件系统中实现HashMap结构，故rocketmq的索引文件其底层实现为hash索引"],["body","\n\n"],["body","混合存储结构"],["body","\n"],["body","在上面的RocketMQ的消息存储整体架构图中可以看出，RocketMQ采用的是混合型的存储结构，"],["body","\n"],["body","为Broker单个实例下所有的队列共用一个日志数据文件（即为CommitLog）来存储。RocketMQ的混合型存储结构(多个Topic的消息实体内容都存储于一个CommitLog中)"],["body","\n"],["body","消息刷盘"],["body","\n"],["body","针对Producer和Consumer分别采用了数据和索引部分相分离的存储结构，Producer发送消息至Broker端，然后Broker端使用同步或者异步的方式对消息刷盘持久化，保存至CommitLog中。只要消息被刷盘持久化至磁盘文件CommitLog中，那么Producer发送的消息就不会丢失。正因为如此，Consumer也就肯定有机会去消费这条消息。"],["body","\n"],["body","等待拉取"],["body","\n"],["body","当无法拉取到消息后，可以等下一次消息拉取，同时服务端也支持长轮询模式，如果一个消息拉取请求未拉取到消息，Broker允许等待30s的时间，只要这段时间内有新消息到达，将直接返回给消费端。这里，RocketMQ的具体做法是，使用Broker端的后台服务线程—ReputMessageService不停地分发请求并异步构建ConsumeQueue（逻辑消费队列）和IndexFile（索引文件）数据。"],["body","\n"],["headingLink","页缓存与内存映射"],["heading","页缓存与内存映射"],["body","\n"],["body","页缓存（PageCache)是OS对文件的缓存，用于加速对文件的读写"],["body","\n"],["body","os对文件缓存"],["body","\n"],["body","一般来说，程序对文件进行顺序读写的速度几乎接近于内存的读写速度，主要原因就是由于OS使用PageCache机制对读写访问操作进行了性能优化，将一部分的内存用作PageCache"],["body","\n"],["body","对于数据的写入，OS会先写入至Cache内，随后通过异步的方式由pdflush内核线程将Cache内的数据刷盘至物理磁盘上"],["body","\n"],["body","对于数据的读取，如果一次读取文件时出现未命中PageCache的情况，OS从物理磁盘上访问读取文件的同时，会顺序对其他相邻块的数据文件进行预读取。"],["body","\n"],["body","ConsumeQueue队列读取"],["body","\n"],["body","在RocketMQ中，ConsumeQueue逻辑消费队列存储的数据较少，并且是顺序读取，在page cache机制的预读取作用下，Consume Queue文件的读性能几乎接近读内存，即使在有消息堆积情况下也不会影响性能"],["body","\n"],["body","CommitLog的随机读取"],["body","\n"],["body","而对于CommitLog消息存储的日志数据文件来说，读取消息内容时候会产生较多的随机访问读取，严重影响性能。如果选择合适的系统IO调度算法，比如设置调度算法为“Deadline”（此时块存储采用SSD的话），随机读的性能也会有所提升。"],["body","\n"],["body","内存映射文件"],["body","\n"],["body","另外，RocketMQ主要通过MappedByteBuffer对文件进行读写操作。其中，利用了NIO中的FileChannel模型将磁盘上的物理文件直接映射到用户态的内存地址中（这种Mmap的方式减少了传统IO将磁盘文件数据在操作系统内核地址空间的缓冲区和用户应用程序地址空间的缓冲区之间来回进行拷贝的性能开销），将对文件的操作转化为直接对内存地址进行操作，从而极大地提高了文件的读写效率（正因为需要使用内存映射机制，故RocketMQ的文件存储都使用定长结构来存储，方便一次将整个文件映射至内存）。"],["body","\n"],["headingLink","消息刷盘"],["heading","消息刷盘"],["body","\n"],["body","(1) 同步刷盘：如上图所示，只有在消息真正持久化至磁盘后RocketMQ的Broker端才会真正返回给Producer端一个成功的ACK响应。同步刷盘对MQ消息可靠性来说是一种不错的保障，但是性能上会有较大影响，一般适用于金融业务应用该模式较多。"],["body","\n"],["body","(2) 异步刷盘：能够充分利用OS的PageCache的优势，只要消息写入PageCache即可将成功的ACK返回给Producer端。消息刷盘采用后台异步线程提交的方式进行，降低了读写延迟，提高了MQ的性能和吞吐量。"],["body","\n"],["headingLink","通信机制"],["heading","通信机制"],["body","\n"],["body","RocketMQ消息队列集群主要包括NameServer、Broker(Master/Slave)、Producer、Consumer4个角色，基本通讯流程如下："],["body","\n\n"],["body","Broker启动后需要完成一次将自己注册至NameServer的操作；随后每隔30s时间定时向NameServer上报Topic路由信息。"],["body","\n"],["body","消息生产者Producer作为客户端发送消息时候，需要根据消息的Topic从本地缓存的TopicPublishInfoTable获取路由信息。如果没有则更新路由信息会从NameServer上重新拉取，同时Producer会默认每隔30s向NameServer拉取一次路由信息。"],["body","\n"],["body","消息生产者Producer根据2）中获取的路由信息选择一个队列（MessageQueue）进行消息发送；Broker作为消息的接收者接收消息并落盘存储。"],["body","\n"],["body","消息消费者Consumer根据2）中获取的路由信息，并再完成客户端的负载均衡后，选择其中的某一个或者某几个消息队列来拉取消息并进行消费。"],["body","\n\n"],["body","从上面1）~3）中可以看出在消息生产者, Broker和NameServer之间都会发生通信（这里只说了MQ的部分通信），因此如何设计一个良好的网络通信模块在MQ中至关重要，它将决定RocketMQ集群整体的消息传输能力与最终的性能。"],["body","\n"],["body","rocketmq-remoting 模块是 RocketMQ消息队列中负责网络通信的模块，它几乎被其他所有需要网络通信的模块（诸如rocketmq-client、rocketmq-broker、rocketmq-namesrv）所依赖和引用。为了实现客户端与服务器之间高效的数据请求与接收，RocketMQ消息队列自定义了通信协议并在Netty的基础之上扩展了通信模块。"],["body","\n"],["headingLink","remoting通信类结构"],["heading","Remoting通信类结构"],["body","\n"],["headingLink","协议设计与编解码"],["heading","协议设计与编解码"],["body","\n"],["body","在Client和Server之间完成一次消息发送时，需要对发送的消息进行一个协议约定，因此就有必要自定义RocketMQ的消息协议。同时，为了高效地在网络中传输消息和对收到的消息读取，就需要对消息进行编解码。在RocketMQ中，RemotingCommand这个类在消息传输过程中对所有数据内容的封装，不但包含了所有的数据结构，还包含了编码解码操作。"],["body","\n"],["body","Header字段"],["body","类型"],["body","Request说明"],["body","Response说明"],["body","\n"],["body","code"],["body","int"],["body","请求操作码，应答方根据不同的请求码进行不同的业务处理"],["body","应答响应码。0表示成功，非0则表示各种错误"],["body","\n"],["body","language"],["body","LanguageCode"],["body","请求方实现的语言"],["body","应答方实现的语言"],["body","\n"],["body","version"],["body","int"],["body","请求方程序的版本"],["body","应答方程序的版本"],["body","\n"],["body","opaque"],["body","int"],["body","相当于requestId，在同一个连接上的不同请求标识码，与响应消息中的相对应"],["body","应答不做修改直接返回"],["body","\n"],["body","flag"],["body","int"],["body","区分是普通RPC还是onewayRPC的标志"],["body","区分是普通RPC还是onewayRPC的标志"],["body","\n"],["body","remark"],["body","String"],["body","传输自定义文本信息"],["body","传输自定义文本信息"],["body","\n"],["body","extFields"],["body","HashMap<String, String>"],["body","请求自定义扩展信息"],["body","响应自定义扩展信息"],["body","\n\n\n"],["body","可见传输内容主要可以分为以下4部分："],["body","\n"],["body","(1) 消息长度：总长度，四个字节存储，占用一个int类型；"],["body","\n"],["body","(2) 序列化类型&消息头长度：同样占用一个int类型，第一个字节表示序列化类型，后面三个字节表示消息头长度；"],["body","\n"],["body","(3) 消息头数据：经过序列化后的消息头数据；"],["body","\n"],["body","(4) 消息主体数据：消息主体的二进制字节数据内容；"],["body","\n"],["headingLink","消息的通信方式和流程"],["heading","消息的通信方式和流程"],["body","\n"],["body","在RocketMQ消息队列中支持通信的方式主要有同步(sync)、异步(async)、单向(oneway) 三种。其中“单向”通信模式相对简单，一般用在发送心跳包场景下，无需关注其Response。这里，主要介绍RocketMQ的异步通信流程。"],["body","\n"],["headingLink","reactor多线程设计"],["heading","Reactor多线程设计"],["body","\n"],["body","RocketMQ的RPC通信采用Netty组件作为底层通信库，同样也遵循了Reactor多线程模型，同时又在这之上做了一些扩展和优化。"],["body","\n"],["body","上面的框图中可以大致了解RocketMQ中NettyRemotingServer的Reactor 多线程模型。一个 Reactor 主线程（eventLoopGroupBoss，即为上面的1）负责监听 TCP网络连接请求，建立好连接，创建SocketChannel，并注册到selector上。RocketMQ的源码中会自动根据OS的类型选择NIO和Epoll，也可以通过参数配置）,然后监听真正的网络数据。拿到网络数据后，再丢给Worker线程池（eventLoopGroupSelector，即为上面的“N”，源码中默认设置为3），在真正执行业务逻辑之前需要进行SSL验证、编解码、空闲检查、网络连接管理，这些工作交给defaultEventExecutorGroup（即为上面的“M1”，源码中默认设置为8）去做。而处理业务操作放在业务线程池中执行，根据 RomotingCommand 的业务请求码code去processorTable这个本地缓存变量中找到对应的 processor，然后封装成task任务后，提交给对应的业务processor处理线程池来执行（sendMessageExecutor，以发送消息为例，即为上面的 “M2”）。从入口到业务逻辑的几个步骤中线程池一直再增加，这跟每一步逻辑复杂性相关，越复杂，需要的并发通道越宽。"],["body","\n"],["body","线程数"],["body","线程名"],["body","线程具体说明"],["body","\n"],["body","1"],["body","NettyBoss_%d"],["body","Reactor 主线程"],["body","\n"],["body","N"],["body","NettyServerEPOLLSelector_%d_%d"],["body","Reactor 线程池"],["body","\n"],["body","M1"],["body","NettyServerCodecThread_%d"],["body","Worker线程池"],["body","\n"],["body","M2"],["body","RemotingExecutorThread_%d"],["body","业务processor处理线程池"],["body","\n\n\n"],["headingLink","消息过滤"],["heading","消息过滤"],["body","\n"],["body","基于Tag的过滤"],["body","\n"],["body","RocketMQ分布式消息队列的消息过滤方式有别于其它MQ中间件，是在Consumer端订阅消息时再做消息过滤的。RocketMQ这么做是在于其Producer端写入消息和Consumer端订阅消息采用分离存储的机制来实现的，Consumer端订阅消息是需要通过ConsumeQueue这个消息消费的逻辑队列拿到一个索引，然后再从CommitLog里面读取真正的消息实体内容，所以说到底也是还绕不开其存储结构。其ConsumeQueue的存储结构如下，可以看到其中有8个字节存储的Message Tag的哈希值，基于Tag的消息过滤正是基于这个字段值的。"],["body","\n"],["headingLink","两种过滤方式"],["heading","两种过滤方式"],["body","\n"],["body","主要支持如下2种的过滤方式"],["body","\n"],["body","(1) Tag过滤方式：Consumer端在订阅消息时除了指定Topic还可以指定TAG，如果一个消息有多个TAG，可以用||分隔。其中，Consumer端会将这个订阅请求构建成一个 SubscriptionData，发送一个Pull消息的请求给Broker端。Broker端从RocketMQ的文件存储层—Store读取数据之前，会用这些数据先构建一个MessageFilter，然后传给Store。Store从 ConsumeQueue读取到一条记录后，会用它记录的消息tag hash值去做过滤，由于在服务端只是根据hashcode进行判断，无法精确对tag原始字符串进行过滤，故在消息消费端拉取到消息后，还需要对消息的原始tag字符串进行比对，如果不同，则丢弃该消息，不进行消息消费。"],["body","\n"],["body","(2) SQL92的过滤方式：这种方式的大致做法和上面的Tag过滤方式一样，只是在Store层的具体过滤过程不太一样，真正的 SQL expression 的构建和执行由rocketmq-filter模块负责的。每次过滤都去执行SQL表达式会影响效率，所以RocketMQ使用了BloomFilter避免了每次都去执行。SQL92的表达式上下文为消息的属性。"],["body","\n"],["headingLink","负载均衡"],["heading","负载均衡"],["body","\n"],["body","RocketMQ中的负载均衡都在Client端完成，具体来说的话，主要可以分为"],["body","\n\n"],["body","Producer端发送消息时候的负载均衡和"],["body","\n"],["body","Consumer端订阅消息的负载均衡。"],["body","\n\n"],["headingLink","producer的负载均衡"],["heading","Producer的负载均衡"],["body","\n"],["body","随机递增取模"],["body","\n"],["body","Producer端在发送消息的时候，会先根据Topic找到指定的TopicPublishInfo，在获取了TopicPublishInfo路由信息后，RocketMQ的客户端在默认方式下selectOneMessageQueue()方法会从TopicPublishInfo中的messageQueueList中选择一个队列（MessageQueue）进行发送消息。具体的容错策略均在MQFaultStrategy这个类中定义。这里有一个sendLatencyFaultEnable开关变量，如果开启，在随机递增取模的基础上，再过滤掉not available的Broker代理。"],["body","\n"],["body","避让机制"],["body","\n"],["body","所谓的\"latencyFaultTolerance\"，是指对之前失败的，按一定的时间做退避。例如，如果上次请求的latency超过550Lms，就退避3000Lms；超过1000L，就退避60000L；如果关闭，采用随机递增取模的方式选择一个队列（MessageQueue）来发送消息，latencyFaultTolerance机制是实现消息发送高可用的核心关键所在。"],["body","\n"],["headingLink","consumer的负载均衡"],["heading","Consumer的负载均衡"],["body","\n"],["body","在RocketMQ中，Consumer端的两种消费模式（Push/Pull）都是基于拉模式来获取消息的，而在Push模式只是对pull模式的一种封装，其本质实现为消息拉取线程在从服务器拉取到一批消息后，然后提交到消息消费线程池后，又“马不停蹄”的继续向服务器再次尝试拉取消息。如果未拉取到消息，则延迟一下又继续拉取。在两种基于拉模式的消费方式（Push/Pull）中，均需要Consumer端在知道从Broker端的哪一个消息队列—队列中去获取消息。因此，有必要在Consumer端来做负载均衡，即Broker端中多个MessageQueue分配给同一个ConsumerGroup中的哪些Consumer消费。"],["body","\n"],["headingLink","consumer端的心跳包发送"],["heading","Consumer端的心跳包发送"],["body","\n"],["body","在Consumer启动后，它就会通过定时任务不断地向RocketMQ集群中的所有Broker实例发送心跳包（其中包含了，消息消费分组名称、订阅关系集合、消息通信模式和客户端id的值等信息）"],["body","\n"],["body","Broker端在收到Consumer的心跳消息后，会将它维护在ConsumerManager的本地缓存变量—consumerTable，同时并将封装后的客户端网络通道信息保存在本地缓存变量—channelInfoTable中，为之后做Consumer端的负载均衡提供可以依据的元数据信息。"],["body","\n"],["headingLink","consumer端实现负载均衡的核心类rebalanceimpl"],["heading","Consumer端实现负载均衡的核心类—RebalanceImpl"],["body","\n"],["body","RebalanceService实现负载均衡"],["body","\n"],["body","在Consumer实例的启动流程中的启动MQClientInstance实例部分，会完成负载均衡服务线程—RebalanceService的启动（每隔20s执行一次）。通过查看源码可以发现，RebalanceService线程的run()方法最终调用的是RebalanceImpl类的rebalanceByTopic()方法，该方法是实现Consumer端负载均衡的核心。这里，rebalanceByTopic()方法会根据消费者通信类型为“广播模式”还是“集群模式”做不同的逻辑处理。这里主要来看下集群模式下的主要处理流程："],["body","\n"],["body","集群模式的处理逻辑"],["body","\n\n"],["body","\n"],["body","从rebalanceImpl实例的本地缓存变量—topicSubscribeInfoTable中，获取该Topic主题下的消息消费队列集合（mqSet）；"],["body","\n"],["body","\n"],["body","\n"],["body","根据topic和consumerGroup为参数调用mQClientFactory.findConsumerIdList()方法向Broker端发送获取该消费组下消费者Id列表的RPC通信请求（Broker端基于前面Consumer端上报的心跳包数据而构建的consumerTable做出响应返回，业务请求码：GET_CONSUMER_LIST_BY_GROUP）"],["body","\n"],["body","\n"],["body","\n"],["body","先对Topic下的消息消费队列、消费者Id排序，然后用消息队列分配策略算法（默认为：消息队列的平均分配算法），计算出待拉取的消息队列。"],["body","\n"],["body","​\t这里的平均分配算法，类似于分页的算法，将所有MessageQueue排好序类似于记录，将所有消费端Consumer排好序类似页数，并求出每一页需要包含的平均size和每个页面记录的范围range，最后遍历整个range而计算出当前Consumer端应该分配到的记录（这里即为：MessageQueue）。"],["body","\n"],["body","\n"],["body","\n"],["body","然后，调用updateProcessQueueTableInRebalance()方法，具体的做法是，先将分配到的消息队列集合（mqSet）与processQueueTable做一个过滤比对。"],["body","\n"],["body","\n\n"],["body","\n"],["body","上图中processQueueTable标注的红色部分，表示与分配到的消息队列集合mqSet互不包含。将这些队列设置Dropped属性为true，然后查看这些队列是否可以移除出processQueueTable缓存变量，这里具体执行removeUnnecessaryMessageQueue()方法，即每隔1s 查看是否可以获取当前消费处理队列的锁，拿到的话返回true。如果等待1s后，仍然拿不到当前消费处理队列的锁则返回false。如果返回true，则从processQueueTable缓存变量中移除对应的Entry；"],["body","\n"],["body","上图中processQueueTable的绿色部分，表示与分配到的消息队列集合mqSet的交集。判断该ProcessQueue是否已经过期了，在Pull模式的不用管，如果是Push模式的，设置Dropped属性为true，并且调用removeUnnecessaryMessageQueue()方法，像上面一样尝试移除Entry；"],["body","\n"],["body","拉取消息队列"],["body","\n"],["body","最后，为过滤后的消息队列集合（mqSet）中的每个MessageQueue创建一个ProcessQueue对象并存入RebalanceImpl的processQueueTable队列中（其中调用RebalanceImpl实例的computePullFromWhere(MessageQueue mq)方法获取该MessageQueue对象的下一个进度消费值offset，随后填充至接下来要创建的pullRequest对象属性中），并创建拉取请求对象—pullRequest添加到拉取列表—pullRequestList中，最后执行dispatchPullRequest()方法，将Pull消息的请求对象PullRequest依次放入PullMessageService服务线程的阻塞队列pullRequestQueue中，待该服务线程取出后向Broker端发起Pull消息的请求。其中，可以重点对比下，RebalancePushImpl和RebalancePullImpl两个实现类的dispatchPullRequest()方法不同，RebalancePullImpl类里面的该方法为空，这样子也就回答了上一篇中最后的那道思考题了。"],["body","\n"],["body","消息消费队列在同一消费组不同消费者之间的负载均衡，其核心设计理念是在一个消息消费队列在同一时间只允许被同一消费组内的一个消费者消费，一个消息消费者能同时消费多个消息队列。"],["body","\n"],["headingLink","事务消息"],["heading","事务消息"],["body","\n"],["body","Apache RocketMQ在4.3.0版中已经支持分布式事务消息，"],["body","\n\n"],["body","这里RocketMQ采用了2PC的思想来实现了提交事务消息，"],["body","\n"],["body","同时增加一个补偿逻辑来处理二阶段超时或者失败的消息，"],["body","\n\n"],["body","如下图所示"],["body","\n"],["body","\n"],["headingLink","rocketmq事务消息流程概要"],["heading","RocketMQ事务消息流程概要"],["body","\n"],["headingLink","事务消息发送及提交"],["heading","事务消息发送及提交"],["body","\n\n"],["body","发送消息（half消息）。"],["body","\n"],["body","服务端响应消息写入结果。"],["body","\n"],["body","根据发送结果执行本地事务（如果写入失败，此时half消息对业务不可见，本地逻辑不执行）。"],["body","\n"],["body","根据本地事务状态执行Commit或者Rollback（Commit操作生成消息索引，消息对消费者可见）"],["body","\n\n"],["headingLink","补偿流程"],["heading","补偿流程"],["body","\n\n"],["body","对没有Commit/Rollback的事务消息（pending状态的消息），从服务端发起一次“回查”"],["body","\n"],["body","Producer收到回查消息，检查回查消息对应的本地事务的状态"],["body","\n"],["body","根据本地事务状态，重新Commit或者Rollback"],["body","\n\n"],["body","其中，补偿阶段用于解决消息Commit或者Rollback发生超时或者失败的情况。"],["body","\n"],["headingLink","rocketmq事务消息设计"],["heading","RocketMQ事务消息设计"],["body","\n"],["headingLink","事务消息在一阶段对用户不可见"],["heading","事务消息在一阶段对用户不可见"],["body","\n"],["body","RocketMQ事务消息的做法是：如果消息是half消息，"],["body","\n\n"],["body","将备份原消息的主题与消息消费队列，"],["body","\n"],["body","然后改变主题为RMQ_SYS_TRANS_HALF_TOPIC"],["body","\n\n"],["body","由于消费组未订阅该主题，故消费端无法消费half类型的消息"],["body","\n\n"],["body","然后RocketMQ会开启一个定时任务，从Topic为RMQ_SYS_TRANS_HALF_TOPIC中拉取消息进行消费，根据生产者组获取一个服务提供者发送回查事务状态请求，根据事务状态来决定是提交或回滚消息。"],["body","\n\n"],["body","消息对应结构"],["body","\n"],["body","\n"],["body","总结"],["body","\n"],["body","RocketMQ的具体实现策略是：写入的如果事务消息，对消息的Topic和Queue等属性进行替换，同时将原来的Topic和Queue信息存储到消息的属性中，正因为消息主题被替换，故消息并不会转发到该原主题的消息消费队列，消费者无法感知消息的存在，不会消费。其实改变消息主题是RocketMQ的常用“套路”，回想一下延时消息的实现机制。"],["body","\n"],["headingLink","commit和rollback操作以及op消息的引入"],["heading","Commit和Rollback操作以及Op消息的引入"],["body","\n"],["body","二阶段的可见"],["body","\n"],["body","在完成一阶段写入一条对用户不可见的消息后，二阶段如果是Commit操作，则需要让消息对用户可见；如果是Rollback则需要撤销一阶段的消息。先说Rollback的情况。对于Rollback，本身一阶段的消息对用户是不可见的，其实不需要真正撤销消息（实际上RocketMQ也无法去真正的删除一条消息，因为是顺序写文件的）。但是区别于这条消息没有确定状态（Pending状态，事务悬而未决），需要一个操作来标识这条消息的最终状态。"],["body","\n"],["body","Op消息"],["body","\n"],["body","RocketMQ事务消息方案中引入了Op消息的概念，用Op消息标识事务消息已经确定的状态（Commit或者Rollback）。如果一条事务消息没有对应的Op消息，说明这个事务的状态还无法确定（可能是二阶段失败了）。引入Op消息后，事务消息无论是Commit或者Rollback都会记录一个Op操作。Commit相对于Rollback只是在写入Op消息前创建Half消息的索引。"],["body","\n"],["headingLink","op消息的存储和对应关系"],["heading","Op消息的存储和对应关系"],["body","\n"],["body","RocketMQ将Op消息写入到全局一个特定的Topic中通过源码中的方法—TransactionalMessageUtil.buildOpTopic()；这个Topic是一个内部的Topic（像Half消息的Topic一样），不会被用户消费。Op消息的内容为对应的Half消息的存储的Offset，这样通过Op消息能索引到Half消息进行后续的回查操作。"],["body","\n"],["body","Op消息存储对应图"],["body","\n"],["body","\n"],["headingLink","half消息的索引构建"],["heading","Half消息的索引构建"],["body","\n"],["body","在执行二阶段Commit操作时，需要构建出Half消息的索引。一阶段的Half消息由于是写到一个特殊的Topic，"],["body","\n\n"],["body","\n"],["body","所以二阶段构建索引时需要读取出Half消息"],["body","\n"],["body","\n"],["body","\n"],["body","并将Topic和Queue替换成真正的目标的Topic和Queue，"],["body","\n"],["body","\n"],["body","\n"],["body","之后通过一次普通消息的写入操作来生成一条对用户可见的消息。"],["body","\n"],["body","\n\n"],["body","所以RocketMQ事务消息二阶段其实是利用了一阶段存储的消息的内容，在二阶段时恢复出一条完整的普通消息，然后走一遍消息写入流程。"],["body","\n"],["headingLink","如何处理二阶段失败的消息"],["heading","如何处理二阶段失败的消息？"],["body","\n"],["body","如果在RocketMQ事务消息的二阶段过程中失败了，例如在做Commit操作时，出现网络问题导致Commit失败，那么需要通过一定的策略使这条消息最终被Commit。"],["body","\n"],["body","回查机制"],["body","\n"],["body","RocketMQ采用了一种补偿机制，称为“回查”"],["body","\n"],["body","Broker端对未确定状态的消息发起回查，将消息发送到对应的Producer端（同一个Group的Producer），由Producer根据消息来检查本地事务的状态，进而执行Commit或者Rollback。Broker端通过对比Half消息和Op消息进行事务消息的回查并且推进CheckPoint（记录那些事务消息的状态是确定的）。"],["body","\n"],["body","查询上限"],["body","\n"],["body","值得注意的是，rocketmq并不会无休止的的信息事务状态回查，默认回查15次，如果15次回查还是无法得知事务状态，rocketmq默认回滚该消息。"],["body","\n"],["headingLink","消息查询"],["heading","消息查询"],["body","\n"],["body","RocketMQ支持按照下面两种维度"],["body","\n\n"],["body","按照Message Id查询消息"],["body","\n"],["body","按照Message Key查询消息"],["body","\n\n"],["headingLink","按照messageid查询消息"],["heading","按照MessageId查询消息"],["body","\n"],["body","16位MessageId标识"],["body","\n"],["body","RocketMQ中的MessageId的长度总共有16字节，其中包含了消息存储主机地址（IP地址和端口），消息Commit Log offset。"],["body","\n"],["body","RocketMQ中具体做法是"],["body","\n"],["body","Client端从MessageId中解析出Broker的地址（IP地址和端口）和Commit Log的偏移地址后封装成一个RPC请求后通过Remoting通信层发送（业务请求码：VIEW_MESSAGE_BY_ID）。Broker端走的是QueryMessageProcessor，读取消息的过程用其中的 commitLog offset 和 size 去 commitLog 中找到真正的记录并解析成一个完整的消息返回。"],["body","\n"],["headingLink","按照message-key查询消息"],["heading","按照Message Key查询消息"],["body","\n"],["body","“按照Message Key查询消息”，主要是基于RocketMQ的IndexFile索引文件来实现的。"],["body","\n"],["body","RocketMQ的索引文件逻辑结构，类似JDK中HashMap的实现。索引文件的具体结构如下："],["body","\n"],["body","\n"],["body","IndexFile索引文件为用户提供通过“按照Message Key查询消息”的消息索引查询服务"],["body","\n"],["body","索引文件"],["body","\n"],["body","IndexFile文件的存储位置是：HOME\\store\\indexHOME\\store\\index{fileName}，文件名fileName是以创建时的时间戳命名的，文件大小是固定的，等于40+500W4+2000W20= 420000040个字节大小"],["body","\n"],["body","索引的Key"],["body","\n"],["body","如果消息的properties中设置了UNIQ_KEY这个属性，就用 topic + “#” + UNIQ_KEY的value作为 key 来做写入操作。如果消息设置了KEYS属性（多个KEY以空格分隔），也会用 topic + “#” + KEY 来做索引。"],["body","\n"],["body","索引字段"],["body","\n"],["body","其中的索引数据包含了Key Hash/CommitLog Offset/Timestamp/NextIndex offset 这四个字段，一共20 Byte"],["body","\n"],["body","hash冲突"],["body","\n"],["body","NextIndex offset 即前面读出来的 slotValue，如果有 hash冲突，就可以用这个字段将所有冲突的索引用链表的方式串起来了"],["body","\n"],["body","timestamp"],["body","\n"],["body","Timestamp记录的是消息storeTimestamp之间的差，并不是一个绝对的时间"],["body","\n"],["body","按照Message Key查询消息"],["body","\n"],["body","RocketMQ的具体做法是，主要通过Broker端的QueryMessageProcessor业务处理器来查询，读取消息的过程就是用topic和key找到IndexFile索引文件中的一条记录，根据其中的commitLog offset从CommitLog文件中读取消息的实体内容。"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","6.消息队列_RocketMQ/MQ的一致性算法实现原理.html"],["title","MQ的一致性算法实现原理 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","物理节点"],["heading","物理节点"],["body","\n"],["body","package org.apache.rocketmq.common.consistenthash;\n\n/**\n * Represent a node which should be mapped to a hash ring\n */\npublic interface Node {\n    /**\n     * @return the key which will be used for hash mapping\n     */\n    String getKey();\n}\n\n"],["body","\n"],["headingLink","虚拟节点"],["heading","虚拟节点"],["body","\n"],["body","package org.apache.rocketmq.common.consistenthash;\n\npublic class VirtualNode<T extends Node> implements Node {\n    final T physicalNode;\n    final int replicaIndex;\n\n    public VirtualNode(T physicalNode, int replicaIndex) {\n        this.replicaIndex = replicaIndex;\n        this.physicalNode = physicalNode;\n    }\n\n    @Override\n    public String getKey() {\n        return physicalNode.getKey() + \"-\" + replicaIndex;\n    }\n\n    public boolean isVirtualNodeOf(T pNode) {\n        return physicalNode.getKey().equals(pNode.getKey());\n    }\n\n    public T getPhysicalNode() {\n        return physicalNode;\n    }\n}\n\n"],["body","\n"],["headingLink","mq节点"],["heading","MQ节点"],["body","\n"],["body","private static class ClientNode implements Node {\n  private final String clientID;\n\n  public ClientNode(String clientID) {\n    this.clientID = clientID;\n  }\n\n  @Override\n  public String getKey() {\n    return clientID;\n  }\n}\n"],["body","\n"],["headingLink","环对象"],["heading","环对象"],["body","\n"],["body","利用 sortedMap 实现的 Hash环"],["body","\n"],["body","private final SortedMap<Long, VirtualNode<T>> ring = new TreeMap<Long, VirtualNode<T>>();\n"],["body","\n"],["headingLink","consistenthashrouter"],["heading","ConsistentHashRouter"],["body","\n"],["headingLink","添加节点"],["heading","添加节点"],["body","\n"],["body","//org.apache.rocketmq.common.consistenthash.ConsistentHashRouter#addNode\n\npublic void addNode(T pNode, int vNodeCount) {\n  if (vNodeCount < 0)\n    throw new IllegalArgumentException(\"illegal virtual node counts :\" + vNodeCount);\n  //先判断 是否之前有加入过环，如果有则 从上次的虚拟节点开始\n  int existingReplicas = getExistingReplicas(pNode);\n  for (int i = 0; i < vNodeCount; i++) {\n    VirtualNode<T> vNode = new VirtualNode<T>(pNode, i + existingReplicas);\n    ring.put(hashFunction.hash(vNode.getKey()), vNode);\n  }\n}\n"],["body","\n"],["headingLink","移除节点"],["heading","移除节点"],["body","\n"],["body","移除物理节点对应的所有虚拟节点"],["body","\n"],["body","public void removeNode(T pNode) {\n  Iterator<Long> it = ring.keySet().iterator();\n  while (it.hasNext()) {\n    Long key = it.next();\n    VirtualNode<T> virtualNode = ring.get(key);\n    if (virtualNode.isVirtualNodeOf(pNode)) {\n      it.remove();\n    }\n  }\n}\n"],["body","\n"],["headingLink","路由"],["heading","路由"],["body","\n"],["body","public T routeNode(String objectKey) {\n  if (ring.isEmpty()) {\n    return null;\n  }\n  Long hashVal = hashFunction.hash(objectKey);\n  SortedMap<Long, VirtualNode<T>> tailMap = ring.tailMap(hashVal);\n  Long nodeHashVal = !tailMap.isEmpty() ? tailMap.firstKey() : ring.firstKey();\n  return ring.get(nodeHashVal).getPhysicalNode();\n}\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","1.容器_docker/docker-compose.html"],["title","docker-compose - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","docker-compose简介"],["heading","Docker-Compose简介"],["body","\n"],["body","Compose允许用户通过一个docker-compose.yml模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）。"],["body","\n"],["body","Compose模板文件是一个定义服务、网络和卷的YAML文件"],["body","\n"],["body","Compose模板文件默认路径是当前目录下的docker-compose.yml"],["body","\n"],["body","Docker-Compose标准模板文件应该包含version、services、networks 三大部分，最关键的是services和networks两个部分。"],["body","\n"],["body","example"],["body","\n"],["body","version: '2'\nservices:\n  web:\n    image: dockercloud/hello-world\n    ports:\n      - 8080\n    networks:\n      - front-tier\n      - back-tier\n\n  redis:\n    image: redis\n    links:\n      - web\n    networks:\n      - back-tier\n\n  lb:\n    image: dockercloud/haproxy\n    ports:\n      - 80:80\n    links:\n      - web\n    networks:\n      - front-tier\n      - back-tier\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock \n\nnetworks:\n  front-tier:\n    driver: bridge\n  back-tier:\n    driver: bridge\n"],["body","\n"],["body","Compose目前有三个版本分别为Version 1，Version 2，Version 3，Compose区分Version 1和Version 2（Compose 1.6.0+，Docker Engine 1.10.0+）。Version 2支持更多的指令。Version 1将来会被弃用。"],["body","\n"],["headingLink","指令"],["heading","指令"],["body","\n"],["headingLink","image"],["heading","image"],["body","\n"],["body","\n"],["body","image是指定服务的镜像名称或镜像ID。如果镜像在本地不存在，Compose将会尝试拉取镜像。"],["body","\n"],["body","\n"],["body","services: \n    web: \n        image: hello-world\n        \n"],["body","\n"],["headingLink","build"],["heading","build"],["body","\n"],["body","服务除了可以基于指定的镜像，还可以基于一份Dockerfile，在使用up启动时执行构建任务，构建标签是build，可以指定Dockerfile所在文件夹的路径。Compose将会利用Dockerfile自动构建镜像，然后使用镜像启动服务容器。"],["body","\n"],["body","build:\n  context: ../\n  dockerfile: path/of/Dockerfile\n"],["body","\n"],["headingLink","context"],["heading","context"],["body","\n"],["body","context选项可以是Dockerfile的文件路径，也可以是到链接到git仓库的url，当提供的值是相对路径时，被解析为相对于撰写文件的路径，此目录也是发送到Docker守护进程的context"],["body","\n"],["headingLink","dockerfile"],["heading","dockerfile"],["body","\n"],["body","使用dockerfile文件来构建，必须指定构建路径"],["body","\n"],["body","build:\n  context: .\n  dockerfile: Dockerfile-alternate\n"],["body","\n"],["headingLink","command"],["heading","command"],["body","\n"],["body","#使用command可以覆盖容器启动后默认执行的命令。\ncommand: bundle exec thin -p 3000\n"],["body","\n"],["headingLink","container_name"],["heading","container_name"],["body","\n"],["body","Compose的容器名称格式是：<项目名称><服务名称><序号>\n可以自定义项目名称、服务名称，但如果想完全控制容器的命名，可以使用标签指定：\ncontainer_name: app"],["body","\n"],["headingLink","depends_on"],["heading","depends_on"],["body","\n"],["body","在使用Compose时，最大的好处就是少打启动命令，但一般项目容器启动的顺序是有要求的，如果直接从上到下启动容器，必然会因为容器依赖问题而启动失败。例如在没启动数据库容器的时候启动应用容器，应用容器会因为找不到数据库而退出。depends_on标签用于解决容器的依赖、启动先后的问题。"],["body","\n"],["body","version: '2'\nservices:\n  web:\n    build: .\n    depends_on:\n      - db\n      - redis\n  redis:\n    image: redis\n  db:\n    image: postgres\n"],["body","\n"],["body","上述YAML文件定义的容器会先启动redis和db两个服务，最后才启动web 服务。"],["body","\n"],["headingLink","pid"],["heading","pid"],["body","\n"],["body","pid: \"host\"\n将PID模式设置为主机PID模式，跟主机系统共享进程命名空间。容器使用pid标签将能够访问和操纵其他容器和宿主机的名称空间。"],["body","\n"],["headingLink","ports"],["heading","ports"],["body","\n"],["body","ports用于映射端口的标签。\n使用HOST:CONTAINER格式或者只是指定容器的端口，宿主机会随机映射端口。"],["body","\n"],["body","ports:\n - \"3000\"\n - \"8000:8000\"\n - \"49100:22\"\n - \"127.0.0.1:8001:8001\"\n"],["body","\n"],["body","当使用HOST:CONTAINER格式来映射端口时，如果使用的容器端口小于60可能会得到错误得结果，因为YAML将会解析xx:yy这种数字格式为60进制。所以建议采用字符串格式。"],["body","\n"],["headingLink","extra_hosts"],["heading","extra_hosts\t"],["body","\n"],["body","添加主机名的标签，会在/etc/hosts文件中添加一些记录。"],["body","\n"],["body","extra_hosts:\n - \"somehost:162.242.195.82\"\n - \"otherhost:50.31.209.229\"\n"],["body","\n"],["body","启动后查看容器内部hosts："],["body","\n"],["body","162.242.195.82  somehost\n50.31.209.229   otherhost\n"],["body","\n"],["headingLink","volumes"],["heading","volumes"],["body","\n"],["body","挂载一个目录或者一个已存在的数据卷容器，可以直接使用 [HOST:CONTAINER]格式，或者使用[HOST:CONTAINER:ro]格式，后者对于容器来说，数据卷是只读的，可以有效保护宿主机的文件系统。\nCompose的数据卷指定路径可以是相对路径，使用 . 或者 .. 来指定相对目录。\n数据卷的格式可以是下面多种形式："],["body","\n"],["body","volumes:\n  // 只是指定一个路径，Docker 会自动在创建一个数据卷（这个路径是容器内部的）。\n  - /var/lib/mysql\n  // 使用绝对路径挂载数据卷\n  - /opt/data:/var/lib/mysql\n  // 以 Compose 配置文件为中心的相对路径作为数据卷挂载到容器。\n  - ./cache:/tmp/cache\n  // 使用用户的相对路径（~/ 表示的目录是 /home/<用户目录>/ 或者 /root/）。\n  - ~/configs:/etc/configs/:ro\n  // 已经存在的命名的数据卷。\n  - datavolume:/var/lib/mysql\n"],["body","\n"],["body","如果不使用宿主机的路径，可以指定一个volume_driver。\nvolume_driver: mydriver"],["body","\n"],["headingLink","volumes_from"],["heading","volumes_from"],["body","\n"],["body","从另一个服务或容器挂载其数据卷："],["body","\n"],["body","volumes_from:\n   - service_name    \n     - container_name\n"],["body","\n"],["headingLink","dns"],["heading","dns"],["body","\n"],["body","自定义DNS服务器。可以是一个值，也可以是一个列表。"],["body","\n"],["body","dns：8.8.8.8\ndns：\n    - 8.8.8.8    \n      - 9.9.9.9\n"],["body","\n"],["headingLink","dns_search"],["heading","dns_search"],["body","\n"],["body","配置DNS搜索域。可以是一个值，也可以是一个列表。"],["body","\n"],["body","dns_search：example.com\ndns_search：\n    - domain1.example.com\n    - domain2.example.com\n"],["body","\n"],["headingLink","entrypoint"],["heading","entrypoint"],["body","\n"],["body","在Dockerfile中有一个指令叫做ENTRYPOINT指令，用于指定接入点。\n在docker-compose.yml中可以定义接入点，覆盖Dockerfile中的定义：\nentrypoint: /code/entrypoint.sh"],["body","\n"],["headingLink","env_file"],["heading","env_file"],["body","\n"],["body","在docker-compose.yml中可以定义一个专门存放变量的文件。\n如果通过docker-compose -f FILE指定配置文件，则env_file中路径会使用配置文件路径。\n如果有变量名称与environment指令冲突，则以后者为准。格式如下：\nenv_file: .env"],["body","\n"],["body","或者根据docker-compose.yml设置多个："],["body","\n"],["body","env_file:\n  - ./common.env\n  - ./apps/web.env\n  - /opt/secrets.env\n"],["body","\n"],["body","如果在配置文件中有build操作，变量并不会进入构建过程中。"],["body","\n"],["headingLink","expose"],["heading","expose"],["body","\n"],["body","暴露端口，但不映射到宿主机，只允许能被连接的服务访问。仅可以指定内部端口为参数，如下所示："],["body","\n"],["body","expose:\n    - \"3000\"\n    - \"8000\"\n"],["body","\n"],["headingLink","extends"],["heading","extends"],["body","\n"],["body","例如，对于webapp服务定义了一个基础模板文件为common.yml："],["body","\n"],["body","# common.yml\nwebapp:\n    build: ./webapp\n    environment:\n        - DEBUG=false\n        - SEND_EMAILS=false\n"],["body","\n"],["body","再编写一个新的development.yml文件，使用common.yml中的webapp服务进行扩展："],["body","\n"],["body","# development.yml\nweb:\n    extends:\n        file: common.yml\n        service: webapp\n    ports:\n        - \"8000:8000\"\n    links:\n        - db\n    environment:\n        - DEBUG=true\ndb:\n    image: mysql\n"],["body","\n"],["body","后者会自动继承common.yml中的webapp服务及环境变量定义。\nextends限制如下：\nA、要避免出现循环依赖\nB、extends不会继承links和volumes_from中定义的容器和数据卷资源\n推荐在基础模板中只定义一些可以共享的镜像和环境变量，在扩展模板中具体指定应用变量、链接、数据卷等信息"],["body","\n"],["headingLink","labels"],["heading","labels"],["body","\n"],["body","为容器添加Docker元数据（metadata）信息。例如，可以为容器添加辅助说明信息："],["body","\n"],["body","labels：\n    com.startupteam.description: \"webapp for a strtup team\"\n"],["body","\n"],["headingLink","links"],["heading","links"],["body","\n"],["body","链接到其它服务中的容器。使用服务名称（同时作为别名），或者“服务名称:服务别名”（如 SERVICE:ALIAS），例如："],["body","\n"],["body","links:\n    - db\n    - db:database\n    - redis\n"],["body","\n"],["body","使用别名将会自动在服务容器中的/etc/hosts里创建。例如："],["body","\n"],["body","172.17.2.186  db\n172.17.2.186  database\n172.17.2.187  redis\n"],["body","\n"],["headingLink","log_driver"],["heading","log_driver"],["body","\n"],["body","指定日志驱动类型。目前支持三种日志驱动类型："],["body","\n"],["body","log_driver: \"json-file\"\nlog_driver: \"syslog\"\nlog_driver: \"none\"\n"],["body","\n"],["headingLink","net"],["heading","net"],["body","\n"],["body","设置网络模式。"],["body","\n"],["body","net: \"bridge\"\nnet: \"none\"\nnet: \"host\"\n"],["body","\n"],["headingLink","环境变量"],["heading","环境变量"],["body","\n"],["body","环境变量可以用来配置Docker-Compose的行为。\nCOMPOSE_PROJECT_NAME\n设置通过Compose启动的每一个容器前添加的项目名称，默认是当前工作目录的名字。\nCOMPOSE_FILE\n设置docker-compose.yml模板文件的路径。默认路径是当前工作目录。\nDOCKER_HOST\n设置Docker daemon的地址。默认使用unix:///var/run/docker.sock。 DOCKER_TLS_VERIFY\n如果设置不为空，则与Docker daemon交互通过TLS进行。\nDOCKER_CERT_PATH\n配置TLS通信所需要的验证(ca.pem、cert.pem 和 key.pem)文件的路径，默认是 ~/.docker 。"],["body","\n"],["headingLink","命令行"],["heading","命令行"],["body","\n"],["headingLink","docker-compose命令格式"],["heading","Docker-Compose命令格式"],["body","\n"],["body","docker-compose [-f <arg>...] [options] [COMMAND] [ARGS...]\n"],["body","\n"],["body","命令选项如下：\n-f，–file FILE指定Compose模板文件，默认为docker-compose.yml，可以多次指定。\n-p，–project-name NAME指定项目名称，默认将使用所在目录名称作为项目名。\n-x-network-driver 使用Docker的可拔插网络后端特性（需要Docker 1.9+版本）\n-x-network-driver DRIVER指定网络后端的驱动，默认为bridge（需要Docker 1.9+版本）\n-verbose输出更多调试信息\n-v，–version打印版本并退出"],["body","\n"],["headingLink","docker-compose-up"],["heading","docker-compose up"],["body","\n"],["body","docker-compose up [options] [--scale SERVICE=NUM...] [SERVICE...]\n"],["body","\n"],["body","选项包括：\n-d 在后台运行服务容器\n–no-color 不使用颜色来区分不同的服务的控制输出\n–no-deps 不启动服务所链接的容器\n–force-recreate 强制重新创建容器，不能与–no-recreate同时使用\n–no-recreate 如果容器已经存在，则不重新创建，不能与–force-recreate同时使用\n–no-build 不自动构建缺失的服务镜像\n–build 在启动容器前构建服务镜像\n–abort-on-container-exit 停止所有容器，如果任何一个容器被停止，不能与-d同时使用\n-t, –timeout TIMEOUT 停止容器时候的超时（默认为10秒）\n–remove-orphans 删除服务中没有在compose文件中定义的容器\n–scale SERVICE=NUM 设置服务运行容器的个数，将覆盖在compose中通过scale指定的参数\n`docker-compose up`\n启动所有服务\n`docker-compose up -d`\n在后台所有启动服务\n-f 指定使用的Compose模板文件，默认为docker-compose.yml，可以多次指定。\n`docker-compose -f docker-compose.yml up -d`\n"],["body","\n"],["headingLink","docker-compose-ps"],["heading","docker-compose ps"],["body","\n"],["body","`docker-compose ps [options] [SERVICE...]`\n`docker-compose ps`\n列出项目中目前的所有容器\n"],["body","\n"],["headingLink","docker-compose-stop"],["heading","docker-compose stop"],["body","\n"],["body","docker-compose stop [options] [SERVICE...]\n选项包括：\n-t, –timeout TIMEOUT 停止容器时候的超时（默认为10秒）\ndocker-compose stop\n停止正在运行的容器，可以通过docker-compose start 再次启动\n"],["body","\n"],["headingLink","docker-compose--h"],["heading","docker-compose -h"],["body","\n"],["body","docker-compose -h\n查看帮助"],["body","\n"],["headingLink","docker-compose-down"],["heading","docker-compose down"],["body","\n"],["body","停止和删除容器、网络、卷、镜像。\n选项包括：\n–rmi type，删除镜像，类型必须是：all，删除compose文件中定义的所有镜像；local，删除镜像名为空的镜像\n-v, –volumes，删除已经在compose文件中定义的和匿名的附在容器上的数据卷\n–remove-orphans，删除服务中没有在compose中定义的容器\ndocker-compose down\n停用移除所有容器以及网络相关"],["body","\n"],["headingLink","docker-compose-logs"],["heading","docker-compose logs"],["body","\n"],["body","docker-compose logs [options] [SERVICE...]\n查看服务容器的输出。默认情况下，docker-compose将对不同的服务输出使用不同的颜色来区分。可以通过–no-color来关闭颜色。\ndocker-compose logs\n查看服务容器的输出"],["body","\n"],["headingLink","docker-compose-build"],["heading","docker-compose build"],["body","\n"],["body","docker-compose build [options] [--build-arg key=val...] [SERVICE...]\n构建（重新构建）项目中的服务容器。\n选项包括：\n–compress 通过gzip压缩构建上下环境\n–force-rm 删除构建过程中的临时容器\n–no-cache 构建镜像过程中不使用缓存\n–pull 始终尝试通过拉取操作来获取更新版本的镜像\n-m, –memory MEM为构建的容器设置内存大小\n–build-arg key=val为服务设置build-time变量\n服务容器一旦构建后，将会带上一个标记名。可以随时在项目目录下运行docker-compose build来重新构建服务"],["body","\n"],["headingLink","docker-compose-pull"],["heading","docker-compose pull"],["body","\n"],["body","docker-compose pull [options] [SERVICE...]\n拉取服务依赖的镜像。\n选项包括：\n–ignore-pull-failures，忽略拉取镜像过程中的错误\n–parallel，多个镜像同时拉取\n–quiet，拉取镜像过程中不打印进度信息\ndocker-compose pull\n拉取服务依赖的镜像"],["body","\n"],["headingLink","docker-compose-restart"],["heading","docker-compose restart"],["body","\n"],["body","docker-compose restart [options] [SERVICE...]\n重启项目中的服务。\n选项包括：\n-t, –timeout TIMEOUT，指定重启前停止容器的超时（默认为10秒）\ndocker-compose restart\n重启项目中的服务"],["body","\n"],["headingLink","docker-compose-rm"],["heading","docker-compose rm"],["body","\n"],["body","docker-compose rm [options] [SERVICE...]\n删除所有（停止状态的）服务容器。\n选项包括：\n–f, –force，强制直接删除，包括非停止状态的容器\n-v，删除容器所挂载的数据卷\ndocker-compose rm\n删除所有（停止状态的）服务容器。推荐先执行docker-compose stop命令来停止容器。"],["body","\n"],["headingLink","docker-compose-start"],["heading","docker-compose start"],["body","\n"],["body","docker-compose start [SERVICE...]\ndocker-compose start\n启动已经存在的服务容器。"],["body","\n"],["headingLink","docker-compose-run"],["heading","docker-compose run"],["body","\n"],["body","docker-compose run [options] [-v VOLUME...] [-p PORT...] [-e KEY=VAL...] SERVICE [COMMAND] [ARGS...]\n在指定服务上执行一个命令。\ndocker-compose run ubuntu ping www.baidu.com\n在指定容器上执行一个ping命令。"],["body","\n"],["headingLink","docker-compose-scale"],["heading","docker-compose scale"],["body","\n"],["body","docker-compose scale web=3 db=2\n设置指定服务运行的容器个数。通过service=num的参数来设置数量"],["body","\n"],["headingLink","docker-compose-pause"],["heading","docker-compose pause"],["body","\n"],["body","docker-compose pause [SERVICE...]\n暂停一个服务容器"],["body","\n"],["headingLink","docker-compose-kill"],["heading","docker-compose kill"],["body","\n"],["body","docker-compose kill [options] [SERVICE...]`\n通过发送SIGKILL信号来强制停止服务容器。 \n支持通过-s参数来指定发送的信号，例如通过如下指令发送SIGINT信号：\n`docker-compose kill -s SIGINT\n"],["body","\n"],["headingLink","dokcer-compose-config"],["heading","dokcer-compose config"],["body","\n"],["body","docker-compose config [options]\n验证并查看compose文件配置。\n选项包括：\n–resolve-image-digests 将镜像标签标记为摘要\n-q, –quiet 只验证配置，不输出。 当配置正确时，不输出任何内容，当文件配置错误，输出错误信息\n–services 打印服务名，一行一个\n–volumes 打印数据卷名，一行一个"],["body","\n"],["headingLink","docker-compose-create"],["heading","docker-compose create"],["body","\n"],["body","docker-compose create [options] [SERVICE...]\n为服务创建容器。\n选项包括：\n–force-recreate：重新创建容器，即使配置和镜像没有改变，不兼容–no-recreate参数\n–no-recreate：如果容器已经存在，不需要重新创建，不兼容–force-recreate参数\n–no-build：不创建镜像，即使缺失\n–build：创建容器前，生成镜像"],["body","\n"],["headingLink","docker-compose-exec"],["heading","docker-compose exec"],["body","\n"],["body","docker-compose exec [options] SERVICE COMMAND [ARGS...]\n选项包括：\n-d 分离模式，后台运行命令。\n–privileged 获取特权。\n–user USER 指定运行的用户。\n-T 禁用分配TTY，默认docker-compose exec分配TTY。\n–index=index，当一个服务拥有多个容器时，可通过该参数登陆到该服务下的任何服务，例如：docker-compose exec –index=1 web /bin/bash ，web服务中包含多个容器"],["body","\n"],["headingLink","docker-compose-port"],["heading","docker-compose port"],["body","\n"],["body","docker-compose port [options] SERVICE PRIVATE_PORT\n显示某个容器端口所映射的公共端口。\n选项包括：\n–protocol=proto，指定端口协议，TCP（默认值）或者UDP\n–index=index，如果同意服务存在多个容器，指定命令对象容器的序号（默认为1）"],["body","\n"],["headingLink","docker-compose-push"],["heading","docker-compose push"],["body","\n"],["body","docker-compose push [options] [SERVICE...]\n推送服务依的镜像。\n选项包括：\n–ignore-push-failures 忽略推送镜像过程中的错误"],["body","\n"],["headingLink","docker-compose-show"],["heading","docker-compose show"],["body","\n"],["body","docker-compose stop [options] [SERVICE...]\n暂停"],["body","\n"],["headingLink","docker-compose-unpause"],["heading","docker-compose unpause"],["body","\n"],["body","docker-compose unpause [SERVICE...]\n恢复处于暂停状态中的服务。"],["body","\n"],["headingLink","docker-compose-version"],["heading","docker-compose version"],["body","\n"],["body","docker-compose version\n打印版本信息。"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","1.容器_docker/DockerFile简介.html"],["title","DockerFile简介 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","基本结构"],["heading","基本结构"],["body","\n"],["body","Dockerfile 由一行行命令语句组成，并且支持以 # 开头的注释行。"],["body","\n"],["body","一般的，Dockerfile 分为四部分："],["body","\n\n"],["body","\n"],["body","基础镜像信息"],["body","\n"],["body","\n"],["body","\n"],["body","维护者信息"],["body","\n"],["body","\n"],["body","\n"],["body","镜像操作指令"],["body","\n"],["body","\n"],["body","\n"],["body","容器启动时执行指令。"],["body","\n"],["body","\n\n"],["body","例如："],["body","\n"],["body","# This dockerfile uses the ubuntu image\n# VERSION 2 - EDITION 1\n# Author: docker_user\n# Command format: Instruction [arguments / command] ..\n\n# Base image to use, this must be set as the first line\nFROM ubuntu\n\n# Maintainer: docker_user <docker_user at email.com> (@docker_user)\nMAINTAINER docker_user docker_user@email.com\n\n# Commands to update the image\nRUN echo \"deb http://archive.ubuntu.com/ubuntu/ raring main universe\" >> /etc/apt/sources.list\nRUN apt-get update && apt-get install -y nginx\nRUN echo \"\\ndaemon off;\" >> /etc/nginx/nginx.conf\n\n# Commands when creating a new container\nCMD /usr/sbin/nginx\n"],["body","\n"],["headingLink","指令"],["heading","指令"],["body","\n"],["body","指令的一般格式为 INSTRUCTION arguments，指令包括 FROM、MAINTAINER、RUN 等。"],["body","\n"],["headingLink","from"],["heading","FROM"],["body","\n"],["body","语法"],["body","\n"],["body","FROM <image>或FROM <image>:<tag>。"],["body","\n"],["headingLink","maintainer"],["heading","MAINTAINER"],["body","\n"],["body","MAINTAINER "],["body","\n"],["headingLink","run"],["heading","RUN"],["body","\n"],["body","RUN <command> 或 RUN [\"executable\", \"param1\", \"param2\"]"],["body","\n"],["body","前者将在 shell 终端中运行命令，即 /bin/sh -c；后者则使用 exec 执行。指定使用其它终端可以通过第二种方式实现，例如 RUN [\"/bin/bash\", \"-c\", \"echo hello\"]。"],["body","\n"],["body","每条 RUN 指令将在当前镜像基础上执行指定命令，并提交为新的镜像。当命令较长时可以使用 \\ 来换行。"],["body","\n"],["headingLink","cmd"],["heading","CMD"],["body","\n"],["body","支持三种格式"],["body","\n\n"],["body","CMD [\"executable\",\"param1\",\"param2\"] 使用 exec 执行，推荐方式；"],["body","\n"],["body","CMD command param1 param2 在 /bin/sh 中执行，提供给需要交互的应用；"],["body","\n"],["body","CMD [\"param1\",\"param2\"] 提供给 ENTRYPOINT 的默认参数；"],["body","\n\n"],["body","指定启动容器时执行的命令，每个 Dockerfile 只能有一条 CMD 命令。如果指定了多条命令，只有最后一条会被执行。"],["body","\n"],["body","如果用户启动容器时候指定了运行的命令，则会覆盖掉 CMD 指定的命令。"],["body","\n"],["headingLink","expose"],["heading","EXPOSE"],["body","\n"],["body","格式为 EXPOSE <port> [<port>...]。"],["body","\n"],["body","告诉 Docker 服务端容器暴露的端口号，供互联系统使用。在启动容器时需要通过 -P，Docker 主机会自动分配一个端口转发到指定的端口。"],["body","\n"],["headingLink","env"],["heading","ENV"],["body","\n"],["body","格式为 ENV <key> <value>。 指定一个环境变量，会被后续 RUN 指令使用，并在容器运行时保持。"],["body","\n"],["body","ENV PG_MAJOR 9.3\nENV PG_VERSION 9.3.4\nRUN curl -SL http://example.com/postgres-$PG_VERSION.tar.xz | tar -xJC /usr/src/postgress && …\nENV PATH /usr/local/postgres-$PG_MAJOR/bin:$PATH\n"],["body","\n"],["headingLink","add"],["heading","ADD"],["body","\n"],["body","格式为 ADD <src> <dest>。"],["body","\n"],["body","该命令将复制指定的 <src> 到容器中的 <dest>。 其中 <src> 可以是Dockerfile所在目录的一个相对路径；也可以是一个 URL；还可以是一个 tar 文件（自动解压为目录）。"],["body","\n"],["headingLink","copy"],["heading","COPY"],["body","\n"],["body","格式为 COPY <src> <dest>。"],["body","\n"],["body","复制本地主机的 <src>（为 Dockerfile 所在目录的相对路径）到容器中的 <dest>。"],["body","\n"],["body","当使用本地目录为源目录时，推荐使用 COPY。"],["body","\n"],["headingLink","entrypoint"],["heading","ENTRYPOINT"],["body","\n"],["body","两种格式："],["body","\n\n"],["body","ENTRYPOINT [\"executable\", \"param1\", \"param2\"]"],["body","\n"],["body","ENTRYPOINT command param1 param2（shell中执行）。"],["body","\n\n"],["body","配置容器启动后执行的命令，并且不可被 docker run 提供的参数覆盖。"],["body","\n"],["body","每个 Dockerfile 中只能有一个 ENTRYPOINT，当指定多个时，只有最后一个起效。"],["body","\n"],["headingLink","volume"],["heading","VOLUME"],["body","\n"],["body","格式为 VOLUME [\"/data\"]。"],["body","\n"],["body","创建一个可以从本地主机或其他容器挂载的挂载点，一般用来存放数据库和需要保持的数据等。"],["body","\n"],["headingLink","user"],["heading","USER"],["body","\n"],["body","格式为 USER daemon。"],["body","\n"],["body","指定运行容器时的用户名或 UID，后续的 RUN 也会使用指定用户。"],["body","\n"],["body","当服务不需要管理员权限时，可以通过该命令指定运行用户。并且可以在之前创建所需要的用户，例如：RUN groupadd -r postgres && useradd -r -g postgres postgres。要临时获取管理员权限可以使用 gosu，而不推荐 sudo。"],["body","\n"],["headingLink","workdir"],["heading","WORKDIR"],["body","\n"],["body","格式为 WORKDIR /path/to/workdir。"],["body","\n"],["body","为后续的 RUN、CMD、ENTRYPOINT 指令配置工作目录。"],["body","\n"],["body","可以使用多个 WORKDIR 指令，后续命令如果参数是相对路径，则会基于之前命令指定的路径。例如"],["body","\n"],["body","WORKDIR /a\nWORKDIR b\nWORKDIR c\nRUN pwd\n"],["body","\n"],["body","则最终路径为 /a/b/c。"],["body","\n"],["headingLink","onbuild"],["heading","ONBUILD"],["body","\n"],["body","格式为 ONBUILD [INSTRUCTION]。"],["body","\n"],["body","配置当所创建的镜像作为其它新创建镜像的基础镜像时，所执行的操作指令。"],["body","\n"],["body","例如，Dockerfile 使用如下的内容创建了镜像 image-A。"],["body","\n"],["body","利用ONBUILD指令,实际上就是相当于创建一个模板镜像，后续可以根据该模板镜像创建特定的子镜像，需要在子镜像构建过程中执行的一些通用操作就可以在模板镜像对应的dockerfile文件中用ONBUILD指令指定。 从而减少dockerfile文件的重复内容编写。"],["body","\n"],["body","[...]\nONBUILD ADD . /app/src\nONBUILD RUN /usr/local/bin/python-build --dir /app/src\n[...]\n"],["body","\n"],["body","使用 ONBUILD 指令的镜像，推荐在标签中注明，例如 ruby:1.9-onbuild。"],["body","\n"],["headingLink","命令总结"],["heading","命令总结"],["body","\n"],["body","对外"],["body","\n\n"],["body","端口暴露"],["body","\n"],["body","路径挂载"],["body","\n\n"],["body","中间"],["body","\n\n"],["body","环境变量"],["body","\n"],["body","文件拷贝"],["body","\n\n"],["body","对内"],["body","\n\n"],["body","镜像操作"],["body","\n"],["body","执行用户"],["body","\n"],["body","工作目录"],["body","\n"],["body","进程执行"],["body","\n\n"],["body","其他"],["body","\n\n"],["body","维护者信息"],["body","\n"],["body","模板镜像"],["body","\n\n"],["headingLink","创建镜像"],["heading","创建镜像"],["body","\n"],["body","docker build [选项] 路径"],["body","\n"],["body","sudo docker build -t myrepo/myapp /tmp/test1/\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","1.容器_docker/使用实例.html"],["title","使用实例 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","docker安装"],["heading","docker安装"],["body","\n"],["body","#安装仓库管理工具\nyum install -y yum-utils\n# 添加阿里云docker仓库\nyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n# 查看docker版本\nyum list docker-ce --showduplicates\n# 安装docker\nyum install docker-ce\n"],["body","\n"],["headingLink","mysql"],["heading","mysql"],["body","\n"],["body","docker hub 链接：https://hub.docker.com/_/mysql"],["body","\n"],["headingLink","docker拉取"],["heading","docker拉取"],["body","\n"],["body","docker pull mysql\n"],["body","\n"],["headingLink","docker从镜像启动容器"],["heading","docker从镜像启动容器"],["body","\n"],["body","docker stop mysql1\ndocker rm mysql1\ndocker container  run   --name  mysql1 --privileged=true     -v /data/mysqldata:/var/lib/mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=root -d mysql:5.7.34  --lower_case_table_names=1\n"],["body","\n"],["headingLink","docker容器管理"],["heading","docker容器管理"],["body","\n"],["body","docker container restart mysql1\ndocker container start mysql1\ndocker container stop mysql1\n"],["body","\n"],["headingLink","直连mysql服务"],["heading","直连mysql服务"],["body","\n"],["body","查询容器IP"],["body","\n"],["body","docker inspect 容器ID | grep IPAddress\ndocker inspect --format='{{.NetworkSettings.IPAddress}}' mycentos3\n"],["body","\n"],["body","启动一个mysql客户端连接"],["body","\n"],["body","docker run -it --rm  mysql:5.7.34   mysql -uroot -proot -h172.17.0.2\n"],["body","\n"],["headingLink","删除无关容器"],["heading","删除无关容器"],["body","\n"],["body","未命名容器"],["body","\n"],["body","docker rmi $(docker images --filter dangling=true -q)\n"],["body","\n"],["headingLink","修改数据源存放路径"],["heading","修改数据源存放路径"],["body","\n"],["body","vi /etc/docker/daemon.json \n{\n  \"data-root\": \"/www/docker\"\n}\nsystemctl restart docker\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","1.容器_docker/docker-volume管理.html"],["title","docker-volume管理 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","use-volumes"],["heading","Use volumes"],["body","\n"],["body","\n"],["body","使用卷"],["body","\n"],["body","\n"],["body","卷是持久化 Docker 容器生成和使用的数据的首选机制。\n虽然绑定挂载依赖于主机的目录结构和操作系统，但卷完全由 Docker 管理。\n与绑定安装相比，卷有几个优点："],["body","\n\n"],["body","卷比绑定安装更容易备份或迁移。"],["body","\n"],["body","您可以使用 Docker CLI 命令或 Docker API 管理卷。 "],["body","\n"],["body","卷适用于 Linux 和 Windows 容器。"],["body","\n"],["body","可以更安全地在多个容器之间共享卷"],["body","\n"],["body","卷驱动程序允许您将卷存储在远程主机或云提供商上，以加密卷的内容或添加其他功能。"],["body","\n"],["body","新卷的内容可以由容器预先填充"],["body","\n"],["body","Volumes on Docker Desktop具有比来自 Mac 和 Windows 主机的绑定挂载更高的性能"],["body","\n\n"],["body","此外，与在容器的可写层中持久化数据相比，卷通常是更好的选择，因为卷不会增加使用它的容器的大小，并且卷的内容存在于给定容器的生命周期之外。"],["body","\n"],["body","\n"],["body","如果您的容器生成非持久状态数据，请考虑使用 tmpfs 挂载以避免将数据永久存储在任何地方，并通过避免写入容器的可写层来提高容器的性能。\nVolumes use rprivate bind propagation, and bind propagation is not configurable for volumes."],["body","\n"],["headingLink","choose-the--v-or---mount-flag"],["heading","Choose the -v or --mount flag"],["body","\n"],["body","一般来说，--mount 更明确和详细。\n最大的区别是 -v 语法将所有选项组合在一个字段中，而 --mount 语法将它们分开。\n这是每个标志的语法比较。\n如果需要指定卷驱动程序选项，则必须使用 --mount。"],["body","\n\n"],["body","\n"],["body","v or --volume ： 由三个字段组成，以冒号字符 (:) 分隔。字段必须按正确顺序排列，每个字段的含义并不是很明显。"],["body","\n\n"],["body","在命名卷的情况下，第一个字段是卷的名称，并且在给定的主机上是唯一的。对于匿名卷，第一个字段被省略。"],["body","\n"],["body","第二个字段是文件或目录在容器中挂载的路径"],["body","\n"],["body","第三个字段是可选的，是一个以逗号分隔的选项列表，例如 ro。"],["body","\n\n"],["body","\n"],["body","\n"],["body","--mount："],["body","\n"],["body","\n\n"],["body","由多个键值对组成，以逗号分隔，每个键值对由一个 = 元组组成。 \n--mount 语法比 -v 或 --volume 更冗长，但键的顺序并不重要，标志的值更容易理解。"],["body","\n\n"],["body","type 挂载的类型， 可以是 bind、volume 或 tmpfs。This topic discusses volumes, so the type is always volume."],["body","\n"],["body","source 挂载源 ：对于命名卷，这是卷的名称，对于匿名卷，此字段被省略。\n可以指定为 source 或 src。"],["body","\n"],["body","destination  将文件或目录安装在容器中的路径作为其值。\n可以指定为  destination、dst 或 target。"],["body","\n"],["body","readonly   if present, causes the bind mount to be mounted into the container as read-only."],["body","\n"],["body","volume-opt  可以多次指定，它采用由选项名称及其值组成的键值对。"],["body","\n\n"],["body","从外部 CSV 解析器转义值"],["body","\n"],["body","如果您的卷驱动程序接受逗号分隔列表作为选项，您必须从外部 CSV 解析器中转义该值。\n要对 volume-opt 进行转义，请用双引号 (\") 将其括起来，并用单引号 (') 将整个挂载参数括起来。"],["body","\n"],["body","$ docker service create \\\n--mount \n'type=volume,src=<VOLUME-NAME>,dst=<CONTAINER-PATH>,volume-driver=local,volume-opt=type=nfs,volume-opt=device=<nfs-server>:<nfs-path>,\"volume-opt=o=addr=<nfs-address>,vers=4,soft,timeo=180,bg,tcp,rw\"'\n    --name myservice \\\n    <IMAGE>\n"],["body","\n"],["headingLink","create-and-manage-volumes"],["heading","Create and manage volumes"],["body","\n"],["body","Create a volume:"],["body","\n"],["body","# 自动创建卷 时会指定 默认挂载位置\n# ${docker_root_dir}/volumes/my-vol/_data\n$ docker volume create my-vol\n"],["body","\n"],["body","List volumes:"],["body","\n"],["body","docker volume ls\n"],["body","\n"],["body","Inspect a volume:"],["body","\n"],["body","$ docker volume inspect my-vol\n[\n    {\n        \"Driver\": \"local\",\n        \"Labels\": {},\n        \"Mountpoint\": \"/var/lib/docker/volumes/my-vol/_data\",\n        \"Name\": \"my-vol\",\n        \"Options\": {},\n        \"Scope\": \"local\"\n    }\n]\n"],["body","\n"],["body","Remove a volume:"],["body","\n"],["body","$ docker volume rm my-vol\n"],["body","\n"],["headingLink","启动一个带卷的容器"],["heading","启动一个带卷的容器"],["body","\n"],["headingLink","使用命令指定"],["heading","使用命令指定"],["body","\n"],["body","如果您使用尚不存在的卷启动容器，Docker 会为您创建该卷。\n以下示例将卷 myvol2 安装到容器中的 /app/ 中。"],["body","\n"],["body","$ docker run -d \\\n  --name devtest \\\n  --mount source=myvol2,target=/app \\\n  nginx:latest\n"],["body","\n"],["headingLink","docker-compose指定"],["heading","docker-compose指定"],["body","\n"],["body","在 首次 调用过程中 会自动创建"],["body","\n"],["body","version: \"3.9\"\nservices:\n  frontend:\n    image: node:lts\n    volumes:\n      - myapp:/home/node/app\nvolumes:\n  myapp:\n"],["body","\n"],["headingLink","start-a-service-with-volumes"],["heading","Start a service with volumes"],["body","\n"],["body","$ docker service create -d \\\n  --replicas=4 \\\n  --name devtest-service \\\n  --mount source=myvol2,target=/app \\\n  nginx:latest\n"],["body","\n"],["body","删除该服务不会删除该服务创建的任何卷。\n卷删除是一个单独的步骤。"],["body","\n"],["body","docker service create 命令不支持 -v 或 --volume 标志。\n将卷挂载到服务的容器中时，您必须使用 --mount 标志。"],["body","\n"],["headingLink","use-a-read-only-volume"],["heading","Use a read-only volume"],["body","\n"],["body","多个容器可以挂载同一个卷，并且可以同时为其中一些容器以读写方式挂载，对其他容器以只读方式挂载"],["body","\n"],["body","$ docker run -d \\\n  --name=nginxtest \\\n  --mount source=nginx-vol,destination=/usr/share/nginx/html,readonly \\\n  nginx:latest\n"],["body","\n"],["headingLink","在机器之间共享数据"],["heading","在机器之间共享数据"],["body","\n"],["body","构建容错应用程序时，您可能需要配置同一服务的多个副本才能访问相同的文件"],["body","\n"],["body","在开发应用程序时，有多种方法可以实现这一点。\n一种是向您的应用程序添加逻辑，以将文件存储在 Amazon S3 等云对象存储系统上。\n另一种方法是使用支持将文件写入外部存储系统（如 NFS 或 Amazon S3）的驱动程序创建卷。"],["body","\n"],["body","卷驱动程序允许您从应用程序逻辑中抽象出底层存储系统。\n例如，如果您的服务使用带有 NFS 驱动程序的卷，您可以更新服务以使用不同的驱动程序，例如将数据存储在云中，而无需更改应用程序逻辑。"],["body","\n"],["headingLink","use-a-volume-driver"],["heading","Use a volume driver"],["body","\n"],["body","当您使用 docker volume create 创建卷时，或者当您启动使用尚未创建的卷的容器时，您可以指定卷驱动程序。\n以下示例首先在创建独立卷时使用 vieux/sshfs 卷驱动程序，然后在启动创建新卷的容器时使用。"],["body","\n"],["headingLink","initial-set-up"],["heading","Initial set-up"],["body","\n"],["body","在 Docker 主机上，安装 vieux/sshfs 插件："],["body","\n"],["body","$ docker plugin install --grant-all-permissions vieux/sshfs\n"],["body","\n"],["headingLink","create-a-volume-using-a-volume-driver"],["heading","Create a volume using a volume driver"],["body","\n"],["body","此示例指定了 SSH 密码，但如果两台主机配置了共享密钥，则可以省略密码。\n每个卷驱动程序可能有零个或多个可配置选项，每个选项都使用 -o 标志指定。"],["body","\n"],["body","docker volume create --driver vieux/sshfs \\\n  -o sshcmd=test@node2:/home/test \\\n  -o password=testpassword \\\n  sshvolume\n"],["body","\n"],["headingLink","创建-nfs-卷的服务"],["heading","创建 NFS 卷的服务"],["body","\n"],["body","此示例说明如何在创建服务时创建 NFS 卷。\n本示例使用 10.0.0.10 作为 NFS 服务器，使用 /var/docker-nfs 作为 NFS 服务器上的导出目录。\n请注意，指定的卷驱动程序是本地的。"],["body","\n"],["body","$ docker service create -d \\\n  --name nfs-service \\\n  --mount 'type=volume,source=nfsvolume,target=/app,volume-driver=local,volume-opt=type=nfs,volume-opt=device=:/var/docker-nfs,volume-opt=o=addr=10.0.0.10' \\\n  nginx:latest\n"],["body","\n"],["body","docker service create -d \\\n    --name nfs-service \\\n    --mount 'type=volume,source=nfsvolume,target=/app,volume-driver=local,volume-opt=type=nfs,volume-opt=device=:/var/docker-nfs,\"volume-opt=o=addr=10.0.0.10,rw,nfsvers=4,async\"' \\\n    nginx:latest\n"],["body","\n"],["headingLink","create-cifssamba-volumes"],["heading","Create CIFS/Samba volumes"],["body","\n"],["body","\n"],["body","请注意，如果使用主机名而不是 IP，则需要 addr 选项，以便 docker 可以执行主机名查找。"],["body","\n"],["body","\n"],["body","docker volume create \\\n\t--driver local \\\n\t--opt type=cifs \\\n\t--opt device=//uxxxxx.your-server.de/backup \\\n\t--opt o=addr=uxxxxx.your-server.de,username=uxxxxxxx,password=*****,file_mode=0777,dir_mode=0777 \\\n\t--name cif-volume\n"],["body","\n"],["body","手动创建"],["body","\n"],["body","docker volume create \\\n\t--driver local \\\n\t--opt type=cifs \\\n\t--opt device=//192.168.1.166/gitrepo/docker_test \\\n\t--opt o=username=networkshare,password=123456,file_mode=0777,dir_mode=0777 \\\n\t--name cif-volume\n"],["body","\n"],["body","docker-compose语法"],["body","\n"],["body","volumes:\n  cif-volume:\n    driver: local\n    driver_opts:\n      type: cifs\n      device: //192.168.1.166/gitrepo/docker_test\n      o:  username=networkshare,password=123456,file_mode=0777,dir_mode=0777\n"],["body","\n"],["headingLink","备份恢复或迁移数据卷"],["heading","备份、恢复或迁移数据卷"],["body","\n"],["headingLink","备份容器"],["heading","备份容器"],["body","\n"],["body","$ docker run -v /dbdata --name dbstore ubuntu /bin/bash\n\n$ docker run --rm --volumes-from dbstore -v $(pwd):/backup ubuntu tar cvf /backup/backup.tar /dbdata\n"],["body","\n"],["headingLink","从备份恢复容器"],["heading","从备份恢复容器"],["body","\n"],["body","$ docker run -v /dbdata --name dbstore2 ubuntu /bin/bash\n$ docker run --rm --volumes-from dbstore2 -v $(pwd):/backup ubuntu bash -c \"cd /dbdata && tar xvf /backup/backup.tar --strip 1\"\n"],["body","\n"],["headingLink","remove-volumes"],["heading","Remove volumes"],["body","\n"],["body","删除容器后，Docker 数据卷仍然存在。\n有两种类型的卷需要考虑："],["body","\n"],["body","命名卷具有来自容器外部的特定来源，例如 awesome:/bar。\n匿名卷没有特定的来源，所以当容器被删除时，指示 Docker 引擎守护进程将它们删除。"],["body","\n"],["headingLink","remove-anonymous-volumes"],["heading","Remove anonymous volumes"],["body","\n"],["body","要自动删除匿名卷，请使用 --rm 选项。\n例如，此命令创建匿名 /foo 卷。\n当容器被移除时，Docker 引擎会移除 /foo 卷而不是 awesome 卷。"],["body","\n"],["body","$ docker run --rm -v /foo -v awesome:/bar busybox top\n"],["body","\n"],["headingLink","remove-all-volumes"],["heading","Remove all volumes"],["body","\n"],["body","To remove all unused volumes and free up space:"],["body","\n"],["body","$ docker volume prune\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","1.容器_docker/docker容器连接.html"],["title","docker容器连接 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","连接到-已启动的容器中"],["heading","连接到 已启动的容器中"],["body","\n"],["headingLink","使用docker-attach进入docker容器"],["heading","使用docker attach进入Docker容器"],["body","\n"],["body","# 创建 守护进程\nsudo docker run -itd ubuntu:14.04 /bin/bash  \n# 连接该容器 的输入输出 到 宿主机的标准输入输出\nsudo docker attach 44fc0f0582d9  \n"],["body","\n"],["headingLink","使用ssh进入docker容器"],["heading","使用SSH进入Docker容器"],["body","\n"],["body","不推荐使用"],["body","\n"],["headingLink","使用nsenter进入docker容器"],["heading","使用nsenter进入Docker容器"],["body","\n"],["body","什么是 nsenter"],["body","\n"],["body","　　nsenter可以访问另一个进程的名称空间。所以为了连接到某个容器我们还需要获取该容器的第一个进程的PID。可以使用docker inspect命令来拿到该PID。"],["body","\n"],["body","docker inspect命令使用如下："],["body","\n"],["body","sudo docker inspect 44fc0f0582d9  \nsudo docker inspect -f {{.State.Pid}} 44fc0f0582d9  \nsudo nsenter --target 3326 --mount --uts --ipc --net --pid \n"],["body","\n"],["headingLink","使用docker-exec进入docker容器"],["heading","使用docker exec进入Docker容器"],["body","\n"],["body","交互式模式终端"],["body","\n"],["body","# i:交互式连接，t：分配一个伪终端\ndocker exec -i -t [容器名] /bin/bash\n"],["body","\n"],["body","运行容器内的脚本"],["body","\n"],["body","docker exec -it [容器名] /bin/sh /root/runoob.sh\n"],["body","\n"],["body","利用容器ID"],["body","\n"],["body","docker exec -it [容器ID] /bin/bash\n"],["body","\n"],["body","参考链接"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","1.容器_docker/index.html"],["title","docker - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["body","{% raw %}"],["body","\n"],["headingLink","简介"],["heading","简介"],["body","\n"],["body","Docker 是一种运行于 Linux 和 Windows 上的软件，用于创建、管理和编排容器。"],["body","\n"],["body","Docker 是在 GitHub 上开发的 Moby 开源项目的一部分。"],["body","\n"],["body","“Docker”一词来自英国口语，意为码头工人（Dock Worker），即从船上装卸货物的人。"],["body","\n"],["headingLink","docker-运行时与编排引擎"],["heading","Docker 运行时与编排引擎"],["body","\n"],["body","多数技术人员在谈到 Docker 时，主要是指 Docker 引擎。"],["body","\n"],["body","Docker 引擎是用于运行和编排容器的基础设施工具。有 VMware 管理经验的读者可以将其类比为 ESXi。"],["body","\n"],["body","ESXi 是运行虚拟机的核心管理程序，而 Docker 引擎是运行容器的核心容器运行时。"],["body","\n"],["body","其他 Docker 公司或第三方的产品都是围绕 Docker 引擎进行开发和集成的。"],["body","\n"],["body","如下图所示，Docker 引擎位于中心，其他产品基于 Docker 引擎的核心功能进行集成。"],["body","\n"],["body","Docker 引擎可以从 Docker 网站下载，也可以基于 GitHub 上的源码进行构建。无论是开源版本还是商业版本，都有 Linux 和 Windows 版本。"],["body","\n"],["body","Docker 引擎主要有两个版本：企业版（EE）和社区版（CE）。"],["body","\n"],["body","每个季度，企业版和社区版都会发布一个稳定版本。社区版本会提供 4 个月的支持，而企业版本会提供 12 个月的支持。"],["body","\n"],["body","社区版还会通过 Edge 方式发布月度版。"],["body","\n"],["body","从 2017 年第一季度开始，Docker 版本号遵循 YY.MM-xx 格式，类似于 Ubuntu 等项目。例如，2018 年 6 月第一次发布的社区版本为 18.06.0-ce。"],["body","\n"],["headingLink","docker开源项目moby"],["heading","Docker开源项目（Moby）"],["body","\n"],["body","Docker”一词也会用于指代开源 Docker 项目。其中包含一系列可以从 Docker 官网下载和安装的工具，比如 Docker 服务端和 Docker 客户端。"],["body","\n"],["body","不过，该项目在 2017 年于 Austin 举办的 DockerCon 上正式命名为 Moby 项目。"],["body","\n"],["body","由于这次改名，GitHub 上的 docker/docker 库也被转移到了 moby/moby，并且拥有了项目自己的 Logo，如下图所示。"],["body","\n"],["body","Moby 项目的目标是基于开源的方式，发展成为 Docker 上游，并将 Docker 拆分为更多的模块化组件。"],["body","\n"],["body","Moby 项目托管于 GitHub 的 Moby 代码库，包括子项目和工具列表。核心的 Docker 引擎项目位于 GitHub 的 moby/moby，但是引擎中的代码正持续被拆分和模块化。"],["body","\n"],["body","作为一个开源项目，其源码是公开可得的，在遵循 Apache 协议 2.0 的情况下，任何人都可以自由地下载、贡献、调整和使用。"],["body","\n"],["body","如果查看项目的提交历史，可以发现其中包含来自如下公司的基础技术：红帽、微软、IBM、思科，以及 HPE。此外，还可以看到一些并非来自大公司的贡献者。"],["body","\n"],["body","多数项目及其工具都是基于 Golang 编写的，这是谷歌推出的一种新的系统级编程语言，又叫 Go 语言。使用 Go 语言的读者，将更容易为该项目贡献代码。"],["body","\n"],["body","Mody/Docker 作为开源项目的好处在于其所有的设计和开发都是开放的，并摒弃了私有代码闭源开发模式下的陈旧方法。"],["body","\n"],["body","因此发布过程也是公开进行的，不会再出现某个秘密的版本提前几个月就宣布要召开发布会和庆功会的荒唐情况。"],["body","\n"],["body","Moby/Docker 不是这样运作的，项目中多数内容都是开放并欢迎任何人查看和作出贡献的。"],["body","\n"],["body","Moby 项目以及更广泛的 Docker 运动一时间掀起了一波热潮。GitHub 上已经有数以千计的提交请求（pull request），以及数以万计的基于容器化技术的项目了，更不用说 Docker Hub 上数十亿的镜像下载。"],["body","\n"],["body","Moby 项目已经给软件产业带来了翻天覆地的变化。"],["body","\n"],["body","这并非妄想，Docker 已经得到了广泛的应用！"],["body","\n"],["headingLink","容器生态"],["heading","容器生态"],["body","\n"],["body","Docker 公司的一个核心哲学通常被称为“含电池，但可拆卸”（Batteries included but removable）。"],["body","\n"],["body","意思是许多 Docker 内置的组件都可以替换为第三方的组件，网络技术栈就是一个很好的例子。"],["body","\n"],["body","Docker 核心产品内置有网络解决方案。但是网络技术栈是可插拔的，这意味着 Docker 内置的网络方案可以被替换为第三方的方案。许多人都会这样使用。"],["body","\n"],["body","早期的时候，经常出现第三方插件比 Docker 提供的内置组件更好的情况。然而这会对 Docker 公司的商业模式造成冲击。毕竟，Docker 公司需要依靠盈利来维持基业长青。"],["body","\n"],["body","因此，“内置的电池”变得越来越好用了。这也导致了生态内部的紧张关系和竞争的加剧。"],["body","\n"],["body","简单来说，Docker 内置的“电池”仍然是可插拔的，然而越来越不需要将它们移除了。"],["body","\n"],["body","尽管如此，容器生态在一种良性的合作与竞争的平衡中还是得以繁荣发展。"],["body","\n"],["body","在谈及容器生态时，人们经常使用到诸如“co-opetition”（意即合作与竞争，英文中 co-operation 与 competition 合并的词）与“frenemy”（英文中朋友 friend 与敌人 enemy 合并的词）这样的字眼。这是一个好现象！因为良性的竞争是创新之母。"],["body","\n"],["headingLink","开放容器计划"],["heading","开放容器计划"],["body","\n"],["body","如果不谈及开放容器计划（The Open Container Initiative, OCI）的话，对 Docker 和容器生态的探讨总是不完整的。"],["body","\n"],["headingLink","docker安装"],["heading","Docker安装"],["body","\n"],["body","$ curl https://get.docker.com/ | sh\n$ systemctl start docker\n$ useradd docker -g docker\n$ docker --version\n$ docker system info\n$ systemctl enable docker\n$ systemctl is-enabled docker\n$ systemctl is-active docker\n"],["body","\n"],["headingLink","镜像"],["heading","镜像"],["body","\n"],["headingLink","介绍"],["heading","介绍"],["body","\n"],["body","首先需要先从镜像仓库服务中拉取镜像。常见的镜像仓库服务是 Docker Hub，但是也存在其他镜像仓库服务。"],["body","\n"],["body","拉取操作会将镜像下载到本地 Docker 主机，可以使用该镜像启动一个或者多个容器。"],["body","\n"],["body","镜像由多个层组成，每层叠加之后，从外部看来就如一个独立的对象。镜像内部是一个精简的操作系统（OS），同时还包含应用运行所必须的文件和依赖包。"],["body","\n"],["body","通常使用docker container run和docker service create命令从某个镜像启动一个或多个容器。"],["body","\n"],["body","一旦容器从镜像启动后，二者之间就变成了互相依赖的关系，并且在镜像上启动的容器全部停止之前，镜像是无法被删除的。尝试删除镜像而不停止或销毁使用它的容器，会导致出错。"],["body","\n"],["headingLink","镜像通常比较小"],["heading","镜像通常比较小"],["body","\n"],["body","容器目的就是运行应用或者服务，这意味着容器的镜像中必须包含应用/服务运行所必需的操作系统和应用文件。"],["body","\n"],["body","但是，容器又追求快速和小巧，这意味着构建镜像的时候通常需要裁剪掉不必要的部分，保持较小的体积。"],["body","\n"],["body","例如，Docker 镜像通常不会包含 6 个不同的 Shell 让读者选择——通常 Docker 镜像中只有一个精简的Shell，甚至没有 Shell。"],["body","\n"],["body","镜像中还不包含内核——容器都是共享所在 Docker 主机的内核。所以有时会说容器仅包含必要的操作系统（通常只有操作系统文件和文件系统对象）。"],["body","\n"],["body","Docker 官方镜像 Alpine Linux 大约只有 4MB，可以说是 Docker 镜像小巧这一特点的比较典型的例子。"],["body","\n"],["headingLink","拉取镜像"],["heading","拉取镜像"],["body","\n"],["body","拉取"],["body","\n"],["body","docker image pull\ndocker pull ubuntu\n"],["body","\n"],["body","默认情况下，镜像会从 Docker Hub 的仓库中拉取。"],["body","\n"],["body","docker image pull alpine:latest \n"],["body","\n"],["body","命令会从 Docker Hub 的 alpine 仓库中拉取标签为 latest 的镜像。"],["body","\n"],["body","位置"],["body","\n"],["body","Linux Docker 主机本地镜像仓库通常位于 /var/lib/docker/<storage-driver>，Windows Docker 主机则是 C:\\ProgramData\\docker\\windowsfilter。"],["body","\n"],["body","查看镜像"],["body","\n"],["body","可以使用以下命令检查 Docker 主机的本地仓库中是否包含镜像。"],["body","\n"],["body","docker image ls\n"],["body","\n"],["headingLink","镜像仓库服务"],["heading","镜像仓库服务"],["body","\n"],["headingLink","镜像仓库服务-1"],["heading","镜像仓库服务"],["body","\n"],["body","Docker 镜像存储在镜像仓库服务（Image Registry）当中。"],["body","\n"],["body","Docker 客户端的镜像仓库服务是可配置的，默认使用 Docker Hub。"],["body","\n"],["body","镜像仓库服务包含多个镜像仓库（Image Repository）。同样，一个镜像仓库中可以包含多个镜像。"],["body","\n"],["body","\n"],["headingLink","官方和非官方镜像仓库"],["heading","官方和非官方镜像仓库"],["body","\n"],["body","Docker Hub 也分为官方仓库（Official Repository）和非官方仓库（Unofficial Repository）。"],["body","\n"],["body","官方仓库中的镜像是由 Docker 公司审查的。这意味着其中的镜像会及时更新，由高质量的代码构成，这些代码是安全的，有完善的文档和最佳实践。"],["body","\n"],["body","非官方仓库更像江湖侠客，其中的镜像不一定具备官方仓库的优点，但这并不意味着所有非官方仓库都是不好的！非官方仓库中也有一些很优秀的镜像。"],["body","\n"],["headingLink","镜像命名和标签"],["heading","镜像命名和标签"],["body","\n"],["body","只需要给出镜像的名字和标签，就能在官方仓库中定位一个镜像（采用“:”分隔）。从官方仓库拉取镜像时，docker image pull 命令的格式如下。"],["body","\n"],["body","docker image pull <repository>:<tag>\n"],["body","\n"],["body","$ docker image pull alpine:latest\n$ docker image pull ubuntu:latest\n"],["body","\n"],["body","这两条命令从 alpine 和 ubuntu 仓库拉取了标有“latest”标签的镜像。"],["body","\n"],["body","如果没有在仓库名称后指定具体的镜像标签，则 Docker 会假设用户希望拉取标签为 latest 的镜像。"],["body","\n"],["body","标签为 latest 的镜像没有什么特殊魔力！标有 latest 标签的镜像不保证这是仓库中最新的镜像！例如，Alpine 仓库中最新的镜像通常标签是 edge。通常来讲，使用 latest 标签时需要谨慎！"],["body","\n"],["body","从非官方仓库拉取镜像也是类似的，读者只需要在仓库名称面前加上 Docker Hub 的用户名或者组织名称。"],["body","\n"],["body","$ docker image pull nigelpoulton/tu-demo:v2\n"],["body","\n"],["headingLink","为镜像打多个标签"],["heading","为镜像打多个标签"],["body","\n"],["body","一个镜像可以根据用户需要设置多个标签。这是因为标签是存放在镜像元数据中的任意数字或字符串。"],["body","\n"],["body","在 docker image pull 命令中指定 -a 参数来拉取仓库中的全部镜像。接下来可以通过运行 docker image ls 查看已经拉取的镜像。"],["body","\n"],["headingLink","镜像查询过滤"],["heading","镜像查询过滤"],["body","\n"],["body","那些没有标签的镜像被称为悬虚镜像，在列表中展示为<none>:<none>"],["body","\n"],["body","$ docker image ls --filter dangling=true\nREPOSITORY TAG IMAGE ID CREATED SIZE\n<none> <none> 4fd34165afe0 7 days ago 14.5MB\n"],["body","\n"],["body","通常出现这种情况，是因为构建了一个新镜像，然后为该镜像打了一个已经存在的标签。"],["body","\n"],["body","当此情况出现，Docker 会构建新的镜像，然后发现已经有镜像包含相同的标签，接着 Docker 会移除旧镜像上面的标签，将该标签标在新的镜像之上。"],["body","\n"],["body","Docker 目前支持如下的过滤器。"],["body","\n\n"],["body","dangling：可以指定 true 或者 false，仅返回悬虚镜像（true），或者非悬虚镜像（false）。"],["body","\n"],["body","before：需要镜像名称或者 ID 作为参数，返回在之前被创建的全部镜像。"],["body","\n"],["body","since：与 before 类似，不过返回的是指定镜像之后创建的全部镜像。"],["body","\n"],["body","label：根据标注（label）的名称或者值，对镜像进行过滤。docker image ls命令输出中不显示标注内容。"],["body","\n\n"],["body","其他的过滤方式可以使用 reference。"],["body","\n"],["body","$ docker image ls --filter=reference=\"*:latest\"\nREPOSITORY TAG IMAGE ID CREATED SIZE\nalpine latest 3fd9065eaf02 8 days ago 4.15MB\ntest latest 8426e7efb777 3 days ago 122MB\n"],["body","\n"],["body","可以使用 --format 参数来通过 Go 模板对输出内容进行格式化。"],["body","\n"],["body","$ docker image ls --format \"{{.Size}}\"\n99.3MB\n111MB\n82.6MB\n88.8MB\n4.15MB\n108MB\n"],["body","\n"],["body","使用下面命令返回全部镜像，但是只显示仓库、标签和大小信息。"],["body","\n"],["body","$ docker image ls --format \"{{.Repository}}: {{.Tag}}: {{.Size}}\"\ndodge: challenger: 99.3MB\nubuntu: latest: 111MB\npython: 3.4-alpine: 82.6MB\npython: 3.5-alpine: 88.8MB\nalpine: latest: 4.15MB\nnginx: latest: 108MB\n"],["body","\n"],["headingLink","通过-cli-方式搜索-docker-hub"],["heading","通过 CLI 方式搜索 Docker Hub"],["body","\n"],["body","docker search 命令允许通过 CLI 的方式搜索 Docker Hub。可以通过“NAME”字段的内容进行匹配，并且基于返回内容中任意列的值进行过滤。"],["body","\n"],["body","简单模式下，该命令会搜索所有“NAME”字段中包含特定字符串的仓库。例如，下面的命令会查找所有“NAME”包含“nigelpoulton”的仓库。"],["body","\n"],["body","$ docker search nigelpoulton\n"],["body","\n"],["body","需要注意，上面返回的镜像中既有官方的也有非官方的。读者可以使用 --filter \"is-official=true\"，使命令返回内容只显示官方镜像。"],["body","\n"],["body","$ docker search alpine --filter \"is-official=true\"\nNAME DESCRIPTION STARS OFFICIAL AUTOMATED\nalpine A minimal Docker.. 2988 [OK]\n"],["body","\n"],["body","重复前面的操作，但这次只显示自动创建的仓库。"],["body","\n"],["body","$ docker search alpine --filter \"is-automated=true\"\nNAME DESCRIPTION OFFICIAL AUTOMATED\nanapsix/alpine-java Oracle Java 8 (and 7).. [OK]\nfrolvlad/alpine-glibc Alpine Docker image.. [OK]\nkiasaki/alpine-postgres PostgreSQL docker.. [OK]\nzzrot/alpine-caddy Caddy Server Docker.. [OK]\n<Snip>\n"],["body","\n"],["body","关于 docker search 需要注意的最后一点是，默认情况下，Docker 只返回 25 行结果。但是，可以通过指定 --limit 参数来增加返回内容行数，最多为 100 行。"],["body","\n"],["headingLink","镜像和分层"],["heading","镜像和分层"],["body","\n"],["body","Docker 镜像由一些松耦合的只读镜像层组成。如下图所示。"],["body","\n"],["body","\n"],["body","Docker 负责堆叠这些镜像层，并且将它们表示为单个统一的对象。"],["body","\n"],["body","查看镜像分层的方式可以通过 docker image inspect 命令。下面同样以 ubuntu:latest 镜像为例。"],["body","\n"],["body","$ docker image inspect ubuntu:latest\n[\n{\n\"Id\": \"sha256:bd3d4369ae.......fa2645f5699037d7d8c6b415a10\",\n\"RepoTags\": [\n\"ubuntu:latest\"\n\n<Snip>\n\n\"RootFS\": {\n  \"Type\": \"layers\",\n  \"Layers\": [\n   \"sha256:c8a75145fc...894129005e461a43875a094b93412\",\n   \"sha256:c6f2b330b6...7214ed6aac305dd03f70b95cdc610\",\n   \"sha256:055757a193...3a9565d78962c7f368d5ac5984998\",\n   \"sha256:4837348061...12695f548406ea77feb5074e195e3\",\n   \"sha256:0cad5e07ba...4bae4cfc66b376265e16c32a0aae9\"\n  ]\n  }\n}\n]\n"],["body","\n"],["body","缩减之后的输出也显示该镜像包含 5 个镜像层。只不过这次的输出内容中使用了镜像的 SHA256 散列值来标识镜像层。不过，两中命令都显示了镜像包含 5 个镜像层。"],["body","\n"],["body","docker history 命令显示了镜像的构建历史记录，但其并不是严格意义上的镜像分层。例如，有些 Dockerfile 中的指令并不会创建新的镜像层。比如 ENV、EXPOSE、CMD 以及 ENTRY- POINT。不过，这些命令会在镜像中添加元数据。"],["body","\n"],["body","所有的 Docker 镜像都起始于一个基础镜像层，当进行修改或增加新的内容时，就会在当前镜像层之上，创建新的镜像层。"],["body","\n"],["body","举一个简单的例子，假如基于 Ubuntu Linux 16.04 创建一个新的镜像，这就是新镜像的第一层；如果在该镜像中添加 Python 包，就会在基础镜像层之上创建第二个镜像层；如果继续添加一个安全补丁，就会创建第三个镜像层。"],["body","\n"],["body","\n"],["body","在外部看来整个镜像只有 6 个文件，这是因为最上层中的文件 7 是文件 5 的一个更新版本。"],["body","\n"],["body","这种情况下，上层镜像层中的文件覆盖了底层镜像层中的文件。这样就使得文件的更新版本作为一个新镜像层添加到镜像当中。"],["body","\n"],["body","Docker 通过存储引擎（新版本采用快照机制）的方式来实现镜像层堆栈，并保证多镜像层对外展示为统一的文件系统。"],["body","\n"],["headingLink","共享镜像层"],["heading","共享镜像层"],["body","\n"],["body","多个镜像之间可以并且确实会共享镜像层。这样可以有效节省空间并提升性能。"],["body","\n"],["body","$ docker image pull -a nigelpoulton/tu-demo\n\nlatest: Pulling from nigelpoulton/tu-demo\n237d5fcd25cf: Pull complete\na3ed95caeb02: Pull complete\n<Snip>\nDigest: sha256:42e34e546cee61adb100...a0c5b53f324a9e1c1aae451e9\n\nv1: Pulling from nigelpoulton/tu-demo\n237d5fcd25cf: Already exists\na3ed95caeb02: Already exists\n<Snip>\nDigest: sha256:9ccc0c67e5c5eaae4beb...24c1d5c80f2c9623cbcc9b59a\n\nv2: Pulling from nigelpoulton/tu-demo\n237d5fcd25cf: Already exists\na3ed95caeb02: Already exists\n<Snip>\neab5aaac65de: Pull complete\nDigest: sha256:d3c0d8c9d5719d31b79c...fef58a7e038cf0ef2ba5eb74c\n\nStatus: Downloaded newer image for nigelpoulton/tu-demo\n\n$ docker image ls\nREPOSITORY TAG IMAGE ID CREATED SIZE\nnigelpoulton/tu-demo v2 6ac...ead 4 months ago 211.6 MB\nnigelpoulton/tu-demo latest 9b9...e29 4 months ago 211.6 MB\nnigelpoulton/tu-demo v1 9b9...e29 4 months ago 211.6 MB\n"],["body","\n"],["body","注意那些以 Already exists 结尾的行。"],["body","\n"],["body","由这几行可见，Docker 很聪明，可以识别出要拉取的镜像中，哪几层已经在本地存在。"],["body","\n"],["body","在本例中，Docker 首先尝试拉取标签为 latest 的镜像。然后，当拉取标签为 v1 和 v2 的镜像时，Docker 注意到组成这两个镜像的镜像层，有一部分已经存在了。出现这种情况的原因是前面 3 个镜像相似度很高，所以共享了很多镜像层。"],["body","\n"],["body","如前所述，Docker 在 Linux 上支持很多存储引擎（Snapshotter）。每个存储引擎都有自己的镜像分层、镜像层共享以及写时复制（CoW）技术的具体实现。"],["body","\n"],["body","但是，其最终效果和用户体验是完全一致的。尽管 Windows 只支持一种存储引擎，还是可以提供与 Linux 相同的功能体验。"],["body","\n"],["headingLink","根据摘要拉取镜像"],["heading","根据摘要拉取镜像"],["body","\n"],["body","咱们前面介绍了通过标签来拉取镜像，这也是常见的方式。但问题是，标签是可变的！这意味着可能偶尔出现给镜像打错标签的情况，有时甚至会给新镜像打一个已经存在的标签。这些都可能导致问题！"],["body","\n"],["body","假设镜像 golftrack:1.5 存在一个已知的 Bug。因此可以拉取该镜像后修复它，并使用相同的标签将更新的镜像重新推送回仓库。"],["body","\n"],["body","一起来思考下刚才发生了什么。镜像 golftrack:1.5 存在 Bug，这个镜像已经应用于生产环境。如果创建一个新版本的镜像，并修复了这个 Bug。"],["body","\n"],["body","那么问题来了，构建新镜像并将其推送回仓库时使用了与问题镜像相同的标签！原镜像被覆盖，但在生产环境中遗留了大量运行中的容器，没有什么好办法区分正在使用的镜像版本是修复前还是修复后的，因为两个镜像的标签是相同的！"],["body","\n"],["body","Docker 1.10 中引入了新的内容寻址存储模型。作为模型的一部分，每一个镜像现在都有一个基于其内容的密码散列值。"],["body","\n"],["body","为了讨论方便，用摘要代指这个散列值。因为摘要是镜像内容的一个散列值，所以镜像内容的变更一定会导致散列值的改变。这意味着摘要是不可变的。这种方式可以解决前面讨论的问题。"],["body","\n"],["body","每次拉取镜像，摘要都会作为 docker image pull 命令返回代码的一部分。只需要在 docker image ls 命令之后添加 --digests 参数即可在本地查看镜像摘要。"],["body","\n"],["body","$ docker image pull alpine\nUsing default tag: latest\nlatest: Pulling from library/alpine\ne110a4a17941: Pull complete\nDigest: sha256:3dcdb92d7432d56604d...6d99b889d0626de158f73a\nStatus: Downloaded newer image for alpine:latest\n\n$ docker image ls --digests alpine\nREPOSITORY TAG DIGEST IMAGE ID CREATED SIZE\nalpine latest sha256:3dcd...f73a 4e38e38c8ce0 10 weeks ago 4.8 MB\n"],["body","\n"],["headingLink","镜像散列值摘要"],["heading","镜像散列值（摘要）"],["body","\n"],["body","从 Docker 1.10 版本开始，镜像就是一系列松耦合的独立层的集合。"],["body","\n"],["body","镜像本身就是一个配置对象，其中包含了镜像层的列表以及一些元数据信息。"],["body","\n"],["body","镜像层才是实际数据存储的地方（比如文件等，镜像层之间是完全独立的，并没有从属于某个镜像集合的概念）。"],["body","\n"],["body","镜像的唯一标识是一个加密 ID，即配置对象本身的散列值。每个镜像层也由一个加密 ID 区分，其值为镜像层本身内容的散列值。"],["body","\n"],["body","这意味着修改镜像的内容或其中任意的镜像层，都会导致加密散列值的变化。所以，镜像和其镜像层都是不可变的，任何改动都能很轻松地被辨别。"],["body","\n"],["body","这就是所谓的内容散列（Content Hash）。"],["body","\n"],["body","到目前为止，事情都很简单。但是接下来的内容就有点儿复杂了。"],["body","\n"],["body","在推送和拉取镜像的时候，都会对镜像层进行压缩来节省网络带宽以及仓库二进制存储空间。"],["body","\n"],["body","但是压缩会改变镜像内容，这意味着镜像的内容散列值在推送或者拉取操作之后，会与镜像内容不相符！这显然是个问题。"],["body","\n"],["body","例如，在推送镜像层到 Docker Hub 的时候，Docker Hub 会尝试确认接收到的镜像没有在传输过程中被篡改。"],["body","\n"],["body","为了完成校验，Docker Hub 会根据镜像层重新计算散列值，并与原散列值进行比较。"],["body","\n"],["body","因为镜像在传输过程中被压缩（发生了改变），所以散列值的校验也会失败。"],["body","\n"],["body","为避免该问题，每个镜像层同时会包含一个分发散列值（Distribution Hash）。这是一个压缩版镜像的散列值，当从镜像仓库服务拉取或者推送镜像的时候，其中就包含了分发散列值，该散列值会用于校验拉取的镜像是否被篡改过。"],["body","\n"],["body","这个内容寻址存储模型极大地提升了镜像的安全性，因为在拉取和推送操作后提供了一种方式来确保镜像和镜像层数据是一致的。"],["body","\n"],["body","该模型也解决了随机生成镜像和镜像层 ID 这种方式可能导致的 ID 冲突问题。"],["body","\n"],["headingLink","多层架构的镜像"],["heading","多层架构的镜像"],["body","\n"],["body","Docker 最值得称赞的一点就是使用方便。例如，运行一个应用就像拉取镜像并运行容器这么简单。无须担心安装、依赖或者配置的问题。开箱即用。"],["body","\n"],["body","但是，随着 Docker 的发展，事情开始变得复杂——尤其是在添加了新平台和架构之后，例如 Windows、ARM 以及 s390x。"],["body","\n"],["body","这是会突然发现，在拉取镜像并运行之前，需要考虑镜像是否与当前运行环境的架构匹配，这破坏了 Docker 的流畅体验。"],["body","\n"],["body","多架构镜像（Multi-architecture Image）的出现解决了这个问题！"],["body","\n"],["body","Docker（镜像和镜像仓库服务）规范目前支持多架构镜像。这意味着某个镜像仓库标签（repository:tag）下的镜像可以同时支持 64 位 Linux、PowerPC Linux、64 位 Windows 和 ARM 等多种架构。"],["body","\n"],["body","简单地说，就是一个镜像标签之下可以支持多个平台和架构。下面通过实操演示该特性。"],["body","\n"],["body","为了实现这个特性，镜像仓库服务 API 支持两种重要的结构：Manifest 列表（新）和 Manifest。"],["body","\n"],["body","Manifest 列表是指某个镜像标签支持的架构列表。其支持的每种架构，都有自己的 Mainfest 定义，其中列举了该镜像的构成。"],["body","\n"],["body","下图使用 Golang 官方镜像作为示例。图左侧是 Manifest 列表，其中包含了该镜像支持的每种架构。"],["body","\n"],["body","Manifest 列表的每一项都有一个箭头，指向具体的 Manifest，其中包含了镜像配置和镜像层数据。"],["body","\n"],["body","\n"],["body","在具体操作之前，先来了解一下原理。"],["body","\n"],["body","假设要在 Raspberry Pi（基于 ARM 架构的 Linux）上运行 Docker。"],["body","\n"],["body","在拉取镜像的时候，Docker 客户端会调用 Docker Hub 镜像仓库服务相应的 API 完成拉取。"],["body","\n"],["body","如果该镜像有 Mainfest 列表，并且存在 Linux on ARM 这一项，则 Docker Client 就会找到 ARM 架构对应的 Mainfest 并解析出组成该镜像的镜像层加密 ID。"],["body","\n"],["body","然后从 Docker Hub 二进制存储中拉取每个镜像层。"],["body","\n"],["body","下面的示例就展示了多架构镜像是如何在拉取官方 Golang 镜像（支持多架构）时工作的，并且通过一个简单的命令展示了 Go 的版本和所在主机的 CPU 架构。"],["body","\n"],["body","需要注意的是，两个例子都使用相同的命令 docker container run。不需要告知 Docker 具体的镜像版本是 64 位 Linux 还是 64 位 Windows。"],["body","\n"],["body","示例中只运行了普通的命令，选择当前平台和架构所需的正确镜像版本是有由 Docker 完成的。"],["body","\n"],["headingLink","删除镜像"],["heading","删除镜像"],["body","\n"],["body","当读者不再需要某个镜像的时候，可以通过 docker image rm 命令从 Docker 主机删除该镜像。其中，rm 是 remove 的缩写。"],["body","\n"],["body","docker image rm 02674b9cb179\n"],["body","\n"],["headingLink","镜像常用命令总结"],["heading","镜像常用命令总结"],["body","\n"],["body","docker image pull\ndocker image pull alpine:latest\ndocker image ls\ndocker image inspect\ndocker image rm\n"],["body","\n"],["headingLink","容器"],["heading","容器"],["body","\n"],["headingLink","简介-1"],["heading","简介"],["body","\n"],["body","docker container run <image> <app>\n\n# 会启动某个 Ubuntu Linux 容器，并运行 Bash Shell 作为其应用。\ndocker container run -it ubuntu /bin/bash\n# 启动 PowerShell 并运行一个应用，则可以使用命令\ndocker container run -it microsoft- /powershell:nanoserver pwsh.exe\n# 运行ubuntu 休眠10s\ndocker container run  ubuntu sleep 10\n# 停止容器\ndocker container stop\n# 开启容器\ndocker container start\n"],["body","\n"],["body","-it 参数可以将当前终端连接到容器的 Shell 终端之上。"],["body","\n"],["body","容器随着其中运行应用的退出而终止。其中 Linux 容器会在 Bash Shell 退出后终止，而 Windows 容器会在 PowerShell 进程终止后退出。"],["body","\n"],["body","一个简单的验证方法就是启动新的容器，并运行 sleep 命令休眠 10s。容器会启动，然后运行休眠命令，在 10s 后退出。"],["body","\n"],["headingLink","容器和虚拟机"],["heading","容器和虚拟机"],["body","\n"],["body","容器和虚拟机都依赖于宿主机才能运行。宿主机可以是笔记本，是数据中心的物理服务器，也可以是公有云的某个实例。"],["body","\n"],["body","在下面的示例中，假设宿主机是一台需要运行 4 个业务应用的物理服务器。"],["body","\n"],["body","在虚拟机模型中，首先要开启物理机并启动 Hypervisor 引导程序。一旦 Hypervisor 启动，就会占有机器上的全部物理资源，如 CPU、RAM、存储和 NIC。"],["body","\n"],["body","Hypervisor 接下来就会将这些物理资源划分为虚拟资源，并且看起来与真实物理资源完全一致。"],["body","\n"],["body","然后 Hypervisor 会将这些资源打包进一个叫作虚拟机（VM）的软件结构当中。这样用户就可以使用这些虚拟机，并在其中安装操作系统和应用。"],["body","\n"],["body","前面提到需要在物理机上运行 4 个应用，所以在 Hypervisor 之上需要创建 4 个虚拟机并安装 4 个操作系统，然后安装 4 个应用。当操作完成后，结构如下图所示。"],["body","\n"],["body","\n"],["body","而容器模型则略有不同。"],["body","\n"],["body","服务器启动之后，所选择的操作系统会启动。在 Docker 世界中可以选择 Linux，或者内核支持内核中的容器原语的新版本 Windows。"],["body","\n"],["body","与虚拟机模型相同，OS 也占用了全部硬件资源。在 OS 层之上，需要安装容器引擎（如 Docker）。"],["body","\n"],["body","容器引擎可以获取系统资源，比如进程树、文件系统以及网络栈，接着将资源分割为安全的互相隔离的资源结构，称之为容器。"],["body","\n"],["body","每个容器看起来就像一个真实的操作系统，在其内部可以运行应用。按照前面的假设，需要在物理机上运行 4 个应用。"],["body","\n"],["body","\n"],["body","从更高层面上来讲，Hypervisor 是硬件虚拟化（Hardware Virtualization）——Hypervisor 将硬件物理资源划分为虚拟资源。"],["body","\n"],["body","容器是操作系统虚拟化（OS Virtualization）——容器将系统资源划分为虚拟资源。"],["body","\n"],["headingLink","虚拟机的额外开销"],["heading","虚拟机的额外开销"],["body","\n"],["body","基于前文所述内容，接下来会着重探讨 Hypervisor 模型的一个主要问题。"],["body","\n"],["body","首先我们的目标是在一台物理机上运行 4 个业务相关应用。每种模型示例中都安装了一个操作系统或者 Hypervisor（一种针对虚拟机高度优化后的操作系统）。"],["body","\n"],["body","虚拟机模型将底层硬件资源划分到虚拟机当中。每个虚拟机都是包含了虚拟 CPU、虚拟 RAM、虚拟磁盘等资源的一种软件结构。"],["body","\n"],["body","因此，每个虚拟机都需要有自己的操作系统来声明、初始化并管理这些虚拟资源。"],["body","\n"],["body","但是，操作系统本身是有其额外开销的。例如，每个操作系统都消耗一点 CPU、一点 RAM、一点存储空间等。"],["body","\n"],["body","每个操作系统都需要独立的许可证，并且都需要打补丁升级，每个操作系统也都面临被攻击的风险。"],["body","\n"],["body","通常将这种现象称作 OS Tax 或者 VM Tax，每个操作系统都占用一定的资源。"],["body","\n"],["body","容器模型具有在宿主机操作系统中运行的单个内核。在一台主机上运行数十个甚至数百个容器都是可能的——容器共享一个操作系统/内核。"],["body","\n"],["body","这意味着只有一个操作系统消耗 CPU、RAM 和存储资源，只有一个操作系统需要授权，只有一个操作系统需要升级和打补丁。同时，只有一个操作系统面临被攻击的风险。简言之，就是只有一份 OS 损耗。"],["body","\n"],["body","在上述单台机器上只需要运行 4 个业务应用的场景中，也许问题尚不明显。但当需要运行成百上千应用的时候，就会引起质的变化。"],["body","\n"],["body","另一个值得考虑的事情是启动时间。因为容器并不是完整的操作系统，所以其启动要远比虚拟机快。"],["body","\n"],["body","切记，在容器内部并不需要内核，也就没有定位、解压以及初始化的过程——更不用提在内核启动过程中对硬件的遍历和初始化了。"],["body","\n"],["body","这些在容器启动的过程中统统都不需要！唯一需要的是位于下层操作系统的共享内核是启动了的！最终结果就是，容器可以在 1s 内启动。唯一对容器启动时间有影响的就是容器内应用启动所花费的时间。"],["body","\n"],["body","这就是容器模型要比虚拟机模型简洁并且高效的原因了。使用容器可以在更少的资源上运行更多的应用，启动更快，并且支付更少的授权和管理费用，同时面对未知攻击的风险也更小。"],["body","\n"],["headingLink","检查-docker-daemon"],["heading","检查 Docker daemon"],["body","\n"],["body","$ docker version\nClient:\nVersion: API 17.05.0-ce\nversion: Go 1.29\nversion: Git go1.7.5\ncommit: 89658be\nBuilt: Thu May 4 22:10:54 2017\nOS/Arch: linux/amd64\n\nServer:\nVersion: 17.05.0-ce\nAPI version: 1.29 (minimum version 1.12)\nGo version: go1.7.5\nGit commit: 89658be\nBuilt: Thu May 4 22:10:54 2017\nOS/Arch: linux/amd64\nExperimental: false\n"],["body","\n"],["body","当命令输出中包含 Client 和 Server 的内容时，可以继续下面的操作。如果在 Server 部分中包含了错误码，这表示 Docker daemon 很可能没有运行，或者当前用户没有权限访问。"],["body","\n"],["body","如果在 Linux 中遇到无权限访问的问题，需要确认当前用户是否属于本地 Docker UNIX 组。如果不是，可以通过usermod -aG docker <user>来添加，然后退出并重新登录 Shell，改动即可生效。"],["body","\n"],["body","如果当前用户已经属于本地 docker 用户组，那么问题可能是 Docker daemon 没有运行导致。"],["body","\n"],["body","//使用 Systemd 在 Linux 系统中执行该命令\n$ service docker status\ndocker start/running, process 29393\n\n//使用Systemd在Linux系统中执行该命令\n$ systemctl is-active docker\nactive\n\n//在Windows Server 2016的PowerShell窗口中运行该命令\n> Get-Service docker\n\nStatus Name DisplayName\n------ ---- -----------\nRunning Docker docker\n"],["body","\n"],["headingLink","启动一个简单容器"],["heading","启动一个简单容器"],["body","\n"],["body","#Windows 示例。\n\n$ docker container run -it microsoft/powershell:nanoserver pwsh.exe\n\n#命令的基础格式为：\n# 示例中使用 docker container run 来启动容器，这也是启动新容器的标准命令。\n\n$ docker container run <options> <im- age>:<tag> <app>\n"],["body","\n"],["headingLink","容器生命周期"],["heading","容器生命周期"],["body","\n"],["body","# 新建容器 名称为 precy\n$ docker container run --name percy -it ubuntu:latest /bin/bash\n#暂停容器\n$ docker container stop <container-id or container-name>\n# 写输入到容器\n$ root@9cb2d2fd1d65:/tmp# echo \"DevOps FTW\" > newfile\n# 退出容器\nctrl +QP\n# 暂停容器\n$ docker container stop percy\n# 查看运行中的容器\n$ docker container ls\n# 所有容器\n$ docker container ls -a\n# 启动容器\n$ docker container start percy\n# 连接容器\n$ docker container exec -it percy bash\n$ docker container stop percy\n# 删除容器\n$ docker container rm percy\n\n"],["body","\n"],["headingLink","优雅地停止容器"],["heading","优雅地停止容器"],["body","\n"],["body","# 向容器内的 PID 1 进程发送了 SIGTERM 这样的信号。\n$ docker container stop\n"],["body","\n"],["body","就像前文提到的一样，会为进程预留一个清理并优雅停止的机会。如果 10s 内进程没有终止，那么就会收到 SIGKILL 信号。这是致命一击。但是，进程起码有 10s 的时间来“解决”自己。"],["body","\n"],["body","docker container rm <container> -f 命令不会先友好地发送 SIGTERM，这条命令会直接发出 SIGKILL。就像刚刚所打的比方一样，该命令悄悄接近并对容器发起致命一击。"],["body","\n"],["headingLink","利用重启策略进行容器的自我修复"],["heading","利用重启策略进行容器的自我修复"],["body","\n"],["body","通常建议在运行容器时配置好重启策略。这是容器的一种自我修复能力，可以在指定事件或者错误后重启来完成自我修复。"],["body","\n"],["body","重启策略应用于每个容器，可以作为参数被强制传入 docker-container run 命令中，或者在 Compose 文件中声明（在使用 Docker Compose 以及 Docker Stacks 的情况下）。"],["body","\n"],["body","容器支持的重启策略包括 "],["body","\n\n"],["body","always"],["body","\n"],["body","unless-stopped "],["body","\n"],["body","on-failed"],["body","\n\n"],["body","always 策略是一种简单的方式。除非容器被明确停止，比如通过 docker container stop 命令，否则该策略会一直尝试重启处于停止状态的容器。"],["body","\n"],["headingLink","容器常用命令"],["heading","容器常用命令"],["body","\n"],["body","# 启动新容器的命令。该命令的最简形式接收镜像和命令作为参数。镜像用于创建容器，而命令则是希望容器运行的应用。\n# Ctrl-PQ 会断开 Shell 和容器终端之间的链接，并在退出后保持容器在后台处于运行（UP）状态。\n$ docker container run\n# 此命令会停止运行中的容器，并将状态置为 Exited(0)。\n$ docker container ls\n# 用于在运行状态的容器中，启动一个新进程。该命令在将 Docker 主机 Shell 连接到一个运行中容器终端时非常有用。\n$ docker container exec\n#此命令会停止运行中的容器，并将状态置为 Exited(0)。\n#该命令通过发送 SIGTERM 信号给容器内 PID 为 1 的进程达到目的。\n#如果进程没有在 10s 之内得到清理并停止运行，那么会接着发送 SIGKILL 信号来强制停止该容器。\n$ docker container stop\n# 重启处于停止（Exited）状态的容器。可以在 docker container start 命令中指定容器的名称或者 ID。\n$ docker container start\n# 删除停止运行的容器\n$ docker container rm\n# 该命令接收容器名称和容器 ID 作为主要参数。\n$ docker container inspect\n"],["body","\n"],["headingLink","应用容器化"],["heading","应用容器化"],["body","\n"],["headingLink","概述"],["heading","概述"],["body","\n"],["body","Docker 的核心思想就是如何将应用整合到容器中，并且能在容器中实际运行。"],["body","\n"],["body","将应用整合到容器中并且运行起来的这个过程，称为“容器化”（Containerizing），有时也叫作“Docker化”（Dockerizing）。"],["body","\n"],["body","容器是为应用而生的，具体来说，容器能够简化应用的构建、部署和运行过程。"],["body","\n"],["body","完整的应用容器化过程主要分为以下几个步骤。"],["body","\n\n"],["body","编写应用代码。"],["body","\n"],["body","创建一个 Dockerfile，其中包括当前应用的描述、依赖以及该如何运行这个应用。"],["body","\n"],["body","对该 Dockerfile 执行 docker image build 命令。"],["body","\n"],["body","等待 Docker 将应用程序构建到 Docker 镜像中。"],["body","\n\n"],["body","一旦应用容器化完成（即应用被打包为一个 Docker 镜像），就能以镜像的形式交付并以容器的方式运行了。"],["body","\n"],["headingLink","单体应用容器化"],["heading","单体应用容器化"],["body","\n"],["body","应用容器化的过程大致分为如下几个步骤："],["body","\n\n"],["body","获取应用代码。"],["body","\n"],["body","分析 Dockerfile。"],["body","\n"],["body","构建应用镜像。"],["body","\n"],["body","运行该应用。"],["body","\n"],["body","测试应用。"],["body","\n"],["body","容器应用化细节。"],["body","\n"],["body","生产环境中的多阶段构建。"],["body","\n"],["body","最佳实践。"],["body","\n\n"],["body","FROM alpine # 以 alpine 镜像作为当前镜像基础\nLABEL maintainer=\"nigelpoulton@hotmail.com\" #指定维护者（maintainer）为“nigelpoultion@hotmail.com”\nRUN apk add --update nodejs nodejs-npm # 安装 Node.js 和 NPM\nCOPY . /src #将应用的代码复制到镜像当中\nWORKDIR /src #设置新的工作目录\nRUN npm install #安装依赖包\nEXPOSE 8080 #记录应用的网络端口，\nENTRYPOINT [\"node\", \"./app.js\"] #最后将 app.js 设置为默认运行的应用。\n"],["body","\n"],["body","Dockerfile 主要包括两个用途："],["body","\n\n"],["body","对当前应用的描述。"],["body","\n"],["body","指导 Docker 完成应用的容器化（创建一个包含当前应用的镜像）。"],["body","\n\n"],["body","不要因 Dockerfile 就是一个描述文件而对其有所轻视！Dockerfile 能实现开发和部署两个过程的无缝切换。"],["body","\n"],["body","同时 Dockerfile 还能帮助新手快速熟悉这个项目。Dockerfile 对当前的应用及其依赖有一个清晰准确的描述，并且非常容易阅读和理解。"],["body","\n"],["body","每个 Dockerfile 文件第一行都是 FROM 指令。"],["body","\n"],["body","FROM 指令指定的镜像，会作为当前镜像的一个基础镜像层，当前应用的剩余内容会作为新增镜像层添加到基础镜像层之上。"],["body","\n"],["body","Example解读"],["body","\n"],["body","本例中的应用基于 Linux 操作系统，所以在 FROM 指令当中所引用的也是一个 Linux 基础镜像；如果要容器化的应用是一个基于 Windows 操作系统的应用，就需要指定一个像 microsoft/aspnetcore-build 这样的 Windows 基础镜像了。"],["body","\n"],["body","截至目前，基础镜像的结构如下图所示。"],["body","\n"],["body","\n"],["body","接下来，Dockerfile 中通过标签（LABLE）方式指定了当前镜像的维护者为“nigelpoulton@hotmail. com”。"],["body","\n"],["body","每个标签其实是一个键值对（Key-Value），在一个镜像当中可以通过增加标签的方式来为镜像添加自定义元数据。"],["body","\n"],["body","RUN apk add --update nodejs nodejs-npm 指令使用 alpine 的 apk 包管理器将 nodejs 和 nodejs-npm 安装到当前镜像之中。"],["body","\n"],["body","RUN 指令会在 FROM 指定的 alpine 基础镜像之上，新建一个镜像层来存储这些安装内容。当前镜像的结构如下图所示。"],["body","\n"],["body","COPY. / src 指令将应用相关文件从构建上下文复制到了当前镜像中，并且新建一个镜像层来存储。COPY 执行结束之后，当前镜像共包含 3 层，如下图所示。"],["body","\n"],["body","\n"],["body","下一步，Dockerfile 通过 WORKDIR 指令，为 Dockerfile 中尚未执行的指令设置工作目录。"],["body","\n"],["body","该目录与镜像相关，并且会作为元数据记录到镜像配置中，但不会创建新的镜像层。"],["body","\n"],["body","然后，RUN npm install 指令会根据 package.json 中的配置信息，使用 npm 来安装当前应用的相关依赖包。"],["body","\n"],["body","目前镜像一共包含 4 层，如下图所示。"],["body","\n"],["body","\n"],["body","因为当前应用需要通过 TCP 端口 8080 对外提供一个 Web 服务，所以在 Dockerfile 中通过 EXPOSE 8080 指令来完成相应端口的设置。"],["body","\n"],["body","这个配置信息会作为镜像的元数据被保存下来，并不会产生新的镜像层。"],["body","\n"],["body","最终，通过 ENTRYPOINT 指令来指定当前镜像的入口程序。"],["body","\n"],["body","ENTRYPOINT 指定的配置信息也是通过镜像元数据的形式保存下来，而不是新增镜像层。"],["body","\n"],["body","启动容器"],["body","\n"],["body","#-d 参数的作用是让应用程序以守护线程的方式在后台运行。\n# -p 80:8080 参数的作用是将主机的80端口与容器内的8080端口进行映射。\n$ docker container run -d --name c1 -p 80:8080  web:latest \n"],["body","\n"],["body","docker image history"],["body","\n"],["body","每行内容都对应了 Dockerfile 中的一条指令（顺序是自下而上）。CREATE BY 这一列中还展示了当前行具体对应 Dockerfile 中的哪条指令。"],["body","\n"],["body","其次，从这个输出内容中，可以观察到只有 4 条指令会新建镜像层（就是那些 SIZE 列对应的数值不为零的指令），分别对应 Dockerfile 中的 FROM、RUN 以及 COPY 指令。"],["body","\n"],["body","虽然其他指令看上去跟这些新建镜像层的指令并无区别，但实际上它们只在镜像中新增了元数据信息。这些指令之所以看起来没有区别，是因为 Docker 对之前构建镜像层方式的兼容。"],["body","\n"],["body","使用 FROM 指令引用官方基础镜像是一个很好的习惯，这是因为官方的镜像通常会遵循一些最佳实践，并且能帮助使用者规避一些已知的问题。"],["body","\n"],["body","通过 docker image build 命令具体的输出内容，可以了解镜像构建的过程。"],["body","\n"],["body","在下面的片段中，可以看到基本的构建过程是，运行临时容器 -> 在该容器中运行 Dockerfile 中的指令 -> 将指令运行结果保存为一个新的镜像层 -> 删除临时容器。"],["body","\n"],["body","Step 3/8 : RUN apk add --update nodejs nodejs-npm\n---> Running in e690ddca785f << Run inside of temp container\nfetch http://dl-cdn...APKINDEX.tar.gz\nfetch http://dl-cdn...APKINDEX.tar.gz\n(1/10) Installing ca-certificates (20171114-r0)\n<Snip>\nOK: 61 MiB in 21 packages\n---> c1d31d36b81f << Create new layer\nRemoving intermediate container << Remove temp container\nStep 4/8 : COPY . /src\n"],["body","\n"],["headingLink","生产环境中的多阶段构建"],["heading","生产环境中的多阶段构建"],["body","\n"],["headingLink","过大体积的问题"],["heading","过大体积的问题"],["body","\n"],["body","对于 Docker 镜像来说，过大的体积并不好！"],["body","\n"],["body","越大则越慢，这就意味着更难使用，而且可能更加脆弱，更容易遭受攻击。"],["body","\n"],["body","鉴于此，Docker 镜像应该尽量小。对于生产环境镜像来说，目标是将其缩小到仅包含运行应用所必需的内容即可。问题在于，生成较小的镜像并非易事。"],["body","\n"],["body","不同的 Dockerfile 写法就会对镜像的大小产生显著影响。"],["body","\n"],["body","使用 && 连接多个命令"],["body","\n"],["body","常见的例子是，每一个 RUN 指令会新增一个镜像层。因此，通过使用 && 连接多个命令以及使用反斜杠（\\）换行的方法，将多个命令包含在一个 RUN 指令中，通常来说是一种值得提倡的方式。"],["body","\n"],["body","构建工具残留"],["body","\n"],["body","另一个问题是开发者通常不会在构建完成后进行清理。当使用 RUN 执行一个命令时，可能会拉取一些构建工具，这些工具会留在镜像中移交至生产环境。"],["body","\n"],["body","采用构建者模式"],["body","\n"],["body","有多种方式来改善这一问题——比如常见的是采用建造者模式（Builder Pattern）。但无论采用哪种方式，通常都需要额外的培训，并且会增加构建的复杂度。"],["body","\n"],["body","建造者模式需要至少两个 Dockerfile，一个用于开发环境，一个用于生产环境。"],["body","\n"],["body","首先需要编写 Dockerfile.dev，它基于一个大型基础镜像（Base Image），拉取所需的构建工具，并构建应用。"],["body","\n"],["body","接下来，需要基于 Dockerfile.dev 构建一个镜像，并用这个镜像创建一个容器。"],["body","\n"],["body","这时再编写 Dockerfile.prod，它基于一个较小的基础镜像开始构建，并从刚才创建的容器中将应用程序相关的部分复制过来。"],["body","\n"],["body","整个过程需要编写额外的脚本才能串联起来。"],["body","\n"],["body","这种方式是可行的，但是比较复杂。"],["body","\n"],["body","多阶段构建（Multi-Stage Build）是一种更好的方式！"],["body","\n"],["body","多阶段构建能够在不增加复杂性的情况下优化构建过程。"],["body","\n"],["headingLink","下面介绍一下多阶段构建方式"],["heading","下面介绍一下多阶段构建方式"],["body","\n"],["body","多阶段构建方式使用一个 Dockerfile，其中包含多个 FROM 指令。每一个 FROM 指令都是一个新的构建阶段（Build Stage），并且可以方便地复制之前阶段的构件。"],["body","\n"],["body","FROM node:latest AS storefront\nWORKDIR /usr/src/atsea/app/react-app\nCOPY react-app .\nRUN npm install\nRUN npm run build\n\nFROM maven:latest AS appserver\nWORKDIR /usr/src/atsea\nCOPY pom.xml .\nRUN mvn -B -f pom.xml -s /usr/share/maven/ref/settings-docker.xml dependency\n\\:resolve\nCOPY . .\nRUN mvn -B -s /usr/share/maven/ref/settings-docker.xml package -DskipTests\n\nFROM java:8-jdk-alpine AS production\nRUN adduser -Dh /home/gordon gordon\nWORKDIR /static\nCOPY --from=storefront /usr/src/atsea/app/react-app/build/ .\nWORKDIR /app\nCOPY --from=appserver /usr/src/atsea/target/AtSea-0.0.1-SNAPSHOT.jar .\nENTRYPOINT [\"java\", \"-jar\", \"/app/AtSea-0.0.1-SNAPSHOT.jar\"]\nCMD [\"--spring.profiles.active=postgres\"]\n"],["body","\n"],["body","首先注意到，Dockerfile 中有 3 个 FROM 指令。每一个 FROM 指令构成一个单独的构建阶段。"],["body","\n"],["body","各个阶段在内部从 0 开始编号。不过，示例中针对每个阶段都定义了便于理解的名字。"],["body","\n\n"],["body","阶段 0 叫作 storefront。"],["body","\n"],["body","阶段 1 叫作 appserver。"],["body","\n"],["body","阶段 2 叫作 production。"],["body","\n\n"],["body","storefront "],["body","\n"],["body","storefront 阶段拉取了大小超过 600MB 的 node:latest 镜像，然后设置了工作目录，复制一些应用代码进去，然后使用 2 个 RUN 指令来执行 npm 操作。"],["body","\n"],["body","这会生成 3 个镜像层并显著增加镜像大小。指令执行结束后会得到一个比原镜像大得多的镜像，其中包含许多构建工具和少量应用程序代码。"],["body","\n"],["body","appserver "],["body","\n"],["body","appserver 阶段拉取了大小超过 700MB 的 maven:latest 镜像。然后通过 2 个 COPY 指令和 2 个 RUN 指令生成了 4 个镜像层。"],["body","\n"],["body","这个阶段同样会构建出一个非常大的包含许多构建工具和非常少量应用程序代码的镜像。"],["body","\n"],["body","production 阶段拉取 java:8-jdk-alpine 镜像，这个镜像大约 150MB，明显小于前两个构建阶段用到的 node 和 maven 镜像。"],["body","\n"],["body","这个阶段会创建一个用户，设置工作目录，从 storefront 阶段生成的镜像中复制一些应用代码过来。"],["body","\n"],["body","之后，设置一个不同的工作目录，然后从 appserver 阶段生成的镜像中复制应用相关的代码。最后，production 设置当前应用程序为容器启动时的主程序。"],["body","\n"],["body","重点在于 COPY --from 指令，它从之前的阶段构建的镜像中仅复制生产环境相关的应用代码，而不会复制生产环境不需要的构件。"],["body","\n"],["body","还有一点也很重要，多阶段构建这种方式仅用到了一个 Dockerfile，并且 docker image build 命令不需要增加额外参数。"],["body","\n"],["body"," docker image build -t multi:stage .\n# 示例中 multi:stage 标签是自行定义的，可以根据自己的需要和规范来指定标签名称。不过并不要求一定必须为多阶段构建指定标签。\n"],["body","\n"],["body","可见它明显比之前阶段拉取和生成的镜像要小。这是因为该镜像是基于相对精简的 java:8-jdk-alpine 镜像构建的，并且仅添加了用于生产环境的应用程序文件。"],["body","\n"],["body","最终，无须额外的脚本，仅对一个单独的 Dockerfile 执行 docker image build 命令，就创建了一个精简的生产环境镜像。"],["body","\n"],["body","多阶段构建是随 Docker 17.05 版本新增的一个特性，用于构建精简的生产环境镜像。"],["body","\n"],["headingLink","最佳实践"],["heading","最佳实践"],["body","\n"],["headingLink","利用构建缓存"],["heading","利用构建缓存"],["body","\n"],["body","Docker 的构建过程利用了缓存机制。观察缓存效果的一个方法，就是在一个干净的 Docker 主机上构建一个新的镜像，然后再重复同样的构建。"],["body","\n"],["body","第一次构建会拉取基础镜像，并构建镜像层，构建过程需要花费一定时间；第二次构建几乎能够立即完成。"],["body","\n"],["body","这就是因为第一次构建的内容（如镜像层）能够被缓存下来，并被后续的构建过程复用。"],["body","\n"],["body","docker image build 命令会从顶层开始解析 Dockerfile 中的指令并逐行执行。而对每一条指令，Docker 都会检查缓存中是否已经有与该指令对应的镜像层。"],["body","\n"],["body","如果有，即为缓存命中（Cache Hit），并且会使用这个镜像层；如果没有，则是缓存未命中（Cache Miss），Docker 会基于该指令构建新的镜像层。"],["body","\n"],["body","缓存命中能够显著加快构建过程。"],["body","\n"],["body","FROM alpine\nRUN apk add --update nodejs nodejs-npm\nCOPY . /src\nWORKDIR /src\nRUN npm install\nEXPOSE 8080\nENTRYPOINT [\"node\", \"./app.js\"]\n"],["body","\n"],["body","缓存策略"],["body","\n"],["body","如果主机中已经存在这个镜像，那么构建时会直接跳到下一条指令；如果镜像不存在，则会从 Docker Hub（docker.io）拉取。"],["body","\n"],["body","下一条指令（RUN apk...）对镜像执行一条命令。"],["body","\n"],["body","此时，Docker 会检查构建缓存中是否存在基于同一基础镜像，并且执行了相同指令的镜像层。"],["body","\n"],["body","在此例中，Docker 会检查缓存中是否存在一个基于 alpine:latest 镜像且执行了 RUN apk add --update nodejs nodejs-npm 指令构建得到的镜像层。"],["body","\n"],["body","如果找到该镜像层，Docker 会跳过这条指令，并链接到这个已经存在的镜像层，然后继续构建；如果无法找到符合要求的镜像层，则设置缓存无效并构建该镜像层。"],["body","\n"],["body","一旦没命中缓存后续再不缓存"],["body","\n"],["body","此处“设置缓存无效”作用于本次构建的后续部分。也就是说 Dockerfile 中接下来的指令将全部执行而不会再尝试查找构建缓存。"],["body","\n"],["body","假设 Docker 已经在缓存中找到了该指令对应的镜像层（缓存命中），并且假设这个镜像层的 ID 是 AAA。"],["body","\n"],["body","下一条指令会复制一些代码到镜像中（COPY . /src）。因为上一条指令命中了缓存，Docker 会继续查找是否有一个缓存的镜像层也是基于 AAA 层并执行了 COPY . /src 命令。"],["body","\n"],["body","如果有，Docker 会链接到这个缓存的镜像层并继续执行后续指令；如果没有，则构建镜像层，并对后续的构建操作设置缓存无效。"],["body","\n"],["body","假设 Docker 已经有一个对应该指令的缓存镜像层（缓存命中），并且假设这个镜像层的 ID 是 BBB。"],["body","\n"],["body","那么 Docker 将继续执行 Dockerfile 中剩余的指令。"],["body","\n"],["body","理解以下几点很重要。"],["body","\n"],["body","首先，一旦有指令在缓存中未命中（没有该指令对应的镜像层），则后续的整个构建过程将不再使用缓存。"],["body","\n"],["body","在编写 Dockerfile 时须特别注意这一点，尽量将易于发生变化的指令置于 Dockerfile 文件的后方执行。"],["body","\n"],["body","这意味着缓存未命中的情况将直到构建的后期才会出现，从而构建过程能够尽量从缓存中获益。"],["body","\n"],["body","通过对 docker image build 命令加入 --nocache=true 参数可以强制忽略对缓存的使用。"],["body","\n"],["body","还有一点也很重要，那就是 COPY 和 ADD 指令会检查复制到镜像中的内容自上一次构建之后是否发生了变化。"],["body","\n"],["body","例如，有可能 Dockerfile 中的 COPY . /src 指令没有发生变化，但是被复制的目录中的内容已经发生变化了。"],["body","\n"],["body","为了应对这一问题，Docker 会计算每一个被复制文件的 Checksum 值，并与缓存镜像层中同一文件的 checksum 进行对比。如果不匹配，那么就认为缓存无效并构建新的镜像层。"],["body","\n"],["headingLink","合并镜像"],["heading","合并镜像"],["body","\n"],["body","合并镜像并非一个最佳实践，因为这种方式利弊参半。"],["body","\n"],["body","总体来说，Docker 会遵循正常的方式构建镜像，但之后会增加一个额外的步骤，将所有的内容合并到一个镜像层中。"],["body","\n"],["body","当镜像中层数太多时，合并是一个不错的优化方式。例如，当创建一个新的基础镜像，以便基于它来构建其他镜像的时候，这个基础镜像就最好被合并为一层。"],["body","\n"],["body","缺点是，合并的镜像将无法共享镜像层。这会导致存储空间的低效利用，而且 push 和 pull 操作的镜像体积更大。"],["body","\n"],["body","执行 docker image build命令时，可以通过增加 --squash 参数来创建一个合并的镜像。"],["body","\n"],["body","\n"],["body","两个镜像的内容是完全一样的，区别在于是否进行了合并。在使用 docker image push 命令发送镜像到 Docker Hub 时，合并的镜像需要发送全部字节，而不合并的镜像只需要发送不同的镜像层即可。"],["body","\n"],["headingLink","使用-no-install-recommends"],["heading","使用 no-install-recommends"],["body","\n"],["body","在构建 Linux 镜像时，若使用的是 APT 包管理器，则应该在执行 apt-get install 命令时增加 no-install-recommends 参数。"],["body","\n"],["body","这能够确保 APT 仅安装核心依赖（Depends 中定义）包，而不是推荐和建议的包。这样能够显著减少不必要包的下载数量。"],["body","\n"],["headingLink","不要安装-msi-包windows"],["heading","不要安装 MSI 包（Windows）"],["body","\n"],["body","在构建 Windows 镜像时，尽量避免使用 MSI 包管理器。因其对空间的利用率不高，会大幅增加镜像的体积。"],["body","\n"],["headingLink","dockerfile简介"],["heading","Dockerfile简介"],["body","\n"],["body","Dockerfile 由一行行命令语句组成，并支持以 # 开头的注释行。例如："],["body","\n"],["body","# Test web-app to use with Pluralsight courses and Docker Deep Dive book\n# Linux x64\nFROM alpine\n\nLABEL maintainer=\"nigelpoulton@hotmail.com\"\n\n# Install Node and NPM\nRUN apk add --update nodejs nodejs-npm\n\n# Copy app to /src\nCOPY . /src\n\nWORKDIR /src\n\n# Install dependencies\nRUN  npm install\n\nEXPOSE 8080\n\nENTRYPOINT [\"node\", \"./app.js\"]\n"],["body","\n"],["body","使用 -t 参数为镜像打标签，使用 -f 参数指定 Dockerfile 的路径和名称，使用 -f 参数可以指定位于任意路径下的任意名称的 Dockerfile。"],["body","\n"],["body","构建上下文是指应用文件存放的位置，可能是本地 Docker 主机上的一个目录或一个远程的 Git 库。"],["body","\n"],["body","Dockerfile 中的 FROM 指令用于指定要构建的镜像的基础镜像。它通常是 Dockerfile 中的第一条指令。"],["body","\n"],["body","Dockerfile 中的 RUN 指令用于在镜像中执行命令，这会创建新的镜像层。每个 RUN 指令创建一个新的镜像层。"],["body","\n"],["body","Dockerfile 中的 COPY 指令用于将文件作为一个新的层添加到镜像中。通常使用 COPY 指令将应用代码赋值到镜像中。"],["body","\n"],["body","Dockerfile 中的 EXPOSE 指令用于记录应用所使用的网络端口。"],["body","\n"],["body","Dockerfile 中的 ENTRYPOINT 指令用于指定镜像以容器方式启动后默认运行的程序。"],["body","\n"],["body","其他的 Dockerfile 指令还有 LABEL、ENV、ONBUILD、HEALTHCHECK、CMD 等。"],["body","\n"],["body","{% endraw %}"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","1.容器_docker/docker网络管理.html"],["title","docker网络管理 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","前言"],["heading","前言"],["body","\n"],["body","Docker 存在 4种网络工作方式，和一些自定义网络模式"],["body","\n"],["body","安装Docker时，它会自动创建三个网络，bridge（创建容器默认连接到此网络）、 none 、host"],["body","\n"],["body","host"],["body","\n"],["body","容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口。"],["body","\n"],["body","Container"],["body","\n"],["body","创建的容器不会创建自己的网卡，配置自己的IP，而是和一个指定的容器共享IP、端口范围。"],["body","\n"],["body","None"],["body","\n"],["body","该模式关闭了容器的网络功能。"],["body","\n"],["body","Bridge"],["body","\n"],["body","此模式会为每一个容器分配、设置IP等，并将容器连接到一个docker0虚拟网桥，通过docker0网桥以及Iptables nat表配置与宿主机通信。"],["body","\n"],["body","Docker内置这三个网络，运行容器时，你可以使用该--network标志来指定容器应连接到哪些网络。"],["body","\n"],["body","指定网络"],["body","\n"],["body","host模式：使用 --net=host 指定。\n\nnone模式：使用 --net=none 指定。\n\nbridge模式：使用 --net=bridge 指定，默认设置。\n\ncontainer模式：使用 --net=container:NAME_or_ID 指定。\n"],["body","\n"],["headingLink","网络模式详解"],["heading","网络模式详解"],["body","\n"],["headingLink","host"],["heading","Host"],["body","\n"],["body","与宿主机在同一个网络中，但没有独立IP地址"],["body","\n"],["body","命名空间隔离资源"],["body","\n"],["body","众所周知，Docker使用了Linux的Namespaces技术来进行资源隔离，"],["body","\n"],["body","如PID Namespace隔离进程，Mount Namespace隔离文件系统，Network Namespace隔离网络等。"],["body","\n"],["body","network 隔离网络"],["body","\n"],["body","一个Network Namespace提供了一份独立的网络环境，包括网卡、路由、Iptable规则等都与其他的Network Namespace隔离。"],["body","\n"],["body","一个Docker容器一般会分配一个独立的Network Namespace。"],["body","\n"],["body","但如果启动容器的时候使用host模式，那么这个容器将不会获得一个独立的Network Namespace，而是和宿主机共用一个Network Namespace。容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口。"],["body","\n"],["body","例如"],["body","\n"],["body","docker run --name nginx1 -p80:80 --net=host -d nginx\n"],["body","\n"],["headingLink","container"],["heading","Container"],["body","\n"],["body","这个模式指定新创建的容器和已经存在的一个容器共享一个Network Namespace"],["body","\n"],["body","而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的IP，而是和一个指定的容器共享IP、端口范围等"],["body","\n"],["body","同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过lo网卡设备通信。"],["body","\n"],["headingLink","none"],["heading","None"],["body","\n"],["body","该模式关闭了容器的网络功能"],["body","\n"],["headingLink","bridge"],["heading","Bridge"],["body","\n"],["body","容器使用独立network Namespace，并连接到docker0虚拟网卡"],["body","\n"],["body","通过docker0网桥以及Iptables nat表配置与宿主机通信；bridge模式是Docker默认的网络设置，此模式会为每一个容器分配Network Namespace、设置IP等，并将一个主机上的Docker容器连接到一个虚拟网桥上。下面着重介绍一下此模式。"],["body","\n"],["headingLink","bridge模式的拓扑结构"],["heading","Bridge模式的拓扑结构"],["body","\n"],["body","创建虚拟网桥"],["body","\n"],["body","当Docker server启动时，会在主机上创建一个名为docker0的虚拟网桥，此主机上启动的Docker容器会连接到这个虚拟网桥上"],["body","\n"],["body","分配容器IP"],["body","\n"],["body","虚拟网桥的工作方式和物理交换机类似，这样主机上的所有容器就通过交换机连在了一个二层网络中"],["body","\n"],["body","接下来就要为容器分配IP了，Docker会从RFC1918所定义的私有IP网段中，选择一个和宿主机不同的IP地址和子网分配给docker0，连接到docker0的容器就从这个子网中选择一个未占用的IP使用，如一般Docker会使用172.17.0.0/16这个网段，并将172.17.0.1/16分配给docker0网桥"],["body","\n"],["body","\n"],["headingLink","网络配置过程"],["heading","网络配置过程"],["body","\n\n"],["body","在主机上创建一对虚拟网卡veth pair设备"],["body","\n\n"],["body","veth设备总是成对出现的，它们组成了一个数据的通道，数据从一个设备进入，就会从另一个设备出来。因此，veth设备常用来连接两个网络设备"],["body","\n\n"],["body","\n"],["body","Docker将veth pair设备的一端放在新创建的容器中，并命名为eth0。另一端放在主机中，以veth65f9这样类似的名字命名，并将这个网络设备加入到docker0网桥中"],["body","\n"],["body","\n"],["body","\n"],["body","从docker0子网中分配一个IP给容器使用，并设置docker0的IP地址为容器的默认网关。"],["body","\n"],["body","\n\n"],["headingLink","bridge模式下容器的通信"],["heading","bridge模式下容器的通信"],["body","\n"],["body","容器间可以相互通信"],["body","\n"],["body","在bridge模式下，连在同一网桥上的容器可以相互通信"],["body","\n"],["body","（若出于安全考虑，也可以禁止它们之间通信，方法是在DOCKER_OPTS变量中设置–icc=false，这样只有使用–link才能使两个容器通信）"],["body","\n"],["body","限制容器间的通信"],["body","\n"],["body","Docker可以开启容器间通信（意味着默认配置--icc=true），也就是说，宿主机上的所有容器可以不受任何限制地相互通信，这可能导致拒绝服务攻击。"],["body","\n"],["body","Docker可以通过--ip_forward和--iptables两个选项控制容器间、容器和外部世界的通信。"],["body","\n"],["body","-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE\n"],["body","\n"],["body","这条规则会将源地址为172.17.0.0/16的包（也就是从Docker容器产生的包），并且不是从docker0网卡发出的，进行源地址转换，转换成主机网卡的地址。\n\n这么说可能不太好理解，举一个例子说明一下。假设主机有一块网卡为eth0，IP地址为10.10.101.105/24，网关为10.10.101.254。从主机上一个IP为172.17.0.1/16的容器中ping百度（180.76.3.151）。IP包首先从容器发往自己的默认网关docker0，包到达docker0后，也就到达了主机上。然后会查询主机的路由表，发现包应该从主机的eth0发往主机的网关10.10.105.254/24。接着包会转发给eth0，并从eth0发出去（主机的ip_forward转发应该已经打开）。这时候，上面的Iptable规则就会起作用，对包做SNAT转换，将源地址换为eth0的地址。这样，在外界看来，这个包就是从10.10.101.105上发出来的，Docker容器对外是不可见的。\n"],["body","\n"],["headingLink","外面的机器是如何访问docker容器的服务呢"],["heading","外面的机器是如何访问Docker容器的服务呢？"],["body","\n"],["body"," docker run --name=nginx_bridge --net=bridge -p 80:80 -d nginx\n"],["body","\n"],["body","iptables -L\n-A DOCKER ! -i docker0 -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.17.0.2:80\n"],["body","\n"],["body","此条规则就是对主机eth0收到的目的端口为80的tcp流量进行DNAT转换，将流量发往172.17.0.2:80，也就是我们上面创建的Docker容器。所以，外界只需访问10.10.101.105:80就可以访问到容器中的服务。\n\n除此之外，我们还可以自定义Docker使用的IP地址、DNS等信息，甚至使用自己定义的网桥，但是其工作方式还是一样的。\n"],["body","\n"],["headingLink","自定义网络"],["heading","自定义网络"],["body","\n"],["body","建议使用自定义的网桥来控制哪些容器可以相互通信，还可以自动DNS解析容器名称到IP地址。Docker提供了创建这些网络的默认网络驱动程序，你可以创建一个新的Bridge网络，Overlay或Macvlan网络。你还可以创建一个网络插件或远程网络进行完整的自定义和控制。"],["body","\n"],["body","你可以根据需要创建任意数量的网络，并且可以在任何给定时间将容器连接到这些网络中的零个或多个网络。此外，您可以连接并断开网络中的运行容器，而无需重新启动容器。当容器连接到多个网络时，其外部连接通过第一个非内部网络以词法顺序提供。"],["body","\n"],["headingLink","自定义桥接网络"],["heading","自定义桥接网络"],["body","\n"],["body","docker network create --driver bridge new_bridge\n\n创建网络后，可以看到新增加了一个网桥（172.18.0.1）。\n\n72: br-2edfc1326986: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default \n    link/ether 02:42:07:cc:f8:33 brd ff:ff:ff:ff:ff:ff\n    inet 172.18.0.1/16 scope global br-2edfc1326986\n       valid_lft forever preferred_lft forever\n"],["body","\n"],["headingLink","macvlan"],["heading","Macvlan"],["body","\n"],["body","简介"],["body","\n"],["body","Macvlan是一个新的尝试，是真正的网络虚拟化技术的转折点。Linux实现非常轻量级，因为与传统的Linux Bridge隔离相比，它们只是简单地与一个Linux以太网接口或子接口相关联，以实现网络之间的分离和与物理网络的连接。"],["body","\n"],["body","Macvlan提供了许多独特的功能，并有充足的空间进一步创新与各种模式。这些方法的两个高级优点是绕过Linux网桥的正面性能以及移动部件少的简单性。删除传统上驻留在Docker主机NIC和容器接口之间的网桥留下了一个非常简单的设置，包括容器接口，直接连接到Docker主机接口。由于在这些情况下没有端口映射，因此可以轻松访问外部服务。"],["body","\n"],["body","Macvlan Bridge模式示例用法"],["body","\n"],["body","略"],["body","\n"],["body","详见"],["body","\n"],["headingLink","docker网络管理命令"],["heading","docker网络管理命令"],["body","\n"],["body","#创建自定义网络\ndocker network create\n# 上线一个网络\ndocker network connect\n# 查看网络\ndocker network ls\n# 删除网络\ndocker network rm\n# 下线网络\ndocker network disconnect\n# 查看网络明细\ndocker network inspect\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","1.容器_docker/docker离线安装.html"],["title","docker离线安装 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","docker离线安装"],["heading","Docker离线安装"],["body","\n"],["headingLink","下载"],["heading","下载"],["body","\n"],["body","下载链接：20.10.7.tgz"],["body","\n"],["headingLink","解压"],["heading","解压"],["body","\n"],["body","将解压出来的docker文件内容移动到 /usr/bin/ 目录下"],["body","\n"],["headingLink","注册为service"],["heading","注册为service"],["body","\n"],["body","vim /etc/systemd/system/docker.service\n"],["body","\n"],["body","[Unit]\nDescription=Docker Application Container Engine\nDocumentation=https://docs.docker.com\nAfter=network-online.target firewalld.service\nWants=network-online.target\n[Service]\nType=notify\n# the default is not to use systemd for cgroups because the delegate issues still\n# exists and systemd currently does not support the cgroup feature set required\n# for containers run by docker\nExecStart=/usr/bin/dockerd\nExecReload=/bin/kill -s HUP $MAINPID\n# Having non-zero Limit*s causes performance problems due to accounting overhead\n# in the kernel. We recommend using cgroups to do container-local accounting.\nLimitNOFILE=infinity\nLimitNPROC=infinity\nLimitCORE=infinity\n# Uncomment TasksMax if your systemd version supports it.\n# Only systemd 226 and above support this version.\n#TasksMax=infinity\nTimeoutStartSec=0\n# set delegate yes so that systemd does not reset the cgroups of docker containers\nDelegate=yes\n# kill only the docker process, not all processes in the cgroup\nKillMode=process\n# restart the docker process if it exits prematurely\nRestart=on-failure\nStartLimitBurst=3\nStartLimitInterval=60s\n \n[Install]\nWantedBy=multi-user.target\n"],["body","\n"],["body","添加文件权限并启动docker，执行如下命令："],["body","\n"],["body","chmod +x /etc/systemd/system/docker.service                      #添加文件权限\nsystemctl daemon-reload                                                       #重载unit配置文件\nsystemctl start docker                                                            #启动Docker\nsystemctl enable docker.service                                            #设置开机自启\n"],["body","\n"],["body","验证docker安装是否成功："],["body","\n"],["body","systemctl status docker                                                         #查看Docker状态\ndocker -v                                                                                #查看Docker版本\n"],["body","\n"],["headingLink","导入离线镜像"],["heading","导入离线镜像"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","1.容器_docker/dockcer-swarm.html"],["title","dockcer-swarm - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","前言"],["heading","前言"],["body","\n"],["body","Docker Swarm 是 Docker 的集群管理工具。"],["body","\n"],["body","支持的工具包括但不限于以下各项："],["body","\n\n"],["body","Dokku"],["body","\n"],["body","Docker Compose"],["body","\n"],["body","Docker Machine"],["body","\n"],["body","Jenkins"],["body","\n\n"],["body","swarm 集群由管理节点（manager）和工作节点（work node）构成。"],["body","\n\n"],["body","swarm mananger：负责整个集群的管理工作包括集群配置、服务管理等所有跟集群有关的工作。"],["body","\n"],["body","work node：即图中的 available node，主要负责运行相应的服务来执行任务（task）。"],["body","\n\n"],["body","Swarm 的配置和状态信息保存在一套位于所有管理节点上的分布式 etcd 数据库中。该数据库运行于内存中，并保持数据的最新状态。关于该数据库最棒的是，它几乎不需要任何配置，作为 Swarm 的一部分被安装，无须管理。"],["body","\n"],["headingLink","集群搭建"],["heading","集群搭建"],["body","\n"],["body","# 会将其切换到 Swarm 模式\ndocker swarm init\n\n# docker swarm init会通知 Docker 来初始化一个新的 Swarm，并将自身设置为第一个管理节点。同时也会使该节点开启 Swarm 模式。\n$ docker swarm init \\\n--advertise-addr 10.0.0.1:2377 \\\n--listen-addr 10.0.0.1:2377\n"],["body","\n"],["body","--advertise-addr 指定其他节点用来连接到当前管理节点的 IP 和端口。这一属性是可选的，当节点上有多个 IP 时，可以用于指定使用哪个IP。此外，还可以用于指定一个节点上没有的 IP，比如一个负载均衡的 IP。"],["body","\n"],["body","--listen-addr 指定用于承载 Swarm 流量的 IP 和端口。其设置通常与 --advertise-addr 相匹配，但是当节点上有多个 IP 的时候，可用于指定具体某个 IP。并且，如果 --advertise-addr 设置了一个远程 IP 地址（如负载均衡的IP地址），该属性也是需要设置的。建议执行命令时总是使用这两个属性来指定具体 IP 和端口。"],["body","\n"],["body","Swarm 模式下的操作默认运行于 2337 端口。虽然它是可配置的，但 2377/tcp 是用于客户端与 Swarm 进行安全（HTTPS）通信的约定俗成的端口配置。"],["body","\n"],["body","添加工作结点"],["body","\n"],["body","docker swarm join --token SWMTKN-1-1h6l8jcv1k6dxwd79tfel7tj3c9q94y9eau875odx56ghje385-4hlef30rwhix3mfruw50h9mpm 192.168.1.73:2377\n"],["body","\n"],["body","添加管理结点"],["body","\n"],["body","docker swarm join-token manager\n"],["body","\n"],["headingLink","swarm-管理器高可用性ha"],["heading","Swarm 管理器高可用性（HA）"],["body","\n"],["body","Swarm 的管理节点内置有对 HA 的支持。这意味着，即使一个或多个节点发生故障，剩余管理节点也会继续保证 Swarm 的运转。"],["body","\n"],["body","从技术上来说，Swarm 实现了一种主从方式的多管理节点的 HA。这意味着，即使你可能有多个管理节点，也总是仅有一个节点处于活动状态。"],["body","\n"],["body","通常处于活动状态的管理节点被称为“主节点”（leader），而主节点也是唯一一个会对 Swarm 发送控制命令的节点。也就是说，只有主节点才会变更配置，或发送任务到工作节点。如果一个备用（非活动）管理节点接收到了 Swarm 命令，则它会将其转发给主节点。"],["body","\n"],["body","关于 HA，有以下两条最佳实践原则。"],["body","\n\n"],["body","部署奇数个管理节点。"],["body","\n"],["body","不要部署太多管理节点（建议 3 个或 5 个）。"],["body","\n\n"],["headingLink","docker-swarm服务的部署及相关操作"],["heading","Docker Swarm服务的部署及相关操作"],["body","\n"],["headingLink","创建集群中的服务"],["heading","创建集群中的服务"],["body","\n"],["body","使用 docker service create 命令创建一个新的服务。"],["body","\n"],["body","docker service create --name web-fe \\\n-p 8080:8080 \\\n--replicas 5 \\\nnigelpoulton/pluralsight-docker-ci\n"],["body","\n\n"],["body","\n"],["body","docker service creale 命令告知 Docker 正在声明一个新服务，"],["body","\n"],["body","\n"],["body","\n"],["body","并传递 --name 参数将其命名为 web-fe。"],["body","\n"],["body","\n"],["body","\n"],["body","将每个节点上的 8080 端口映射到服务副本内部的 8080 端口。"],["body","\n"],["body","\n"],["body","\n"],["body","接下来，使用 --replicas 参数告知 Docker 应该总是有 5 个此服务的副本。"],["body","\n"],["body","\n"],["body","\n"],["body","最后，告知 Docker 哪个镜像用于副本，重要的是，要了解所有的服务副本使用相同的镜像和配置。"],["body","\n"],["body","\n\n"],["body","敲击回车键之后，主管理节点会在 Swarm 中实例化 5 个副本，管理节点也会作为工作节点运行。相关各工作节点或管理节点会拉取镜像，然后启动一个运行在 8080 端口上的容器。"],["body","\n"],["body","轮询监控"],["body","\n"],["body","所有的服务都会被 Swarm 持续监控，Swarm 会在后台进行轮训检查（Reconciliation Loop），来持续比较服务的实际状态和期望状态是否一致。如果一致，则无须任何额外操作；如果不一致，Swarm 会使其一致。换句话说，Swarm 会一直确保实际状态能够满足期望状态的要求。"],["body","\n"],["body","假如运行有 web-fe 副本的某个工作节点宕机了，则 web-fe 的实际状态从 5 个副本降为 4 个，从而不能满足期望状态的要求。Docker 变回启动一个新的 web-fe 副本来使实际状态与期望状态保持一致。这一特性功能强大，使得服务在面对节点宕机等问题时具有自愈能力。"],["body","\n"],["headingLink","服务命令"],["heading","服务命令"],["body","\n"],["body","查看服务"],["body","\n"],["body","# 只能在管理结点运行\ndocker service ls\n"],["body","\n"],["body","查看实际进程"],["body","\n"],["body","# 只能在管理结点运行\ndocker service ps service_name\n"],["body","\n"],["body","查看详细命令"],["body","\n"],["body","docker service inspect\n"],["body","\n"],["headingLink","副本服务-vs-全局服务"],["heading","副本服务 vs 全局服务"],["body","\n"],["body","服务的默认复制模式（Replication Mode）是副本模式（replicated）。"],["body","\n"],["body","这种模式会部署期望数量的服务副本，并尽可能均匀地将各个副本分布在整个集群中。"],["body","\n"],["body","另一种模式是全局模式（global），在这种模式下，每个节点上仅运行一个副本。可以通过给 docker service create 命令传递 --mode global 参数来部署一个全局服务。"],["body","\n"],["headingLink","服务的扩缩容"],["heading","服务的扩缩容"],["body","\n"],["body","docker service scale web-fe=10\n"],["body","\n"],["headingLink","删除服务"],["heading","删除服务"],["body","\n"],["body","docker service rm\n"],["body","\n"],["headingLink","滚动升级"],["heading","滚动升级"],["body","\n"],["body","# 同时升级 两台\n--update-parallelism 2 \\\n# 20s 延迟\n--update-delay 20s \n"],["body","\n"],["body","docker service update\n"],["body","\n"],["headingLink","docker-swarm服务日志及相关配置"],["heading","Docker Swarm服务日志及相关配置"],["body","\n"],["body","日志驱动"],["body","\n"],["body","Docker Swarm 服务的日志可以通过执行 docker service logs 命令来查看，然而并非所有的日志驱动（Logging Driver）都支持该命令。"],["body","\n"],["body","默认日志驱动"],["body","\n"],["body","Docker 节点默认的配置是，服务使用 json-file 日志驱动，其他的驱动还有 journald（仅用于运行有 systemd 的 Linux 主机）、syslog、splunk 和 gelf。"],["body","\n"],["body","json-file 和 journald 是较容易配置的，二者都可用于 docker service logs 命令。"],["body","\n"],["body","docker service logs <service-name>\n"],["body","\n"],["body","如下是在 daemon.json 配置文件中定义使用 syslog 作为日志驱动的示例。"],["body","\n"],["body","{\n  \"log-driver\": \"syslog\"\n}\n"],["body","\n"],["body","通过在执行 docker service create 命令时传入 --logdriver 和 --log-opts 参数可以强制某服务使用一个不同的日志驱动，这会覆盖 daemon.json 中的配置。"],["body","\n"],["body","服务日志能够正常工作的前提是，容器内的应用程序运行于 PID 为 1 的进程，并且将日志发送给 STDOUT，错误信息发送给 STDERR。日志驱动会将这些日志转发到其配置指定的位置。"],["body","\n"],["headingLink","swarm常用命令"],["heading","SWARM常用命令"],["body","\n"],["body","命令"],["body","说明"],["body","\n"],["body","docker swarm init"],["body","用于创建一个新的 Swarm。执行该命令的节点会成为第一个管理节点，并且会切换到 Swarm 模式。"],["body","\n"],["body","docker swarm join-token"],["body","用于查询加入管理节点和工作节点到现有 Swarm 时所使用的命令和 Token。 要获取新增管理节点的命令，请执行 docker swarm join-token manager 命令； 要获取新增工作节点的命令，请执行 docker swarm join-token worker 命令。"],["body","\n"],["body","docker node ls"],["body","用于列出 Swarm 中的所有节点及相关信息，包括哪些是管理节点、哪个是主管理节点。"],["body","\n"],["body","docker service create"],["body","用于创建一个新服务。"],["body","\n"],["body","docker service ls"],["body","用于列出 Swarm 中运行的服务，以及诸如服务状态、服务副本等基本信息。"],["body","\n"],["body","docker service ps "],["body","该命令会给出更多关于某个服务副本的信息"],["body","\n"],["body","docker service inspect"],["body","用于获取关于服务的详尽信息。附加 --pretty 参数可限制仅显示重要信息。"],["body","\n"],["body","docker service scale"],["body","用于对服务副本个数进行增减。"],["body","\n"],["body","docker service update"],["body","用于对运行中的服务的属性进行变更。"],["body","\n"],["body","docker service logs"],["body","用于查看服务的日志。"],["body","\n"],["body","docker service rm"],["body","用于从 Swarm 中删除某服务。该命令会在不做确认的情况下删除服务的所有副本，所以使用时应保持警惕。"],["body","\n\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","1.容器_docker/docker搭建私有镜像库.html"],["title","docker搭建私有镜像库 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","前言"],["heading","前言"],["body","\n"],["body","主要介绍registry、harbor两种私有仓库搭建。"],["body","\n"],["headingLink","registry-的搭建"],["heading","registry 的搭建"],["body","\n"],["body","Docker 官方提供了一个搭建私有仓库的镜像 registry ，只需把镜像下载下来，运行容器并暴露5000端口，就可以使用了。"],["body","\n"],["body","docker pull registry:2\ndocker run -d -v /opt/registry:/var/lib/registry -p 5000:5000 --name myregistry registry:2\n\n"],["body","\n"],["body","Registry服务默认会将上传的镜像保存在容器的/var/lib/registry，我们将主机的/opt/registry目录挂载到该目录，即可实现将镜像保存到主机的/opt/registry目录了。"],["body","\n"],["body","浏览器访问http://127.0.0.1:5000/v2，出现下面情况说明registry运行正常。"],["body","\n"],["body","现在通过push镜像到registry来验证一下。"],["body","\n"],["body","要通过docker tag将该镜像标志为要推送到私有仓库："],["body","\n"],["body","使用docker tag将session-web:latest这个镜像标记为 127.0.0.1:5000/session-web:latest\n#格式为\ndocker tag IMAGE[:TAG][REGISTRY_HOST[:REGISTRY_PORT]/]REPOSITORY[:TAG]\ndocker tag session-web:latest 127.0.0.1:5000/session-web:latest\n#使用docker push上传标记的镜像\ndocker push 127.0.0.1:5000/session-web:latest\n"],["body","\n"],["body","下载私有仓库的镜像，使用如下命令："],["body","\n"],["body","docker pull localhost:5000/镜像名:版本号\ndocker pull localhost:5000/nginx:latest\n"],["body","\n"],["headingLink","harbor-的搭建"],["heading","harbor 的搭建"],["body","\n"],["body","docker 官方提供的私有仓库 registry，用起来虽然简单 ，但在管理的功能上存在不足。 Harbor是一个用于存储和分发Docker镜像的企业级Registry服务器，harbor使用的是官方的docker registry(v2命名是distribution)服务去完成。harbor在docker distribution的基础上增加了一些安全、访问控制、管理的功能以满足企业对于镜像仓库的需求。"],["body","\n"],["headingLink","搭建"],["heading","搭建"],["body","\n"],["headingLink","下载"],["heading","下载"],["body","\n"],["body","github"],["body","\n"],["headingLink","配置"],["heading","配置"],["body","\n"],["body","tar -xvf harbor-offline-installer-{version}.tgz\n"],["body","\n"],["body","**修改 harbor.yml **"],["body","\n"],["body","#hostname 改为 0.0.0.0\nhostname = \nhttps 配置注释掉\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","1.容器_docker/docker搭建registry仓库.html"],["title","docker搭建registry仓库 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","总览"],["heading","总览"],["body","\n"],["body","\n"],["body","registry 是一个无状态、高度可扩展的服务器端应用程序，用于存储和分发 Docker 镜像"],["body","\n"],["body","\n"],["headingLink","基本命令"],["heading","基本命令"],["body","\n"],["body","建立镜像"],["body","\n"],["body","docker run -d -p 5000:5000 --name registry registry:2\n"],["body","\n"],["body","推送与拉取镜像"],["body","\n"],["body","docker pull ubuntu\ndocker image tag ubuntu localhost:5000/myfirstimage\ndocker push localhost:5000/myfirstimage\ndocker pull localhost:5000/myfirstimage\n"],["body","\n"],["body","删除镜像"],["body","\n"],["body","docker container stop registry && docker container rm -v registry\n"],["body","\n"],["headingLink","镜像命名"],["heading","镜像命名"],["body","\n"],["body","典型 docker 命令中使用的 镜像名称反映了它们的来源："],["body","\n\n"],["body","docker pull ubuntu instructs docker to pull an image named ubuntu from the official Docker Hub. This is simply a shortcut for the longer docker pull docker.io/library/ubuntu command"],["body","\n"],["body","docker pull myregistrydomain:port/foo/bar instructs docker to contact the registry located at myregistrydomain:port to find the image foo/bar"],["body","\n\n"],["headingLink","搭建外部访问的-registry"],["heading","搭建外部访问的 registry"],["body","\n"],["headingLink","获取自签名证书"],["heading","获取自签名证书"],["body","\n"],["body","修改  /etc/pki/tls/openssl.cnf"],["body","\n"],["body","sed  '/^\\[ v3_ca \\]/  a subjectAltName = IP:192.168.1.73'   /etc/pki/tls/openssl.cnf\n"],["body","\n"],["body","生成证书"],["body","\n"],["body","openssl req \\\n  -newkey rsa:4096 -nodes -sha256 -keyout certs/domain.key \\\n  -x509 -days 365 -out certs/domain.crt\n  \n  #-addext \"subjectAltName = IP:192.168.1.73\" \\\n\n"],["body","\n"],["body","加入到docker"],["body","\n"],["body","docker secret rm domain.crt\ndocker secret rm domain.key\ndocker secret create domain.crt certs/domain.crt\ndocker secret create domain.key certs/domain.key\n"],["body","\n"],["body","copy证书到 所有docker"],["body","\n"],["body","cp certs/domain.crt  /etc/docker/certs.d/192.168.1.73:5000/ca.crt\n\n"],["body","\n"],["body","创建镜像"],["body","\n"],["body","docker service create \\\n  --name registry \\\n  --secret domain.crt \\\n  --secret domain.key \\\n  --constraint 'node.role==manager' \\\n  --mount type=bind,source=/data/registry,destination=/var/lib/registry \\\n  -e REGISTRY_HTTP_TLS_CERTIFICATE=/run/secrets/domain.crt \\\n  -e REGISTRY_HTTP_TLS_KEY=/run/secrets/domain.key \\\n  --publish published=5000,target=5000 \\\n  --replicas 1 \\\n  registry:2\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","1.容器_docker/docker远程连接配置.html"],["title","docker远程连接配置 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","以idea为例"],["heading","以IDEA为例"],["body","\n"],["body","配置 TLS 实现安全的 Docker 远程连接。"],["body","\n"],["headingLink","非安全的连接方式"],["heading","非安全的连接方式"],["body","\n"],["body","以CentOS为例"],["body","\n"],["headingLink","配置-socket-service"],["heading","配置 socket-service"],["body","\n"],["body","vim /etc/systemd/system/docker-tcp.socket\n\n[Unit]\nDescription=Docker Socket for the API\n\n[Socket]\n# ListenStream=127.0.0.1:2375\nListenStream=2375\nBindIPv6Only=both\nService=docker.service\n\n[Install]\nWantedBy=sockets.target\n"],["body","\n"],["headingLink","重新启动服务"],["heading","重新启动服务"],["body","\n"],["body","$ sudo systemctl daemon-reload\n$ sudo systemctl enable docker-tcp.socket\n$ sudo systemctl stop docker\n$ sudo systemctl start docker-tcp.socket\n$ sudo systemctl start docker\n\n# 注意：这种方法必须先启动 docker-tcp.socket，再启动 Docker，一定要注意启动顺序！\n"],["body","\n"],["headingLink","客户端测试连接"],["heading","客户端测试连接"],["body","\n"],["body","\ndocker -H 192.168.57.110:2375 info\n"],["body","\n"],["headingLink","配置环境变量简化连接"],["heading","配置环境变量简化连接"],["body","\n"],["body"," export DOCKER_HOST=\"tcp://0.0.0.0:2375\"\n docker info\n"],["body","\n"],["headingLink","配置tls安全连接"],["heading","配置TLS安全连接"],["body","\n"],["body","官方文档"],["body","\n"],["body","步骤"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","1.容器_docker/docker-swarm集群搭建实例.html"],["title","docker-swarm集群搭建实例 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","端口放开"],["heading","端口放开"],["body","\n\n"],["body","TCP port 2377 for cluster management communications 用于集群管理通信"],["body","\n"],["body","TCP and UDP port 7946 for communication among nodes  节点之间通信"],["body","\n"],["body","UDP port 4789 for overlay network traffic  用于 overlay 网络 的流量通信"],["body","\n"],["body","5000 端口 私有仓库访问端口"],["body","\n\n"],["body","firewall-cmd --add-port=2377/tcp --permanent\nfirewall-cmd --add-port=7946/tcp --permanent\nfirewall-cmd --add-port=7946/udp --permanent\nfirewall-cmd --add-port=5000/tcp --permanent\nfirewall-cmd --add-port=443/tcp --permanent\nfirewall-cmd --reload\n"],["body","\n"],["headingLink","集群管理命令"],["heading","集群管理命令"],["body","\n"],["headingLink","初始化节点"],["heading","初始化节点"],["body","\n"],["body","# 初始化为 管理节点\t\ndocker swarm init\n"],["body","\n"],["headingLink","以-工作节点加入到管理节点"],["heading","以 工作节点加入到管理节点"],["body","\n"],["body"," docker swarm join \\\n    --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \\\n    192.168.99.100:2377\n"],["body","\n"],["headingLink","查看以工作节点加入到集群中的命令"],["heading","查看以工作节点加入到集群中的命令"],["body","\n"],["body","docker swarm join-token worker\n"],["body","\n"],["headingLink","查看以管理节点加入到集群中的命令"],["heading","查看以管理节点加入到集群中的命令"],["body","\n"],["body","docker swarm join-token manager\n"],["body","\n"],["headingLink","新建registry-私有仓库"],["heading","新建Registry 私有仓库"],["body","\n"],["body","docker service create  --constraint node.role==manager    --name registry --publish published=5000,target=5000 registry:2\n"],["body","\n"],["headingLink","新建虚拟化服务"],["heading","新建虚拟化服务"],["body","\n"],["body","docker pull dockersamples/visualizer:latest\ndocker service create \\\n--name=viz \\\n--publish=8081:8080/tcp \\\n--constraint=node.role==manager \\\n--mount=type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \\\ndockersamples/visualizer:latest\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","1.容器_docker/redis主从与哨兵.html"],["title","redis主从与哨兵 - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","全局修改地方"],["heading","全局修改地方"],["body","\n"],["body","protected-mode no\nappendonly yes\n"],["body","\n"],["headingLink","变量定义"],["heading","变量定义"],["body","\n"],["headingLink","redis通用变量"],["heading","redis通用变量"],["body","\n"],["body","port 6001\npidfile \"/var/run/redis_6001.pid\"\nbind 172.16.48.129 127.0.0.1\n"],["body","\n"],["headingLink","redis进程"],["heading","redis进程"],["body","\n"],["body","requirepass \"123456\"\nmasterauth \"123456\"\n"],["body","\n"],["headingLink","redis从进程"],["heading","redis从进程"],["body","\n"],["body","#新增\nrequirepass \"123456\"\nmasterauth \"123456\"\nslaveof 192.168.1.88 6001\n"],["body","\n"],["headingLink","哨兵进程"],["heading","哨兵进程"],["body","\n"],["body","sentinel monitor mymaster 192.168.1.32 6001 2\nsentinel down-after-milliseconds mymaster 5000\nsentinel failover-timeout mymaster 15000\nsentinel parallel-syncs mymaster 2\nsentinel auth-pass mymaster 123456\n"],["body","\n"],["headingLink","ip配置"],["heading","IP配置"],["body","\n"],["body","#通用变量\nREDIS_IMAGE_NAME=myredis-server\nSENTINEl_IMAGE_NAME=myredis-sentinel\n\n# 创建 一主两从，三哨兵\n\nMASTER1=172.18.1.2\nSLAVE1=172.18.1.3\nSLAVE2=172.18.1.4\nSENTINEL1=172.18.1.5\nSENTINEL2=172.18.1.6\nSENTINEL3=172.18.1.7\n\n\n\n# 函数定义  --------------------------------------------------------------------------------------\n# 创建容器\ncreate_container(){\n ip=$1;shift;\n image_name=$1;shift;\n docker run -d --name ${image_name}_${ip} --ip $ip --network=mynet   $image_name\n}\n\nfunction delete_container(){\n        ip=$1;shift;\n        image_name=$1;shift;\n        docker stop ${image_name}_${ip}\n        docker rm ${image_name}_${ip}\n}\n\nfunction restartContainer(){\n        ip=$1;shift;\n        image_name=$1;shift;\n        docker restart ${image_name}_${ip}\n}\n\n\n# 函数定义结束  --------------------------------------------------------------------------------------\n"],["body","\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]],[["_relative_fp","7.消息队列_KafaKa/index.html"],["title","KafaKa - 中间件"],["body","\n    \n        \n        \n\n        \n        \n\n        \n        \n\n        \n        \n\n        \n\n        \n\n            \n                                \n                \n                    \n                        "],["body","\n                            \n                        "],["body","\n                        "],["body","\n                            \n                        "],["body","\n                        \n                            "],["body","Light"],["body","\n                            "],["body","Rust"],["body","\n                            "],["body","Coal"],["body","\n                            "],["body","Navy"],["body","\n                            "],["body","Ayu"],["body","\n                        \n                    \n\n                    "],["heading","中间件"],["body","\n\n                    \n\n                    \n                \n\n\n                \n                \n\n                \n                    "],["body","\n                        \n\n"],["body","\n"],["body","\n\n"],["headingLink","kafka是如何保证数据的可靠性"],["heading","Kafka是如何保证数据的可靠性?"],["body","\n"],["headingLink","副本机制"],["heading","副本机制"],["body","\n"],["body","单个topic可以有多个 分区, 每个分区 都可以设置 副本数量,只有一个leader副本进行外部数据的写入,然后由leader将数据转发给各个副本,保证集群间数据的一致."],["body","\n"],["headingLink","acks参数"],["heading","acks参数"],["body","\n"],["body","三种选项 不等待答复|等待leader答复|等待所有节点答复"],["body","\n"],["headingLink","isr机制"],["heading","ISR机制"],["body","\n"],["body","ISR机制保证了leader写入数据成功并且至少有一个follower同步完成leader的数据，才会认为消息发送成功"],["body","\n"],["body","每个partition中会维护着一个ISR列表包含leader，还有与它同步的follower"],["body","\n"],["body","只要某个follower会同步leader的数据，那么肯定会在该列表中"],["body","\n"],["body","如果某个follower因为自身发生问题，不能同步数据，那么会被认为“out of sync”，从ISR列表中删除"],["body","\n"],["headingLink","kafka是如何保证数据的顺序性"],["heading","Kafka是如何保证数据的顺序性?"],["body","\n"],["body","Kafka只能保证 单个topics 的 单个partition, 的数据顺序"],["body","\n"],["body","因为在partition 内维护了 offset"],["body","\n"],["headingLink","kafka的消息处理消息交付语义"],["heading","Kafka的消息处理/消息交付语义"],["body","\n"],["body","最多一次：消息可能丢失也可能被处理，但最多只会被处理一次。"],["body","\n"],["body","至少一次：消息不会丢失，但可能被处理多次"],["body","\n"],["body","精确一次：消息被处理且只会被处理一次"],["body","\n"],["headingLink","kafka如何判断节点是否还活着"],["heading","Kafka如何判断节点是否还活着"],["body","\n\n"],["body","建立与zk的连接,在zk上建立一个连接节点"],["body","\n"],["body","如果是follower节点 能及时的同步leader写操作,不能延时太久"],["body","\n\n"],["headingLink","pull模式与push模式的优缺点"],["heading","PULL模式与PUSH模式的优缺点"],["body","\n"],["body","push "],["body","\n\n"],["body","由broker决定消息推送速率,当broker推送的速率远大于consumer 消费的效率,consumer会崩溃"],["body","\n"],["body","但能够及时的推送"],["body","\n\n"],["body","pull"],["body","\n"],["body","consumer能根据自己的消费能力去决定"],["body","\n"],["body","需要不断轮询"],["body","\n"],["headingLink","topic-partition"],["heading","TOPIC partition"],["body","\n"],["headingLink","分配规则"],["heading","分配规则"],["body","\n"],["body","接受 topic create 请求的 结点 为 0号分区,按照 集群ID 依次有序分配, 当partitions数量 > brokers数量,会轮回再次分配"],["body","\n"],["headingLink","命名规则"],["heading","命名规则"],["body","\n"],["body","paritions名称为:topic-name-index,  index分区索引编号，从0开始依次递增"],["body","\n"],["headingLink","分区文件存储方式"],["heading","分区文件存储方式"],["body","\n\n"],["body","\n"],["body","将大文件切割成 segment file"],["body","\n"],["body","\n"],["body","\n"],["body","segment file组成：由2大部分组成，分别为segment data file和segment index file,此2个文件一一对应，成对出现."],["body","\n"],["body","\n"],["body","\n"],["body","partition文件命名"],["body","\n\n"],["body","partion全局的第一个segment从0开始，"],["body","\n"],["body","后续每个segment文件名为上一个segment文件最后一条消息的offset值"],["body","\n"],["body","数值最大为64位long大小，19位数字字符长度，没有数字用0填充"],["body","\n\n"],["body","\n"],["body","\n"],["body","partition.log文件 由 很多message 顺序存储"],["body","\n"],["body","message结构"],["body","\n"],["body","\n"],["body","参数说明"],["body","\n"],["body","关键字"],["body","解释说明"],["body","\n"],["body","8 byte offset"],["body","在parition(分区)内的每条消息都有一个有序的id号，这个id号被称为偏移(offset),它可以唯一确定每条消息在parition(分区)内的位置。即offset表示partiion的第多少message"],["body","\n"],["body","4 byte message size"],["body","message大小"],["body","\n"],["body","4 byte CRC32"],["body","用crc32校验message"],["body","\n"],["body","1 byte “magic”"],["body","表示本次发布Kafka服务程序协议版本号"],["body","\n"],["body","1 byte “attributes”"],["body","表示为独立版本、或标识压缩类型、或编码类型。"],["body","\n"],["body","4 byte key length"],["body","表示key的长度,当key为-1时，K byte key字段不填"],["body","\n"],["body","K byte key"],["body","可选"],["body","\n"],["body","value bytes payload"],["body","表示实际消息数据。"],["body","\n\n"],["body","\n"],["body","\n"],["body","索引文件"],["body","\n\n"],["body","稀疏索引存储方式"],["body","\n"],["body","记录了 message 在 log文件中的 序号以及 对应的 字节偏移"],["body","\n\n"],["body","\n"],["body","\n"],["body","如何通过 offset 查找相应的 message"],["body","\n\n"],["body","根据文件列表的 数字命名 二分查找 定位  相应的 log文件,与index文件"],["body","\n"],["body","通过索引文件定位  该 offset下的 物理字节偏移"],["body","\n"],["body","最后通过 去log文件中查找数据"],["body","\n\n"],["body","\n\n"],["headingLink","消息队列优点"],["heading","消息队列优点"],["body","\n"],["body","解耦,冗余,扩展,灵活,峰值处理,可恢复性,顺序保证,异步通信"],["body","\n"],["headingLink","kafka架构"],["heading","kafka架构"],["body","\n\n"],["body","\n"],["body","生产者"],["body","\n"],["body","\n"],["body","\n"],["body","消费者"],["body","\n\n"],["body","消费者组 中的 多个客户端 不能重复消息"],["body","\n"],["body","同一个Topic的同一个分区的数据"],["body","\n\n"],["body","\n"],["body","\n"],["body","集群"],["body","\n"],["body","集群 -> broker(节点) -> TOPIC 主题 -> 分区"],["body","\n"],["body","\n"],["body","\n"],["body","zookeeper"],["body","\n"],["body","systemctl stop firewalld && ./zookeeper/bin/zkServer.sh start && ./zookeeper/bin/zkServer.sh status\n"],["body","\n"],["body","\n\n"],["body","操作命令"],["body","\n"],["body","解压\ntar -xf kafka_2.13-2.5.0.tgz\n\n启动\nbin/kafka-server-start.sh config/server.properties\n--daemon  后台\n创建topic\nbin/kafka-topics.sh --create --zookeeper 192.168.3.8:2181 --partitions 2 --replication-factor 3 --topic second\nCreated topic first.\n\n查看topic\nbin/kafka-topics.sh --list --zookeeper 192.168.3.8:2181\n\n生产者\nbin/kafka-console-producer.sh --broker-list 192.168.3.8:9092 --topic second \n\n控制台消费者-消费\nkafka_2.13-2.5.0/bin/kafka-console-consumer.sh --broker-list 192.168.3.8:9092  --topic second\n从开始消费数据 --from-begining\n\n新版本,offset维护在本地,\n--bootstrap-server 192.168.3.7:9092\n\n描述卡夫卡\nbin/kafka-topics.sh --zookeeper 192.168.3.7:2181 --describe --topic first\npartitionCount:2 分区2,relicationFactor:2 复本2\npartition0,leader:0,replicas:0,2, ISR:0,2 ,分区0 有两块副本,broker0为使用分区,ISR为分区不可用时的副本使用顺序\n\n"],["body","\n"],["headingLink","分区原则"],["heading","分区原则"],["body","\n\n"],["body","指定了分区 直接使用分区"],["body","\n"],["body","未指定partition 但指定key,通过key 的valuehash分区"],["body","\n"],["body","key未指定,轮询选出 一个partition"],["body","\n\n"],["headingLink","副本replication"],["heading","副本(replication)"],["body","\n"],["headingLink","描述"],["heading","描述"],["body","\n\n"],["body","同一个分区的有多个副本"],["body","\n"],["body","需要在 这几个副本中选出 leader, producer与consumer 只与leader交互 ,其他replication作为follower 从leader中复制数据"],["body","\n"],["body","副本创建好之后,会有一个 可用序列, 决定了 leader 不可用时,下一个可用的副本broker节点"],["body","\n"],["body","replication 副本之间不可能在同一个 broker"],["body","\n\n"],["headingLink","副本之间的同步机制"],["heading","副本之间的同步机制"],["body","\n"],["body","ALL"],["body","\n"],["body","producer向leader写入 数据, 等待所有follower同步完成"],["body","\n"],["body","1"],["body","\n"],["body","producer 本地写到日志中后 就立刻返回"],["body","\n"],["body","0"],["body","\n"],["body","producer 不等待任何确认即返回,一般确认数为 -1"],["body","\n"],["headingLink","javaapi"],["heading","JavaAPI"],["body","\n"],["headingLink","生产者"],["heading","生产者"],["body","\n\n"],["body","\n"],["body","生产者配置信息"],["body","\n\n"],["body","\n"],["body","bootstrap.servers  :"],["body","\n"],["body","Kafka集群地址,host:port,host:port"],["body","\n"],["body","\n"],["body","\n"],["body","acks:应答级别"],["body","\n"],["body","\n"],["body","\n"],["body","retries:重试次数"],["body","\n"],["body","\n"],["body","\n"],["body","生产者提交数据的 阈值"],["body","\n\n"],["body","batch.size :批量大小"],["body","\n"],["body","linger.ms:提交延时"],["body","\n\n"],["body","\n"],["body","\n"],["body","buffer.memory"],["body","\n\n"],["body","缓存区大小"],["body","\n\n"],["body","\n"],["body","\n"],["body","KV的序列化类"],["body","\n\n"],["body","key.serializer*:"],["body","\n\n"],["body","\n"],["body","\n"],["body","位于ProducerConfig"],["body","\n"],["body","\n\n"],["body","\n"],["body","\n"],["body","实例化生产者"],["body","\n"],["body","KafkaProducer<String, String> producer = new KafkaProducer<>(p);\n"],["body","\n"],["body","\n"],["body","\n"],["body","发送消息"],["body","\n"],["body","  producer.send(new ProducerRecord<String, String>(\"first\", i + \"\"),(metadata, exception) -> {\n                System.out.println(\"offset:\"+metadata.offset()+\";partition:\"+metadata.partition());\n            });\n            producer.flush();\n"],["body","\n"],["body","\n\n"],["body","自定义分区"],["body","\n\n"],["body","\n"],["body","实现Partitioner接口"],["body","\n"],["body","\n"],["body","\n"],["body","关键方法"],["body","\n"],["body","public int partition"],["body","\n"],["body","(String topic, Object key, byte[] keyBytes, "],["body","\n"],["body","Object value, byte[] valueBytes, "],["body","\n"],["body","Cluster cluster);"],["body","\n"],["body","根据,topic,key,序列化的keybytes,值,序列化的值,集群配置等决定分区"],["body","\n"],["body","\n"],["body","\n"],["body","默认方法onNewBatch "],["body","\n\n"],["body","创建新的批次时通知"],["body","\n"],["body","API默认一个批次一个批次的推送消息过去"],["body","\n\n"],["body","\n"],["body","\n"],["body","配置属性 PARTITIONER_CLASS_CONFIG 批次类"],["body","\n"],["body","\n\n"],["headingLink","消费者"],["heading","消费者"],["body","\n"],["body","属性"],["body","\n\n"],["body","\n"],["body","bootstrap.servers 集群"],["body","\n"],["body","\n"],["body","\n"],["body","消费组ID group.id"],["body","\n"],["body","\n"],["body","\n"],["body","enable.auto.commit  自动提交offset"],["body","\n"],["body","\n"],["body","\n"],["body","auto.commit.interval.ms 提交延时"],["body","\n"],["body","\n"],["body","\n"],["body","KV反序列化 "],["body","\n"],["body","\n"],["body","\n"],["body","AUTO_OFFSET_RESET_CONFIG"],["body","\n"],["body","earliest , lastest,每次连接到 Kafka集群 自动 offset"],["body","\n"],["body","\n\n"],["body","对象实例化"],["body","\n"],["body","KafkaConsumer"],["body","\n"],["body","订阅主题"],["body","\n"],["body","subscribe"],["body","\n"],["body","拉取消息"],["body","\n"],["body","poll"],["body","\n"],["body","直接定位offset"],["body","\n"],["body","seek"],["body","\n"],["headingLink","消费者细化api"],["heading","消费者细化API"],["body","\n"],["body","步骤"],["body","\n\n"],["body","\n"],["body","根据指定分区 从主题元数据中找到 主副本"],["body","\n"],["body","findLeader"],["body","\n"],["body","\n"],["body","\n"],["body","获取分区最新消费进度"],["body","\n"],["body","getLastOffset"],["body","\n"],["body","\n"],["body","\n"],["body","从主副本拉去分区信息"],["body","\n"],["body","run"],["body","\n"],["body","\n"],["body","\n"],["body","识别主副本的变化 重试"],["body","\n"],["body","findNewLeader"],["body","\n"],["body","\n\n"],["headingLink",""],["body","\n"],["headingLink","拦截器"],["heading","拦截器"],["body","\n"],["body","ProducerInterceptor "],["body","\n"],["body","方法"],["body","\n"],["body","configure"],["body","\n"],["body","获取配置信息和初始化数据时调用"],["body","\n"],["body","onsend(ProducerRecord)"],["body","\n"],["body","发送前,序列化前 调用"],["body","\n"],["body","onAcknowledement(RecordMetaData,Exception)"],["body","\n"],["body","在producer的回调 之前调用"],["body","\n"],["body","close "],["body","\n"],["body","关闭 "],["body","\n"],["body","设置拦截器"],["body","\n"],["body","Interceptor_classes_config"],["body","\n"],["headingLink","kafka-eos-语义"],["heading","Kafka EOS 语义"],["body","\n"],["body","exactly once semantics"],["body","\n"],["body","精确一次处理语义"],["body","\n"],["headingLink","kafka-幂等性-idempotent"],["heading","Kafka 幂等性 Idempotent"],["body","\n\n"],["body","Kafka的幂等性实现 引入了 PID(producerID) 和 sequenceNumber"],["body","\n"],["body","pid:对于用户透明,每个producer在初始化的时候会被分配一个唯一的PID"],["body","\n"],["body","sequenceNumber ,对于每一个PID,该producer发送到每个partition的数据都有对应的序列号,这些序列号时从0开始单调递增,broker只接收序号大于其缓存中 1 的,否则就丢弃"],["body","\n"],["body","涉及的参数是 enable.idempotence = true"],["body","\n"],["body","只能保证同个 producer 单会话,单个partition 的exactlyOnce语义"],["body","\n\n"],["headingLink","kafka事务"],["heading","Kafka事务"],["body","\n\n"],["body","正是因为 Kafka幂等性不提供跨多个partition和 跨会话场景下的保证能够原子的处理多个partition 的写入操作"],["body","\n\n"],["headingLink","使用事务api注意事项"],["heading","使用事务API注意事项"],["body","\n\n"],["body","需要消费者的自动模式设置为 false"],["body","\n"],["body","不能手动执行consumer#commitSync或者consumer#commitAsyc"],["body","\n"],["body","生产者配置 transactional.id 属性"],["body","\n"],["body","生产者不需要再配置 enable.idempotence，因为如果配置了transaction.id，则此时 enable.idempotence 会被设置为true"],["body","\n"],["body","消费者需要配置 isolation.level 属性，有两个可选值：\"read_committed\"，\"read_uncommitted\"，默认\"read_uncommitted\""],["body","\n\n"],["headingLink","事务api"],["heading","事务API"],["body","\n"],["body","为producer提供了"],["body","\n"],["body","initTransactions"],["body","\n"],["body","beginTransaction"],["body","\n"],["body","sendOffsetsToTransaction"],["body","\n"],["body","commitTransaction"],["body","\n"],["body","abortTransaction "],["body","\n"],["body"," Properties props = new Properties();\n        props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n        props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n        props.put(\"client.id\", \"ProducerTranscationnalExample\");\n        props.put(\"bootstrap.servers\", \"192.168.3.7:9092\");\n        props.put(\"transactional.id\", \"test-transactional\");\n        props.put(\"acks\", \"all\");\n\n        KafkaProducer producer = new KafkaProducer(props);\n\n        producer.initTransactions();\n\n        try {\n\n            String msg = \"matt test\";\n            producer.beginTransaction();\n            producer.send(new ProducerRecord(\"first\", \"1\", msg.toString()));\n            producer.send(new ProducerRecord(\"first\", \"1\", msg.toString()));\n            producer.send(new ProducerRecord(\"first\", \"1\", msg.toString()));\n            if(1 == 1)\n            throw  new ProducerFencedException(\"自定义异常\");\n\n            producer.commitTransaction();\n        } catch (ProducerFencedException e1) {\n\n            e1.printStackTrace();\n            producer.close();\n        } catch (KafkaException e2) {\n            e2.printStackTrace();\n            producer.abortTransaction();\n\n        }\n\n        producer.close();\n"],["body","\n"],["headingLink","事务具体实现"],["heading","事务具体实现"],["body","\n"],["body","寻找TC"],["body","\n\n"],["body","\n"],["body","Transaction Coordinator 运行在 Kafka 服务端，下面简称 TC 服务。"],["body","\n"],["body","\n"],["body","\n"],["body","Kafka 有个特殊的事务 topic，名称为*__transaction_state*  负责持久化事务消息，有50个分区，每个分区负责一部分事务。事务划分是根据 transaction id，计算出该事务属于哪个分区。这个分区的 leader 所在的机器，负责这个事务的TC 服务地址"],["body","\n"],["body","\n"],["body","\n"],["body","Producer 会首先从 Kafka 集群中选择任意一台机器，然后向其发送请求，获取 TC 服务的地址"],["body","\n"],["body","\n\n"],["body","初始化事务"],["body","\n\n"],["body","Producer 在使用事务功能，必须先自定义一个唯一的 transaction id。有了 transaction id，即使客户端挂掉了，它重启后也能继续处理未完成的事务。"],["body","\n"],["body","Kafka 实现事务需要依靠幂等性，而幂等性需要指定 producer id 。所以Producer在启动事务之前，需要向 TC 服务申请 producer id。TC 服务在分配 producer id 后，会将它持久化到事务 topic。"],["body","\n\n"],["body","发送消息"],["body","\n\n"],["body","\n"],["body","Producer 在接收到 producer id 后，就可以正常的发送消息了。不过发送消息之前，需要先将这些消息的分区地址，上传到 TC 服务，TC 服务会将这些分区地址持久化到事务 topic"],["body","\n"],["body","\n"],["body","\n"],["body","然后 Producer 才会真正的发送消息，这些消息与普通消息不同，它们会有一个字段，表示自身是事务消息。"],["body","\n"],["body","\n\n"],["body","发送提交请求"],["body","\n\n"],["body","TC 服务收到事务提交请求后，会先将提交信息先持久化到事务 topic"],["body","\n"],["body","持久化成功后，服务端就立即发送成功响应给 Producer"],["body","\n"],["body","TC服务找到该事务涉及到的所有分区，为每 个分区生成提交请求，存到队列里等待发送"],["body","\n\n"],["body","发送事务结果信息给分区"],["body","\n\n"],["body","后台线程会不停的从队列里，拉取请求并且发送到分区。当一个分区收到事务结果消息后，会将结果保存到分区里，并且返回成功响应到 TC服务。当 TC 服务收到所有分区的成功响应后，会持久化一条事务完成的消息到事务 topic。至此，一个完整的事务流程就完成了"],["body","\n\n\n\n\n"],["body","\n\n                    "],["body","\n\n                    \n                \n            \n\n            \n\n        \n\n\n\n\n        \n\n\n\n        \n        \n        \n\n        \n\n\n    \n    \n"]]]