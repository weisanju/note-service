<!DOCTYPE HTML>
<html lang="zh_CN" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>redis集群 - 中间件</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="/note-service/favicon.svg">
        <link rel="shortcut icon" href="/note-service/favicon.png">
        <link rel="stylesheet" href="/note-service/css/variables.css">
        <link rel="stylesheet" href="/note-service/css/general.css">
        <link rel="stylesheet" href="/note-service/css/chrome.css">

        <!-- Fonts -->
        <link rel="stylesheet" href="https://xjq-note.oss-cn-hangzhou.aliyuncs.com/FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="https://xjq-note.oss-cn-hangzhou.aliyuncs.com/fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="https://xjq-note.oss-cn-hangzhou.aliyuncs.com/highlight.css">
        <link rel="stylesheet" href="https://xjq-note.oss-cn-hangzhou.aliyuncs.com/tomorrow-night.css">
        <link rel="stylesheet" href="https://xjq-note.oss-cn-hangzhou.aliyuncs.com/ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item affix "><a href="../index.html">中间件</a></li><li class="chapter-item affix "><li class="part-title">容器</li><li class="chapter-item "><a href="../1.容器_docker/index.html">docker</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../1.容器_docker/DockerFile简介.html">DockerFile简介</a></li><li class="chapter-item "><a href="../1.容器_docker/dockcer-swarm.html">dockcer-swarm</a></li><li class="chapter-item "><a href="../1.容器_docker/docker-compose.html">docker-compose</a></li><li class="chapter-item "><a href="../1.容器_docker/docker-swarm集群搭建实例.html">docker-swarm集群搭建实例</a></li><li class="chapter-item "><a href="../1.容器_docker/docker-volume管理.html">docker-volume管理</a></li><li class="chapter-item "><a href="../1.容器_docker/docker容器连接.html">docker容器连接</a></li><li class="chapter-item "><a href="../1.容器_docker/docker搭建registry仓库.html">docker搭建registry仓库</a></li><li class="chapter-item "><a href="../1.容器_docker/docker搭建私有镜像库.html">docker搭建私有镜像库</a></li><li class="chapter-item "><a href="../1.容器_docker/docker离线安装.html">docker离线安装</a></li><li class="chapter-item "><a href="../1.容器_docker/docker网络管理.html">docker网络管理</a></li><li class="chapter-item "><a href="../1.容器_docker/docker远程连接配置.html">docker远程连接配置</a></li><li class="chapter-item "><a href="../1.容器_docker/redis主从与哨兵.html">redis主从与哨兵</a></li><li class="chapter-item "><a href="../1.容器_docker/使用实例.html">使用实例</a></li></ol></li><li class="chapter-item "><div>k8s</div><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../2.容器_k8s/k8s/index.html">k8s</a></li></ol></li><li class="chapter-item "><li class="part-title">运维与部署</li><li class="chapter-item "><div>ansible</div><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../3.运维与部署_ansible/ansible/index.html">ansible</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../3.运维与部署_ansible/ansible/1.入门.html">入门</a></li><li class="chapter-item "><a href="../3.运维与部署_ansible/ansible/Ansible.html">Ansible</a></li><li class="chapter-item "><a href="../3.运维与部署_ansible/ansible/Loops.html">Loops</a></li><li class="chapter-item "><a href="../3.运维与部署_ansible/ansible/ansible中的变量.html">ansible中的变量</a></li><li class="chapter-item "><a href="../3.运维与部署_ansible/ansible/ansible示例.html">ansible示例</a></li><li class="chapter-item "><a href="../3.运维与部署_ansible/ansible/inventor文件.html">inventor文件</a></li><li class="chapter-item "><a href="../3.运维与部署_ansible/ansible/playbooks.html">playbooks</a></li><li class="chapter-item "><a href="../3.运维与部署_ansible/ansible/临时命令行模式.html">临时命令行模式</a></li><li class="chapter-item "><a href="../3.运维与部署_ansible/ansible/2.AnsibleConcepts/index.html">2.AnsibleConcepts</a></li><li class="chapter-item "><a href="../3.运维与部署_ansible/ansible/3.内置模块/index.html">3.内置模块</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../3.运维与部署_ansible/ansible/3.内置模块/1.Command.html">Command</a></li><li class="chapter-item "><a href="../3.运维与部署_ansible/ansible/3.内置模块/2.Shell.html">Shell</a></li></ol></li></ol></li></ol></li><li class="chapter-item "><li class="part-title">持续集成</li><li class="chapter-item "><div>jenkins</div><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../4.持续集成_jenkins/Jenkins.html">Jenkins</a></li><li class="chapter-item "><a href="../4.持续集成_jenkins/jenkins_pipeline.html">jenkins_pipeline</a></li></ol></li><li class="chapter-item "><li class="part-title"></li><li class="chapter-item "><div>日志收集</div><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../5.日志收集/filebeat/index.html">filebeat</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../5.日志收集/filebeat/1.HowFilebeatWorks.html">HowFilebeatWorks</a></li><li class="chapter-item "><a href="../5.日志收集/filebeat/ConfigureFilebeat/index.html">ConfigureFilebeat</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../5.日志收集/filebeat/ConfigureFilebeat/1.Input.html">Input</a></li><li class="chapter-item "><a href="../5.日志收集/filebeat/ConfigureFilebeat/2.InputFileStream.html">InputFileStream</a></li></ol></li></ol></li></ol></li><li class="chapter-item "><div>Redis</div><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../10.Redis/命令杂项.html">命令杂项</a></li></ol></li><li class="chapter-item "><li class="part-title">消息队列</li><li class="chapter-item "><a href="../6.消息队列_RocketMQ/index.html">RocketMQ</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../6.消息队列_RocketMQ/MQ消息的投递机制.html">MQ消息的投递机制</a></li><li class="chapter-item "><a href="../6.消息队列_RocketMQ/MQ的一致性算法实现原理.html">MQ的一致性算法实现原理</a></li><li class="chapter-item "><a href="../6.消息队列_RocketMQ/架构和设计.html">架构和设计</a></li><li class="chapter-item "><a href="../6.消息队列_RocketMQ/案例.html">案例</a></li><li class="chapter-item "><a href="../6.消息队列_RocketMQ/概念和特性.html">概念和特性</a></li></ol></li><li class="chapter-item "><a href="../7.消息队列_KafaKa/index.html">KafaKa</a></li><li class="chapter-item affix "><li class="part-title">缓存中间件</li><li class="chapter-item expanded "><a href="../8.缓存中间件_Redis/index.html">Redis</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../8.缓存中间件_Redis/Redis应用-位图.html">Redis应用-位图</a></li><li class="chapter-item "><a href="../8.缓存中间件_Redis/Redis应用-分布式锁.html">Redis应用-分布式锁</a></li><li class="chapter-item "><a href="../8.缓存中间件_Redis/redis-keys与scan.html">redis-keys与scan</a></li><li class="chapter-item "><a href="../8.缓存中间件_Redis/redis-serverAndRedis-cli.html">redis-serverAndRedis-cli</a></li><li class="chapter-item "><a href="../8.缓存中间件_Redis/redis主从与哨兵.html">redis主从与哨兵</a></li><li class="chapter-item "><a href="../8.缓存中间件_Redis/redis发布与订阅.html">redis发布与订阅</a></li><li class="chapter-item expanded "><a href="../8.缓存中间件_Redis/redis集群.html" class="active">redis集群</a></li><li class="chapter-item "><div>redis数据结构</div><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../8.缓存中间件_Redis/redis数据结构/Redis有序集合(sortedSet).html">Redis有序集合(sortedSet)</a></li><li class="chapter-item "><a href="../8.缓存中间件_Redis/redis数据结构/基本数据结构.html">基本数据结构</a></li></ol></li></ol></li><li class="chapter-item "><li class="part-title">源码管理</li><li class="chapter-item "><a href="../9.源码管理_GIT/index.html">GIT</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../9.源码管理_GIT/GIT回滚.html">GIT回滚</a></li><li class="chapter-item "><a href="../9.源码管理_GIT/Git库管理.html">Git库管理</a></li><li class="chapter-item "><a href="../9.源码管理_GIT/gitflow.html">gitflow</a></li><li class="chapter-item "><a href="../9.源码管理_GIT/git使用案例.html">git使用案例</a></li><li class="chapter-item "><a href="../9.源码管理_GIT/git分支.html">git分支</a></li><li class="chapter-item "><a href="../9.源码管理_GIT/git命令杂项.html">git命令杂项</a></li><li class="chapter-item "><a href="../9.源码管理_GIT/git基础.html">git基础</a></li><li class="chapter-item "><a href="../9.源码管理_GIT/git指针与引用.html">git指针与引用</a></li><li class="chapter-item "><a href="../9.源码管理_GIT/git错误提交后场景.html">git错误提交后场景</a></li><li class="chapter-item "><a href="../9.源码管理_GIT/克隆.html">克隆</a></li><li class="chapter-item "><a href="../9.源码管理_GIT/历史提交.html">历史提交</a></li><li class="chapter-item "><a href="../9.源码管理_GIT/基本操作.html">基本操作</a></li><li class="chapter-item "><a href="../9.源码管理_GIT/杂项.html">杂项</a></li><li class="chapter-item "><div>GIT协作</div><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../9.源码管理_GIT/GIT协作/Git支持的协议.html">Git支持的协议</a></li><li class="chapter-item "><a href="../9.源码管理_GIT/GIT协作/冲突解决.html">冲突解决</a></li><li class="chapter-item "><a href="../9.源码管理_GIT/GIT协作/补丁文件交互.html">补丁文件交互</a></li><li class="chapter-item "><a href="../9.源码管理_GIT/GIT协作/远程版本库.html">远程版本库</a></li></ol></li><li class="chapter-item "><div>命令集合</div><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../9.源码管理_GIT/命令集合/AMEND.html">AMEND</a></li><li class="chapter-item "><a href="../9.源码管理_GIT/命令集合/LOG.html">LOG</a></li><li class="chapter-item "><a href="../9.源码管理_GIT/命令集合/MERGE.html">MERGE</a></li><li class="chapter-item "><a href="../9.源码管理_GIT/命令集合/PUSH.html">PUSH</a></li><li class="chapter-item "><a href="../9.源码管理_GIT/命令集合/REBASE.html">REBASE</a></li><li class="chapter-item "><a href="../9.源码管理_GIT/命令集合/RESET.html">RESET</a></li><li class="chapter-item "><a href="../9.源码管理_GIT/命令集合/TAG.html">TAG</a></li></ol></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">中间件</h1>

                    <div class="right-buttons">

                    </div>
                </div>


                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <link rel="stylesheet" href="/note-service/mw/infisearch_assets/search-ui-light.css">
<style>.light .infi-root,
.rust .infi-root,
.coal .infi-root,
.navy .infi-root,
.ayu .infi-root {
    --infi-shadow: none;
    --infi-border: 3px solid var(--table-header-bg);
    --infi-bg: var(--bg);
    --infi-triangle-bg: var(--table-header-bg);
    --infi-item-box-shadow: 0 1px 5px rgba(196, 192, 187, 0.8);
    --infi-title-fg: var(--fg);
    --infi-title-hover-fg: var(--fg);
    --infi-title-hover-bg: var(--table-header-bg);
    --infi-title-border-bottom-hover: 2px solid var(--table-header-bg);
    --infi-heading-fg: var(--fg);
    --infi-heading-hover-fg: var(--fg);
    --infi-body-fg: var(--fg);
    --infi-body-hover-fg: var(--fg);
    --infi-sub-bg: var(--bg);
    --infi-highlight: var(--search-mark-bg);
    --infi-highlight-bg: none;
    --infi-header-fg: var(--fg);
    --infi-checkbox-bg: #f8f8f8;
    --infi-checkbox-checked-bg: #fff;
    --infi-checkbox-border: #414141;
    --infi-filter-header-active: var(--infi-title-bg);
    --infi-error-fg: var(--fg);
    --infi-fine-print-fg: var(--fg);
    --infi-loading-bg: var(--fg);
    --infi-loading-secondary-bg: var(--fg);
    --infi-load-more-fg: var(--infi-title-fg);
    --infi-load-more-bg: var(--infi-title-bg);
    --infi-load-more-hover-fg: var(--infi-title-hover-fg);
    --infi-load-more-hover-bg: var(--infi-title-hover-bg);
    --infi-scrollbar-bg: none;
    --infi-scrollbar-thumb-bg: var(--sidebar-non-existant);
    --infi-fs-button-input-fg: var(--searchbar-shadow-color);
    --infi-fs-border: 3px solid var(--sidebar-bg);
    --infi-fs-box-shadow: none;
    --infi-fs-header-bg: var(--sidebar-bg);
    --infi-fs-header-box-shadow: none;
    --infi-tip-table-header-border: var(--table-border-color);
    --infi-tip-table-border: transparent;
    --infi-tip-table-alternate: var(--table-alternate-bg);
    --infi-tip-bg: var(--sidebar-bg);
    --infi-tip-fg: var(--sidebar-fg);
    --infi-tip-code-fg: var(--inline-code-color);
    --infi-tip-code-bg: transparent;
    --infi-tip-icon-bg: rgb(230, 230, 230);
    --infi-tip-icon-fg: rgb(80, 80, 80);
}

.light .infi-root {
    --infi-fs-header-close-fg: var(--searchresults-header-fg);
    --infi-fs-header-close-hover-fg: var(--fg);
}

.ayu .infi-root,
.rust .infi-root,
.coal .infi-root,
.navy .infi-root {
    --infi-fs-header-close-fg: var(--sidebar-fg);
    --infi-fs-header-close-hover-fg: white;
}

.ayu .infi-root,
.rust .infi-root {
    --infi-tip-code-fg: var(--search-mark-bg) !important;
    --infi-tip-icon-bg: rgb(200, 200, 200);
    --infi-tip-icon-fg: rgb(50, 50, 50);
}

.light .infi-root .infi-list-item.focus,
.rust .infi-root .infi-list-item.focus,
.coal .infi-root .infi-list-item.focus,
.navy .infi-root .infi-list-item.focus,
.ayu .infi-root .infi-list-item.focus {
    outline: 2px solid grey;
}

.light .infi-root,
.coal .infi-root,
.navy .infi-root,
.ayu .infi-root {
    --infi-title-bg: var(--theme-hover);
    --infi-sub-hover-bg: var(--table-alternate-bg);
    --infi-title-border-bottom: 2px solid var(--theme-hover);
}

.light .infi-root {
    --infi-highlight: #82a6c4;
    --infi-sub-hover-bg: #ebebeb;
}

.rust .infi-root {
    --infi-highlight: #bc8e6a;

    --infi-tip-table-alternate: var(--sidebar-bg);
    --infi-title-bg: var(--table-header-bg);
    --infi-title-border-bottom: 2px solid var(--table-header-bg);

    --infi-sub-hover-bg: #c6bbb1;
    --infi-title-bg: #bbada1;
    --infi-title-border-bottom: 2px solid #bbada1;
    --infi-title-hover-fg: #000;
    --infi-title-hover-bg: #a19488;
    --infi-title-border-bottom-hover: 2px solid #a19488;
    --infi-body-hover-fg: #1e1e1e;
    --infi-heading-hover-fg: #1e1e1e;
}

.coal .infi-root {
    --infi-highlight: #496c8a;
    --infi-sub-hover-bg: #272a2b;
}

.infi-theme .infi-root .infi-tip-item code,
.rust .infi-root .infi-tip-item code {
    color: var(--infi-tip-code-fg) !important;;
}

.coal .infi-root,
.navy .infi-root,
.ayu .infi-root {
    --infi-item-box-shadow: 0 1px 5px rgb(50, 50, 50);
    --infi-fs-input-fg: var(--fg);
    --infi-fs-input-focus-border: 2px solid #4f95cc;
    --infi-fs-input-focus-box-shadow: 0 0 5px -1px #63baff;
    --infi-key-fg: #fff;
    --infi-key-bg: #7d7d7d;
    --infi-checkbox-bg: #313233;
    --infi-checkbox-checked-bg: #424243;
    --infi-checkbox-border: #525354;
}

.rust .infi-root {
    --infi-fs-input-fg: var(--sidebar-fg);
    --infi-fs-input-bg: #29201d;
    --infi-fs-input-border: 2px solid #584d4a;
    --infi-fs-input-focus-border: 2px solid #4f95cc;
    --infi-fs-input-focus-box-shadow: 0 0 5px -1px #63baff;
}

.coal .infi-root {
    --infi-fs-input-bg: #1d1f21;
    --infi-fs-input-border: 2px solid #3e4144;
}

.navy .infi-root {
    --infi-fs-input-bg: #1e222f;
    --infi-fs-input-border: 2px solid #3d4252;
    --infi-sub-hover-bg: #242734;
}

.ayu .infi-root {
    --infi-fs-input-fg: var(--fg);
    --infi-fs-input-bg: #2b3035;
    --infi-fs-input-border: 2px solid #43474c;
    --infi-fs-input-focus-border: 2px solid #4f95cc;
    --infi-fs-input-focus-box-shadow: 0 0 5px -1px #63baff;
    --infi-sub-hover-bg: #282e35;
}

#infi-search {
    width: 100%;
    border-radius: 3px;
    box-sizing: border-box;
    padding: 10px 16px;
    border: 1px solid var(--searchbar-border-color);
    background: var(--searchbar-bg);
    color: var(--searchbar-fg);
}

#infi-search:focus:not(.infi-button-input) {
    box-shadow: 0 0 3px var(--searchbar-shadow-color);
}

#infi-search.infi-button-input {
    width: 100px;
}

#infi-search.infi-button-input::placeholder {
    position: relative;
    left: 14px;
}

#infi-search.infi-button-input:hover {
    transition: 0.3s ease-out;
    background: var(--infi-fs-button-input-bg) !important;
    outline: 2px solid var(--infi-fs-button-input-bg);
}

#infi-search.infi-button-input:hover::placeholder {
    color: var(--infi-fs-button-input-fg) !important;
}

@media print {
    #infi-search {
        display: none;
    }
}

#infisearch-mdbook-target {
    position: relative;
}

/*
 * For this plugin, don't show the controls until there is a query.
 */
#infisearch-mdbook-target.infi-empty-input > * {
    display: none;
}

.infi-root:not(.infi-fs-root) {
    display: block;
}

.light .infi-root .infi-title::after,
.rust .infi-root .infi-title::after,
.coal .infi-root .infi-title::after,
.navy .infi-root .infi-title::after,
.ayu .infi-root .infi-title::after {
    content: none;
    display: none;
}

.infi-header {
    padding-bottom: 9px;
}

.infi-load-more {
    padding: 7px 15px;
}
</style>
<p><input
    type="search"
    id="infi-search"
    placeholder="Search this book ..."
/></p>
<p><span style="font-weight: 600;"><!--preload weight 600--></span></p>
<div id="infisearch-mdbook-target"></div>
<h1 id="概念"><a class="header" href="#概念"><strong>概念</strong></a></h1>
<h2 id="主从哨兵的局限性"><a class="header" href="#主从哨兵的局限性"><strong>主从+哨兵的局限性</strong></a></h2>
<p>之前介绍过Redis主从集群+哨兵的搭建，架构如下图所示</p>
<p><img src="https://weisanju.github.io/note-service/mw/images/redis-master-slaver-sentinel.jpg" alt="" /></p>
<p>这种集群模式下水平扩容和垂直扩容都可以实现，并且可以实现高可用性和易用性</p>
<ul>
<li>水平扩容：比如增加一套主从集群，在predixy代理处配置hash寻址，让部分数据可以被新加入的主从集群存储，水平扩容的实现强烈<strong>依赖于predixy代理</strong>。</li>
<li>垂直扩容：比如增加某个集群的内存，提升单机/单集群的处理能力</li>
<li>高可用性：一套哨兵集群监控多套redis主从集群，高可用的实现依赖于哨兵</li>
<li>易用性：指的是客户端的易用性，也是依赖于predixy代理来实现</li>
</ul>
<p><strong>但是此架构下还是有缺陷的：</strong></p>
<ul>
<li>水平扩容问题：水平扩容依赖于predixy实现既是优点也是缺点，因为predixy的实现是根据crc16计算key的哈希值，然后通过modula也就是求模的办法将key分布到不同的集群中去，所以说当水平扩容的时候会涉及到大量缓存重建</li>
<li>可用性问题：集群的高可用性是建立在哨兵集群之上的，假设哨兵集群全部宕机，那么整个集群的故障转移功能将会丧失，也不能动态发现新加入的集群，最终导致集群的可用性受到影响。</li>
</ul>
<p><strong>那么redis cluster又是怎么解决上面两个问题的呢？</strong></p>
<ul>
<li>针对水平扩容问题：redis使用hash槽算法，默认分配16384个hash槽位（2^14），然后将槽位均匀分配到不同的redis实例中去，找数据的时候通过crc16(key) % 16384找到对应的槽位，再看槽位在哪台实例上，最后去实例上取数据，使用槽位将具体的数据与redis实例解耦，当新增或者减少redis实例的时候自动将槽位均匀迁移到其他可用的redis实例上去。</li>
<li>针对可用性问题：redis使用流行病协议，即Gossip/ˈɡɒsɪp/ Protocol ，每台redis主机即使客户端也是服务端，随时都在向整个集群扩散自己的可用性状态，实际上就是基于P2P的 <strong>去中心化网络拓扑架构</strong>，没有中心节点，所有节点通过Gossip协议通信，所有节点既是数据存储节点，也是控制节点。</li>
</ul>
<p>下面就针对水平扩容问题的hash寻址算法和针对可用性问题的流行病协议详细讨论</p>
<h2 id="hash寻址算法"><a class="header" href="#hash寻址算法">hash寻址算法</a></h2>
<h3 id="普通hash"><a class="header" href="#普通hash">普通hash</a></h3>
<p>普通hash也就是最简单的hash算法，即</p>
<pre><code class="language-text">index = hash(key) % N
</code></pre>
<p>新增加了一个节点，那么所有key取模的结果都变了，导致所有的数据都要重新迁移一遍，如果节点下线了呢？那么毫无疑问所有数据都要还原回去，就redis而言，这就叫<strong>大量缓存的重建</strong>，那么有没有新增/删除节点影响不那么大的hash算法呢？答案肯定是有，下面轮到一致性hash出场。</p>
<h3 id="一致性hash"><a class="header" href="#一致性hash">一致性hash</a></h3>
<blockquote>
<p>一致哈希由MIT的Karger及其合作者提出，现在这一思想已经扩展到其它领域。在这篇1997年发表的学术论文中介绍了“一致哈希”如何应用于用户易变的分布式Web服务中。哈希表中的每一个代表分布式系统中一个节点，在系统添加或删除节点只需要移动K/n （方法K是总key的个数，n是节点个数）</p>
</blockquote>
<p>一致性hash的特性</p>
<ul>
<li><strong>平衡性</strong>：尽可能让数据尽可能分散到所有节点上，避免造成极其不均匀</li>
<li><strong>单调性</strong>：要求在新增或者减少节点的时候，原有的结果绝大部分不受影响，而新增的数据尽可能分配到新加的节点</li>
<li><strong>分散性</strong>：好的算法在不同终端，针对相同的数据的计算，得到的结果应该是一样的，一致性要很强</li>
<li><strong>负载</strong>：针对相同的节点，避免被不同终端映射不同的内容</li>
<li><strong>平滑性</strong>：对于增加节点或者减少节点，应该能够平滑过渡</li>
</ul>
<h3 id="hash环"><a class="header" href="#hash环">hash环</a></h3>
<p>普通hash算法导致大量数据迁移的根本原因是N的不确定性，有没有在N变化的时候影响范围更小的算法呢？有人提出了<strong>环</strong>的概念</p>
<p>hash环通过构建环状的hash空间代替线性hash空间的方法解决了上面的问题，假设将0~2^32-1的hash空间分布到一个环上</p>
<ul>
<li>节点加入环：将节点通过hash(节点的信息如ip端口等) % 2^32-1取节点在环上位置</li>
<li>数据读写：读写数据时同样取key的hash，即hash(key) % 2^32-1落到环上的某一位置，再<strong>顺时针</strong>找到离环最近的那个节点进行读写</li>
</ul>
<p><img src="https://weisanju.github.io/note-service/mw/images/redis-hash-cycle.jpg" alt="" /></p>
<p><strong>新增与删除</strong></p>
<p>新增一个节点4，只会影响到节点2到节点4之间的数据，其他的数据不会被影响到，这也是<strong>一致性</strong>的体现</p>
<p>删除一个节点也是同样的道理，假设删除节点4，也只是会影响到节点2到原节点4之间的数据</p>
<p><strong>缺点</strong></p>
<p>假设节点分布不均匀（hash算法并不能保证绝对的平衡性），那么大部分数据都会落在一个节点上，导致请求和数据倾斜，这样就不能很好的保证负载均衡。</p>
<p><img src="https://weisanju.github.io/note-service/mw/images/redis-hash-cycle-nonuniform.jpg" alt="" /></p>
<p>那么解决办法就是增加虚拟节点（注意，此时环上<strong>全部都是虚拟节点</strong>），对每一个节点计算多个hash，尽量保证环上的节点是均匀的，如下图</p>
<p><img src="https://weisanju.github.io/note-service/mw/images/redis-hash-cycle-uniform.jpg" alt="" /></p>
<h3 id="hash槽"><a class="header" href="#hash槽">hash槽</a></h3>
<p>hash槽（hash slot）<strong>是redis中一致性hash的实</strong>现，很多文章将一致性hash环和hash槽分开来讲，其实hash槽也是一致性hash的一种实现。</p>
<p>redis默认分配16384个hash槽位，然后将槽位均匀分配到不同的redis实例中去，找数据的时候通过CRC16算法计算后再取模找到对应的槽位（CRC16我们应该不陌生，这个winrar里面使用的CRC32是一样的，只是校验长度不一样而已），算法如下</p>
<pre><code>CRC16(key) % 16384
</code></pre>
<p><img src="https://weisanju.github.io/note-service/mw/images/redis-hash-slot-algorithm.jpg" alt="" /></p>
<p>使用槽位将具体的数据与redis实例解耦，当新增或者减少redis实例的时候用redis cluster总线通过Ping/Pong报文进行广播，告知整个redis集群新节点上线/下线，并迁移槽位和更新集群中的槽位映射表，整个过程尽量保证hash槽的平均分</p>
<p><strong>那么是基于什么样的考虑，redis的作者没有用hash环呢？</strong></p>
<p>redis的作者认为他的CRC16(key) mod 16384的效果已经不错了，虽然没有一致性hash灵活，但实现很简单，节点增删时处理起来也很方便</p>
<p>当然还有个原因是hash槽的分布更加均匀，如果有N个节点，那么每个节点都负载1/N，此处引用一句话总结</p>
<p><strong>那为什么hash槽是16384个呢？</strong></p>
<p><strong>分布均匀</strong></p>
<p>实际上是因为CRC16会输出16bit的结果，可以看作是一个分布在0~2^16-1之间的数，redis的作者测试发现这个数对2^14求模的会将key在0-2^14-1之间分布得很均匀，2^14即16384</p>
<p><strong>节省空间</strong></p>
<p>还有个说法是为了节省存储空间，每个节点用一个Bitmap来存放其对应的槽，2k = 2*1024*8 = 16384，也就是说，每个节点用2k的内存空间，总共16384个比特位，就可以存储该结点对应了哪些槽。然后这2k的信息，通过Gossip协议，在节点之间传递</p>
<h2 id="gossip协议流行病协议"><a class="header" href="#gossip协议流行病协议">Gossip协议(流行病协议)</a></h2>
<p>redis cluster正是通过Gossip协议在节点之间同步数据的，所有节点都是对等的，既是数据存储节点，也是控制节点。redis cluster启动的时候会开两个端口，一个是常规的6379端口，另外一个端口一般是（6379+ <strong>10000</strong>），这个就是所谓的Cluster总线，这个端口的作用就是就是利用Gossip协议进行节点之间的通信。</p>
<p>这里顺便提一下<strong>反熵</strong>（Anti-Entropy），熵描述的是一个系统的混乱程度，大名鼎鼎的<strong>熵增定律</strong>指的是一个有序系统在无外力的作用下，会慢慢转化到无序的状态，所谓反熵就是需要借助外力来减少系统的混乱程度，redis通过Gossip协议传播节点之间的可用信息，使得整个系统有序可用，是反熵行为，假设redis集群奉行无为而治，那么整个集群会随着各种不确定性（比如内存满了、网络抖动等）变得越来越无序，可用性降低，符合熵增定律。</p>
<h2 id="容错机制"><a class="header" href="#容错机制">容错机制</a></h2>
<h3 id="主观下线"><a class="header" href="#主观下线"><strong>主观下线</strong></a></h3>
<p>集群中每个节点都会定期向其他节点发送ping消息，接受节点回复ping消息作为响应。如果在cluster-node-timeout时间内通信一直失败，则发送节点会认为接收节点存在故障，把接受节点标记为主观下线(pfail)状态。</p>
<h3 id="客观下线"><a class="header" href="#客观下线"><strong>客观下线</strong></a></h3>
<ul>
<li>当某个节点判断另一个节点主观下线后，相应的节点状态会跟随消息在集群内传播</li>
<li>假设节点a标记节点b为主观下线，一段时间后节点a通过消息把节点b的状态发送到其他节点，当其他节点收到消息并解析出消息体中含有b的pfail状态，把节点b加入下线报告链表；</li>
<li>当某一节点c收到节点b的pfail状态时，此时有超过一半的槽主节点都标记了节点b为pfail状态时，则标记故障节点b为客观下线；</li>
<li>向集群广播一条pfail消息，通知集群内的所有节点标记故障节点b为客观下线状态并立刻生效，同时通知故障节点b的从节点触发故障转移流程</li>
</ul>
<h3 id="故障恢复"><a class="header" href="#故障恢复"><strong>故障恢复</strong></a></h3>
<p><strong>资格检查</strong></p>
<p>若从节点与主节点断线时间超过一定时间，则不具备资格</p>
<p><strong>准备选举时间</strong></p>
<p>当从节点符合故障转移资格后，要等待一段选举时间后才开始选举</p>
<p>在故障节点的所有从节点中，复制偏移量最大的那个从节点最先开始（与主节点的数据最一致）进行选举，然后是次大的节点开始选举.....剩下其余的从节点等待到它们的选举时间到达后再进行选举</p>
<p><strong>发起选举</strong></p>
<p><strong>选举投票</strong></p>
<p>只有持有槽的主节点才具有一张唯一的选票，从从节点收集到N/2 + 1个持有槽的主节点投票时，从节点可以执行替换主节点操作</p>
<p><strong>替换主节点</strong></p>
<p>当从节点收集到足够的选票之后，触发替换主节点操作</p>
<ul>
<li>当前从节点取消复制变为主节点</li>
<li>撤销故障主节点负责的槽，并把这些槽委派给自己</li>
<li>向集群广播自己的pong消息，通知集群内所有的节点当前从节点变为主节点并接管了故障主节点的槽信息</li>
</ul>
<h1 id="redis-cluster集群"><a class="header" href="#redis-cluster集群">Redis Cluster集群</a></h1>
<h2 id="概念-1"><a class="header" href="#概念-1">概念</a></h2>
<ul>
<li>
<p>Redis 集群是一个提供在<strong>多个Redis间节点间共享数据</strong>的程序集。</p>
</li>
<li>
<p>Redis集群并不支持处理多个keys的命令,因为这需要在不同的节点间移动数据,从而达不到像Redis那样的性能,在高负载的情况下可能会导致不可预料的错误.</p>
</li>
<li>
<p>Redis 集群通过分区来提供<strong>一定程度的可用性</strong>,在实际环境中当某个节点宕机或者不可达的情况下继续处理命令. Redis 集群的优势:</p>
<ul>
<li>自动分割数据到不同的节点上。</li>
<li>整个集群的部分节点失败或者不可达的情况下能够继续处理命令。</li>
</ul>
</li>
</ul>
<h2 id="redis-集群的数据分片"><a class="header" href="#redis-集群的数据分片">Redis 集群的数据分片</a></h2>
<p>Redis 集群没有使用一致性hash, 而是引入了 <strong>哈希槽</strong>的概念.</p>
<p>Redis 集群有16384个哈希槽,每个key通过CRC16校验后对16384取模来决定放置哪个槽.集群的每个节点负责一部分hash槽,举个例子,比如当前集群有3个节点,那么:</p>
<ul>
<li>节点 A 包含 0 到 5500号哈希槽.</li>
<li>节点 B 包含5501 到 11000 号哈希槽.</li>
<li>节点 C 包含11001 到 16384号哈希槽.</li>
</ul>
<p>这种结构很容易添加或者删除节点. 比如如果我想新添加个节点D, 我需要从节点 A, B, C中得部分槽到D上. 如果我想移除节点A,需要将A中的槽移到B和C节点上,然后将没有任何槽的A节点从集群中移除即可. 由于从一个节点将哈希槽移动到另一个节点并不会停止服务,所以无论添加删除或者改变某个节点的哈希槽的数量都不会造成集群不可用的状态.</p>
<h2 id="redis-集群的主从复制模型"><a class="header" href="#redis-集群的主从复制模型">Redis 集群的主从复制模型</a></h2>
<p>为了使在部分节点失败或者大部分节点无法通信的情况下集群仍然可用，所以集群使用了主从复制模型,每个节点都会有N-1个复制品.</p>
<p>在我们例子中具有A，B，C三个节点的集群,在没有复制模型的情况下,如果节点B失败了，那么整个集群就会以为缺少5501-11000这个范围的槽而不可用.</p>
<p>然而如果在集群创建的时候（或者过一段时间）我们为每个节点添加一个从节点A1，B1，C1,那么整个集群便有三个master节点和三个slave节点组成，这样在节点B失败后，集群便会选举B1为新的主节点继续服务，整个集群便不会因为槽找不到而不可用了</p>
<p>不过当B和B1 都失败后，集群是不可用的.</p>
<h2 id="redis-一致性保证"><a class="header" href="#redis-一致性保证">Redis 一致性保证</a></h2>
<h3 id="不保证强一致性"><a class="header" href="#不保证强一致性">不保证强一致性</a></h3>
<p><strong>异步复制</strong></p>
<p>Redis 并不能保证数据的<strong>强一致性</strong>. 这意味这在实际中集群在特定的条件下可能会丢失写操作.</p>
<p>第一个原因是因为集群是用了异步复制. 写操作过程:</p>
<ul>
<li>客户端向主节点B写入一条命令.</li>
<li>主节点B向客户端回复命令状态.</li>
<li>主节点将写操作复制给他得从节点 B1, B2 和 B3.</li>
</ul>
<p><strong>性能与一致性的权衡</strong></p>
<p>主节点对命令的复制工作发生在返回命令回复之后， 因为如果每次处理命令请求都需要等待复制操作完成的话， 那么主节点处理命令请求的速度将极大地降低 —— 我们必须在性能和一致性之间做出权衡。 注意：Redis 集群可能会在将来提供同步写的方法。 </p>
<p><strong>网络分裂导致非一致性</strong></p>
<p>Redis 集群另外一种可能会丢失命令的情况是集群出现了网络分区， 并且一个客户端与至少包括一个主节点在内的少数实例被孤立。</p>
<p>举个例子 假设集群包含 A 、 B 、 C 、 A1 、 B1 、 C1 六个节点， 其中 A 、B 、C 为主节点， A1 、B1 、C1 为A，B，C的从节点， 还有一个客户端 Z1 假设集群中发生网络分区，那么集群可能会分为两方，大部分的一方包含节点 A 、C 、A1 、B1 和 C1 ，小部分的一方则包含节点 B 和客户端 Z1 .</p>
<p>Z1仍然能够向主节点B中写入, 如果网络分区发生时间较短,那么集群将会继续正常运作,如果分区的时间足够让大部分的一方将B1选举为新的master，那么Z1写入B中得数据便丢失了.</p>
<p>注意， 在网络分裂出现期间， 客户端 Z1 可以向主节点 B 发送写命令的最大时间是有限制的， 这一时间限制称为节点超时时间（node timeout）， 是 Redis 集群的一个重要的配置选项：</p>
<h2 id="集群原理"><a class="header" href="#集群原理">集群原理</a></h2>
<h3 id="键分布模型"><a class="header" href="#键分布模型">键分布模型</a></h3>
<p>键空间被分割为 16384 槽（slot），事实上集群的最大节点数量是 16384 个。（然而建议最大节点数量设置在1000这个数量级上）</p>
<p>所有的主节点都负责 16384 个哈希槽中的一部分。当集群处于稳定状态时，集群中没有在执行重配置（reconfiguration）操作，每个哈希槽都只由一个节点进行处理（不过主节点可以有一个或多个从节点，可以在网络断线或节点失效时替换掉主节点）。</p>
<p>以下是用来把键映射到哈希槽的算法（下一段哈希标签例外就是按照这个规则）：</p>
<h3 id="键哈希标签keys-hash-tags"><a class="header" href="#键哈希标签keys-hash-tags">键哈希标签（Keys hash tags）</a></h3>
<p><strong>设置hash标签确保同一个slot</strong></p>
<p>哈希标签是确保两个键都在同一个哈希槽里的一种方式</p>
<p>为了实现哈希标签，哈希槽是用另一种不同的方式计算的。基本来说，如果一个键包含一个 “{…}” 这样的模式，只有 { 和 } 之间的字符串会被用来做哈希以获取哈希槽。但是由于可能出现多个 { 或 }，计算的算法如下：</p>
<ul>
<li>如果键包含一个 { 字符。</li>
<li>那么在 { 的右边就会有一个 }。</li>
<li>在 { 和 } 之间会有一个或多个字符，第一个 } 一定是出现在第一个 { 之后。</li>
</ul>
<p>然后不是直接计算键的哈希，只有在第一个 { 和它右边第一个 } 之间的内容会被用来计算哈希值。</p>
<p>例子：</p>
<ul>
<li>比如这两个键 {user1000}.following 和 {user1000}.followers 会被哈希到同一个哈希槽里，因为只有 user1000 这个子串会被用来计算哈希值。</li>
<li>对于 foo{}{bar} 这个键，整个键都会被用来计算哈希值，因为第一个出现的 { 和它右边第一个出现的 } 之间没有任何字符。</li>
</ul>
<pre><code class="language-ruby">def HASH_SLOT(key)
    s = key.index &quot;{&quot;
    if s
        e = key.index &quot;}&quot;,s+1
        if e &amp;&amp; e != s+1
            key = key[s+1..e-1]
        end
    end
    crc16(key) % 16384
end
</code></pre>
<h3 id="集群节点属性"><a class="header" href="#集群节点属性">集群节点属性</a></h3>
<h3 id="固定唯一id"><a class="header" href="#固定唯一id">固定唯一ID</a></h3>
<p>在集群中，每个节点都有一个唯一的名字。节点名字是一个十六进制表示的160 bit 随机数，这个随机数是节点第一次启动时获得的（通常是用 /dev/urandom）。 节点会把它的ID保存在配置文件里，以后永远使用这个ID，只要这个节点配置文件没有被系统管理员删除掉。</p>
<p>节点ID是用于在整个集群中标识每个节点。一个给定的节点可以在不改变节点ID的情况下改变 IP 和地址。集群能检测到 IP 或端口的变化，然后使用在集群连接（cluster bus）上的 gossip 协议来发布广播消息，通知配置变更。</p>
<p>每个节点都有其他相关信息是所有节点都知道的：</p>
<ul>
<li>节点的 IP 地址和 TCP 端口号。</li>
<li>各种标识。</li>
<li>节点使用的哈希槽。</li>
<li>最近一次用集群连接发送 ping 包的时间。</li>
<li>最近一次在回复中收到一个 pong 包的时间。</li>
<li>最近一次标识节点失效的时间。</li>
<li>该节点的从节点个数。</li>
<li>如果该节点是从节点，会有主节点ID信息。（如果它是个主节点则该信息置为0000000…）</li>
</ul>
<p>使用 CLUSTER NODES 命令可以获得以上的一些信息，这个命令可以发送到集群中的所有节点，无论主节点还是从节点</p>
<pre><code>redis-cli cluster nodes

d1861060fe6a534d42d8a19aeb36600e18785e04 127.0.0.1:6379 myself - 0 1318428930 1 connected 0-1364
3886e65cc906bfd9b1f7e7bde468726a052d1dae 127.0.0.1:6380 master - 1318428930 1318428931 2 connected 1365-2729
d289c575dcbc4bdd2931585fd4339089e461a27d 127.0.0.1:6381 master - 1318428931 1318428931 3 connected 2730-4095
各个域依次表示的是：节点ID，IP地址：端口号，标识，上一次发送 ping 包的时间，上一次收到 pong 包的时间，连接状态，节点使用的哈希槽。
</code></pre>
<h3 id="集群拓扑结构"><a class="header" href="#集群拓扑结构">集群拓扑结构</a></h3>
<p>Redis 集群是一个网状结构，每个节点都通过 TCP 连接跟其他每个节点连接。</p>
<p>在一个有 N 个节点的集群中，每个节点都有 N-1 个流出的 TCP 连接，和 N-1 个流入的连接。 这些 TCP 连接会永久保持，并不是按需创建的。</p>
<h3 id="节点握手"><a class="header" href="#节点握手">节点握手</a></h3>
<p>节点总是在集群连接端口接受连接，甚至会回复接收到的 ping 包，即使发送 ping 包的节点是不可信的。 然而如果某个节点不被认为是在集群中，那么所有它发出的数据包都会被丢弃掉。</p>
<p>只有在两种方式下，一个节点才会认为另一个节点是集群中的一部分：</p>
<ul>
<li>
<p>当一个节点使用 MEET 消息介绍自己。一个 meet 消息跟一个 PING 消息完全一样，但它会强制让接收者接受发送者为集群中的一部分。 只有在系统管理员使用以下命令要求的时候，节点才会发送 MEET 消息给其他节点：</p>
<pre><code>CLUSTER MEET ip port
</code></pre>
</li>
</ul>
<ul>
<li>
<p>一个已被信任的节点能通过传播gossip消息让另一个节点被注册为集群中的一部分</p>
<p>如果 A 知道 B，B 知道 C，那么 B 会向 A 发送 C 的gossip消息。A 收到后就会把 C 当作是网络中的一部分，并且尝试连接 C</p>
<p>从根本上来说，这表示集群能自动发现其他节点，但前提是有一个由系统管理员强制创建的信任关系</p>
</li>
</ul>
<h3 id="moved-重定向"><a class="header" href="#moved-重定向">MOVED 重定向</a></h3>
<p>一个 Redis 客户端可以自由地向集群中的任意节点（包括从节点）发送查询。接收的节点会分析查询，如果这个命令是集群可以执行的（就是查询中只涉及一个键），那么节点会找这个键所属的哈希槽对应的节点。</p>
<p>如果刚好这个节点就是对应这个哈希槽，那么这个查询就直接被节点处理掉。否则这个节点会查看它内部的 哈希槽 -&gt; 节点ID 映射，然后给客户端返回一个 MOVED 错误。</p>
<pre><code>GET x
-MOVED 3999 127.0.0.1:6381
</code></pre>
<p>当集群是稳定的时候，所有客户端最终都会得到一份哈希槽 -&gt; 节点的映射表，这样能使得集群效率非常高：客户端直接定位目标节点，不用重定向、或代理或发生其他单点故障（single point of failure entities）。</p>
<p>一个客户端也应该能处理本文后面将提到的 -ASK 重定向错误。</p>
<h3 id="集群在线重配置live-reconfiguration"><a class="header" href="#集群在线重配置live-reconfiguration">集群在线重配置（live reconfiguration）</a></h3>
<p>Redis 集群支持在集群运行过程中添加或移除节点。实际上，添加或移除节点都被抽象为同一个操作，那就是把哈希槽从一个节点移到另一个节点。</p>
<ul>
<li>向集群添加一个新节点，就是把一个空节点加入到集群中并把某些哈希槽从已存在的节点移到新节点上。</li>
<li>从集群中移除一个节点，就是把该节点上的哈希槽移到其他已存在的节点上。</li>
<li>所以实现这个的核心是能把哈希槽移来移去。从实际角度看，哈希槽就只是一堆键，所以 Redis 集群在重组碎片（reshard）时做的就是把键从一个节点移到另一个节点。</li>
</ul>
<p>CLUSTER 的子命令，这些命令是用来操作 Redis 集群节点上的哈希槽转换表（slots translation table）。</p>
<ul>
<li>
<p><code>CLUSTER ADDSLOTS</code> slot1 [slot2] … [slotN]</p>
</li>
<li>
<p><code>CLUSTER DELSLOTS</code> slot1 [slot2] … [slotN]</p>
</li>
<li>
<p><code>CLUSTER SETSLOT</code> slot NODE node</p>
</li>
<li>
<p><code>CLUSTER SETSLOT</code> slot MIGRATING node</p>
</li>
<li>
<p><code>CLUSTER SETSLOT</code> slot IMPORTING node</p>
</li>
<li>
<p>头两个命令，ADDSLOTS 和 DELSLOTS，就是简单地用来给一个 Redis 节点指派（assign）或移除哈希槽。 在哈希槽被指派后，节点会将这个消息通过 gossip 协议向整个集群传播。ADDSLOTS 命令通常是用于在一个集群刚建立的时候快速给所有节点指派哈希槽。</p>
</li>
<li>
<p>当 SETSLOT 子命令使用 NODE 形式的时候，用来给指定 ID 的节点指派哈希槽。 除此之外哈希槽能通过两个特殊的状态来设定，MIGRATING 和 IMPORTING：</p>
<ul>
<li>当一个槽被设置为 MIGRATING，原来持有该哈希槽的节点仍会接受所有跟这个哈希槽有关的请求，但只有当查询的键还存在原节点时，原节点会处理该请求，否则这个查询会通过一个 -ASK 重定向（-ASK redirection）转发到迁移的目标节点。</li>
<li>当一个槽被设置为 IMPORTING，只有在接受到 ASKING 命令之后节点才会接受所有查询这个哈希槽的请求。如果客户端一直没有发送 ASKING 命令，那么查询都会通过 -MOVED 重定向错误转发到真正处理这个哈希槽的节点那里。</li>
</ul>
</li>
</ul>
<p>假设我们有两个 Redis 节点，称为 A 和 B。我们想要把哈希槽 8 从 节点A 移到 节点B，所以我们发送了这样的命令：</p>
<ul>
<li>我们向 节点B 发送：CLUSTER SETSLOT 8 IMPORTING A</li>
<li>我们向 节点A 发送：CLUSTER SETSLOT 8 MIGRATING B</li>
</ul>
<p>其他所有节点在每次被询问到的一个键是属于哈希槽 8 的时候，都会把客户端引向节点”A”。具体如下：</p>
<ul>
<li>所有关于已存在的键的查询都由节点”A”处理。</li>
<li>所有关于不存在于节点 A 的键都由节点”B”处理。</li>
</ul>
<h3 id="ask-重定向"><a class="header" href="#ask-重定向">ASK 重定向</a></h3>
<p>为什么我们不能单纯地使用 MOVED 重定向呢？因为当我们使用 MOVED 的时候，意味着我们认为哈希槽永久地被另一个不同的节点处理，并且希望接下来的所有查询都尝试发到这个指定的节点上去。</p>
<p>而 ASK 意味着我们只要<strong>下一个查询发送到指定节点上去。</strong></p>
<p>这个命令是必要的，因为下一个关于哈希槽 8 的查询需要的键或许还在节点 A 中，所以我们希望客户端尝试在节点 A 中查找，如果需要的话也在节点 B 中查找。 由于这是发生在 16384 个槽的其中一个槽，所以对于集群的性能影响是在可接受的范围。</p>
<p>然而我们需要强制客户端的行为，以确保客户端会在尝试 A 中查找后去尝试在 B 中查找。如果客户端在发送查询前发送了 ASKING 命令，那么节点 B 只会接受被设为 IMPORTING 的槽的查询。 本质上来说，ASKING 命令在客户端设置了一个一次性标识（one-time flag），强制一个节点可以执行一次关于带有 IMPORTING 状态的槽的查询。</p>
<p>所以从客户端看来，ASK 重定向的完整语义如下：</p>
<ul>
<li>如果接受到 ASK 重定向，那么把查询的对象调整为指定的节点。</li>
<li>先发送 ASKING 命令，再开始发送查询。</li>
<li>现在不要更新本地客户端的映射表把哈希槽 8 映射到节点 B。</li>
</ul>
<p>一旦完成了哈希槽 8 的转移，节点 A 会发送一个 MOVED 消息，客户端也许会永久地把哈希槽 8 映射到新的 ip:端口号 上。</p>
<h3 id="失效检测failure-detection"><a class="header" href="#失效检测failure-detection">失效检测（Failure detection）</a></h3>
<p><strong>大部分失效</strong></p>
<p>Redis 集群失效检测是用来识别出大多数节点何时无法访问某一个主节点或从节点。当这个事件发生时，就提升一个从节点来做主节点；若如果无法提升从节点来做主节点的话，那么整个集群就置为错误状态并停止接收客户端的查询。</p>
<p>每个节点都有一份跟其他已知节点相关的标识列表。其中有两个标识是用于失效检测，分别是 PFAIL 和 FAIL。*</p>
<ul>
<li>PFAIL 表示可能失效（Possible failure），这是一个非公认的（non acknowledged）失效类型。</li>
<li>FAIL 表示一个节点已经失效，而且这个情况已经被大多数主节点在某段固定时间内确认过的了。</li>
</ul>
<p><strong>PFAIL 标识:</strong></p>
<p>当一个节点在超过 NODE_TIMEOUT 时间后仍无法访问某个节点，那么它会用 PFAIL 来标识这个不可达的节点。无论节点类型是什么，主节点和从节点都能标识其他的节点为 PFAIL。</p>
<p>Redis 集群节点的不可达性（non reachability）是指，发送给某个节点的一个活跃的 ping 包（active ping）(一个我们发送后要等待其回复的 ping 包)已经等待了超过 NODE_TIMEOUT 时间，那么我们认为这个节点具有不可达性。为了让这个机制能正常工作，NODE_TIMEOUT 必须比网络往返时间（network round trip time）大。</p>
<p>节点为了在普通操作中增加可达性，当在经过一半 NODE_TIMEOUT 时间还没收到目标节点对于 ping 包的回复的时候，就会马上尝试重连接该节点。这个机制能保证连接都保持有效，所以节点间的失效连接通常都不会导致错误的失效报告。</p>
<p><strong>FAIL 标识:</strong></p>
<p>单独一个 PFAIL 标识只是每个节点的一些关于其他节点的本地信息，它不是为了起作用而使用的，也不足够触发从节点的提升。要让一个节点真正被认为失效了，那需要让 PFAIL 状态上升为 FAIL 状态。 </p>
<p>每个节点向其他每个节点发送的 gossip 消息中有包含一些随机的已知节点的状态。最终每个节点都能收到一份其他每个节点的节点标识。使用这种方法，每个节点都有一套机制去标记他们检查到的关于其他节点的失效状态。</p>
<p>当下面的条件满足的时候，会使用这个机制来让 PFAIL 状态升级为 FAIL 状态：</p>
<ul>
<li>某个节点，我们称为节点 A，标记另一个节点 B 为 PFAIL。</li>
<li>节点 A 通过 gossip 字段收集到集群中大部分主节点标识的节点 B 的状态信息。</li>
<li>大部分主节点标记节点 B 为 PFAIL 状态，或者在 NODE_TIMEOUT * FAIL_REPORT_VALIDITY_MULT 这个时间内是处于 PFAIL 状态。</li>
</ul>
<p>如果以上所有条件都满足了，那么节点 A 会：</p>
<ul>
<li>标记节点 B 为 FAIL。</li>
<li>向所有可达节点发送一个 FAIL 消息。</li>
</ul>
<p>FAIL 消息会强制每个接收到这消息的节点把节点 B 标记为 FAIL 状态。</p>
<p>注意，FAIL 标识基本都是单向的，也就是说，一个节点能从 PFAIL 状态升级到 FAIL 状态，但要清除 FAIL 标识只有以下两种可能方法：</p>
<ul>
<li>节点已经恢复可达的，并且它是一个从节点。在这种情况下，FAIL 标识可以清除掉，因为从节点并没有被故障转移。</li>
<li>节点已经恢复可达的，而且它是一个主节点，但经过了很长时间（N * NODE_TIMEOUT）后也没有检测到任何从节点被提升了。</li>
</ul>
<p><strong>PFAIL -&gt; FAIL 的转变使用一种弱协议（agreement）：</strong></p>
<ul>
<li>节点是在一段时间内收集其他节点的信息，所以即使大多数主节点要去”同意”标记某节点为 FAIL，实际上这只是表明说我们在不同时间里从不同节点收集了信息，得出当前的状态不一定是稳定的结论</li>
<li>当每个节点检测到 FAIL 节点的时候会强迫集群里的其他节点把各自对该节点的记录更新为 FAIL，但没有一种方式能保证这个消息能到达所有节点。比如有个节点可能检测到了 FAIL 的节点，但是因为分区，这个节点无法到达其他任何一个节点。</li>
</ul>
<p>然而 Redis 集群的失效检测有一个要求：最终所有节点都应该同意给定节点的状态是 FAIL，哪怕它处于分区。有两种情况是来源于脑裂情况（？），或者是小部分节点相信该节点处于 FAIL 状态，或者是相信节点不处于 FAIL 状态。在这两种情况中，最后集群都会认为给定的节点只有一个状态：</p>
<p>**第 1 种情况: **如果大多数节点都标记了某个节点为 FAIL，由于链条反应，这个主节点最终会被标记为 FAIL。</p>
<p><strong>第 2 种情况:</strong> 当只有小部分的主节点标记某个节点为 FAIL 的时候，从节点的提升并不会发生（它是使用一个更正式的算法来保证每个节点最终都会知道节点的提升。），并且每个节点都会根据上面的清除规则（在经过了一段时间 &gt; N * NODE_TIMEOUT 后仍没有从节点提升操作）来清除 FAIL 状态。</p>
<p><strong>本质上来说，FAIL 标识只是用来触发从节点提升（slave promotion）算法的安全部分</strong></p>
<p>理论上一个从节点会在它的主节点不可达的时候独立起作用并且启动从节点提升程序，然后等待主节点来拒绝认可该提升（如果主节点对大部分节点恢复连接）。PFAIL -&gt; FAIL 的状态变化、弱协议、强制在集群的可达部分用最短的时间传播状态变更的 FAIL 消息，这些东西增加的复杂性有实际的好处。由于这种机制，如果集群处于错误状态的时候，所有节点都会在同一时间停止接收写入操作，这从使用 Redis 集群的应用的角度来看是个很好的特性。还有非必要的选举，是从节点在无法访问主节点的时候发起的，若该主节点能被其他大多数主节点访问的话，这个选举会被拒绝掉。</p>
<h3 id="集群阶段cluster-epoch"><a class="header" href="#集群阶段cluster-epoch">集群阶段（Cluster epoch）</a></h3>
<p>Redis 集群使用一个类似于木筏算法（Raft algorithm）”术语”的概念。在 Redis 集群中这个术语叫做 阶段（epoch），它是用来记录事件的版本号，所以当有多个节点提供了冲突的信息的时候，另外的节点就可以通过这个状态来了解哪个是最新的。 currentEpoch 是一个 64bit 的 unsigned 数。</p>
<p>Redis 集群中的每个节点，包括主节点和从节点，都在创建的时候设置了 currentEpoch 为0。</p>
<p>当节点接收到来自其他节点的 ping 包或 pong 包的时候，如果发送者的 epoch（集群连接消息头部的一部分）大于该节点的 epoch，那么更新发送者的 epoch 为 currentEpoch。</p>
<p>由于这个语义，最终所有节点都会支持集群中较大的 epoch。</p>
<p>这个信息在此处是用于，当一个节点的状态发生改变的时候为了执行一些动作寻求其他节点的同意（agreement）。</p>
<p>目前这个只发生在从节点的提升过程，这个将在下一节中详述。本质上说，epoch 是一个集群里的逻辑时钟，并决定一个给定的消息赢了另一个带着更小 epoch 的消息。</p>
<h3 id="配置阶段configuration-epoch"><a class="header" href="#配置阶段configuration-epoch">配置阶段（Configuration epoch）</a></h3>
<p>每一个主节点总是通过发送 ping 包和 pong 包向别人宣传它的 configEpoch 和一份表示它负责的哈希槽的位图。</p>
<p>当一个新节点被创建的时候，主节点中的 configEpoch 设为零。</p>
<p>从节点由于故障转移事件被提升为主节点时，为了取代它那失效的主节点，会把 configEpoch 设置为它赢得选举的时候的 configEpoch 值。</p>
<p>configEpoch 用于在不同节点提出不同的配置信息的时候（这种情况或许会在分区之后发生）解决冲突</p>
<p>从节点也会在 ping 包和 pong 包中向别人宣传它的 configEpoch 域，不过从节点的这个域表示的是上一次跟它的主节点交换数据的时候主节点的 configEpoch 值。这能让其他个体检测出从节点的配置信息是不是需要更新了（主节点不会给一个配置信息过时的从节点投票）。</p>
<p>每次由于一些已知节点的值比自己的值大而更新 configEpoch 值，它都会永久性地存储在 nodes.conf 文件中。</p>
<p>当一个节点重启，它的 configEpoch 值被设为所有已知节点中最大的那个 configEpoch 值。</p>
<h3 id="丛节点的选举和提升"><a class="header" href="#丛节点的选举和提升">丛节点的选举和提升</a></h3>
<p>从节点的选举和提升都是由从节点处理的，主节点会投票要提升哪个从节点。一个从节点的选举是在主节点被至少一个具有成为主节点必备条件的从节点标记为 FAIL 的状态的时候发生的。</p>
<p>当以下条件满足时，一个从节点可以发起选举：</p>
<ul>
<li>该从节点的主节点处于 FAIL 状态。</li>
<li>这个主节点负责的哈希槽数目不为零。</li>
<li>从节点和主节点之间的重复连接（replication link）断线不超过一段给定的时间，这是为了确保从节点的数据是可靠的。</li>
<li>一个从节点想要被推选出来，那么第一步应该是提高它的 currentEpoch 计数，并且向主节点们请求投票。</li>
</ul>
<p>从节点通过广播一个 FAILOVER_AUTH_REQUEST 数据包给集群里的每个主节点来请求选票。然后等待回复（最多等 NODE_TIMEOUT 这么长时间）。一旦一个主节点给这个从节点投票，会回复一个 FAILOVER_AUTH_ACK，并且在 NODE_TIMEOUT * 2 这段时间内不能再给同个主节点的其他从节点投票。在这段时间内它完全不能回复其他授权请求。</p>
<p>从节点会忽视所有带有的时期（epoch）参数比 currentEpoch 小的回应（ACKs），这样能避免把之前的投票的算为当前的合理投票。</p>
<p>一旦某个从节点收到了大多数主节点的回应，那么它就赢得了选举。否则，如果无法在 NODE_TIMEOUT 时间内访问到大多数主节点，那么当前选举会被中断并在 NODE_TIMEOUT * 4 这段时间后由另一个从节点尝试发起选举。</p>
<p>从节点并不是在主节点一进入 FAIL 状态就马上尝试发起选举，而是有一点点延迟，这段延迟是这么计算的：</p>
<pre><code>DELAY = 500 milliseconds + random delay between 0 and 500 milliseconds +
        SLAVE_RANK * 1000 milliseconds.
</code></pre>
<p>固定延时（fixed delay）确保我们会等到 FAIL 状态在集群内广播后，否则若从节点尝试发起选举，主节点们仍然不知道那个主节点已经 FAIL，就会拒绝投票。</p>
<p>一旦有从节点赢得选举，它就会开始用 ping 和 pong 数据包向其他节点宣布自己已经是主节点，并提供它负责的哈希槽，设置 configEpoch 为 currentEpoch（选举开始时生成的）。</p>
<p>为了加速其他节点的重新配置，该节点会广播一个 pong 包 给集群里的所有节点（那些现在访问不到的节点最终也会收到一个 ping 包或 pong 包，并且进行重新配置）。</p>
<p>其他节点会检测到有一个新的主节点（带着更大的configEpoch）在负责处理之前一个旧的主节点负责的哈希槽，然后就升级自己的配置信息。 旧主节点的从节点，或者是经过故障转移后重新加入集群的该旧主节点，不仅会升级配置信息，还会配置新主节点的备份。</p>
<h3 id="主节点回复从节点的投票请求"><a class="header" href="#主节点回复从节点的投票请求">主节点回复从节点的投票请求</a></h3>
<p>主节点接收到来自于从节点、要求以 FAILOVER_AUTH_REQUEST 请求的形式投票的请求。 要授予一个投票，必须要满足以下条件：</p>
<ul>
<li>在一个给定的时段（epoch）里，一个主节点只能投一次票，并且拒绝给以前时段投票：每个主节点都有一个 lastVoteEpoch 域，一旦认证请求数据包（auth request packet）里的 currentEpoch 小于 lastVoteEpoch，那么主节点就会拒绝再次投票。当一个主节点积极响应一个投票请求，那么 lastVoteEpoch 会相应地进行更新。</li>
<li>一个主节点投票给某个从节点当且仅当该从节点的主节点被标记为 FAIL。</li>
<li>如果认证请求里的 currentEpoch 小于主节点里的 currentEpoch 的话，那么该请求会被忽视掉。因此，主节点的回应总是带着和认证请求一致的 currentEpoch。如果同一个从节点在增加 currentEpoch 后再次请求投票，那么保证一个来自于主节点的、旧的延迟回复不会被新一轮选举接受。</li>
</ul>
<p>主节点的 currentEpoch 是 5， lastVoteEpoch 是 1（在几次失败的选举后这也许会发生的）</p>
<ul>
<li>从节点的 currentEpoch 是 3。</li>
<li>从节点尝试用 epoch 值为 4（3+1）来赢得选票，主节点回复 ok，里面的 currentEpoch 是 5，可是这个回复延迟了。</li>
<li>从节点尝试用 epoch 值为 5（4+1）来再次赢得选票，收到的是带着 currentEpoch 值为 5 的延迟回复，这个回复会被当作有效的来接收。</li>
<li>
<ol start="4">
<li>主节点若已经为某个失效主节点的一个从节点投票后，在经过 NODE_TIMEOUT * 2 时间之前不会为同个失效主节点的另一个从节点投票。这并不是严格要求的，因为两个从节点用同个 epoch 来赢得选举的可能性很低，不过在实际中，系统确保正常情况当一个从节点被选举上，那么它有足够的时间来通知其他从节点，以避免另一个从节点发起另一个新的选举。</li>
</ol>
</li>
<li>
<ol start="5">
<li>主节点不会用任何方式来尝试选出最好的从节点，只要从节点的主节点处于 FAIL 状态并且投票主节点在这一轮中还没投票，主节点就能进行积极投票。</li>
</ol>
</li>
<li>
<ol start="6">
<li>若一个主节点拒绝为给定从节点投票，它不会给任何负面的回应，只是单纯忽略掉这个投票请求。</li>
</ol>
</li>
<li>
<ol start="7">
<li>主节点不会授予投票给那些 configEpoch 值比主节点哈希槽表里的 configEpoch 更小的从节点。记住，从节点发送了它的主节点的 configEpoch 值，还有它的主节点负责的哈希槽对应的位图。本质上来说，这意味着，请求投票的从节点必须拥有它想要进行故障转移的哈希槽的配置信息，而且信息应该比它请求投票的主节点的配置信息更新或者一致。</li>
</ol>
</li>
</ul>
<h3 id="从节点选举的竞争情况"><a class="header" href="#从节点选举的竞争情况">从节点选举的竞争情况</a></h3>
<p>这一节解释如何使用 epoch 概念来使得从节点提升过程对分区操作更有抵抗力。</p>
<ul>
<li>主节点不是无限期地可达。它拥有三个从节点 A，B，C。</li>
<li>从节点 A 赢得了选举并且被推选为主节点。</li>
<li>一个分区操作使得集群中的大多数节点无法访问节点 A。</li>
<li>节点 B 赢得了选举并且被推选为主节点。</li>
<li>一个分区操作使得集群中大多数节点无法访问节点 B。</li>
<li>之前分区操作的问题被修复了，节点 A 又恢复可访问状态。</li>
</ul>
<p>此刻，节点 B 仍然失效，节点 A 恢复可访问，会与节点 C 竞选去获得选票对节点 B 进行故障转移。</p>
<p>这两个有同样的哈希槽的从节点最终都会请求被提升，然而由于它们发布的 configEpoch 是不一样的，而且节点 C 的 epoch 比较大，所以所有的节点都会把它们的配置更新为节点 C 的。</p>
<p>节点 A 会从来源于节点 C（负责同样哈希槽的节点）的 ping 包中检测出节点 C 的 epoch 是更大的，所以它会重新设置自己为节点 C 的一个从节点。</p>
<h3 id="服务器哈希槽信息的传播规则"><a class="header" href="#服务器哈希槽信息的传播规则">服务器哈希槽信息的传播规则</a></h3>
<p>Redis 集群很重要的一个部分是用来传播关于集群节点负责哪些哈希槽的信息的机制。这对于新集群的启动和提升从节点来负责处理哈希槽（它那失效的主节点本该处理的槽）的能力来说是必不可少的。</p>
<p>个体持续交流使用的 ping 包和 pong 包都包含着一个头部，这个头部是给发送者使用的，为了向别的节点宣传它负责的哈希槽。这是主要用来传播变更的机制，不过集群管理员手动进行重新配置是例外（比如为了在主节点间移动哈希槽，通过 redis-trib 来进行手动碎片整理）。</p>
<p>当一个新的 Redis 集群节点创建的时候，它的本地哈希槽表（表示给定哈希槽和给定节点 ID 的映射关系表）被初始化，每个哈希槽被置为 nil，也就是，每个哈希槽都是没赋值的。</p>
<p>一个节点要更新它的哈希槽表所要遵守的第一个规则如下：</p>
<p><strong>规则 1：如果一个哈希槽是没有赋值的，然后有个已知节点认领它，那么我就会修改我的哈希槽表，把这个哈希槽和这个节点关联起来。</strong></p>
<p>由于这个规则，当一个新集群被创建的时候，只需要手动给哈希槽赋值上（通常是通过 redis-trib 命令行工具使用 CLUSTER 命令来实现）负责它的主节点，然后这些信息就会迅速在集群中传播开来。</p>
<p>然而，当一个配置更新的发生是因为一个从节点在其主节点失效后被提升为主节点的时候，这个规则显然还不足够。新的主节点会宣传之前它做从节点的时候负责的哈希槽，但从其他节点看来这些哈希槽并没有被重新赋值，所以如果它们只遵守第一个规则的话就不会升级配置信息。</p>
<p>由于这个原因就有第二个规则，是用来把一个已赋值给以前节点的哈希槽重新绑定到一个新的认领它的节点上</p>
<p><strong>规则 2：如果一个哈希槽已经被赋值了，有个节点它的 configEpoch 比哈希槽当前拥有者的值更大，并且该节点宣称正在负责该哈希槽，那么我们会把这个哈希槽重新绑定到这个新节点上。</strong></p>
<p>因为有这第二个规则，所以集群中的所有节点最终都会同意<strong>哈希槽的拥有者是所有声称拥有它的节点中 configEpoch 值最大的那个</strong>。</p>
<h3 id="update-消息"><a class="header" href="#update-消息">UPDATE 消息</a></h3>
<p>上面描述的传播哈希槽配置信息的系统只使用节点间交换信息的普通 ping 包和 pong 包。 这要求存在一个节点（可以是负责给定哈希槽的主节点或从节点）拥有更新后的配置信息，</p>
<p>然而也存在例外。当有一个节点，它是唯一一个负责处理给定哈希槽的节点，有可能在分区操作后它恢复正常，但拥有的配置信息是过时的。</p>
<p>例子：一个给定的哈希槽是由节点 A 和 B 负责的。节点 A 是一个主节点，然后它在某个时刻失效了，所以节点 B 被提升为主节点。过了一段时间节点 B 也失效了，集群没有其他备份节点可以来处理这个哈希槽，所以只能开始修复操作。</p>
<p>在一段时间过后节点 A 恢复正常了，并且作为一个可写入的主节点重新加入集群，但它的配置信息是过时的。此时没有任何备份节点能更新它的配置信息。这就是 UPDATE 消息存在的目的：当一个节点检测到其他节点在宣传它的哈希槽的时候是用一份过时的配置信息，那么它就会向这个节点发送一个 UPDATE 消息，这个消息包含新节点的 ID 和它负责的哈希槽（以 bitmap 形式发送）。</p>
<p>注意：目前更新配置信息可以用 ping 包/ pong 包，也可以用 UPDATE 消息，这两种方法是共享同一个代码路径（code path）。这两者在更新一个带有老旧信息的节点的配置信息时会有功能上的重复。然而这两种机制都是非常有用的，因为 ping / pong 包在一段时间后能填充（populate）新节点的哈希槽路由表，而 UPDATE 消息只是在一个过时配置信息被检测出来时才被发送出去，并且只覆盖那些需要修复的错误配置信息。</p>
<h3 id="从结点迁移"><a class="header" href="#从结点迁移">从结点迁移</a></h3>
<p>Redis 集群实现了一个叫做备份迁移（replica migration）的概念，以提高系统的可用性。</p>
<p>在集群中有主节点-从节点的设定，如果主从节点间的映射关系是固定的，那么久而久之，当发生多个单一节点独立故障的时候，系统可用性会变得很有限。</p>
<p>例如有一个每个主节点都只有一个从节点的集群，当主节点或者从节点故障失效的时候集群能让操作继续执行下去，但如果主从节点都失效的话就没法让操作继续执行下去。然而这样长期会积累很多由硬件或软件问题引起的单一节点独立故障。例如：</p>
<ul>
<li>主节点 A 有且只有一个从节点 A1。</li>
<li>主节点 A 失效了。A1 被提升为新的主节点。</li>
<li>三个小时后，A1 因为一个独立事件（跟节点 A 的失效无关）失效了。由于没有其他从节点可以提升为主节点（因为节点 A 仍未恢复正常），集群没法继续进行正常操作。</li>
</ul>
<p>如果主从节点间的映射关系是固定的，那要让集群更有抵抗力地面对上面的情况的唯一方法就是为每个主节点添加从节点。然而这要付出的代价也更昂贵，因为要求 Redis 执行更多的实例、更多的内存等等。</p>
<p>一个候选方案就是在集群中创建不对称性，然后让集群布局时不时地自动变化。例如，假设集群有三个主节点 A，B，C。节点 A 和 B 都各有一个从节点，A1 和 B1。节点 C 有两个从节点：C1 和 C2。</p>
<p>备份迁移是从节点自动重构的过程，为了迁移到一个没有可工作从节点的主节点上。在上面提到的例子中，备份迁移过程如下：</p>
<ul>
<li>主节点 A 失效。A1 被提升为主节点。</li>
<li>节点 C2 迁移成为节点 A1 的从节点，要不然 A1 就没有任何从节点。</li>
<li>三个小时后节点 A1 也失效了。</li>
<li>节点 C2 被提升为取代 A1 的新主节点。</li>
<li>集群仍然能继续正常工作。</li>
</ul>
<h3 id="迁移算法"><a class="header" href="#迁移算法">迁移算法</a></h3>
<p>迁移算法不用任何形式的协议，因为 Redis 集群中的从节点布局不是集群配置信息（配置信息要求前后一致并且/或者用 config epochs 来标记版本号）的一部分。 它使用的是一个避免在主节点没有备份时从节点大批迁移的算法。这个算法保证，一旦集群配置信息稳定下来，<strong>最终每个主节点都至少会有一个从节点作为备份</strong>。</p>
<ul>
<li>
<p>每个从节点若检测出存在至少一个没有好的从节点的单一主节点，那么就会触发这个算法的执行</p>
</li>
<li>
<p>采取行动的从节点是属于那些拥有最多从节点的主节点，并且不处于 FAIL 状态及拥有最小的节点 ID。</p>
</li>
<li>
<p>如果有 10 个主节点，它们各有 1 个从节点，另外还有 2 个主节点，它们各有 5 个从节点。会尝试迁移的从节点是在那 2 个拥有 5 个从节点的主节点中的所有从节点里，节点 ID 最小的那个</p>
</li>
</ul>
<p><strong>竞争情况</strong></p>
<p>在集群配置信息不稳定的情况下，有可能发生一种竞争情况：多个从节点都认为自己是不处于 FAIL 状态并且拥有较小节点 ID（实际上这是一种比较难出现的状况）。如果这种情况发生的话，结果是多个从节点都会迁移到同个主节点下，不过这种结局是无害的。这种竞争发生的话，有时候会使得割让出从节点的主节点变成没有任何备份节点，当集群再次达到稳定状态的时候，本算法会再次执行，然后把从节点迁移回它原来的主节点。</p>
<p>最终每个主节点都会至少有一个从节点作为备份节点。通常表现出来的行为是，一个从节点从一个拥有多个从节点的主节点迁移到一个孤立的主节点。</p>
<p>这个算法能通过一个用户可配置的参数 cluster-migration-barrier 进行控制。这个参数表示的是，一个主节点在拥有多少个好的从节点的时候就要割让一个从节点出来。例如这个参数若被设为 2，那么只有当一个主节点拥有 2 个可工作的从节点时，它的一个从节点会尝试迁移。</p>
<h3 id="发布订阅publishsubscribe"><a class="header" href="#发布订阅publishsubscribe">发布/订阅（Publish/Subscribe）</a></h3>
<p>在一个 Redis 集群中，客户端能订阅任何一个节点，也能发布消息给任何一个节点。集群会确保发布的消息都会按需进行转发。 目前的实现方式是单纯地向所有节点广播所有的发布消息，在将来的实现中会用 bloom filters 或其他算法来优化。</p>
<h1 id="集群配置"><a class="header" href="#集群配置">集群配置</a></h1>
<pre><code class="language-shell">#端口配置
port 7000
#集群启用
cluster-enabled yes
#保存节点配置文件的路径
cluster-config-file nodes.conf
#节点超时
cluster-node-timeout 5000

appendonly yes
</code></pre>
<h1 id="实战"><a class="header" href="#实战">实战</a></h1>
<h2 id="经典三主三从"><a class="header" href="#经典三主三从">经典三主三从</a></h2>
<h3 id="规划"><a class="header" href="#规划"><strong>规划</strong></a></h3>
<pre><code>7000 7001 7002 7003 7004 7005 自动配置主从
</code></pre>
<h3 id="基础配置"><a class="header" href="#基础配置"><strong>基础配置</strong></a></h3>
<pre><code class="language-sh"># 拷贝命令
sed  's/bind 127.0.0.1/bind 0.0.0.0/g' ../redis.conf &gt;redis.conf
sed -i 's/^port 6379/port 7005/g' redis.conf
sed -i 's/^protected-mode yes/protected-mode no/g' redis.conf
sed -i 's/^daemonize no/daemonize yes/g' redis.conf
</code></pre>
<h3 id="集群配置-1"><a class="header" href="#集群配置-1"><strong>集群配置</strong></a></h3>
<pre><code class="language-shell">
# 启动cluster
sed -i 's/# cluster-enabled yes/cluster-enabled yes/g' redis.conf
# 启用集群配置文件
sed -i 's/# cluster-config-file nodes-6379.conf/cluster-config-file nodes-6379.conf/g'  redis.conf
# 集群不需要所有槽都被映射也能提供服务
sed -i 's/# cluster-require-full-coverage yes/cluster-require-full-coverage no/g' redis.conf
</code></pre>
<h3 id="启停"><a class="header" href="#启停"><strong>启停</strong></a></h3>
<pre><code class="language-shell">../../redis-server redis.conf
kill -15 `ps aux | grep -v grep | grep redis | awk '{ print $2 }'`
</code></pre>
<h3 id="完整配置"><a class="header" href="#完整配置"><strong>完整配置</strong></a></h3>
<pre><code class="language-shell">kill -15 `ps aux | grep -v grep | grep redis | awk '{ print $2 }'`
for i in 7000 7001 7002 7003 7004 7005;
do
    rm -rf $i;
    mkdir $i;
    cd $i;
    sed  's/bind 127.0.0.1/bind 0.0.0.0/g' ../redis.conf &gt;redis.conf
    sed -i &quot;s/^port 6379/port $i/g&quot; redis.conf
    sed -i 's/^protected-mode yes/protected-mode no/g' redis.conf
    sed -i 's/^daemonize no/daemonize yes/g' redis.conf
    sed -i 's/# cluster-enabled yes/cluster-enabled yes/g' redis.conf
    sed -i 's/# cluster-config-file nodes-6379.conf/cluster-config-file nodes-6379.conf/g'  redis.conf 
    ../redis-server redis.conf
    cd ../
done
</code></pre>
<h3 id="启动集群"><a class="header" href="#启动集群"><strong>启动集群</strong></a></h3>
<p><strong>随机指定主从</strong></p>
<pre><code class="language-shell">./redis-cli --cluster create 192.168.3.16:7000 192.168.3.16:7001 192.168.3.16:7002 192.168.3.16:7003 192.168.3.16:7004 192.168.3.16:7005  --cluster-replicas 1
</code></pre>
<p><strong>手动指定</strong></p>
<pre><code class="language-shell">./redis-cli --cluster create 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002  --cluster-replicas 0

./redis-cli --cluster add-node --slave 127.0.0.1:7003 127.0.0.1:7000 --cluster-slave
./redis-cli --cluster add-node --slave 127.0.0.1:7004 127.0.0.1:7001 --cluster-slave
./redis-cli --cluster add-node --slave 127.0.0.1:7005 127.0.0.1:7002 --cluster-slave
</code></pre>
<h3 id="查看集群"><a class="header" href="#查看集群">查看集群</a></h3>
<p><strong>查看集群信息</strong></p>
<pre><code class="language-text">cluster info
</code></pre>
<p><strong>查看集群节点</strong></p>
<pre><code>cluster nodes
</code></pre>
<h3 id="操作集群"><a class="header" href="#操作集群">操作集群</a></h3>
<p><strong>删除节点</strong></p>
<pre><code>如果有slave，先删除slave或者将slave转移到其他master下
</code></pre>
<p><strong>删除slave</strong></p>
<pre><code>redis-cli --cluster del-node 172.17.0.7:6379 'f9e78f563314a1d88796ec0ca2b13e4ac3cae75f'
</code></pre>
<p><strong>转移slot</strong></p>
<p>因为master上面有slot，所以首先reshard转移slot，假设将  127.0.0.1:7000 上面的slot转移到 127.0.0.1:7001 和127.0.0.1:7002  上</p>
<pre><code class="language-shell">#非交互式
redis-cli --cluster reshard  127.0.0.1:7000

redis-cli --cluster reshard 172.17.0.4:6379 --cluster-from '46623a0b2ec8abb8a0688769337e91268df3c73f' --cluster-to '14d129294d95867777a91d29b708413baa8a276c' --cluster-slots 2500 --cluster-yes


 redis-cli --cluster reshard host:port --cluster-from &lt;arg&gt; --cluster-to &lt;arg&gt; --cluster-slots &lt;arg&gt; --cluster-yes --cluster-timeout &lt;arg&gt; --cluster-pipeline &lt;arg&gt;

# 参数说明：
# host：port：必传参数，集群内任意节点地址，用来获取整个集群信息。
# --cluster-from：制定源节点的id，如果有多个源节点，使用逗号分隔，如果是all源节点变为集群内所有主节点，在迁移过程中提示用户输入。
# --cluster-to：需要迁移的目标节点的id，目标节点只能填写一个，在迁移过程中提示用户输入。
# --cluster-slots：需要迁移槽的总数量，在迁移过程中提示用户输入。
# --cluster-yes：当打印出reshard执行计划时，是否需要用户输入yes确认后再执行reshard。
# --cluster-timeout：控制每次migrate操作的超时时间，默认为60000毫秒。
# --cluster-pipeline：控制每次批量迁移键的数量，默认为10。
</code></pre>
<pre><code class="language-shell">#然后将剩余的slot转移到172.17.0.3上，执行下面一条命令即可，还剩余2962个slot
redis-cli --cluster reshard 172.17.0.4:6379 --cluster-from '46623a0b2ec8abb8a0688769337e91268df3c73f' --cluster-to '8a7b1a4cf2980c031c0e5e912cf366981588e3c9' --cluster-slots 2962 --cluster-yes
</code></pre>
<p><strong>删除master</strong></p>
<pre><code>redis-cli --cluster del-node 172.17.0.4:6379 '46623a0b2ec8abb8a0688769337e91268df3c73f'
</code></pre>
<p><strong>添加节点</strong></p>
<pre><code>redis-cli --cluster add-node 172.17.0.4:6379 172.17.0.2:6379
</code></pre>
<h3 id="故障转移"><a class="header" href="#故障转移">故障转移</a></h3>
<p>上面的删除节点都是在已经情况下操作的，假设现在在未知情况下宕机了，那么会发生什么？</p>
<h4 id="master宕机"><a class="header" href="#master宕机">master宕机</a></h4>
<p>假设master宕机，让127.0.0.1:7000 宕机，过了一会儿，看cluster nodes信息发现 127.0.0.1:7000 有fail标志</p>
<p>稍等片刻之后发现127.0.0.1:7004 变成了master</p>
<p>在看看127.0.0.1:7004 的日志输出有下面这句话</p>
<pre><code>Failover election won: I'm the new master.
</code></pre>
<p>也就是说当集群内的mater宕机后，slave被选举一个出来当做master，集群依然可用，假设原来的master恢复了，那么它将变成slave追随现在的master</p>
<h4 id="master-slave宕机"><a class="header" href="#master-slave宕机">master-slave宕机</a></h4>
<p>假如master和slave双双宕机了呢？此时将都变成fail</p>
<p>默认情况下此时集群将变得不可用，执行get指令时会报错</p>
<pre><code>(error) CLUSTERDOWN The cluster is down
</code></pre>
<p>但是如果配置了cluster-require-full-coverage，那么集群依然<strong>部分可用</strong>，所谓部分可用即宕机的slot不可用，其他的slot还是可用的，参考：<a href="https://link.zhihu.com/?target=https%3A//stackoverflow.com/questions/53594257/clusterdown-the-cluster-is-down-in-redis">https://stackoverflow.com/quest</a></p>
<pre><code>cluster-require-full-coverage no
</code></pre>
<p>很多文章都说此时slot会自动转移，但是我测试时并不会自动转移，仔细想一想，master-slave双双都在不可抗力下宕机了，那么里面的数据肯定是拿不出来的，怎么转移slot呢？</p>
<h2 id="代理"><a class="header" href="#代理">代理</a></h2>
<h3 id="安装"><a class="header" href="#安装">安装</a></h3>
<pre><code># 安装编译环境，各种工具等，注意libstdc++-static可能安装不上
yum install -y git wget gcc gcc-c++ libstdc++-static make telnet
# 创建文件夹
cd ~ &amp;&amp; mkdir soft &amp;&amp; cd soft
# clone代码到本地
git clone https://github.com/joyieldInc/predixy.git
# 编译安装
cd predixy &amp;&amp; make 
# 拷贝
mkdir -p /opt/predixy &amp;&amp; cp src/predixy /opt/predixy
# 添加环境变量
cat &gt;&gt; /etc/profile &lt;&lt;EOF
export PATH=$PATH:/opt/predixy
EOF
echo  'source /etc/profile' &gt;&gt; ~/.bashrc &amp;&amp; source ~/.bashrc
# 帮助命令
predixy -h
</code></pre>
<h3 id="配置predixconf"><a class="header" href="#配置predixconf">配置predix.conf</a></h3>
<pre><code># 开启日志
sed -i 's/# Log .\/predixy.log/Log .\/predixy.log/g' predixy.conf
#predixy 默认运行在7617端口
# 引入cluster.conf
sed -i 's/# Include cluster.conf/Include cluster.conf/g' predixy.conf
# 注释测试 try.conf
sed -i 's/Include try.conf/# Include try.conf/g' predixy.conf
</code></pre>
<h3 id="配置clusterconf"><a class="header" href="#配置clusterconf">配置cluster.conf</a></h3>
<pre><code>cat &gt; cluster.conf &lt;&lt;EOF
ClusterServerPool {
    #这个是主节点访问权重，如果是只把备节点用作备份不去做读写分离，直接将这个配置成100只去读主节点就好了。
    MasterReadPriority 100
    # redis实例的访问密码
    # Password sjwkk123456
    # 读写分离功能，从静态redis slave节点执行读请求的优先级，所谓静态节点，是指在本配置文件中显示列出的redis节点，不指定的话为0
    StaticSlaveReadPriority 50 
    # 功能见上，所谓动态节点是指在本配置文件中没有列出，但是通过redis sentinel动态发现的节点，不指定的话为0
    DynamicSlaveReadPriority 50
    # predixy会周期性的请求redis sentinel以获取最新的集群信息，该参数以秒为单位指定刷新周期，不指定的话为1秒
    RefreshInterval 1
    # 请求在predixy中最长的处理/等待时间，如果超过该时间redis还没有响应的话，那么predixy会关闭同redis的连接，并给客户端一个错误响应，对于blpop这种阻塞式命令，该选项不起作用，为0则禁止此功能，即如果redis不返回就一直等待，不指定的话为0
    ServerTimeout 1
    # 一个redis实例出现多少次才错误以后将其标记为失效，不指定的话为10
    ServerFailureLimit 10
    # 一个redis实例失效后多久后去检查其是否恢复正常，不指定的话为1秒
    ServerRetryTimeout 1
    #predixy与redis的连接tcp keepalive时间，为0则禁止此功能，不指定的话为0
    KeepAlive 120
    Servers {
        # 配置所有节点地址
        + 127.0.0.1:7000
        + 127.0.0.1:7001
        + 127.0.0.1:7002
        + 127.0.0.1:7003
        + 127.0.0.1:7004
        + 127.0.0.1:7005
    }
}
EOF
</code></pre>
<h3 id="启动并测试"><a class="header" href="#启动并测试">启动并测试</a></h3>
<pre><code class="language-shell"># 启动
predixy /etc/predixy/conf/predixy.conf &amp;

# 查看日志
tail -f /etc/predixy/conf/predixy.log 

# 连接代理
redis-cli -h 172.17.0.8 -p 7617

# 设置值
set k1 aaa

# 获取值
get k1
</code></pre>
<p><a href="https://zhuanlan.zhihu.com/p/362143353">参考文章</a></p>
<p><strong>下载链接</strong></p>
<pre><code>curl -O  http://download.redis.io/releases/redis-6.0.6.tar.gz
tar -xf redis-6.0.6.tar.gz
cd redis-6.0.6
</code></pre>
<script src="/note-service/mw/infisearch_assets/search-ui.chinese.bundle.js" type="text/javascript" charset="utf-8"></script>
<script src="/note-service/mw/infisearch_assets/mark.min.js" type="text/javascript" charset="utf-8"></script>
<script>
const base_url = '/note-service/mw/';
const mode = 'target';
infisearch.init({
  searcherOptions: {
    url: base_url + 'infisearch_output/',
  },
  uiOptions: {
    mode,
    dropdownAlignment: 'bottom-start',
    target: document.getElementById('infisearch-mdbook-target'),
    fsButtonPlaceholder: 'Search',
    sourceFilesUrl: base_url,
    resultsRenderOpts: {
      searchedTermsParam: 'search',
    },
    multiSelectFilters: [
      { fieldName: 'partTitle', displayName: 'Section', defaultOptName: 'None' },
    ],
  },
});

document.getElementById('infi-search').addEventListener('keydown', (ev) => {
  if (['ArrowLeft', 'ArrowRight'].includes(ev.key)) {
    ev.stopPropagation(); // used in global listener to change pages
    return;
  }
});

if (window.location.search) {
  // Adapted from the original searcher.js for mdbook
  // https://github.com/rust-lang/mdBook/blob/master/src/theme/searcher/searcher.js
  const target = document.getElementById('content');
  const marker = new Mark(target);

  function doSearchOrMarkFromUrl() {
    // Check current URL for search request
    var url = new URL(window.location.href);
    var urlParams = new URLSearchParams(url.search);

    if (urlParams.has('search')) {
      var words = JSON.parse(decodeURIComponent(urlParams.get('search')));
      marker.mark(words);

      var markers = document.querySelectorAll('mark');
      function hide() {
        for (var i = 0; i < markers.length; i++) {
          markers[i].classList.add('fade-out');
          window.setTimeout(function () { marker.unmark(); }, 300);
        }
      }
      for (var i = 0; i < markers.length; i++) {
        markers[i].addEventListener('click', hide);
      }
    }
  }
  doSearchOrMarkFromUrl();
}
</script>
<p><span data-infisearch-part-title="缓存中间件"></span></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../8.缓存中间件_Redis/redis发布与订阅.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="../8.缓存中间件_Redis/redis数据结构/Redis有序集合(sortedSet).html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../8.缓存中间件_Redis/redis发布与订阅.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="../8.缓存中间件_Redis/redis数据结构/Redis有序集合(sortedSet).html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>



        <script src="https://xjq-note.oss-cn-hangzhou.aliyuncs.com/clipboard.min.js"></script>
        <script src="https://xjq-note.oss-cn-hangzhou.aliyuncs.com/highlight.js"></script>
        <script src="/note-service/book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>